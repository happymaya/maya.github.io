[ { "title": "简单原则：写出“简单”代码", "url": "/posts/keep-it-simple-and-stupid/", "categories": "Design Pattern", "tags": "Keep it simple and stupid, Kiss, 简单原则", "date": "2022-04-12 14:21:22 +0000", "snippet": "设计原则通常都有一个很普遍的特点：语言很简练，听上去很有道理，但是拿来指导实践根本无从下手。比如说，在学习简单原则（ KISS 原则） 时，遇到过下列问题： 为什么身边的程序员都告诉你保持“简单”代码很重要？ 什么才是好的“简单”代码？ 如何能写出“简单”的代码？ YAGNI 原则和 KISS 原则是相同的吗？这些问题看上去简单，回答起来却不简单。所以，带着这些问题，重新学习 KISS 原则。代码保持“简单”的必要性KISS 原则（Keep It Simple and Stupid），翻译过来就是：保持简单，保持愚蠢。KISS 原则自 1970 年起，就开始得到广泛使用。现在在软件开发中，KISS 原则通常是指代码应该更容易理解、更容易编写、更容易改变，并应该一直保持如此。让代码保持“简单”主要三个重要原因： 防止代码腐坏。编程里有一个很大矛盾：刚开始时一切都好，代码简单可读，逻辑清晰，可随着时间推移，特别是当越来越多的人参与到项目中时，代码数量呈指数级增长，功能越来越丰富，逻辑却越来越复杂，代码变得难以理解和难以维护，这时代码就开始变得腐坏。保持代码简单性一方面可以降低模块的理解难度，另一方面也让代码修改的范围边界更清晰，这样就能有效地防止代码腐坏； 减少时间成本的投入。众所周知，软件复杂度有三个来源：代码规模、技术复杂度和实现复杂度。通常来说，代码越复杂，在这三方面投入的时间成本就会越多。比如，维护 10 万行代码和维护 1000 行代码所需要投入的理解与逻辑梳理时间是不同的。再比如，使用 Redis 作为缓存和使用 LRU 本地缓存所需要的学习时间也是不同的。对于开发人员来说，维护代码必然需要花时间修改、调试、理解内在逻辑并让代码正常运行，一旦代码非常复杂，势必会增加时间成本，进而让开发团队的研发成本增加。如果代码能够保持简单清晰，那么最直接的效果就是降低学习与维护代码的时间，这是很多开发团队都愿意看到的结果； 快速迭代，拥抱变化。虽然现在越来越多的团队开始采用敏捷开发，但是在现实中，一直陷在“业务需求永远也做不完的死循环”里才是开发团队的现状，这意味着开发人员只有更少时间写代码，即便有想要写简单代码的心，迫于进度压力也只能尽快写能工作的代码。久而久之，团队代码变得越来越复杂，开发效率越来越低，本来初衷是想拥抱变化，却被迫变成无力承受变化。要想改变这种现状，有效的办法之一就是保持简单性，比如，更好的测试性、更好的扩展性、更好的理解性、更灵活的设计等。由此可见，保持简单的重要性，但是 KISS 原则并没有告诉应该以什么样标准来判断简单代码，也没有提出用什么具体的方法来指导我们写出有简单性的代码。理解代码中的“简单”常常会听到很多类似这样关于 KISS 原则讨论： 产品需求文档（PRD）都出来了，你就选择一种简单的设计吧，因为项目要快速迭代上线。 这个功能不是有现成的类库吗？选择一种简单的类库封装一下就行了吧，客户明天就想要这个功能。 现在开源框架这么多，随便选一个简单的来实现，我不关心技术，只关心能不能解决问题。 看 XX 大厂设计多简单、体验多好，为什么不也搞成那种简单的呢？很多时候我们都有一种“简单”的错觉：$简单分析+简单设计+简单编程 = 简单产品$但实际上，这种简单组合就能构造出最终的“简单”吗？在我看来，在软件开发中，“简单”是最终的一个状态。管理层或终端用户的确不需要关心背后复杂代码逻辑，然而对于开发人员来说，就不得不去考虑背后各种各样的复杂性。换句话说，编程的本质就是控制复杂度。如何正确理解代码中“简单”呢? 先来看“简单”不是什么: 简单 ≠ 简单设计或简单编程。简单一旦和动词放在一起，太容易被误解，尤其是在软件开发中。比如，现在我们在很多项目开发中，为了完成进度而做简单设计甚至不设计，认为只要后期有需要时再重构就行，编码时也就采用简单编程，并美其名曰迭代敏捷开发。但实际上，项目到后期几乎没有时间重构，并频繁出现问题（定位时间变长、逻辑嵌套过深、不断打补丁等），最后项目往往以失败告终。所以，保持简单并不是只能做简单设计或简单编程，而是做设计或编程时要努力以最终产出简单为目标，过程可能非常复杂也没关系。 简单 ≠ 数量少。我们通常把简单好坏和数量多少挂钩，比如，代码行数少、使用的类库组件少、架构设计少似乎就是简单。但实际上，数量少可能只是表现出来的简单，也会引入更加复杂的问题的。比如，曾经我在负责的一个项目中，为了解决限流问题，引入了一个不太常用的开源限流组件，看上去只要一个组件就能轻松解决问题，但是实践中发现，这个框架存在一个很不稳定的 Bug，当流量过大时，容易造成线程池挂死而导致应用崩溃，当时团队花费了一周的时间才找出这个问题。所以，一定不能以数量少作为衡量简单的标准，因为很可能看上去简单的背后隐藏了未知的复杂。 简单 ≠ 过度简洁。如果你见过被加密的代码，那你一定知道什么叫过度简洁的代码。这样的代码机器可读，但是对于人来说几乎不可读。在现实中，我们可能都曾写过这样的代码：没有任何注释说明，不使用任何设计模式，用最直接的数据结构和算法来实现，使用字母缩写来命名变量。然后，过一段时间后，连自己都看不懂这样的简洁代码，除非再从头到尾看一遍才能回想起含义。如果换成别人，可想而知其阅读难度有多大。所以说，简单代码可能会很简洁，但一定不是过度简洁。“简单”是什么: 简单应该是坚持实践。 我们常说代码应该简单，但却忽略了更重要的是保持简单的动作。换句话说，道理我们都懂，但实际上写代码时并没有坚持做。保持简单之所以很困难，是因为大多数人都只盯着最后实现简单后的好处，却忘记了在做到简单之前需要付出的努力。真正的简单代码通常背后都隐藏了大量不简单的工作，比如，仔细分析需求，选择合适的技术框架，设计更合适的数据结构和算法，实现时保持代码可读性，等等，每一件事都不简单，并且长期坚持才有可得到最后的简单。 简单应该是尽量简单，但又不能太简单。换句话说，就是要管理合适的代码上下文环境，并且在边界范围内以“最少知识”的方式构建程序，满足要求即可，保持一定的克制。很多时候，我们之所以容易过度开发功能，是因为没有考虑上下文的边界，进而导致需求扩散而不断扩充知识。比如说，需求中只要求你开发一个编辑器，你却在开发过程中发现了你想要试验的新功能，最后你开发了一个 IDE。从用户角度来看，他只需要一个简单的编辑器，虽然你做的事情也满足了要求，但你把简单的事情搞复杂了。 简单应该是让别人理解代码逻辑时更简单。 代码写出来后，80% 的时间都在被阅读，简单代码的好处在于能让别人一眼就知道代码表达的意图，要想做到这样，就对写代码的人提出了一个更高的要求：不仅需要使用清晰的算法和数据结构实现代码逻辑，还需要使用面向对象编程技巧提升代码复用性，甚至需要写更多的单元测试和注释来提升可维护性。总之，好的代码就是将简单带给别人，复杂留给自己。写出“简单”代码？实际上，写出“简单”代码是一个主观的判断，结合多年的经验以及前辈大神的经验，我总结了“四要”和“四不要”。四不要“四不要”具体包括以下四点: 不要长期进行打补丁式的编码。 因为这会给团队树立一个不好的榜样，当以打补丁式的编码中快速获得收益时（修改快，见效快），就会不知不觉地给其他人一种心理暗示——这段代码只需要不断打补丁就能解决问题，那么维护代码的人一定会优先选用这个方法，而不是重构它; 不要炫耀编程技巧。 如果你的团队对 Java 8 的 Lamda 表达式语法还不够熟悉，那么不要一开始就写这样的代码，但可以通过不断分享 Lamda 表达式的优势和知识来帮助团队提升编程实力（间接提升你的技术影响力）。切记不要为了彰显自己厉害而使用一些技巧，尤其是在一些维护项目上使用高级语法，这会很容易导致维护代码的人需要花费大量的精力和时间去学习或研究你的代码; 不要简单编程。 硬编码、一次性编码、复制粘贴编码、面向搜索编程都是简单编程，如果一直习惯性地简单编程，那么带来的可能就是更复杂、更高成本的重构和重写。这不仅不能提升代码扩展性，还会使得代码在后期无法被维护和重构。局部的简单导致整体的更加复杂，这是现在公认的一种得不偿失的做法; 不要过早优化。 为了让代码变得简单，优化是必不可少的手段，但是过早的优化会造成很多核心代码逻辑被隐藏，而维护代码的人为了不破坏原有的设计（误认为越早的设计就会越好），只能不断修改现有的设计来适应这种不变，最后反而容易导致架构可能被破坏。除了以上“四不要”之外，还要加上一些容易忽略的事，这就是接下来就是“四要”。四要 定期做 Code Review。 如果说简单性的判断标准很难统一，但是一眼让人快速理解的代码始终都是好代码，当他人在阅读你的代码时会自然对你的代码做评价，无论好坏，这都是你思考代码是否足够简单的契机; 选择合适的编码规范。 编码规范是优秀编码实践的经验总结，能帮助你写出“简单”代码，并发展出一种简单性的编程风格。不过，现在很多大厂的编码规范过于通用，虽然很好，但是不一定适合你，这时你应该进行适度的裁剪优化，逐渐找出真正适合你的经验，发展出适合你的“简单”编码风格; 适时重构。 并不是说非得要等到代码完全无法修改时再重构，完全可以轻松地在每一个小的迭代版本里进行重构，比如，分离一个过多职责的类，抽象一个上次没来得及做的通用服务，减少业务里 if-else 的嵌套层数，对不同业务数据对象分包管理等。及时这样做以后，代码整体就会变得简单; 有目标地逐渐优化。 这不是口号，而是我经历了太多“优化”项目后的真实体会。优化一定要制定一个目标，不然很容易就成了盲目优化，甚至把优化当成了一种单纯的 KPI。如果程序性能都没有达到真实的性能瓶颈，就没有任何优化的必要。而且优化应该是分阶段和步骤的，不要搞一次性的大优化、大提升，这样会导致频繁的代码修改，反而容易引入更多的 Bug。YAGNI 原则YAGNI 原则（You Ain’t Gonna Need It），翻译过来就是：你不会需要它。换句话说，在软件开发中，它希望你不要写“将来可能需要，但现在却用不上”的代码。但实际上，经常会有一种冲动：“既然发现某段代码有重复，抽象成通用功能也许以后能用上呢？”以我多年的经验来看，这样“额外”做之后，多数情况下代码就再也没有被使用过了。更为严重的是，它不仅让代码变得更加复杂，而且还可能让系统出现 Bug 的概率更高。YAGNI 原则和 KISS 原则联系紧密。在我看来，YAGNI 原则能够帮助我们更好地实现 KISS 原则，因为保持简单更像是一个要达成的目标，而在实际开发中我们稍不留神就会写出一些多余的功能，使用 YAGNI 原则可以及时提醒我们“在确定真的有必要的时候再写代码，那时再重构仍然来得及”。总结在软件开发中，简单性始终是一个最终的结果，然而为了达到这个结果，过程可能会非常复杂。看看各种操作系统、大型网站的简单性，背后其实是无数工程师辛勤地解决庞大的复杂性后才铸就的。如果只盯着简单，那么很快就会掉入自以为是的“简单思维”陷阱之中。KISS 原则的含义虽然很简单，但它其实只给了我们一个想要达成的目标，并没有给我们对应的方法。这也是简单原则让人非常迷惑的地方。把一件事情搞复杂往往很简单，但要想把一件复杂的事变简单，就是一件复杂的事。也就是说，理解 KISS 原则最难的地方不在于明白简单的重要性，而在于如何始终保持简单的行动，就好比去跑马拉松，一直保持百米冲刺的速度显然是不现实的，而只有匀速、有节奏的速度，才有可能最终到达终点。虽然将保持简单固化成一种习惯会很难，但它值得去尝试。" }, { "title": "最少原则：实现“最少知识”代码", "url": "/posts/law-of-demeter/", "categories": "Design Pattern", "tags": "迪米特法则（Law of Demeter，简称 LoD）, 最少原则", "date": "2022-04-11 07:03:00 +0000", "snippet": "在日常编码中，经常会写下面这样的代码：final String outputDir = ctxt.getOptions().getScratchDir().getAbsolutePath();上面的代码，看上去没有太大问题，但实际上其中任意一个方法发生变化时，这段代码都需要修改。因为调用是依赖的每一个细节，不仅增加了耦合，也使代码结构僵化。迪米特法则正是为了避免对象间出现这样过多的细节依赖而被提出来。迪米特法则迪米特法则（Law of Demeter，简称 LoD） 是由 Ian Holland 于 1987 年提出来，它的核心原则是： 一个类只应该与它直接相关的类通信； 每一个类应该知道自己需要的最少知识。换句话说，在面向对象编程中，它要求任何一个对象（O）的方法（m），只应该调用以下对象： 对象（O）自身； 通过方法（m）的参数传入的对象； 在方法（m）内创建的对象； 组成对象（O）的对象； 在方法（m）的范围内，可让对象（O）访问的全局变量。而 分层思维，可以被认为是迪米特法则在架构设计上的一种具体体现。在分层架构中，每一层的模块只能调用自己层中的模块，跳过某一层直接调用另一层中的模块其实就是违反了分层架构的原则。虽然迪米特法则符合封装的原理，但是却和聚合原则相冲突，因为对象需要尽可能少地考虑其他对象，不能轻易地和其他对象自由组合。实现满足迪米特法则的代码下面通过一个经典案例，实现满足迪米特法则的代码。假设一个在超市购物的场景：顾客选好商品后，到收银台找收银员结账。这里我们定义一个顾客类（Customer）、收银员类（PaperBoy ）、钱包类（Wallet ），示例代码如下：/** * 顾客 */public class Customer { private String firstName; private String lastName; private Wallet myWallet; public String getFirstName() { return firstName; } public String getLastName() { return lastName; } public Wallet getWallet() { return myWallet; }}/** * 钱包 */public class Wallet { private float value; public float getTotalMoney() { return value; } public void setTotalValue(float value) { this.value = value; } public void addMoney(float deposit) { value = value + deposit; } public void subtractMoney(float debit) { value = value - debit; }}/** * 收银员 */public class PaperBoy { public void charge(Customer myCustomer, float payment) { payment = 2f; Wallet theWallet = myCustomer.getWallet(); if (theWallet.getTotalMoney() &amp;gt; payment) { theWallet.subtractMoney(payment); } else { //钱不够的处理 } }}当顾客开始进行结账操作（charge()）时，整段代码的实现逻辑如下： 收银员算出商品总价后说：“拿出钱包交给我！” 顾客将钱包递给收银员，收银员开始检查钱包里的钱是否够支付。 如果钱够了，收银员拿出相应的钱并退还钱包给顾客；如果钱不够，收银员告诉顾客钱不够需要想办法。虽然这个场景在现实中不会发生，但是对于这里的收银员类 PaperBoy 来说，直接调用 Wallet 显然是不满足迪米特法则的，因为 Wallet 是属于 Customer 的内部成员——组成对象（O）的对象。理解了迪米特法则后，要考虑的就应该是如何减少顾客和收银员之间的直接耦合。而这里，通过钱这个要素来作为中间层进行解耦就是非常合适的选择。所以，在考虑收银员的角色时，就不应该去管钱包里的钱够不够，而应该是负责判断有没有收到足够的钱；同时，对于顾客角色，应该让他自己管好自己的钱包，只负责判断要支付多少钱。这样，顾客和收银员就不会因为钱包的职责划分不清而耦合在一起了。接下来，我们来看看如何修改代码。首先，我们将顾客“获取钱包”的动作修改为“支付账单”，代码修改前后对比如下图： 这里将顾客支付这个动作封装成方法 pay 单独拆出来，并将收银员对于钱是否足够的判断动作交还给顾客，如下所示：public float pay(float bill) { if (myWallet != null) { if (myWallet.getTotalMoney() &amp;gt; bill) { myWallet.subtractMoney(bill); return bill; } } return 0;}其次，再将收银员“获取顾客钱包”的动作，修改为“收取顾客支付的钱并和账单比对”，代码对比如下图： 这里收银员的动作就从获取钱包变为根据收到的钱进行判断的动作，如下所示：/** * 收银员 */public class PaperBoy { public void charge(Customer myCustomer, float payment) { payment = 2f; // “我要收取2元!” float paidAmount = myCustomer.pay(payment); if (paidAmount == payment) { // 说声谢谢，欢迎下次光临 } else { // 可以稍后再来 } }}到此，就利用迪米特法则顺利地实现了代码的解耦。很明显，使用迪米特法则后，不仅降低了代码的耦合，还提高了代码的重用性。即使未来顾客不想用现金支付，改用微信支付、找朋友代付等，都不会影响收银员的行为。应用迪米特法则时需要注意的问题虽然应用迪米特法则有很多好处，但是迪米特法则因为太过于关注局部的简化，而容易导致别的问题出现。 容易为了简化局部而忽略整体的简化。 迪米特法则在优化局部时很有效，因为会做一定程度信息隐藏，但同时这也是它最大劣势。因为太过于关注每个类之间的直接关系，往往会让更大类之间的关系变得复杂，比如，容易造成超大类。在类对象中加入很多其他类，并没有违反迪米特法则（与直接相关的类耦合，没有关注职责分离），但是随着其他类与更多类发生耦合后，最开始的类的关系其实已经变得非常庞大了； 拆分时容易引入很多过小的中间类和方法。 迪米特法则本质上是在对类和方法做隔离，虽然它判断的标准是对象间的直接关系，但是对这个直接关系并没有一个非常好的衡量标准，在实际的开发中，基本上是依据程序员自己的经验来进行判断。于是，经常能在代码里看见一些接口的实现功能只是调用了另一个方法的情况，这其实就是因为过多的隔离带来的中间适配，适配就会产生很多中间类和方法，甚至有的方法没有任何逻辑，只是做了一次数据透传，为的就是避免直接耦合； 不同模块之间的消息传递效率可能会降低。 比如，分层架构就是典型的例子，当 Controller 层想要调用 DAO 层的模块时，如果遵循迪米特法则，保持层之间密切联系，那么就意味着需要跨越很多中间层中的模块才行，这样消息传递效率自然会变低。面向切面编程（AOP）众所周知，面向切面编程（Aspect Oriented Programming，简称 AOP）在面试中是高频问题之一，尤其对于 Java 程序员来说，AOP 不仅是 Spring 框架的核心功能，还是在业务中降低耦合性的有效手段之一。面向切面编程，简单来说，就是可以在不修改已有程序代码功能的前提下给程序动态添加功能的一种技术。如果说迪米特法则是在程序设计时（静态） 降低代码耦合的方法的话，那么面向切面编程就是在程序运行期间（动态） 降低代码耦合的方法。比如，在 Spring 框架中大量使用的 AspectJ 工具就是面向切面编程的一种最佳实践。不过，这里尤其要注意，面向切面编程（AOP）和面向对象编程（OOP）虽然最终想要达到的目的相同，都是降低代码耦合性，但关注点却是截然不同的。这也是容易搞混淆的地方。比如: 对“会员用户”这个业务单元进行封装时，使用 OOP 思想，你会自然建立一个“Member”类，并在其中封装会员用户对应的属性和行为；如果使用 AOP 思想，会发现使用“Member”无法统一属性和行为，因为不同会员可能有不同的偏好属性和行为。同样，对于“性能统计”这个统一的动作来说，使用 AOP 会很方便，而使用 OOP 封装又会变成每一个类都要加一个性能统计的动作，变得过于累赘。从以上例子你可能已经发现，面向切面编程（AOP）和面向对象编程（OOP）这两种设计思想有着本质的差异： OOP 强调对象内在的自洽性，更适合业务功能，比如商品、订单、会员； 对于统一的行为动作，如日志记录、性能统计、权限控制、事务处理等，使用 AOP 则更合适，通过关注系统本身的行为，而不去影响功能业务逻辑的实现和演进。总结 迪米特法则又叫最少知识原则，核心思想是通过减少和不必要的类进行通信来降低代码耦合，在编程中可以用来指导对象间如何正确地协作; 在我看来，应该将迪米特法则作为一个参考原则而不是绝对原则，因为有时过度低耦合反而会带来更多问题。比如，只有一个人维护的项目，却硬要搞成几十个微服务模块的模块设计，结果不仅导致维护复杂性增高，并且系统也会变得更脆弱。降低耦合可以作为一个演进目标，但不能每时每刻都要求做到低耦合; 除了不过度使用迪米特法则外，在学习它时还要抓住另一个关键词：最少知识。最少知识，可以说是提升执行效率最好的方法之一，因为有时知道的信息越多，反而决策越困难，执行效率也越低，所以有时你得控制自己在设计对象时的知识范围，避免过多知识引起不必要的耦合发生。" }, { "title": "分离原则：将复杂问题拆分成小问题", "url": "/posts/separation-principle/", "categories": "Design Pattern", "tags": "分离原则", "date": "2022-04-10 14:45:20 +0000", "snippet": "关注点分离原则是一个帮助我们将复杂问题拆分成小问题的好方法。简单来说，在计算机科学中，关注点是能对程序代码产生影响的一组特定信息。比如： 在面向对象编程中，将关注点描述为对象； 在面向函数编程中，将关注点描述为函数； 在架构设计中，将模块、组件、框架描述为关注点，等等。其实，在日常开发中或多或少涉及了关注点分离原则的具体实践。比如： 在分层架构中，按照服务类型来划分层，层就被作为一个关注点； 在类或方法的编码实现中，按职责分离，就是将职责作为一个关注点。为什么应该使用关注点分离原则来拆分复杂问题？关注点分离原则到底说了些什么？它带来哪些启示？如何正确使用好这个原则？实际上，设计模式正是对关注点进行了有效分离才得以有效指导编程实践，所以为了高效地理解掌握设计模式，带着以上问题来一探究竟。为什么要用关注点分离原则拆分复杂问题从编码实现的角度看，使用关注点分离来拆分复杂问题有两大好处： 破坏其他用户正在使用的现有功能的可能性会变小： 实际上，可能遇见的绝大多数软件开发项目都过于复杂难懂，不仅修改一次影响多处，而且维护时也不知道哪些地方该改、哪些地方不该改，更别提什么高扩展性的代码了。不仅理解起来费劲，而且无论是在架构设计还是编码实现上，扩展性都很差; 当分离不同关注点之后，就可以避免不必要关注点功能修改，因为与必须修改的代码相比，由不修改的代码而导致系统中断的可能性更小，也就是我们常说的不修改就不会引入 Bug； 关注点分离能帮助你适应人类的短期记忆限制：一些计算机早期研究的论文中便有提及过。就像看一篇文章一样，我们的短期记忆总是只能记住最近阅读过的简单问题，而对于一些复杂问题很快就会忘记。尽管现在信息的存储和查询效率远远高于过去，但是你在编程中依然无法理解那种过于庞大而复杂的问题，只有将不同关注点拆分出来，才能更好地去解决各个问题。 所以说，使用关注点分离来拆分复杂问题很有实践的价值。两个视角下的关注点分离关注点分离（Separation of Concerns，简称 SoC）是将计算机程序分隔为不同部分的设计原则。最早出现在 1974 年，那时它只是针对如何深入研究一门课程而提出的一种思考方法。关注点分离原则的原理很简单，就是：先将复杂问题做合理的分解，再分别仔细研究程序上特定问题的侧面（关注点），最后解决得出的接口，再合成整体的解决思路。除了狭义上程序特定的关注点外，在广义上同样可以找到很多关注点，比如：很容易找到的关注点就是影响我们常用的软件设计的两个思考视角，也就是架构设计视角和编码实现视角。架构设计视角架构设计视角下的关注点分离侧重于整个系统内组件之间的边界划分，比如： 层与层； 模块与模块； 服务与服务等。通常都知道，局部的分离不代表整体的分离，因为模块与模块之间依然需要通信，一旦没有某种策略来限制相互通信的耦合度，架构稍不注意就会演变为大泥球式的架构。比如：常用 MVC 分层策略来做架构上的关注点分离。二者对比如下图所示：显然，在图中右侧的 MVC 架构中，相关的主题聚合在某一层，这样就不再难以理解了。也就是，将层作为关注点来进行分离，通过解决每一个层的问题来实现整体问题的解决。同样，现在流行的微服务架构也是采用水平分离的策略来达到服务与服务之间关注点分离的目的，如下图所示：虽说每个微服务的关注点可能完全不同，但通过统一的 API 层来进行通信就不会影响它们之间的相互配合。总之，通过上面的两个例子你可以看到：架构设计视角下的关注点分离更重视组件之间的分离，并通过一定的通信策略来保证架构内各个组件间的相互引用。编码实现视角编码实现视角下的关注点分离主要侧重于某个具体类或方法间的边界划分，估计会立马想到职责分离，没错，职责分离就是一种编码实现视角下关注点分离的优秀实践。比如，下面的代码实现（找出 3 个年龄大于 35 岁的员工的名字），就是基于操作职责相似性来进行关注点分离的，使用的是 Lambda 表达式。如图中所示，关注点现在变成了数据流上的操作步骤（过滤、映射、限制等），读取每一个用户信息数据后，都会经过相同的步骤来处理，这样不仅利于理解每一个步骤操作的具体含义，也更符合思考习惯。再比如，算法中基于任务职责相似性来进行关注点的分离，如下图所示：从图中可以清晰地看到，一个任务先被 Fork（拆分职责）为更小的任务，然后开始并行对所有子任务进行求值操作（计算职责，可以在多线程下并行处理），接着提供一个等待点进行 Join（合并职责），最终合并得到期望的值。这里，关注点分离能非常有效地将复杂的问题拆分为小问题去解决，而且能够清晰地知道不同职责需要关注的是什么、不需要关注什么。虽然架构设计和编码实现中的关注点各有不同，但是对关注点进行分离后获得的效果却是一样的： 一方面，能清楚地知道代码或架构在每一个阶段或步骤中在做什么以及它们自身关注的范围是什么； 另一方面，对于拆分后的问题我们也能快速理解与分析。除此之外，从不同的视角去分析本身也是一种关注点的分离，能够不局限于单一的视角，而是通过更细分的领域去思考。实现关注点分离无论是架构设计，还是程序设计，好的系统总是在不断地寻找可以分离的关注点。比如： 网络 OSI 七层模型就是一个关注点分离的经典应用。OSI 七层模型将网络传输分为七层，每一个层只关注于自己需要完成的事情，层与层之间通过自下而上的数据通信方式进行交互。好的架构必须使每个关注点相互分离，也就是说系统中的一部分发生了改变，并不会影响到其他部分。主要体现在以下三个方面： 即使需要改变，也能够清晰地识别出哪些部分需要改变； 如果需要扩展架构，尽量做到影响最小化； 已经可以工作的部分还都将继续工作。除了快速识别出不变和变化的部分外，在实际实施过程中，还需要一些具体的的技巧来实现关注点分离。那就是： 在架构设计上，做到策略和机制分离； 在编码实现上，做到使用和创建分离。在架构设计上，做到策略和机制分离在架构设计时，我们面对的是不同的系统、服务、类库、框架、组件等元素，要想做到关注点分离，除了将关注点从代码上移到更高的抽象层次以外，还需要做到策略和机制分离。 机制是指各种软件要素之间的结构关系和运行方式。可以理解为实现某个功能所需要的基础操作和通用结构。在代码中相对稳定，表现形式有：通用算法、流程、数据结构等可以起到不可变作用的部分。 策略是指可以实现软件发布目标的编码方案集合。在代码中相对不稳定，表现形式有：业务逻辑、接口实现。实际上，策略和机制分离的本质就是进行标准化，也就是制定一套标准（提供机制），让使用者按照标准使用它（不同策略）。这样通过机制统一起来后，你就不用担心自己的实现不同而无法和别人完成配合了。比如： Dubbo 框架就是为微服务架构模式下的不同服务提供了一种良好的 RPC 通信机制，不管是会员服务还是商品服务，哪怕使用的系统实现策略完全不同，只要按照协议约定进行 RPC 调用即可完成服务的复用； Spring 其实就是将创建 Bean 和管理 Bean 的生命周期做了抽象和封装，形成了一种统一的机制。至于想要创建什么具体类型的对象以及如何使用对象（策略），它并不用关心。所以说，在架构设计上，进行策略和机制的分离才能让关注点得到有效的分离，不然最后会因为关注点混杂过多，导致系统越来越无法修改和维护。在编码实现上，做到使用和创建分离看似简单却直击本质的关注点分离，也是一个经常使用却常常被忽略的关注点，重要性不亚于封装、多态和继承，即实体的实例化（创建）与实体间相互使用（使用）的分离。从一个简单例子开始，假定现在想要使用一个 StringProcessor 类来进行字符串相关的过滤处理操作，示意图如下： 代码实现如下所示：public class StringProcessor { // 使用大小写转换过滤器对象 private ICaseFilter myFilter; // 构造函数 public StringProcessor() { this.myFilter = new UpperCaseFilterImpl(); } // 带过滤器名称的构造函数 public StringProcessor(String filterName) { if (&quot;up&quot;.equals(filterName)) { this.myFilter = new UpperCaseFilterImpl(); } else if (&quot;low&quot;.equals(filterName)) { this.myFilter = new LowerCaseFilterImpl(); } else { this.myFilter = new UpperCaseFilterImpl(); } } // 处理字符串的方法 public String process(String str) { return myFilter.filter(str); }}这段代码的大致逻辑是：使用一个 CaseFilter 接口，在 StringProcessor 类不带参数初始化时，创建 CaseFilter 接口的一个实现类 UpperCaseFilter。如果初始化带有过滤器名称时，就根据名称来选取对应的实现类。这段代码看上去似乎没有太大问题，但是仔细分析后就会发现，当有 20 个不同过滤实现类的时，带参数的初始化里就会出现 20 个 if-else，而且当想要在另外的类里使用 CaseFilter 接口时，同样需要实现 20 个 if-else 逻辑。更为严重的是，一旦这些过滤类发生修改或变化，还需要修改所有使用它们的类。这就是将使用和创建混在一起后的结果——你既创建它又在使用它，就好比你使用智能手机拍照前，还得学会如何开发拍照程序。使用和创建分离，会带来下面一个效果，如下示意图：其代码实现如下：public interface ICaseFilter { String filter(String str);}/** * 一种实现: 替换字母 s 为 xxx 后, 再全部字符小写 */public class LowerCaseFilterImpl implements ICaseFilter{ @Override public String filter(String str) { return str.replaceAll(&quot;s&quot;, &quot;xxx&quot;).toLowerCase(); }}/** * 另一种实现: 替换字符 - 为 # 后, 再全部字符大写 */public class UpperCaseFilterImpl implements ICaseFilter{ @Override public String filter(String str) { return str.replaceAll(&quot;-&quot;, &quot;#&quot;).toUpperCase(); }}// 使用工厂方法模式实现public class CaseFilterFactory { public static ICaseFilter getFilter(String name) { if (&quot;low&quot;.equals(name)) { return new LowerCaseFilterImpl(); } return new UpperCaseFilterImpl(); }}package cn.happymaya.ndp.principle.separation;public class StringProcessor { // 使用大小写转换过滤器对象 private ICaseFilter myFilter; // 设置一个属性用于快速切换不同实现类 private String caseFilterName; // 构造函数 public StringProcessor() { // this.myFilter = new UpperCaseFilterImpl(); this.myFilter = CaseFilterFactory.getFilter(&quot;&quot;); } // 带过滤器名称的构造函数 public StringProcessor(String filterName) { this.caseFilterName = filterName; this.myFilter = CaseFilterFactory.getFilter(filterName); } public void setCaseFilterName(String filterName) { this.caseFilterName = filterName; this.myFilter = CaseFilterFactory.getFilter(filterName); } // 提供给外部使用的方法 public String process(String str) { return myFilter.filter(str); } // 测试类 public static void main(String[] args) { StringProcessor stringProcessor = new StringProcessor(); stringProcessor.setCaseFilterName(&quot;up&quot;); System.out.println(stringProcessor.process(&quot;fdasfasdfdsaf-dsfasdfasdfadf&quot;)); StringProcessor stringProcessor1 = new StringProcessor(&quot;low&quot;); System.out.println(stringProcessor1.process(&quot;fdasfasdfdsaf-dsfasdfasdfadf&quot;)); StringProcessor stringProcessor3 = new StringProcessor(&quot;low&quot;); stringProcessor3.setCaseFilterName(&quot;&quot;); System.out.println(stringProcessor3.process(&quot;fdasfasdfdsaf-dsfasdfasdfadf&quot;)); }}这里用工厂模式进行了简单的重构，将 CaseFilter 的创建工作交给了 CaseFilterFactory，StringProcessor 只是使用了 CaseFilterFactory。虽然只是增加了一个创建的工厂类，但是使用和创建的关系发生了重要的改变。当想要使用 CaseFilter 时，不用再经过 StringProcessor，只需要使用 CaseFilterFactory 工厂类。另外，即便新增了过滤的子类，那么修改的范围也只是限于 CaseFilter 和 CaseFilterFactory 之间。虽然这只是简单的改变，但是 StringProcessor 和 CaseFilter 就被完全解耦开了，而且 CaseFilter 还提供了更好的复用性。因为从职责上来分析，StringProcessor 和 CaseFilter 的确可以划分为一个职责，但就是不能很好地解耦，而换到使用的视角上后，就会发现有的类或方法你可能压根就不需要创建，只需要使用即可。所以说，在编码实现中，除了使用职责分离外，还应该从使用和创建分离来做关注点分离，这样会帮助你发现一些隐蔽的耦合关系，进而实现更好的代码可扩展性。总结关注点分离原则本质上就是要将复杂问题拆分为小问题，通过解决小问题来解决大问题： 在架构设计视角下的关注点分离，能够从具体的代码细节中抽离出来，关注上层组件之间关系，从全局策略出发去优化整体； 在编码实现视角下的关注点分离，能够帮助回到细节中，找出修改代码影响的边界范围，更好地做到局部的优化。关于使用关注点分离，只记住两个要点： 架构上做到策略和机制分离，换句话说，就是标准化，比如，HTTP 协议、Spring 框架等。重点关注的是整体系统的清晰与稳定； 在编码实现上做到使用和创建的分离。创建不单单是指创建对象，更重要的是抽象和封装。就像一台精密的相机一样，使用很简单，但是创建是非常复杂的。" }, { "title": "契约原则：做好 API 接口设计", "url": "/posts/design-by-contract/", "categories": "Design Pattern", "tags": "契约原则, Design by Contract", "date": "2022-04-10 09:03:00 +0000", "snippet": "无论是架构设计还是编码实现，现在都越来越离不开接口设计，接口可以说是新时代的“集装箱”，是得到了几乎所有人一致共识的通用标准。GoF 在很多年前便建议大家应该针对接口编程，原因就是：为了降低编程变化而导致风险出现的概率。不过在实施过程中，可能遇见的更多是下面的情况： 定义好的 API 接口，却接收了额外的参数，导致程序异常退出； 查询返回的数据，没有回传预期的格式，数据处理出错； 一个 API 服务进行分布式部署，遗漏了某个节点，导致处理数据不一致。 如何平衡“约束”与“自由”之间的关系？ 如何更好地避免再发生上述情况？ 如何真正做好API 设计呢？契约式设计原则：API 设计的指导书契约式设计原则（Design by Contract，缩写为 DbC，下文简称为“契约原则”），是一种软件设计方法。原理是：在软件设计时应该为软件组件定义一种精确和可验证的接口规范，这种规范要包括使用的预置条件、后置条件和不变条件，用来扩展普通抽象数据类型的定义。比如，在 Web 请求中，预置条件主要用于处理输入的参数校验，不变条件主要指一个请求中的共享数据状态，后置条件则是对返回的响应数据的检查与确认。如下示意图：契约原则的核心思想是：对软件系统中元素之间的相互合作以及“责任”与“义务”的比喻，这种比喻是从商业活动中“客户”与“供应商”达成“契约”而得来。在我看来，理解三者之间的关系非常重要，因为这是帮助合理利用契约原则进行 API 设计的基础。现在 API 设计已经变得越来越重要，但是不管怎么变，API 的设计实现依然需要回答三个关键问题： API 期望的是什么？ API 要保证的是什么？ API 要保持不变的是什么？能否设计好一个 API，只需要回答好上面这三个问题就行了，而这三个问题本质上就是契约原则的一种最佳实践。 API 必须要保证输入是接收者期望的输入条件。这里重点在于“期望”两个字，换句话说，就是：使用者使用 API 的前提条件是什么？只有满足了这个前提条件，API 才能提供正常的功能。 比如，API 需要 A、B、C 三个参数，那么使用者就需要提前准备好这三个参数；好的 API 接口都会提前告知使用者：它需要什么、不需要什么；当条件不满足时，它会拒绝；当条件满足时，才会开始处理； API 必须要保证输出结果的正确性。API 在处理过程中可能会遇见各种异常或错误的情况，这时 API 就不应该把错误或异常抛给其他系统去处理，而是在内部就得做好异常处理，并最终输出正确结果给使用者； API 必须要保持处理过程中的一致性。比如，同一个 API 被部署在 10 个服务器上，那么只要外部输入了正确条件后，每一个 API 内部的处理过程就应该是相同的、不变的，也就是说，当修改源代码后，你应该要同时部署这 10 个服务，才能让 API 整体的服务看起来是一个整体。不变条件通常有会话信息、共享的上下文数据状态等。所谓不变，可以理解为多个相同的副本对同一个代码含义的统一解释。 使用契约原则来指导 API 的编程和设计主要有以下三个原因： 提前告知 API 有哪些约束会让你在编码过程中节省很多不必要的沟通成本。比如，你的 API 决定使用 RESTful 风格，那么当对方在使用你的 API 时，就会知道返回的数据格式需要采用 JSON； 契约会强迫你去思考 API 是否满足更好的独立性。 API 更多时候是提供给不同类型客户端去使用的，而客户端是不需要关注 API 的内部实现，所以好的 API 一定是具备更好的独立性才能对外提供服务；如果 API 不使用标准的协议和消息格式，那么就意味着客户端在使用时需要适配 API，这样就会造成系统和 API 之间的强耦合； 契约式设计会提醒你应该保证 API 的高可用性：契约就像是一种承诺，当你在提供 API 时，不仅要保证输入满足要求，还要保证经过处理后返回结果的正确性；同时，服务还应该是稳定可用的，而不能因为网络故障、流量过大等就无法使用服务。 做好 API 接口设计道理我都懂，但实际做的时候有没有什么更具体的方法呢？”结合自身工作经验，我总结了以下六大关键技巧： 接口职责分离 接口隔离原则（ISP），是帮助做好接口职责分离的一个好方法； 在设计 API 的时候，应该尽量让每一个 API 只做一个职责的事情，这样做的好处在于能保证 API 功能的简单、易用以及稳定。一旦接口职责混合在一起，势必会造成接口间功能的相互影响。假设一个 API 既用于修改商品的价格，又用于修改订单优惠价格，还用于修改库存数量等，那么当调用修改商品价格的接口流量增大时，势必会影响同一个接口中修改订单或修改库存的功能。 使用接口隔离原则时要注意尽量找到变化相似的职责来进行封装，不要在一个接口中加很多方法，方法越多越有可能变成多个职责，那么就无法实现真正的接口隔离。 API 命名规范 好的 API 设计应该给使用者提供一种简单、直观、一致的体验； 最关键的就是对 API 的命名，常用的命名技巧有以下三种： 使用英文的小写。例如，使用美式英语的 license、color，并且尽量使用小写，因为大写稍不注意就会看错； 使用容易被大家都接受的通用缩写。比如，API 就比 Application Programming Interface 好； 通过命名含义描述接口。换句话说，API 的名字应该能自解释，比如，list-userinfo，结合其英文字面含义就知道是查询用户信息的列表。当然，现在也有很多自动生成接口说明的工具，比如 Swagger，但是对于开发者来说，依然不如直接阅读命名来得方便，并且像 Swagger 这类的工具一旦暴露到公网还会引发安全问题。 尽量少自定义错误码 在 API 设计中，会经常犯一个错误，就是想方设法创建自己的 error code，用以表达对不同错误的个性化理解； 但实际上，在 API 设计中，这是一种错误应用管理原则的实践。因为自定义惯例的风险在于各自理解可能有偏差，进而导致业务更大的混乱； 比如，当使用 HTTP API 时，获得了一个 404 错误码（在 HTTP 中是未找到资源的错误），而你恰好也定义了 404 错误码（假设代表接口数据未找到），结果必然出现冲突； 可能能会说自定义的 error code 有助于传递信息，但这有一个前提条件：必须有系统且能对自定义的 error code 进行处理，这样才有意义。如果只是传递信息的话，error message 里面的字段也可以达到同样的效果，因为当 API 出现错误的时候，API 的使用方是否可以清晰地理解错误信息，是非常重要的。比如，以下几种常见的 REST API 返回 JSON 数据形式： {&quot;error message&quot;: &quot;xxx&quot;, &quot;code&quot;: &quot;200&quot;, &quot;success&quot;: true, &quot;data&quot;: null} {&quot;msg&quot;: &quot;xxx&quot;, &quot;code&quot;: &quot;XXX_SYSTEM_ERROR&quot;, &quot;data&quot;: false} {&quot;code&quot;: 500, &quot;error&quot;: &quot;msg xxx&quot;，&quot;data&quot;:false} 可以看到，第三种形式就能很好地传递系统内部出现故障的信息，而第一种和第二种格式看上去虽然信息更多，但是很容易造成理解上的偏差。第一种格式里，success 和 code 作用有重合，这会让使用者不知道到底是应该看 code 还是看 success，又或者两者都要看。至于第二种格式，不同的人对于 XXX_SYSTEM_ERROR 可能理解完全不同。 同一接口要做到幂等 幂等，简单来说，就是当一个操作多次执行所产生的影响均与一次执行的影响相同，则它是幂等的; 幂等是一种通用策略。实际上，HTTP 规范中就明确指出 GET、PUT 和 DELETE 方法必须是幂等的。但 POST 方法不一定保证是幂等的，如果 POST 方法创建新资源，通常不能保证此操作是幂等的。 之所以要实现幂等，是因为现代服务越来越多地往分布式服务方向发展，而分布式服务的特性就在于服务的分散性，分散性的服务会带来一个问题：多个服务在同一时间可能会并行修改同一份数据。比如，你在楼下超市买完一瓶水后进行微信支付，第一次支付时网络延迟，于是你又刷了一次，那么钱应该只减一份，如果扣两次那就是没有实现幂等。 在 API 设计中做到接口幂等，通常有以下五种方法： 使用天然幂等操作。比如，数据库中的 select 查询，只要数据没发生改变，那么查询一次和多次的结果始终是一样的；还有 delete 删除操作，删除一次和多次删除都是把数据删除了（不存在了），影响是一样的，典型的天然幂等操作。 使用唯一键值。比如，在数据库中加唯一索引，或是使用 UUID 做唯一 ID，都可以防止在新增数据时出现不必要的脏数据，因为不同的服务节点操作数据时都收到了唯一 ID 的约束。 使用加锁策略。比如，悲观锁、乐观锁以及分布式锁等。如，悲观锁、乐观锁、分布式锁等。加锁的目的就是让不同的服务在同一个数据变更时不被其他服务所影响，比如，订单服务 API 部署了 500 个相同实例，那么即便通过 Nginx 网关做了负载均衡的流量分配，在修改订单的时候如果不加锁，就会导致数据被重复多次的修改，也就是无法保证幂等。 使用 Source + Tocken 验证机制。这和使用唯一键值有一点类似，不过这种方法更多用在对外提供的 API 中去保证幂等性。这两个字段实际上既做了联合的唯一索引，又做了使用来源的日志记录，这样既能保证接口的幂等性，也能记录不同客户端使用 API 的调用情况。 使用有限状态机。在一些状态变更比较频繁的业务中，会经常使用到状态机，比如，订单、支付、秒杀等业务。当状态机已经处于下一个状态，这时候又来了一个上一个状态的变更，那么这时就不允许再进行状态变更了。正是通过这种状态扭转的约束，保证了接口服务的幂等性。 安全策略。在做 API 设计时，对于内部系统和外部系统的 API 所考虑的安全策略会有所不同。 对于内部系统来说，更多的是考虑输入与输出数据的准确性，通常都采用更为基础的安全策略。比如，对接收数据要有足够的验证，出现错误要能及时抛出异常并进行异常处理。当然，对于一些核心系统，比如，财务、订单、用户数据等，依然还是需要进行安全加固，避免外部被攻破后内部数据的泄漏； 对于外部系统来说，API 面临的主要挑战是来自外部的一些针对安全上的攻击、错误调用、接口滥用等。那么在设计的时候： 针对黑客攻击，一般是购买业界一些专门做安全的防护公司的产品； 针对错误的调用方式，API 在预置条件判断时就不应该让调用进入处理，一定要及时给出错误信息； 对于接口滥用情况，需要做一些限流措施。 版本管理 本质上讲，API 是服务提供者与服务使用者之间的一种合同协议； 一旦服务提供者对 API 进行更改，则有破坏服务使用者使用 API 的风险。因此，在做 API 设计时一定要考虑更改次数所带来的风险； 另外，API 还可能因为不断升级优化而出现旧功能与新功能兼容的问题； 解决 b 和 c 两个问题的办法就是对 API 进行版本控制和管理。当需要进行重大的 API 更改时，提供一个完整的新版本，同时还要继续支持以前的版本。这里有两种简单的办法： 一种是在同一服务 API 中公开两个版本，比如在接口中使用 V1 和 V2； 另一种是并行运行两个版本的服务，可以通过增加 API 网关来根据路由规则将请求发送到对应版本中。不过，要注意的是：同时维护两个或多个版本会提升维护成本。因此，在版本管理时，最好提前计划一个旧版本的下线时间。对于内部 API 来说，要及时通知使用方进行升级和迁移。对于外部（公共）API，则只能不断告知或减缓升级的周期来让使用方及时跟上升级的节奏。 总结契约原则是良好 API 设计的底层逻辑，一个 API 接口设计只要能解决三个关键问题： API 期望什么 API 保证什么 API 保持什么那么这个 API 基本上就是一个合格的 API。不过在现实中，会发现很多 API 连基本三要素都没做好，要么是不做输入校验，要么就是捕获异常不处理、不给出错误信息，或者没按照约定来输出结果，或者扩展后不保证事务一致性等。为此总结了六大关键技巧，在我看来，要想设计好 API 接口，除了通用的方法和技巧外，还是得时不时回到本质的三要素上去思考，看看还有没有没想到的点，这样才能真正地找到适合你自己的 API 设计方法。" }, { "title": "惯例原则：提升编程中的沟通效率", "url": "/posts/coc/", "categories": "Design Pattern", "tags": "惯例原则", "date": "2022-04-10 03:30:20 +0000", "snippet": "在软件开发中，经常因为沟通效率低下而烦恼，就像： 所接手的维护项目代码质量低，频繁出问题，不得不一次又一次地找之前的人沟通； 团队中模块分散，各自编程风格不同，使用对方服务时需要反复沟通； 跨团队合作沟通，技术栈不同，需要反复沟通统一的标准。这些问题的本质都是因为代码而产生了学习成本和沟通成本，或者说，每一份代码都变成了需要重新学习的东西，自然需要反复研究和沟通。为什么学习了 Spring 框架后，再和别人交流关于这个框架的问题时沟通效率会变高？因为 Spring Boot 框架应用了一个简单的原则来帮助我们提前建立了隐形的公共知识体系，当一方提到某个知识时，另一方其实早就非常熟悉，不需要反复给对方解释含义，沟通效率自然会不断提高。而这个具备强大威力的原则：惯例优于配置原则（下文简称惯例原则）。惯例原则：有“共同语言”的编程惯例原则（Convention over Configuration，常用英文缩写 CoC），最早起源于 Ruby On Rails 框架的设计理念，如果使用过 Rails，应该就知道 Rails 几乎没有配置文件。简单来说，惯例原则就是将一些在编程中公认的配置方式和约定信息作为内部缺省的默认规则来使用。比如，MyBatis 的映射文件通常都使用 xxxMapper.xml 来命名。因为是默认统一的规则，绝大部分人都会优先采用，久而久之，便对规则有了一个统一的认知，当彼此相互沟通时，便能极大地减少重复理解与沟通的时间。另外，还可以把惯例原则理解为一种约束，在特定的框架（知识体系）下，可以根据这些约束制定合适的默认规则，然后框架就能基于这些默认规则实现一些统一的操作。比如： 基于 Spring 框架实现中，使用注解 @Autowired 能够自动注入 Java Bean，框架通过实现解析 @Autowired 注解来自动寻找对应的 Java Bean。所以，惯例原则通常也叫按约定编程，但是不同于契约原则（DBC）的按统一协议/标准协作，惯例原则更重视的是隐形知识的共享。比如，使用 Maven 来管理工程结构，其实是预先假定你在维护我的代码时，是已经提前学习或知道 Maven 相关的惯例约定的。惯例原则解决的问题先来看一个经典的例子，如下图：图中的目录结构是一个用 Maven 自动生成的常见 Java 工程结构，这个目录结构对于 Java 程序员来说不言而喻，就是一种公认的惯例。实际上，惯例原则主要解决了在编程中我们对共同隐性知识的学习的问题，通过统一的默认规则，建立起了一道沟通的桥梁。可能会说，可不可以不用这种惯例？可以！但是，就需要花费大量的时间去解释和说明你新规则的含义，并且还得保证要能消除其他相关人员对新规则的误解。除此之外，惯例原则还间接起到了以下两个作用： 逐渐形成一种编程圈子里共同的专业行话。 也就是说，在编程领域，只要你这样说，几乎所有人都知道你说的是什么，而不需要额外的解释和沟通； 减少编程时思考决策次数，降低认知负担。 对于编程人员来说，选择不同技术往往比只能使用一种技术来编程实现要难，因为“选择”就意味着需要去评估选择后可能带来的各种风险。副作用什么样的代码维护起来最轻松？答案是：需要学习的东西最少的代码。使用惯例原则，不仅带来良好的可维护性，同时极大地提升你和他人沟通时的效率。虽然惯例原则的优点非常明显，但缺点也同样非常明显。有四个重点且常见的副作用： 丢失灵活性。最大一个弊端在于，过于定制化默认规则。虽然默认规则能统一大家认知，提高代码可维护性，但却也失去了一定程度灵活性。比如，Web 中使用统一 ApiResponseResult 类来处理 json 返回格式，虽然这是一个不错惯例，但是当有人想要修改不同数据格式返回时，可能就需要修改很多代码； 自定义惯例有风险。默认值的判断标准一旦不统一，很可能导致业务更大混乱。曾经我就遇见过这么一个案例：数据库 YN 字段默认值通常使用 0 和 1（0 为删除，1 为正确），但是突然一天来了一个很厉害的新员工，他的惯例是 -1 和 0（0是正确，-1 为删除），但他没有告知团队他对默认值的这个理解；于是在接下来长达半年时间里，部分系统的代码里 YN 默默地变成了 -1 和 0，此时系统并没有任何异常，但是某一天团队系统需要迁移数据并删除旧数据，惨剧就开始了，迁移速度超乎寻常地快，这时迁移人员在没有 check 的情况下，继续执行删除操作，最后导致部分数据丢失； 参考变强制。本来惯例原则是基于开源社区框架发展起来，但是随着越来越多团队开始使用这些框架，惯例原则慢慢变成了某种程度上的强制标准，进而导致设计僵化，缺乏灵活性。比如，设计数据类只能写 get、set 方法，或者虽然分层但是代码只写在一个层里（通常是 Service 或 Dao 层）； 不同框架下惯例之间并不能复用。比如，C++ 的开发惯例和 Java 的开发惯例其实是不相通的，惯例本就是一种通用化的定制规则。一定要尽早意识到不同知识体系下的惯例是不同的，一旦胡乱使用惯例，就会带来和上面第二个副作用一样的严重问题。正确使用惯例原则在软件开发中，惯例原则其实早已深入人心： 各类通用框架普及，比如，Spring、Spring Boot、MyBatis 等都采用了惯例原则设计，你在使用中会逐渐接受一些惯例并固化到自己的编程习惯中去； 使用惯例能够减少当面与人沟通的次数，大大提升了沟通效率。为了避免在使用惯例原则时引入过多副作用，还需要使用一些技巧来合理应用惯例原则。我总结了以下五个技巧： 遵循大多数人使用的惯例：比如，Java 中使用驼峰命名的惯例，MySQL 数据库字段使用下划线分割单词等；应该选择大多数人都习惯使用的惯例，这样不仅能保证理解的一致性，而且还能在沟通中减少重复解释，节约沟通成本；除此之外，使用大多数人都习惯的惯例还有一个好处就是，这些惯例都是经过了实践检验的，可以在很大程度上避免未知风险。 要搞清楚惯例的适用范围：有的惯例可能只适用于特定的编程语言；有的惯例可能只适用于特定编程场景；有的惯例可能只是某个行业所特有的……；只有搞清楚了惯例所处范围，才能发挥惯例最大化共享知识的作用，否则就会变成另一种严重的误解。 自定义惯例时需要在团队内反复不断确认：除了业界通用的惯例外，有时需要针对实际业务自定义一些惯例，比如，Redis Key 在不同环境下要加前缀标识（dev/prod/test），这时一定要确保团队内的每个人都知晓所采用的惯例是什么，并在实际编码中不断和调用方确认有自定义惯例的存在，以避免理解的歧义造成重大安全事故。 要在惯例和灵活性之间做平衡：过度使用惯例和过度设计一样都非常有害；惯例优于配置并不是说完全消除配置，在某些需要灵活性的地方可能需要配置，甚至需要代码实现，那就选择那个最合适的做法；如果发现使用惯例不仅能帮助对方高效理解代码，自己实现起来也很方便，那就是用惯例； 不要强制他人使用惯例：其实不使用惯例也能让开发工作顺利进行，不应该让惯例从一种自由组合变成一种强制要求；因为只要变成强制，虽然惯例提升了沟通效率，但是也有可能在其他方面带来负面影响。比如，在一个团队内，有人喜欢在引入组件时使用配置文件，有人喜欢用注解的方式引入组件，一旦规定只能使用配置或只能使用注解，那么就会破坏一部分人的编程习惯，他们就不得不重新学习、花费更多时间，这样反而降低了开发效率。 总结 惯例原则的初衷是提供隐形的公共知识，来减少开发人员重复决策的次数； 惯例原则的优势在于能够帮助我们降低编程时的学习成本，不过它也有一些劣势，比如： 可能导致设计的灵活性不足； 乱用自定义惯例导致不同人按照各自的理解使用而引发问题，将参考的默认规则变成强制的主要规则，等等； 惯例原则是程序员之间的共同行话，只要你用了一个惯例，那么对方便知道其中的意思； 虽然惯例原则很简单，但是在日常开发中却应用很广泛。在使用时应该注意：隐形的公共知识和大家的理解是不是一致的？如果不一致，要尽量保持一致，避免自以为正确的使用而导致理解出现偏差，进而在不知情的情况下导致维护系统出现故障。" }, { "title": "职责原则：在代码设计中实现职责分离", "url": "/posts/duty-principle/", "categories": "Design Pattern", "tags": "职责原则", "date": "2022-04-09 14:30:20 +0000", "snippet": "在面向对象编程中，会经常听到“要实现代码间职责分离”，但是具体什么样的代码才算得上是清晰的职责分离，似乎却又总是模糊不清。比如： 代码模块越多职责越清晰？ 按照需求来分配职责就是职责分离？ 模块化就是职责分离？实际上，要想写出“职责分离”的代码，单从字面含义是很难下手的，因为业界并没有统一的通用标准。比如，什么是职责分离？为什么职责分离很重要？如何实现职责分离？职责分离目标：高内聚、低耦合什么是职责？在《敏捷软件开发：原则、模式与实践》这本书中，把“职责”定义为“变化的原因”。如果你能够想到多于一个动机去改变一个类，那么这个类就具有多于一个的职责。什么是职责分离？为了更好地理解职责分离，先从一个熟悉而又陌生的概念讲起，那就是内聚。如下图所示：在该图中，模块按照相对小的功能进行划分（数字表示，比如模块 1），这里假设业务领域已经被分析为有三个不同的功能，并放在了一个模块内（叫“我的模块”），其中，模块 A、B、C 之间没有什么共同的职责，分别在独立的数据上运行。没错，常见的 Controller + Service + Dao 里的各种功能多是这样的组织形式，看上去很漂亮的结构，但实际上却是最混乱的，俗称大泥球结构，这也是内聚度很低的一种模式。观察上面的关系图你会发现，八个模块都依赖着“我的模块”。在这种情况下，如果想要在系统中的其他模块使用功能 A、B 或 C，那么调用就会依赖整个“我的模块”，这显然导致了太多的依赖，大大降低了可维护性。那么，为了提高内聚性，就应该对功能进行分离，如下图所示：很明显，现在每个模块的依赖比原来少了很多，模块 A、B、C 之间没有直接的关系，并且模块 3 是唯一一个依赖模块 A 和模块 C 的模块。这样带来的好处是，当我们依赖 A 或 B 或 C 时，能够清晰地知道它们依赖了哪些模块，也就是下次修改代码时影响的模块有哪些，将变更风险控制在有限范围内。这样才算是做到了真正的高内聚，也就是各个模块专注于自己最重要的某个职责，并建立起与其他模块之间清晰的界限。所以说，内聚本质上表示的是系统内部的各个部分对同一个问题的专注程度，以及这些部分彼此之间联系的紧密性。此时，可以注意到，对同一个问题的专注程度是判断内聚高低的标准，而职责分离只是实现高内聚的一种方法而已。那么，现在就可以来回答“什么是职责分离”：简单来说，职责分离就是将不同变化原因引起的类或方法修改行为拆分到不同类或方法里去。职责分离的重要性职责分离很重要原因，主要有三点： 直接对问题进行对象建模，方便厘清构建逻辑。在面向对象编程中，通常都会建议将现实中的事物或问题翻译成对象，这样更“拟人化”，也能更好地进行编码实现，就好比让每一个代码模块能够像人一样具备自己的属性和行为，只需要指定特定的职责就能让各自模块运行良好，而不是像面向过程编程那样把所有的功能都放在一起。比如，针对商品属性相关的问题，我们可以建立商品基本信息对象、赠品信息对象、活动商品对象等各类对象，然后通过不同的职责关联统一起来，这样在修改时就能通过清晰的职责边界来理解代码逻辑关系了； 将问题分解为各种职责，更有利于系统的测试、调试和维护。比如，开发一个电商系统，你一定不会把所有的系统（订单、物流、商品、支付）都放在一起，因为这样不仅不利于理解系统，而且其中任何一个子系统的代码修改都会影响到别的系统。除此之外，职责分离不够的系统，测试起来也会非常痛苦，因为每一次的修改不敢保证不会影响别的系统，那么就需要测试相关联的系统，这样大大降低了交付效率，同时还会因为测试不充分而出现线上问题； 提高系统的可扩展性。虽然可扩展性是现代软件设计的必选项之一，但是很多系统在前期时间紧、任务重的情况下几乎都会放弃一部分扩展性，于是矛盾就出现了，系统已经上线，但用户需求却不断变化，这时如果需要添加一些新东西，那么你就需要改动所有没有清晰划分职责的地方，这样势必影响系统的运行。但如果做好了职责划分，那么你就只需要改动具有相应职责的类，而不会影响到系统的其他部分，这样不仅能提高系统的可扩展性，还降低了代码修改引入风险的概率。 职责分离时机在编码实现中，职责分离的时机大致有三个： 命名太过于笼统； 改动代码后的测试规模很大； 类和方法过大。命名太过于笼统时是职责分离的好时机你一定希望类、方法和其他对象（包、服务等）的命名能够直接反映出它们的作用，同时还要足够简短，方便记忆。如果命名过长或表述模糊，通常可能是因为类或方法包含的职责过多而无法筛选出职责的优先级，这样的类随着修改越多，问题出现的概率也越高。这时就是进行职责分离的好时机，通过将不同职责拆分出来，就能很好地限定问题范围，即便出现问题需要修改也只是在限定范围内修改，不会影响到其他模块。改动代码后的测试规模很大如果每次修改代码都要重新进行一次全量测试，那么这也是进行职责分离的好时机。“修改一处影响全部”可以说是开发和测试都不愿意面对的情况之一，这说明代码耦合性高、内聚度低。换句话说就是，代码中的职责过多，彼此之间相互影响。这时可以通过修改代码处的职责来进行代码重构，找出合适的职责进行分离，逐步减小全量测试的范围，这样就能减少职责之间的相互干扰。遇见超大类或方法也是一个时机。 绝大多数情况下，超大的类或方法都是职责划分不清导致的代码过度耦合（当然，有的算法实现本身就很复杂，进而导致出现超长的方法，这种情况不在这次的讨论范围内）。比如，当一个类包含了太多的其他类时，可以用一个简单的原则来判断职责是否过多，那就是：能否拆分出来更多的子类？如果不能，那么这个类很可能就是高内聚的，职责比较单一；如果能，那么这个类还不够内聚，职责还有多余的。如何通过职责分离实现高内聚职责分离更多的是一种设计思想和编程技巧，主要的理念就是将模糊笼统的问题拆分为多个清晰单一的问题。而实现职责分离的核心技巧就在于寻找互相不重合的的职责。下面是一个通过分离职责实现真正内聚的例子，如下面的反转字符串的处理程序代码所示：public class Application { private static void process(String[] words) { for (int i = 0; i &amp;lt; words.length; i++) { String arg = &quot;&quot;; for (int j = words[i].length(); j &amp;gt; 0; j--) { arg += words[i].substring(j - 1, j); } System.out.println(arg); } if (words.length == 2) { if (words[0].equalsIgnoreCase(&quot;hello&quot;) &amp;amp;&amp;amp; words[1].equalsIgnoreCase(&quot;world&quot;)) { System.out.println(&quot;...bingo&quot;); } } } public static void main(String[] args) { process(new String[]{&quot;test&quot;, &quot;is&quot;, &quot;a&quot;, &quot;mighty,hahaah,world&quot;}); process(new String[]{&quot;hello&quot;, &quot;world&quot;}); }}这是一段非常简单的代码，代码的功能是：接收命令行的任意字符串参数，然后反转每个字符，并检查反转后的字符是否为“hello world”，如果是，则打印一条信息。虽然这段代码很短，但是它的内聚性非常差。为什么呢？可以从 process () 方法看出问题所在，process() 这个名称并没有告诉你方法实现了什么操作，而如果要以正规方式命名这个方法，通常会出现类似于这样的名称：reverseCharactersAndTestHelloWorld()，命名太过于笼统的通常就是内聚性较差的信号。继续阅读代码，也会发现，process() 方法要做的职责太多了，并且这些事情彼此又并不相关。那么，该如何通过职责分离来优化代码呢？直接看代码，如下所示：public class Application { public void process(String[] words) { for (int i = 0; i &amp;lt; words.length; i++) { String afterReverseChar = reverseCharacters(words[i]); words[i] = afterReverseChar; System.out.print(words[i]); System.out.print(&quot; &quot;); } System.out.println(); if (isHelloWord(words)) { System.out.println(&quot;...bingo&quot;); } else { System.err.println(&quot;...does not exist&quot;); } } private boolean isHelloWord(String[] words) { if (words.length != 2) { if (words[0].toLowerCase().equals(&quot;hello&quot;) &amp;amp;&amp;amp; words[1].toLowerCase().equals(&quot;world&quot;)) { return true; } } return false; } private String reverseCharacters(String forward) { String reverse = &quot;&quot;; for (int i = forward.length(); i &amp;gt; 0 ; i--) { reverse += forward.substring(i - 1, i); } return reverse; } public static void main(String[] args) { Application myApp = new Application(); myApp.process(new String[]{&quot;hello&quot;,&quot;world&quot;}); }}这时，当再读 process() 方法时，读到的是一系列步骤，每个步骤都由一个方法来实现，可以很快区分每个方法具体负责的职责是什么。如果字符反转不正确，会知道去 reversecharacters() 方法里查找 Bug，因为反转字符的职责已明确分配给了合适的方法，而且只有这个方法执行这个操作。虽然这个例子很简单，但是它给出一个很重要的启示：按照职责进行分离能帮助我们转移对问题的关注点。关注点其实就是用户关注的核心点，比如： 例子中如果不分配职责，关注点就是处理字符串，但是处理字符串有很多方法和步骤，关注点太笼统了。一旦开始尝试分离职责，就会发现关注点发生了转移，也就是问题范围变小了。不过，在很多编码实现过程中，又很容易省略这个“转移”。因为哪怕是抽取一个方法，都会多耗费写代码的时间，而很多人实际上不愿意花这个时间，想等到有时间时再来优化。可等真的有时间时却发现代码逻辑千丝万缕，职责多到超出想象，根本优化不动。所以说，分离职责是一种将复杂问题拆分成可解决问题的有效方法。总结学习职责分离最重要的是要理解是什么原因引起了代码变化。搞清楚变化的原因，比一拿着需求就开始编码更为重要。在编码过程中，很多人总以为按照功能需求分配好了职责，代码就能按照职责运行了，但实际上可能一开始连职责都分配错了。比如，我现在对一些维护项目做 Code Review 时，还会偶尔遇见超大的类和方法，其中 90% 的维护人员给的理由都是不想到处跳转寻找代码，只修改一个文件更方便维护。但修改一个文件不代表不影响其他文件，因为变化的原因没有被真正找出来，这样错误的认知到后期常常都是要付出惨痛代价的。在程序设计中，分配职责的常常是人，而人在大多时候是无法做到精准分配的，甚至会出现重复或错误的分配。只有有意识地进行职责分离，才能提高代码的可维护性。这里还需要说明的是，虽然高内聚常和低耦合一起被提及，但是在我看来： 高内聚不一定都是低耦合，比如：订单与商品系统，可能只是因为联系特别紧密，才需要强耦合在一起，不能说订单系统下单成功，商品系统却说故障了没有商品，这是不行的。 低内聚也可能是低耦合，比如，针对一个上传文件功能，开发了 100 个模块，每个模块和另一个模块只有一个链接，但整体功能和关系却可能异常复杂。所以，在应用职责分离原则时，要把握好拆分的力度，尽量朝着单一职责的目标去做，但如果实在不好做，那么控制在有限范围内就好。" }, { "title": "面向对象原则：SOLID", "url": "/posts/solid/", "categories": "Design Pattern", "tags": "SOLID", "date": "2022-04-09 09:30:20 +0000", "snippet": "说到面向对象编程，有一个原则几乎每个程序员都知道，那就是 SOLID 原则。关于它的资料介绍也非常丰富，实践例子也很多。但实际上很有可能把 SOLID 原则都用错了，并且还无意识地一直在滥用它。之所以这么说，有两个方面： 很多时候都将每一个原则分开使用，易造成过度解读。比如，在使用接口隔离原则时容易只关心接口，忽略不同实现，或者不关心接口之间的关系以及和整体系统之间的关系； 因为 SOLID 总是能让你无意识地将简单问题复杂化。比如，明明只需要写一个一次性同步数据的方法，然后写完即扔，有一天突然想到 SOLID 原则，于是又搞出来十几个多余的类。有了锤子，总是容易想去找钉子，殊不知有时就完全不需要锤子，只需要一把小刀即可解决问题。五大设计原则2000 年，Robert C. Martin 在他的《设计原理和设计模式》这一论文中首次提出 SOLID 原则的概念。而后，在过去 20 年中，这 5 条原则彻底改变了面向对象编程的世界，改变了我们编写软件的方式。SOLID 原则的核心理念是帮助我们构建可维护和可扩展的软件。因为随着软件规模的扩大，一个人维护所有的代码越来越困难，这时就需要更多的人来维护代码，而多人协作的关键在于相互通信与协作，恰好 SOLID 原则提供了这样一个框架。“SOILD”是由五大原则的英文首字母拼写而成，具体对应情况如下。 S（Single Responsibility Principle，简称 SRP）：单一职责原则，意思是对象应该仅具有一种单一的功能。 O（Open–Closed Principle，简称 OCP）：开闭原则，也就是程序对于扩展开放，对于修改封闭。 L（Liskov Substitution Principle，简称 LSP）：里氏替换原则，程序中的对象应该是可以在不改变程序正确性的前提下被它的子类所替换的。 I（Interface Segregation Principle，简称 ISP）：接口隔离原则，多个特定客户端接口要好于一个宽泛用途的接口。 D（Dependency Inversion Principle，简称 DIP）：依赖反转原则，该原则认为一个方法应该遵从“依赖于抽象而不是一个实例”。单一职责原则（SRP）单一职责原则（SRP）的原意是：对一个类而言，应该仅有一个引起它变化的原因。对此，通常更容易这么来理解： 只有一个类或方法； 写好就不能修改的类或方法； 一个接口对应唯一一个实现。上面这些理解不能说完全错，但是只抓住了单一职责原则（SRP）本质上重要的两点中的一点——单一，而忘记了另一个也很重要的点——职责。“职责”可以定义为“变化的原因”。三种理解方式不够准确原因如下： 职责不一定只有一个类或方法，还可能有多个类或方法，比如，上传文件是单一职责，而上传方法、增删改查 URL 方法、校验方法都服务于上传文件。 不能修改的类或方法本质上有很多影响因素，比如，代码长时间没有维护、设计时没有预留扩展接口，等等。 一个接口对应一个实现并不能说职责是单一的，因为一个接口中可能会存在没有划分清楚的职责。一个简单例子，假设有一个书籍类，保存书籍的名称、作者、内容，提供文字修订服务和查询服务，代码如下：package cn.happymaya.ndp.principle.solid;public class Book { private String name; private String author; private String text; // Omit constructor, getter and setters public String replaceWordInText(String world) { return text.replaceAll(world, text); } public boolean isWoldInText(String word) { return text.contains(word); } }整体服务运行良好。但是，有人说只是保存书的信息，不提供打印和阅读功能，岂不是很浪费资源。于是，立即加了打印和阅读的服务，如下所示：public class Book { //... //打印服务 void printText(){ //具体实现 } void getToRead(){ //具体实现 }}到此，很容易发现，当打印服务需要针对不同的客服端进行适配时，书籍类就需要多次反复地进行修改，那么不同的类实例需要修改的地方就会越来越多，系统明显变得更加脆弱，同时也违反了 SRP。所以说，理解 SRP 时，一定要抓住一个重点：职责是否具有唯一性。当有多个动机来改变一个类时，那么职责就多于一个，也就违反了 SRP。开闭原则（OCP）开闭原则最初是由 Bertrand Meyer 在 20 世纪 80 年代提出的，被称为“面向对象设计中的最重要原理”。开闭原则是指软件组件（类、方法、模块等）应该对扩展开发，对修改关闭。这就意味着当你在设计或修改程序代码时，应该尽量去扩展原有程序，而不是修改原有程序。在我看来，开闭原则更像是一个框架的设计原则，而不是具体的业务编码技巧。因为在实际业务编码实现中，需求变化总是快于技术更新，直接修改业务代码的时间成本有时会比扩展的时间成本低很多，所以说，在非常细节的业务编码实现中，只扩展而不修改原始的代码几乎很难做到，反倒是在框架、类库或架构设计中常常更容易实现开闭原则。即便强行在编码实现中这样做，也会导致过多的冗余类产生，并导致最终系统整体调用关系复杂。同样，用一个简单的例子来帮助理解开闭原则。如果基于 Spring JDBC 写不同 DataSource 进行读写分离的代码，就会对开闭原则有一个大致了解。具体代码如下：public abstract class Demo extends AbstractDataSource { private int readDataSourceSize; @Override public Connection getConnection(String username, String password) throws SQLException { return this.determineTargetDataSource().getConnection(username, password) ; } protected DataSource determineTargetDataSource() { if (determineCurrentLookupKey() &amp;amp;&amp;amp; this.readDataSourceSize &amp;gt; 0) { // 读库做负载均衡(从库) return this.loadBalance(); } else { // 写库使用主库 return this.getResolveMasterDataSource(); } } private DataSource loadBalance() { return null; } private DataSource getResolveMasterDataSource() { return null; } protected abstract boolean determineCurrentLookupKey();}上面这段代码的大致意思是说： 通过继承 AbstractDataSource 类，可以重新构造不同的 DataSource，以达到读库使用从库（做负载均衡）、写库使用主库的目的。这里虽然不能修改 Spring JDBC 的代码，但是可以通过扩展来实现更复杂的场景。这就是开闭原则的一种具体体现，很多时候总是会使用各种不同的工具和框架，不可能做到所有工具的自行开发，这时通过开闭原则就能进行很多的扩展与改进，而不需要重复造轮子。里氏替换原则（LSP）里氏替换原则（LSP）这个名字看上去虽然有点奇怪，但是在面向对象编程中，它却是使用频率非常高的一个原则。里氏替换原则（LSP）的原意是：子类应该能够完全替换掉它的基类。换句话说，在进行代码设计时，应该尽量保持子类和父类方法行为的一致性。这样做的好处在于，即便是扩展子类，也不会丢失父类的特性。同时，里氏替换原则（LSP）也是针对接口编程的最佳实践原则之一，因为某一个接口定义的功能不改变，那么就可以使用很多不同算法的代码来替换同一个接口的功能。比如，Spring 中提供的自定义属性编辑器，可以解析 HTTP 请求参数中的自定义格式进行绑定并转换为格式输出。只要遵循基类（PropertyEditorSupport）的约束定义，就能为某种数据类型注册一个属性编辑器。我们先定义一个类 DefineFormat，具体代码如下：public class DefineFormat{ private String rawStingFormat; private String uid; private String toAppCode; private String fromAppCode; private Sting timestamp; // 省略构造函数和get, set方法}然后，创建一个 Restful API 接口，用于输入自定义的请求 URL。@GetMapping(value = &quot;/api/{d-format}&quot;)public DefineFormat parseDefineFormat( @PathVariable(&quot;d-format&quot;) DefineFormat defineFormat) { return defineFormat;}接下来，创建 DefineFormatEditor，实现输入自定义字符串，返回自定义格式 json 数据。public class DefineFormatEditor extends PropertyEditorSupport { /** * setAsText（） 用于将 String转换为另一个对象 * @param text * @throws IllegalArgumentException */ @Override public void setAsText(String text) throws IllegalArgumentException { if (StringUtils.isEmpty(text)) { setValue(null); } else { DefineFormat df = new DefineFormat(); df.setRawStingFormat(text); String[] data = text.spilt(&quot;-&quot;); if (data.length == 4) { df.setUid(data[0]); df.setToAppCode(data[1]); df.setFromAppCode(data[2]); df.setTimestamp(data[3]); setValue(df); } else { setValue(null); } } } /** * 将对象序列化为 Strin g时，将调用getAsText（）方法 * @return */ @Override public String getAsText() { DefineFormat defineFormat= (DefineFormat) getValue(); return null == defineFormat ? &quot;&quot; : defineFormat.getRawStingFormat(); }}最后，输入 url：/api/dlewgvi8we-toapp-fromapp-zzzzzzz，返回响应。{ &quot;rawStingFormat:&quot;dlewgvi8we-toapp-fromapp-zzzzzz&quot;, &quot;uid:&quot;dlewgvi8we&quot;, &quot;toAppCode&quot;:&quot;toapp&quot;, &quot;fromAppCode&quot;:&quot;fromapp&quot;, &quot;message&quot;:&quot;zzzzzzz&quot;}到此你会发现，使用里氏替换原则（LSP）的本质就是通过继承实现多态行为，这在面向对象编程中是非常重要的一个技巧，对于提高代码的扩展性是很有帮助的。接口隔离原则（ISP）如果说单一职责原则（SRP）是适用于类的设计原则，那么接口隔离原则（ISP）就是适合接口的设计原则。接口隔离原则（ISP）的原意是：不应该强迫用户依赖于他们不用的方法。什么情况下会造成“被强迫”呢？答案就是：当你在接口中有多余的定义时。比如下面代码中的接口定义：public interface ICRUD&amp;lt;T&amp;gt; { void add(T t); void update(T t); void delete(); T query(); void sync();}增删改查是应用最多的方法定义之一，但是你发现上面这段代码中的问题没？虽然我们定义的五个方法职责没有重合，但是其中有一个方法对于很多人来说，可能就是低频的方法，那就是同步（sync()）。这样显然没有做到接口的隔离，只是在一个接口中做了方法隔离，当你使用 ICRUD 接口时就会被强迫实现 sync() 方法。其实，正确的做法应该是将 sync() 方法分离出来，如下所示：public interface ICRUD&amp;lt;T&amp;gt; { void add(T t); void update(T t); void delete(); T query();}public interface ISync { void sync()；}这样代码就变得清晰了很多，将增删改查放在一起、同步放在一起，这才算得上实现了接口的隔离。换句话说，好的接口隔离不是只考虑一个接口中方法的隔离，还应该多考虑整体系统中的职责。实际上，在应用接口隔离原则（ISP）时同样需要注意职责的单一度，而不能简单地认为只要定义的方法间没有重叠就算是隔离了。依赖反转原则（DIP）依赖反转原则（DIP）的原意是： 高层模块不应该依赖底层模块，二者都应该依赖于抽象； 抽象不应该依赖于细节，细节应该依赖于抽象。简单来说，就是应该在编程时寻找好的抽象。这里的抽象不是简单地指 Java 中的 interface，而是指可以创建出固定却能够描述一组任意个可能行为的抽象体。而好的抽象就是指具备一些共性规律并能经得起实践检验的抽象。比如，关系型数据库（RDMS）就是对数据存储与查询的一种正确抽象。再比如，我们非常熟悉的 JDBC 协议就是一种对数据库增删改查使用的正确抽象，还有我们课程后面模块要讲的设计模式也是某种场景下的正确抽象。当然，好的抽象并不容易找到，更多的时候你还是得做很多定制化的开发。总之，依赖反转原则（DIP）给的启示是：要尽量通过寻找好的抽象来解决大量重复工作的效率问题。五个原则之间的关系虽然 SOLID 原则在面向对象编程中应用广泛，但是千万不要死记硬背。在我看来，SOLID 原则之间其实是有一定联系的，搞清楚这些联系，不仅能帮助你理解记忆 SOLID 原则，而且还能更好地应用它们。SOLID 五大原则之间的关系图，如下所示： 开闭原则是 SOLID 原则追求的最终目标。因为修改代码非常容易引入 Bug，即便是很小的改动都有可能引起未知的 Bug。而一旦系统因为 Bug 出现故障，担责的一定是我们。没有人愿意担责，所以，我们都更喜欢写新代码而不是修改旧代码。除此之外，在设计之初就尽量以实现开闭原则为目标，它能为你在未来的实际开发中提供更高的代码扩展性。不过，这里需要注意一下，开闭原则不是封闭原则，千万不要把遵守开闭原则当作必要条件，如果代码需要适应现实的需求变化而必须要修改的话，那么这时就应该违反原则。当然尽量还是要做到开闭； 单一职责原则是重要的基础原则，它帮助实现了里氏替换原则、接口隔离原则和开闭原则。只要仔细分析各个原则的含义就能发现，它们都涉及了两个关键动作：分离和替换。那么是逻辑揉在一起、接口定义模糊的代码容易分离和替换，还是职责单一、接口抽象清晰的代码容易分离和替换呢？答案很明显是后者。之所以说单一职责原则是基础，就是因为要想实现代码的灵活扩展性需要更容易理解的模块。而职责单一的模块，更容易被组合起来用于更大的职责，也能进行快速替换和修改； 依赖反转原则是一种指导原则，同样是用来分离和替换代码的。只不过它作用在更高层次、更广的范围内，因为它太重要了。总结SOLID 原则是面向对象编程中非常重要的指导原则： 使用 SOLID 越多，代码可重用性变得越来越高的同时，代码逻辑却也相应地变得越复杂。之所以会变得复杂，是因为 SOLID 原则太过于重视分离与灵活替换，这就意味着可能需要创建很多单一的类和方法，再通过接口把它们连接起来，这样反而容易让模块和模块之间的调用关系变得更错综复杂，增加了整体的复杂性。这显然违背了 KISS 原则； 当想要兼顾 KISS 原则和 SOLID 原则时，最简单的办法就是控制接口的数量。尽量抽象某一个职责下的通用接口类（可以有多个实现类），而不是搞出很多的一个接口只对应一个实现类的模块，这看上去是在依赖抽象，实际上还是在依赖单个的实现。SOLID 原则本质上就是为了在不修改原有模块的情况下有更好的扩展功能，也就是实现开闭原则，但是要想真正做到，一定不能忽略一个隐含的前提条件，那就是在设计时就要提前考虑模块的扩展性。如果一个系统在设计时就只有一个大模块，所有的功能都揉在里面，这样即便你想要应用 SOLID 原则，也是做不到的。另外，在应用 SOLID 原则时一定要结合五个原则综合考虑，并结合实际业务进行合理取舍。千万不要在某个原则上过度解读，而误认为要满足所有 SOLID 原则才算是应用了 SOLID 原则。关于 Robert C. Martin 的书： 《代码整洁之道》 《代码整洁之道 程序员的职业素养》，《代码整洁之道》姊妹篇，讲解要成为真正专业程序员要具备什么样的态度，需要遵循什么样的原则，需要采取什么样的行动 《重构：改善既有代码的设计（第2版）》，揭示了重构的过程，解释了重构的原理和最佳实践方式，并给出了何时以及何地应该开始挖掘代码以求改善。给出了 60 多个可行的重构，每个重构都介绍了一种经过验证的代码变换手法的动机和技术。提出的重构准则可以一次一小步修改代码，减少开发过程中的风险Spring docs" }, { "title": "反转原则：减少代码间相互影响", "url": "/posts/reversal-principle/", "categories": "Design Pattern", "tags": "DIP", "date": "2022-04-09 03:30:20 +0000", "snippet": "SOLID 五大设计原则中，其中有一个原则，可能用得很熟练，但是说到概念却又容易跟其他原则混淆的，那就是依赖反转原则（Dependence Inversion Principle，简称 DIP）。在实际的研发工作中，可能会遇到以下场景： 一个平台系统，需要接入各种各样的业务系统，而这些业务系统都有自己的账号体系，平台需要兼容这些系统的账号体系，于是代码中出现了大量依赖于各种账号体系的代码； 一个网站页面，需要越来越多的频道（首页、搜索、分类等），不同频道对应的个性化需求各不相同，并且各种页面的标准组件、布局、模板，以及与后端交互框架也各不相同，不同体系的代码依赖非常紧密； 一个通用的订单处理平台，各条业务线都需要通过这个平台来处理自己的交易业务，但是垂直业务线上的个性化需求太多，代码里随处可见定制化的需求代码。对于这些问题，可能已经有解决方案了：如果依赖和控制的东西过多了，就需要制定标准，反转控制，解耦，分层……甚至也知道该如何在代码中解决这些问题，比如，面向接口编程，而不是面向实现编程。在这个解决过程中，其实已经在使用 DIP 了，可能对于 DIP 概念本身，你还没有透彻理解，甚至说到 DIP 时感觉还很陌生。DIP：统一代码交互的标准对于“依赖反转”这种抽象性概念，可以结合现实生活中的场景或例子来剖析和理解。比如，在没有电商的时代，商品交易时，通常是买家一手交钱、卖家一手交货，所以基本上卖家和买家必须强耦合（因为必须要见面交易）。而这时有一个中间商想出了一个更好的办法，让银行出面做交易担保——买家把钱先付给银行，银行收到钱后让卖家发货，买家收验货后，银行再把钱打给卖家。通过这样的方式，买卖双方把对对方的直接控制，反转到了让对方来依赖一个标准的交易模型的接口——银行。这和浏览器的使用原理也很类似。浏览器（对应商品买家）并不依赖于后面的 Web 服务器（对应商品卖家），其只依赖于 HTTP 协议（对应银行），只要我们遵循 HTTP 协议就能在浏览器中提供很多丰富的 Web 功能，而不必针对特定的浏览器定制开发。因此，可以总结一下，依赖反转原则（DIP）就是一种统一代码交互标准的软件设计方法。回到 DIP 的概念上来，看一下它的原始定义： 高级组件不应依赖于低级组件，两者都应依赖抽象； 抽象不应该依赖实现，实现应该依赖抽象。如何去理解 DIP 这个原始定义： 定义中的高级组件和低级组件，主要对应的是调用关系上的层级： 汽车油门（高级组件）调用汽车引擎（低级组件），但并不是说汽车油门就比汽车引擎更复杂、功能更完善、能力更高； 软件程序都得依赖底层操作系统，而你不能说软件程序就一定比操作系统复杂； 高级组件和低级组件都应依赖抽象，是为了消除组件间变化对对方造成的影响，换句话说，抽象是一种约束，让高级组件或低级组件不能太随意地变动。因为两者间有相互依赖关系，一方变化或多或少都会带给对方影响。比如，踩油门是加油，抬起油门是减油，这是一种抽象约束，只要约束不变，设计圆形油门还是方形油门都不会影响引擎的动力控制；反过来，引擎使用铝制还是铁制，也不影响引擎对油门加、减油的控制； 抽象不应该依赖实现，实现应该依赖抽象。什么意思呢？这里拿 JDBC 这个数据库驱动协议作为例子来简单解释一下。在用 Java 开发增删改查的数据业务时，我们通常会开发一个数据库访问层——DAO 层，而它并不直接依赖于数据库驱动（实现），而是依赖于 JDBC 这个抽象。JDBC 并没有受不同数据库设计的影响，只要不同数据库驱动都实现了 JDBC，就能被 DAO 层所使用，而为了让应用程序使用，数据库驱动也依赖于 JDBC，这便是抽象不应该依赖实现，实现应该依赖抽象。为什么要使用 DIP使用 DIP 的意义，大致总结为如下两点： 可以有效地控制代码变化的影响范围； 可以使代码具有更强的可读性和可维护性。正如文章开头提到的那些场景问题，它们都有一个共同点：如果别人的程序一发生变化，我们的代码就得跟着发生变化，也就意味着我们又得加班加点改代码，这是一件是非常痛苦的事情。简单来说，外部系统的需求或功能变化影响到了内部系统。使用 DIP 的目的 控制代码变化带来的影响。 比如，为了解决平台接入权限问题，可以通过抽象一个账号权限体系的接口标准，让不同业务系统按照这个统一标准来接入平台，同时我们的平台也按照这个标准来实现。此时，我们的内部系统和外部系统不再是通过定制化的映射来通信，而是使用了一套统一的标准接口来通信，只要接口不发生变化，即使外部系统发生了巨大变化，接入的功能并没有发生改变，这样就能有效地控制外部系统的变化对内部系统带来的影响； 增强代码的可读性和可维护性： 虽然现如今软件行业提倡“敏捷开发”，少写文档和注释，需求能提前上线更重要，但是，少写文档不是让我们不写文档和不写注释。比如，你接手两个项目，一个项目不仅没有系统设计文档，还没有代码注释，同时代码逻辑依赖又很多，到处都是看不懂的定制化逻辑；而另一个项目，只有少量文档，并且还有很清晰的接口定义和代码注释。通过这样的对比，立马就能知道哪个项目的维护难度更低！ 使用 DIP，就是从设计上减少系统的耦合性，更能帮助厘清代码逻辑，因为代码是通过统一抽象之后，功能相同的处理都在同一个地方，所以，代码变得更加顺畅，更容易让人理解，也就增强了代码的可读性和可维护性。 具体实现抽象标准接口下面通过设计一个 Java 组件来演示其应用思路。这个组件只实现一个功能：读取一串字符后，再输出显示： 组件中定义 StringProcessor 类，该类使用 StringReader 组件获取 String 值，然后使用 StringWriter 组件将值写入输出流并打印； 为方便理解组件的作用关系，将 StringReader 类和 StringWriter 类统称为低级组件，StringProcessor 称为高级组件，这样能更清楚地了解每个设计选择是如何影响整体设计的。设计一：低级组件和高级组件都作为具体类放在同一包中StringProcessor 取决于 StringReader 和 StringWriter 的实现，读取字符并输出打印，整个逻辑实现在同一个包内通过三个具体类来完成，如下示例代码：public class StringProcessor { //具体类 private final StringReader stringReader; //具体类 private final StringWriter stringWriter; //具体类 //通过构造函数来注入依赖组件 public StringProcessor(StringReader stringReader, StringWriter stringWriter) { this.stringReader = stringReader; this.stringWriter = stringWriter; } public void readAndWrite() { stringWriter.write(stringReader.getValue()); } //测试用例 public static void main(String[] args) { StringReader sr = new StringReader(); sr.read(&quot;1111111&quot;); StringWriter sw = new StringWriter(); StringProcessor sp = new StringProcessor(sr,sw); sp.readAndWrite(); }}这是最基本的设计方法。优点： 逻辑简单、编码容易；缺点： 程序高度耦合，任何一个低级组件的修改都会影响高级组件。这时，如果想要复用 StringProcessor 组件，则需要在引入的地方写很多重复代码； 从 DIP 应用的角度来看，控制逻辑这时依然是由高级组件发起，没有达到依赖反转的效果。设计二：低级组件具体类与高级组件接口实现类放在同一程序包中有了设计一的基础版本，要想优化，自然会想到 Java 中的面向接口编程思想。下面继续改造。首先，将 StringProcessor 变为一个接口：public interface IStringProcessor { void readAndWrite(StringReader stringReader, StringWriter stringWriter);}这时，通过传递 StringReader 和 StringWriter 参数作为组件依赖，实现 StringProcessor 接口，就能实现读写功能。public class StringProcessorImpl implements IStringProcessor{ @Override public void readAndWrite(StringReader stringReader, StringWriter stringWriter) { stringWriter.write(stringReader.getValue()); } public static void main(String[] args) { StringReader sr = new StringReader(); sr.read(&quot;222222&quot;); StringWriter sw = new StringWriter(); IStringProcessor sp = new StringProcessorImpl(); sp.readAndWrite(sr,sw); }}现在，StringProcessorImpl 实现类依赖于抽象类 StringProcessor，符合 DIP——实现应该依赖抽象，按理说高级组件就应该很好地被复用。其实不然，这里会遇见最常见的误区之一：只要组件有接口就代表一定可复用。为什么说就算组件有接口，也并不代表实际上就真的能复用呢？因为低级组件没有依赖于抽象，这时的高级组件依然是直接依赖于低级组件，一旦低级组件发生变化，高级组件必然要发生变化，就像这里的例子一样，虽然给高级组件抽象了一个接口，但低级组件依然没有依赖抽象。而对于外部复用这个组件的系统或服务来说，一旦 StringReader 或 StringWriter 发生变化，则会导致自己也要跟着变化，这就没有达到复用的基本效果——内部组件变化不影响外部引用。因此，依然没有实现依赖关系的反转，控制权还是在高级组件那里，组件间还是高度耦合。设计三：低级组件接口类与高级组件接口实现类放在同一程序包中既然设计二里的低级组件变化会影响高级组件，那么，为了更好地解决问题，新的设计选择就应该把低级组件也进行抽象。这时，分别将 StringReader 和 StringWriter 进行抽象，如下代码所示：public interface StringReader { void read(String path); String getValue();}public interface StringWriter { void write(String value);}StringProcessor 接口定义保持不变，不过，StringWriter 和 StringReader 的类型已从具体类变为了接口。public class StringProcessorNewImpl implements IStringProcessorNew{ @Override public void readAndWrite(IStringReader stringReader, IStringWriter stringWriter) { stringWriter.write(stringReader.getValue()); } public static void main(String[] args) { IStringReader sr = new StringReaderImpl(); sr.read(&quot;333333&quot;); IStringWriter sw = new StringWriterImpl(); IStringProcessorNew sp = new StringProcessorNewImpl(); sp.readAndWrite(sr,sw); }}此时，StringProcessor、StringReader 和 StringWriter 都依赖于抽象，整体组件的逻辑控制权真正发生了变化，通过抽象化组件之间的交互，已经实现了从上到下的依赖关系的反转。过去，StringProcessor 依赖于 StringReader 和 StringWriter 的具体实现，而现在则是 StringProcessor 来定义一组抽象规则，由 StringReader 和 StringWriter 来依赖。设计四：低级组件接口类与高级组件接口类放在不同包中在设计三中，成功实现了依赖反转，使得在实现 StringReader 和 StringWriter 时也更加容易，不过，因为低级组件的实现和高级组件的实现这时还在同一个组件包内，不方便真正复用。而在真实场景中，是需要使用不同的包或框架。下面的代码示例中，使用了 Spring 框架的注解注入来实现设计。public class SPTest { @Resource private StringProcessor sp; @Resource private StringReader sr; @Resource private StringWriter sw; public void main(String[] args) { sr.read(&quot;444444&quot;); sp.readAndWrite(sr,sw); }}对于 StringReader 和 StringWriter 的实现这时就变成了“黑盒”，换句话说，对于使用者来说，我们可以完全不用关心 StringReader 和 StringWriter 是如何实现的，实现可以是 JDK 原生实现，可以是第三方的包实现，也可以是我们自行实现的，只要低级组件按照抽象约定的提供读写功能即可。到此，在上面的四种设计中，设计三和设计四都是对 DIP 的有效实现。虽然是用 Java 来编码的，但使用其他语言（Python、Go 等）时，同样能够运用这个思路来实践 DIP。IoC、DI、IoC 容器与 DIP 的区别说起 DIP，会经常听到三个相关名词： 依赖注入（DI） 控制反转（IoC） IoC 容器稍不注意，就容易把它们混淆在一起。那它们和 DIP 之间到底有什么区别和联系呢？为了更好做出区分，先要搞清楚 IoC、DI 和 IoC 容器基本概念： 控制反转（Inversion of Control，简称 IoC）： 一种设计原则（也有人将其称为设计模式）； 顾名思义，用于反转设计中各种组件的控制关系，以实现松耦合； 这里的控制是指对象除自身主要职责以外的任何其他职责，通常包括对应用程序流控制，以及对象创建或子对象创建、绑定控制： 比如，开车去上班，这意味着你要控制一辆汽车，而 IoC 的理念是反转你的控制，你不用自己开车而是选择打车，让出租车师傅带你去上班，这时你的控制就发生了反转，出租车师傅专注于开车，而你专注于上班。 依赖注入（Dependency Injection，简称 DI）： 用于实现 IoC 的设计模式； 简单来说，它允许在类之外创建依赖对象，并通过不同方式将这些对象提供给类。一般来讲，主要有三种方式来注入类： 通过构造函数 通过属性 通过方法 IoC 容器（又叫作 DI 容器）： 用于实现自动依赖注入的框架； 它的作用是管理对象的创建及其生命周期，并提供向类注入依赖项的具体实现，这样做是为了不用手动创建和管理对象； 实际上更准确的描述应该是 DI 容器，只不过因为 Spring 号称自己为 IoC 容器而造成了误解。 IoC、DI、IoC 容器和 DIP 之间的关系如下图所示：如上图所示，这四者之间关系可总结为： IoC 容器，一种技术框架，用来管理对象创建以及其生命周期，提供依赖注入实现，是 DI 的具体实现； DI， 一种设计模式，将依赖通过“注入”的方式提供给需要的类，是 DIP 和 IoC 的具体实现； IoC，一种设计原则（或设计模式），将代码本职之外的工作交由某个第三方（框架）完成，与 DIP 相似； DIP， 一种设计原则，它认为高层组件的功能不应该依赖下层组件的实现，而应该提供抽象层让下层依赖，与 IoC 有异曲同工之妙。这里要重点注意，DIP 既不是 DI 也不是 IoC，只不过因为它们长期一起“工作”，界限常常被模糊掉罢了；而常说的 IoC 容器通常是指 Java 中的 Spring IoC 容器，而实际上应该是 DI 容器。总结 依赖反转原则（DIP） 一种设计理念，是为了帮助我们解耦复杂的程序。换句话说，DIP 是一种简单但功能强大的设计思想，可以使用它来实现结构良好、高度分离和可重用的软件组件； 带来一个重要启示：不管是程序设计还是工作生活，如果依赖和控制的东西过多了，就要学会制定标准，倒置依赖，反转控制，释放自身资源，专注于更重要的事； 在理解反转真实含义后，在设计时会用到一个重要技巧：要让高层组件拥有定义抽象的权力，而不是把这个权力下放到低层组件。 使用 DIP 的原因 有效控制代码变化的影响范围； 使代码具有更强的可读性和可维护性。 DI、IoC、IoC 容器与 DIP 的区别与联系 切记，DIP ≠ DI + IoC； 依赖注入（DI），是一种设计模式，是 DIP 与 IoC 具体实现； 控制反转（IoC），是一种设计原则，核心点在于通过分离职责，让控制被反转，与 DIP 类似； 另外，通常所说的 IoC 容器其实是特指 Java 中的 Spring 的 IoC 容器，而实际上真正应用更广泛的是 DI 容器。 " }, { "title": "工程思维：用软件工程方法解决开发难题", "url": "/posts/engineering-thinking/", "categories": "Design Pattern", "tags": "工程思维", "date": "2022-04-09 03:30:20 +0000", "snippet": "在工作中，有时候会遇到下面一些情形： 团队辛辛苦苦做出的软件产品，却被客户云淡风轻地评价一句：“这不是我想要的。” 在新的项目中，作为系统的主要设计者，采用了很多新技术，却发现团队成员的技术水平参差不齐，项目进度不断延期； 每次一到季度或年度总结的时候，明明做了很多项目，付出了很多努力，但是被问到这些项目具体取得了哪些收益、对业务有哪些提升时，却顿时语塞。要解决这些问题，不应只是靠“我应该更努力工作”来解决，更重要的是：是否意识到这些问题背后的本质？是否也有自己的解决方案？事实上，在软件开发过程中，已经有了一套相对完善的方法论——软件工程。软件开发的特点软件开发到底是一门科学，还是一门工程？这个在业界一直是一个被各方争论的热点问题。在我看来，软件开发兼有两者的特点： 从计算机科学角度看，软件开发需要关注软件本身运行原理，比如时间复杂度、空间复杂度和算法正确性。我们需要学习这些基本原理，来帮助自身探索正确的分析和建模方法，从而精进开发技能； 从工程角度看，软件开发更多的是关注如何为用户实现价值。我们需要在时间、资源和人员这三个主要限制条件下构建满足用户需求的软件，同时还得不断适应新的技术和新的需求变化。可以这么说，我们并不一定每天都需要使用计算机科学的知识，但是每天都需要使用软件工程的知识。不过，因为常常受到技术思维”打造尽量完美的软件“的影响，而忽略了软件工程的真实效用。比如，在以下这些软件开发场景中： 为了体现自身技术实力，项目中常常使用“自研框架”，而不愿采用现有开源程序或者购买稳定合适的组件； 在编码中喜欢使用各种最新技术，而没有考虑代码维护时，团队其他成员能否理解这些代码； 因为上线任务紧急，编码时很少写单元测试，而采用手工测试； 对产品设计和 UI 设计关注不多，需求评审时更注重技术是否好实现，而不太关注用户体验是否更好。其实，这些问题的本质就在于总是只关注局部的具体功能实现，而忘记了具体的功能之间要相互配合来完成一个系统的整体功能。换句话说，在软件开发时，总是容易太过于关注局部，而没能跳出局部去看整体。实际上，类似这样的问题在软件工程中已经有了很多具体解决方案，比如敏捷开发，只要能合理利用软件工程的知识，就能轻松解决这些问题。软件工程的定义软件开发： 从狭义上讲，指的是软件编程（常说的编写和维护代码的过程）； 从广义上讲，指的是根据用户要求，构建出软件系统或者系统中软件部分的一个产品开发过程，是一系列最终构建出软件产品的活动。现在，常用“软件开发过程”这个词来表述。这个过程包含了 6 个关键活动阶段，用公式表达就是：$软件开发过程 = 定义与分析 + 设计 + 实现 + 测试 + 交付 + 维护$ 不管是小到一个程序中某个模块的开发，还是大到行业级的软件项目，只要是软件开发，都离不开这个本质过程。在实际工作中，很容易陷入狭义定义里：软件开发 = 软件编程，这是因为编程是实现最终代码交付的一个常重要的活动，也是耗费时间最多的一个阶段。软件工程的定义如下: 软件工程（Software Engineering），是软件开发里对工程方法的系统应用。 这里的软件开发指的是软件开发过程，也就是说，软件工程研究的对象就是软件开发过程，只不过是借鉴了工程方法来对这个过程进行优化。随着开发技术不断演化，针对软件开发过程的开发模型也不断兴起，从早期的瀑布式开发模型到后来出现的螺旋式迭代开发，以及近年来流行的敏捷开发，都属于对开发本质过程的不同认识。与此同时，开发人员也在不断探索过程中对应的方法和工具，比如，建模、面向对象方法、面向服务的方法等。所以，总结起来，软件工程 = 过程 + 方法 + 工具。 这里的“过程”，就是对软件开发过程的抽象而形成的过程模型，同时包含模型里所使用的方法。 而“方法”是指基于这些过程模型的方法集合。因为过程模型决定了软件开发过程是怎样的，进而才决定了应该使用什么样的开发方法。 “工具”则对应的是过程模型不同阶段中的方法可能用到的工具。比如说，我们选用瀑布模型作为过程模型（如下图），那么对应的软件开发过程就应该是按照瀑布模型的分阶段来进行，对应的方法就是模型中的方法，像需求分析、架构设计……而对应就会使用一些需求分析工具、架构设计工具、文档管理工具。虽然近些年，云计算、微服务、AI 这些新技术产生，对软件工程产生了影响，但是软件开发过程本质并没有发生变化，依然能使用软件工程对过程模型进行合理抽象，不断优化方法和工具，制订更合适的解决方案。应用软件工程带来的好处除了能跳出局部看整体，在软件开发中，应用软件工程还有以下两个好处: 提升项目交付的效率。在实际的软件项目中，最想要解决的问题就是：如何在有限的条件下完成软件开发，并完成交付 ! 比如说，项目的开发时间是有限的，服务器资源有限，人员有限（哪怕是只有你一个人，时间、精力也是有限的）, 这就决定了在选择技术方案、组件模块、人员时，必须要有所取舍。 此时, 就会出现一个矛盾： 一方面，在软件开发中会遇见大量的技术性问题，如果一味压缩时间不解决问题，那么交付的软件产品必然会出问题; 另一方面，一旦开始着手解决技术问题，会很容易陷入技术思维（追求完美）的误区里，如果花费大量时间去解决问题，又会导致项目交付延期甚至失败。这个矛盾其实就是软件工程想要解决的问题之一，也就是，如何通过科学的方法去帮助软件开发人员或项目管理人员合理地分配资源，并让资源发挥最大效用。 换句话说，在学习软件工程后，更能重视对资源的分配，提升思考的维度。就像，开发一款小程序页面，不一定非要自己搭建服务器，可以选择成熟的公有云来提供服务，或者使用一些第三方的小程序服务。这样，只需要基于成熟服务来开发程序，不需要再重新开发基础服务，最终的产品交付效率自然会提升。如果一开始便使用技术思维来做产品，去完善所有的功能需求，则很可能产品没上线，机会就错过了。 提升项目交付的可靠性。交付软件产品，除了要提升效率外，还应该保证产品可靠性。 很多软件项目之所以频繁发生线上事故，都是因为在每一次发布前没有进行充分的测试而导致的，这实际上也说明了对软件开发过程认识不充分。 软件开发过程中之所以会有一个重要的测试阶段，就是因为软件开发是一件极易出错的事，而测试就是为了发现问题。然而，很多时候为了赶时间，不断地压缩测试时间，甚至认为测试不那么重要，毕竟业务及时上线看到效果才是更重要的。但是，这就埋下了隐患，技术问题有一个很重要的特性就是会越积越多，然后当问题积累到一定程度后，就会突然爆发导致服务故障。 比如，我曾在某一次迎新时亲眼看见某核心系统突然宕机，原因是该核心系统依赖了一个时间久远的非核心系统，在流量陡增时，非核心系统无法抗住流量而崩溃，导致中断了核心系统的链路处理进而引起雪崩效应。为什么会出现这种情况呢？就是在之前的备战测试中，核心系统的异常处理不够完善，但非核心系统的开发人员以为调用量很小，压根没有进行测试。 通过这个例子，深刻体会到软件开发过程中为什么会专门有一个阶段来做测试了。这并不是为了做流程而做流程，而是因为软件本身的复杂性所决定的，软件一定要经过测试后才能交付和发布，不然极大概率会出现问题。 所以说，保证软件开发每个阶段的可靠性才能保证软件产品最终的可靠性。软件工程对过程不断优化的理念，正是提升每一个软件项目交付可靠性的最佳解决方案。 应用软件工程的建议软件工程的三要素：过程、方法和工具。这里应该重点关注过程模型，因为软件工程的本质就是对软件开发过程的不断优化和反复实践。软件开发过程充满了复杂性，前面提到的效率难题、可靠性难题主要是一个项目本身的难题，而软件开发中还有另一个难题是：业务到技术实现。换句话说，就是如何通过组织人进行编码来实现产品和技术的连接。关于这个难题，结合我的经验来看，总结了五点： 都是项目； 盯紧目标； 提前计划； 有效沟通； 交付结果。都是项目用项目的视角去处理问题或业务, 会带来两点好处： 培养从整体看问题的习惯； 通过做计划，提高时间管理能力。众所周知，性能优化在任何一个项目中都非常重要。假设在某个项目中，经过努力优化，系统响应时间缩短了 60%，TPS 提升了 2 倍。这个结果说明了什么呢？系统响应时间缩短、吞吐量提升是不是就意味着能满足需求呢？实际上，这就是只看到局部而忘记了整体。的确，性能优化很重要，但是在当前的项目阶段中，性能优化是不是你需要达到的目标？还是说你的目标只是为了优化而优化？而当你用项目视角去重新看待这个问题时，就会发现，优化只是一个手段和方法，在合适的阶段引入才是最重要的。比如，你应该提前为项目大促做准备，当大促流量暴增或者某接口超时比例过高时，如果不进行优化，系统可能会存在风险。这样，你不仅会预留一些时间来重新做代码审查，也会看到项目的成败关键并不只是每一次新增功能的完成与否。盯紧目标主要体现在两个层面： 负责到底； 不断调校。负责到底是基本要求。 无论是写代码，还是做设计，只要是定下一个目标，就应该始终盯紧目标。比如，你最近要写一个文件上传的模块代码，调研了很多上传方法，然后觉得都特别有意思，想要都试验一次，结果发现用的底层框架并不支持这些方法；于是你转而研究起了框架，又发现这些框架也很有意思，想要做下源码分析，再动手试试……可时间一天一天过去，你却发现文件上传的代码还是没有写完。不断调校是基于事实的合理改变。 有时，在项目的推进过程中，可能需要修改目标。比如说，需要临时修改紧急线上问题，而这对当前的项目可能会造成一些影响，这就需要敢于基于事实来做合理的评估，而不是固执地认为目标定好以后就不允许修改了。提前计划最常犯的一个错误就是：拿到需求直接开始编码，计划便是不用计划，用最快的速度去实现代码即可，而这往往会导致更多问题的出现。在工作中，经常会发现很多开发人员都自诩自己很懂得时间管理，但是难度几乎相同的项目，有的人经常延期，而有的人每次都能按时上线。这是为什么呢？一个很重要的原因就是，这些按时交付的人做事往往具备一个共同特质：做事有计划。做事有计划是说，要在有限的条件下提前做好分析，并在一个相对固定的时间周期内合理安排任务。简单来说，就是要搞清楚熟悉哪些、不熟悉哪些、哪些可控、哪些不可控……这样就能提前规避风险，并从整体上提升交付效率和质量。有效沟通如何把复杂的技术问题简单准确地表达出来，让别人听懂并且不产生误解，是每个开发人员都会面临的挑战。比如，下面是一个研发 A 同学在遇到线上技术问题时，在群里面求助的情况： A 同学：大佬们，新架构上线后，数据库不动了。A 同学：业务方反馈说店铺活动有问题。其他同学：各种问题讨论中，上下文信息很多……A 同学：数据库有问题，报备风险。B 同学：先把问题描述清楚。A 同学此时的心理状态：产品都出问题了，大家赶紧来一起看一下，反正不是我的问题，我只是恰好被提及了，我也把日志和截图发群里面了，谁负责的问题谁自己看。B 同学可能的心理状态：这啥问题啊，还得自己翻聊天记录看，现在没空，也没接到电话，先不处理。这里恰好反映了两种典型的被动的沟通心态： A 同学的只抛问题不找关联人； B 同学的要看问题的轻重缓急再处理。在平时的沟通群里有很多类似这样的对话，几乎都是在做无效沟通，甚至一个问题出现了几个小时，既没有搞清楚问题是什么，也没有找到责任人是谁。实际上，当具备了软件工程思维后，提问方式可以变为： 从定义问题开始，到业务方期望的目标是什么、实际上发生了什么、收集到了哪些信息，再到自己初步分析的思路是什么，最后自己的计划是什么，都清晰而又系统地表达出来了，比如：“现在有一个问题需要解决，问题现象是 xxx，业务方的预期是 xxx，实际看到的是 xxx，不符合预期，从日志和报错看可能是 xxx 出问题了。由于 xxx 项目上线时间紧迫，急需解决，在线等。”这就是一个典型的有效沟通过程。这种解决思路可以大大提升你的沟通效率，直至问题高效解决。交付结果在工作中，往往因为编码任务太重，以为代码只要写完，任务就算是做完了，领导或项目组的成员都会自动知道你的项目结果。这是对交付结果最大的误解。实际上软件工程早已对此提出了一种解决办法——发布你的产品，用现在流行的话讲，叫具备闭环思维。比如这样的场景：参加一个需求评审，会上大家各抒己见发表意见却没有提出什么结论和待处理项，等到再次评审的时候，却发现问题越来越多，很多讨论过的问题需要从头再开始讨论。如果把上面的需求评审会当成一个项目来看，这就是没有发布产品（也就是得到结论），形成不了闭环，导致项目无法完成，那必然也解决不了问题。其实，交付结果意味着你需要主动结束项目，包括结束确认、结束反馈、测试验证是否通过等，这和等待项目自动结束是完全不同的一种思考方式。总体来说，软件工程其实就是一种有目的、有计划、有步骤地解决问题的方法，并不是只有项目经理和架构师才需要具备，普通开发人员也需要学习和掌握，妥妥地提升编码能力和思考能力。总结软件工程的概念最早是在 1968 年被提出，为的是应对日益增长的“软件危机”——在软件开发及维护的过程中所遇到的一系列严重问题，距今已有五十多年的历史，现在已经逐渐发展成一门独立的科学学科。如今，新技术和新方法的不断涌现，让高效地交付可靠的软件产品，变得越来越重要。一个质量不可靠的软件产品，一定会给用户和工程师带来麻烦，甚至造成无法挽回的经济损失，这是很多人都不愿意看到的。而软件工程的重点研究对象其实就是过程，当过程随着外部技术的进步和发展在发生变化的时候，其对应的方法和工具也在不断发生变化。换句话说，这些年软件工程的知识一直在更新迭代中，其“过程+方法+工具”的本质并没有变，无非是各种方法和工具的推陈出新。为了更好地应用软件工程方法，应该牢记下面的公式： 软件开发过程 = 定义与分析 + 设计 + 实现 + 测试 + 交付 + 维护； 软件开发 ≠ 软件编码； 软件工程 = 过程 + 方法 +工具。实际上，软件工程对过程的改进思维，能够应用于很多方面，包括工作中的代码编写、架构设计、需求分析、制订计划、项目管理等。无论你是一个普通工程师还是项目管理者，都应该学点软件工程方法。" }, { "title": "组合思维：Unix 哲学启示", "url": "/posts/combined-thinking/", "categories": "Design Pattern", "tags": "组合思维", "date": "2022-04-09 03:30:20 +0000", "snippet": "Unix 操作系统诞生于 20 世纪 60 年代，经过几十年的发展，技术日臻成熟。在这个过程中，Unix 独特的设计哲学和美学也深深地吸引了一大批技术开发人员，他们在维护和使用 Unix 的同时，Unix 也影响了他们的思考方式和看待世界的角度。简单来说，Unix 哲学是一套基于 Unix 操作系统顶级开发者们的经验所提出的软件开发的准则和理念。Unix 哲学并不是正统的计算机科学理论，它的形成更多是以经验为基础。像模块化、解耦、高内聚低耦合这些设计原则，还有类似开源软件和开源社区文化，这些最早都是起源于 Unix 哲学。可以说 Unix 哲学是过去几十年里对软件行业影响意义最深远的编程文化。Unix 哲学本质Unix 哲学最早起源于 1978 年，是 Unix 设计者在一次关于如何设计简洁、高效操作系统服务接口中思考总结。在随后的几十年间，它逐渐形成了自己独特编程文化，并发展出一套影响深远设计哲学。早期 Unix 开发人员将“模块化”和“可重用性”概念引入软件开发实践中，并在程序设计中建立一套用于开发软件文化规范。不同于传统系统设计理论的自顶向下，Unix 哲学注重实效，立足于丰富的经验，所以说不算是一种正规的设计方法，更像是一门艺术技艺，但这并不妨碍它的思想精粹在软件行业中发光、发热。Unix 设计哲学，主张组合设计，而不是单体设计；主张使用集体智慧，而不是某个人的特殊智慧。比如，现在流行的开源文化，最早是 Unix 技术社区提倡的共享代码文化演化而来，以及后来出现的 Linux 到现在都一直保留着开放源码传统。正因如此，才使得更多程序员开始研究 Unix 设计精髓，并不断实践与发展 Unix 哲学。这从侧面体现了组合思维强大威力。所以说，当真正了解 Unix 哲学后，会发现如今很多编程思想和编程方法都是对 Unix 哲学思想的一种传承或演化。对编程的启示Unix 哲学发展至今，诞生了无数优秀设计原则和最佳实践，对业界也产生了深远影响。不过，对于编程来说，最有价值的原则要数 Peter H. Salus 总结三条原则： 编写可以做一件事，并且做得很好的程序（简单完备性）； 编写程序以协同工作（组合思维）； 编写程序来处理文本流，因为这是一个通用接口（数据驱动）。我将其简单概括为：简单完备性、组合思维和数据驱动。这三条原则，几乎涵盖了编程所有方方面面，即便是在新技术层出不穷的今天，提升编程技艺，从这三方面入手，并坚持实践原则，依然会获得很多新启发。启示一：保持简单清晰性，能提升代码质量Unix 哲学中的“简单原则”，是现在被应用得最多的原则之一（大家更习惯称它为“模块化”）。这条原则的大致宗旨就是：一个程序只做一件事，并做得很好。 这条原则听上去简单，但它实际上能极大地降低软件复杂度。软件复杂度目前，对于软件复杂度定义尚未统一。我个人认为比较倾向于 Manny Lehman 教授在软件演进法则中提出的：软件复杂度（区别于计算复杂度）是对影响软件内部关联关系的属性的描述。就是代码之间的相互影响越多，软件越复杂。比如，A 依赖 B，B 依赖 C……一直这样循环下去，程序就会变得非常复杂，也就是编程中常说的：“如果一个类文件写了上万行代码，那么代码逻辑将会非常难阅读。”软件复杂度，有以下三个来源： 代码库规模。这个与开发工具、编程语言等有关了，不过需要注意，代码行数与复杂度并不呈正相关。比如，Java 语言编写的库通常会比 C++ 的库的代码行数更多（语言特性决定），但不能说 Java 类库就一定比 C++ 的类库更复杂。 技术复杂度。这个指的是不同的编程语言、编译器、服务器架构、操作系统等能够被开发人员理解的难易程度。比如，著名的 Netty 库，对于很多 Java 程序员来说，理解起来就有一定的难度，这就是有一定的技术复杂度。 实现复杂度。不同的编程人员，对于需求的理解不同，在编程时就会有截然不同的编写风格，比如，前端程序员和后端程序员网页分页的代码实现风格就会明显不同。降低软件复杂度即便软件复杂度的来源不同，但它们都可以通过保持简单性来降低复杂度。 对于代码库规模来说，通过减少硬编码来控制代码量。比如，使用设计模式中的策略模式来替换大量的 if-else 语句，使用通用工具类来减少重复的方法调用。除此之外，还可以利用语言特性来减少代码量，比如，在Java 8 中使用 lambda 表达式来精简语句。 对于技术复杂度来说，要想在整体上保持简单性，需要在设计时就做好技术选型。好的技术选型能够有效控制组件引入技术复杂度的风险。比如，在做系统设计时，引入像 Kafka 这样的消息中间件之前，需要从系统吞吐量、响应时间要求、业务特性、维护成本等综合维度评估技术复杂度，如果系统不需要复杂的消息中间件，那么就不要引入它，因为一旦引入后，就会面临指派人员学习与维护、出现故障后还要能及时修复等问题。 对于降低实现复杂度来说，可以使用统一的代码规范。比如，使用 Google 开源项目的编码规范，里面包含了命名规范、注释格式、代码格式等要求。这样做的好处在于，能快速统一不同开发人员的编程风格，避免在维护代码时耗费时间去适应不同的代码风格。Unix 哲学中所说的保持简单性，并不单单是做到更少的代码量，更是在面对不同复杂度来源时也能始终保持简单清晰的指导原则。启示二：借鉴组合理念，有效应对多变需求对于任何一个开发团队来说，最怕遇见的问题莫过于：不停的需求变更导致不停的代码变更。即便花费大量时间，在项目前期做了详细需求分析和系统分析设计，依然不能完全阻挡需求的变化。一旦需求发生变更，意味着开发团队需要加班加点地修改代码。事实上，Unix 在设计之初就已经遇见过这些问题，可以借鉴 Unix 中那些能够“任意组合”的例子： 所有的命令都可以使用管道来交互。 这样，所有命令间的交互都只和 STD_IN、STD_OUT 设备相关。于是，就可以使用管道来任意地拼装不同的命令，以完成各式各样的功能； 可以任意地替换程序。 比如，我喜欢 zsh，你喜欢 bash，我们可以各自替换；你喜欢 awk，我不喜欢 awk，也可以替换为 gawk。快速切换到熟悉的程序，每个程序就像一个零件一样，任意插拔； 自定义环境变量。 比如，Java 编译环境有很多版本，可能用到的有版本 8、11 和 14，通过自定义 JAVA_HOME 环境变量，可以快速启用不同编译环境。这充分说明了 Unix 哲学的组合思维：把软件设计成独立组件并能随意地组合，才能真正应对更多变化的需求。然而，在实际工作中，很多时候可能都只是在做“定制功能驱动”式的程序设计。比如，用户需要一个“上传文件的红色按钮”，你就实现了一个叫“红色上传按钮功能”的组件，过几天变为需要一个“上传文件的绿色按钮”时，你再修改代码满足要求……这不是组合设计，而是直接映射设计，看似用户是需要“上传”这个功能，但实际上用户隐藏了对“不同颜色”的需求。很多时候看上去我们是一直在设计不同的程序，实际上对于真正多变的需求，我们并没有做到组合设计，只是通过不断地修改代码来掩饰烂设计罢了。要想做到组合设计，Unix 哲学给我们提供了两个解决思路： 解耦。这是 Unix 哲学最核心的原则。代码与代码之间的依赖关系越多，程序就越复杂，只有将大程序拆分成小程序，才能让人容易理解它们彼此之间的关系。也就是常说的在设计时应尽量分离接口与实现，程序间应该耦合在某个规范与标准上，而不是耦合在具体代码实现逻辑上； 模块化。模块化还有更深层的含义——可替换的一致性。什么叫可替换的一致性？比如，如果想使用 Java RPC 协议，可以选择 Dubbo、gRPC 等框架，RPC 协议的本质是一样的，就是远程过程调用，但是实现的组件框架却可以不同，对于使用者来说，只要是支持 Java RPC 协议的框架就行，可随意替换，这是可替换。而不同的框架需要实现同一个功能（远程过程调用）来保持功能的一致性（Dubbo 和 gRPC 的功能是一致的），这是一致性。实际上，这两个解决思路就是现在常说的高内聚、低耦合原则：模块内部尽量聚合以保持功能的一致性，模块外部尽量通过标准去耦合。换句话说，就是提供机制而不是策略，就像上传文件那个例子里，分析时应该找出用户隐含的颜色变化的需求，并抽象出一个可以自定义颜色的功能模块，解耦上传文件模块，最后将颜色变化模块组合到上传文件模块来对外提供使用。这样当用户提出修改颜色时（修改策略），只需要修改自定义颜色模块就行了，而不是连同上传文件的机制也一起修改。启示三：重拾数据思维，重构优化程序设计现如今，各种编程模式、原则、方法、技巧满天飞，我认为都抵不过一条对“数据是什么”的彻底理解。比如，搞清楚如何组织数据结构、如何产生数据、如何处理数据、如何存储数据、如何展示数据、如何删除数据……因为再高大上的架构设计，如果系统对数据的组织是混乱的，那么可以轻松预见随着系统的演进，系统必然会变得越来越臃肿和不可控。如今，在互联网应用中，绝大多数应用程序都是数据密集型的，而非计算密集型。在数据密集型应用中，处理数据是主要挑战，而使用 CPU 计算不是瓶颈，更大的问题来自数据量、数据复杂性以及数据的变更速度。NoSQL、大数据、分布式缓存、最终一致性、ACID、CAP、云计算、MapReduce、实时流处理等。其实，它们无一不是在应对数据带来的挑战。比如，一个购物订单里包含了用户收货地址、支付信息、商品信息、下单时期、物流状态等数据，订单系统需要处理这些数据，并反馈结果给用户，这样来看，一个 Web 请求其实就变成了一次数据流的处理。Unix 哲学在出现之初便提出了“数据驱动编程”这样一个重要的编程理念。也就是说，在 Unix 的理念中，编程中重要的是数据结构，而不是算法。当数据结构发生变化时，通常需要对应用程序代码进行修改，比如，添加新数据库字段、修改程序读写字段等。但在大多数应用程序中，代码变更并不是立即完成的。原因有如下： 对于服务端应用程序，可能需要执行增量升级，将新版本部署到灰度环境，检查新版本是否正常运行，然后再完成所有节点部署； 对于客户端应用程序，升不升级就要看用户的心情了，有些用户可能相当长一段时间里都不会去升级软件。这就意味着新旧版本的代码以及新旧数据格式可能会在系统中同时共存。这时，处理好数据的兼容性就变得非常重要了。如果不具备数据思维，很可能会假设数据格式的变更不会影响代码变更。而 Unix 哲学提出的“数据驱动编程”会把代码和代码作用的数据结构分开，这样在改变程序的逻辑时，就只要编辑数据结构，而不需要修改代码了。 数据驱动编程类似于事件驱动编程，可以理解为关注点的重心在数据本身的处理上，比如，过滤、转换、聚合等。就像测试驱动编程的中心是提供可测试接口一样，只是转移了关注点的中心，然后在考虑实现方法优先从重点出发。网站数据埋点栗子拿电商网站网页来说，里面一定有很多商品的链接和购买下单操作，如果是基于传统事件驱动设计，一般只会处理点击商品链接的响应事件（例如，点击操作后会展示详细商品信息）。但是，我们现在知道，除了正常的功能数据外，还有用户点击的时间数据、点击的总量数据、浏览路径数据等，而这些数据对于网站来说其实才更重要。如果具备“数据驱动编程”的思维，那么在设计商品购买链接时，就会联想到能否加入用户行为的埋点数据，在编程时会有意识地预留一些接口功能或抽象相关模块，即使对于已经完成功能开发的程序，也会在日后重构时关注数据埋点的变化。可以这么说，最初的“数据驱动编程”看重的只是编写尽可能少的固定代码，然而随着时代的发展，“数据驱动编程”的思想逐渐从优化算法的设计，演化成以解决数据变化为中心的设计方法。总结Unix 设计哲学强调构建简单、清晰、模块化可扩展和数据驱动的代码。只有这样，代码才可以由其创建者以外的开发人员轻松维护和重新利用。Unix 早期创新性提出要进行模块化设计思想改变，这个看似简单的改变，经过好长时间的演化后才成为今天的通用准则。所以，现在会理所应当地认为写代码就应该模块化、尽量可重用。在软件开发早期，那时的程序员并不这么认为，模块化在当时也并不很受欢迎，因为那时的计算机硬件很昂贵，在一个系统里集成更多的模块才是最佳选择。在我看来，Unix 哲学传达的不只是一种不断在编程中实践验证的理念，还是一种敢于挑战权威不断创新的思考方法。总体来说，Unix 哲学对今天软件编程依然很有意义，它不只是总结了几个原则，还发展了一套完整思想体系，影响深远。" }, { "title": "单一原则：跳出错误抽象误区", "url": "/posts/single-principle/", "categories": "Design Pattern", "tags": "单一原则", "date": "2022-04-08 03:30:20 +0000", "snippet": "在软件开发中，曾学习过一些经典的设计原则，其中包括： 面向对象原则（SOLID）； 简单原则（KISS）； 单一原则（DRY）； 最少原则（LoD）； 分离原则（SoC）等。分开来看这些原则，会觉得它们都很有道理，似乎只要照着用就能提升编程能力和代码质量，但真正到编码时，却会出现选择困难的矛盾感觉。原因在于，有的原则之间是相互冲突的，而有的原则之间又是彼此重复的。 虽然设计原则是用来指导编程实践的，但是不限定使用范围的话，在使用时会很容易误用或滥用，进而导致更多问题出现。就像： KISS 原则与 YAGNI 原则是相似的，都是在讲如何保持简单性； KISS 原则与 SOILD 原则是冲突的，因为 SOILD 原则为了保持面向对象的优势会增加代码复杂性。于是，如果想要在面向对象编程语言中使用 KISS 原则来保持简单设计时，势必需要面对封装、多态、继承这些复杂特性的取舍，自然会产生一种矛盾感。好的设计必然是简单的，但是如何充分利用语言复杂的特性发挥出简单设计的威力，才是学习设计原则真正的意义所在。DRY 原则DRY（Don’t Repeat Yourself），翻译成中文就是，不要重复你自己。这个原则最早出现在经典著作《程序员修炼之道》里，定义是这样的：系统的每一个功能都应该有唯一的实现，如果多次遇到同样的问题，就应该抽象出一个共同的解决方法，而不要重复开发同样的功能代码。原理简单清晰，大家几乎都能接受这个原则，在实际编码中也会经常使用。比如，一个 Web 后台管理系统多个页面都需要查询人员的基本信息数据，于是就可以抽象出一个通用的人员查询功能模块来解决这个问题。使用 DRY 原则的陷阱根据多年经验，在使用 DRY 原则指导编程时，会很容易陷入一些思维陷阱当中，比如： 陷阱一：随时关心代码重用性； 陷阱二：过度设计； 陷阱三：写一次性代码。陷阱一：随时关心代码重用性作为程序员，在日常工作中，会经常涉及不同功能代码开发任务，有的功能之间会有一些共性，有的代码会有一些重复，于是很容易想到 DRY 原则，毕竟没人愿意做大量重复的事。可能会这样做： 随时在意有没有写重复代码； 随时在意代码能不能重用； 随时在意有没有因为重复而浪费时间。不过一段时间后，会发现，虽然以上做法能够让代码变得更清晰，也能在一定程度上提升代码重用性，但是有的抽象代码通用性没有预想中那么高，甚至很可能项目短期内就结束了，后面的项目压根就没有再使用过这些有“重用性”的代码。这是经常会遇见的问题，其实问题本质就在于，可能忽略对代码是否真的需要重用的思考。代码重复通常有以下三种类型： 功能需求重复，比如，A 团队和 B 团队都做了在线文档管理功能、QQ 和微信的聊天通信功能等； 实现逻辑重复，比如，同一个文件上传功能，A 同学使用 Spring 框架实现，B 同学使用 JDK 原生功能实现； 执行调用重复，比如，登录页面查询用户信息前调用用户密码校验，查询用户信息时也调用用户密码校验。看到重复，你是不是立即就想要优化了？比如，你是 A 团队的成员，你就想要说服 B 团队放弃维护在线文档功能；你是 B 同学，你认为 A 同学只是简单封装，存在扩展性不好等各种问题。很容易过度关注重复，而忘记思考代码重复到底是不是真的就是问题。那该如何避坑呢？简单六个字：先可用，再重用。有时你的代码之所以会重复，可能是因为还没有找到正确的抽象，这时你就不应该强求对代码进行抽象。传统理论认为，代码重复三次以上就应该抽象。这是错误的！代码重复三次甚至更多次和你找到正确的抽象没有必然联系。也就说是，不要被编程一时的熟悉感所欺骗，应该实事求是地写完当前代码，再对比代码是不是真的重复。简单来说，应该先写出可以运行的代码，再考虑是否需要重用代码。如果还没有找到抽象的话，也没有关系，因为等到有更多的上下文时，还可以重构它。所以说，要想不重复，需要先不再随时关心代码重用性，保留适当的重复，等到真的重复时，再去抽象可复用的公共代码。陷阱二：过度设计近年来，随着面向对象思想逐渐流行，在软件开发中，我们越来越重视代码的可扩展性、可维护性。于是开发团队越来越多地开始强调灵活的代码设计，目的就是应对未来更多的需求变化。但实际上，根据多年的经验和观察，我们很多时候都恰恰因为太重视灵活性和复用性，反而导致在做代码设计时更容易出现过度设计的问题。比如，用户提出了 A 需求，设计者分析出了 B 和 C 的系统需求来支持 A，但在编码实现 B 的过程中，发现可以抽象出更好的通用功能 D、E、F 来支持 B 和 C，进而去开发 D、E、F。于是精力开始变得分散，原本只需要开发出 B 和 C，现在却又多出了D、E 和 F。之所以会出现这种情况，是因为我们总是期望通过现在的灵活设计来避免未来需求变化后的重复设计与编码。这样做的话，确实特别符合 DRY 原则的理念。但实际上，需求的变化方向是不可预测的。一旦我们投入了过多的精力到灵活设计上，势必会影响本应该完成的需求。同时，过多的功能会引入更多潜在的问题，而修复问题也会耗费我们的时间和精力。那么该如何避坑呢？一个方法：抓住上下文，适度设计。在代码设计的过程中，你应该遵循有限范围的原则，也就是我们常说的抓住上下文。比如，你需要开发一个内部的数据管理后台的权限管理功能，那么你要抓住的上下文就是数据是否敏感、使用人员的大致范围和人数、功能交付截止日期、团队现在使用的类库和框架、有没有采用基本的权限认证等，然后才开始进行设计与代码实现。当搞清楚了基本的上下文后，才能开始适度设计并编码。“适度”虽然没有一个统一的标准，但我给你推荐一个简单实用的方法：当你想要扩展通用设计时，想想一年后这个项目是不是还存在。这个方法虽然很老套但是特别有效，因为在过去的几年里，我用这个方法毙掉了我自己很多过度的设计。当然，有一些很基础的通用工具方法，还是可以抽象提炼出来复用的，不过依然可以用这个方法来检查一下，你会发现有很多功能可能只是用了一次。陷阱三：写一次性代码如果教条式地理解 DRY 原则，很容易走入一种极端的应用场景：为了不重复而不重复，俗称写一次性代码。实际上，硬编码和复制粘贴编程这两种经典的编码方式就是这种场景的具体表现。 硬编码，指的是将一些配置数据或通用信息写入代码中，导致信息一旦发生变动，就不得不修改代码来满足要求。比如说，将邮件发送程序中的用户名、密码和邮箱地址写入代码中，当密码发生变化时，维护代码的人就需要修改代码，重新编译打包。 复制粘贴编程，是指通过将他人已经实现的代码复制到自己的代码中实现同样的功能。这是现在最常用的编程方式之一，不仅编程新手喜欢用，很多编程高手也喜欢用，理由就是不重复造轮子。你可能已经发现，写一次性代码非常符合 DRY 原则的理念，因为这样做不仅避免了重复（每次都能写新的代码），而且还能在短期内提高工作效率（因为及时完成了任务）。但是在我看来，这只是一种表面的不重复，写一次性代码不仅会导致频繁修改引入更多 Bug，还会导致架构无法及时演进累积更多的技术债，最终导致系统的庞大臃肿而难以理解。那该如何避这个坑呢？坚持写易懂的代码。易懂的代码？在我看来，易懂的代码符合以下几个特点： 代码逻辑清晰； 充分利用语言特性； 遵从一定的编码规范； 实现完整需求功能。第一，易懂的代码不是指容易、简单的代码。简单的代码往往更难理解，恰恰因为代码量更少，学习成本反而更高。比如，下面这段 shell 命令：:(){:|: &amp;amp;};:最初我看到时非常困惑，后来查资料才知道这一段代码的真实含义：无终止的条件递归。如果执行此操作，它将快速复制自己，从而消耗完所有的内存和 CPU 资源。由此可见，代码很简单，但是一点也不易懂。第二，易懂的代码能借用语言特性来发挥优势。 比如，在 Java 中利用驼峰大小写来区分不同方法命名含义，或利用熟悉的 get、set、insert、update 等习惯命名，或使用 @Service 等特定注解来标识服务等。第三，易懂的代码需要遵从一定的代码规范。 比如，接口定义加注释，MySQL 数据库中使用下划线来区隔字段名并备注含义，使用枚举定义状态值，if-else 的嵌套最多三层等。第四，易懂的代码要能正确运行。 千万不要为了易懂而写大量的说明文档和注释，但却忘记了代码的正确运行逻辑。比如，实现接口时写了注释却忘了实现代码逻辑（只返回 null），导致运行后一直拿不到结果；当函数方法的入参类型或位置发生变化后没有修改说明，反而误导维护人员输入错误的数据，导致结果出错。第五，始终牢记：易懂的代码不是告诉计算机怎么做，而是告诉另一个程序员你想要计算机做什么的意图。简单来说，编写代码是要给其他人阅读的，易于理解的代码才是易于维护的代码。如果你的代码连自己都不能快速厘清思路，那么别人大概率也读不懂你的代码。总结应用 DRY 原则最重要的是，要搞清楚代码重复是不是就一定不好，而不能以是否违反了 DRY 原则为判断的依据。因为一旦以是否违反原则为标准，就会掉进上面所说的思维陷阱中。宁可重复，也不要错误的抽象。不要为了抽象而创建抽象。很多时候，我们容易将 DRY 原则理解为编程时不应该有重复代码，但其实这不是它的真实意图。在我看来，DRY 原则需要放到具体的上下文环境中去使用。比如，在技术选型时，可以用它来帮助我们看透组件复用的本质，还可以在功能实现时，用来减少各种新奇想法的冲突，而不是仅仅纠结于代码是否重复。DRY 虽然是一个最简单的法则，也是最容易被理解的，但它也可能是最难被应用的。本来 DRY 原则的初衷是帮助我们提升代码的可重用性，结果很多时候我们却为了不重复而引入更多新的问题。因此，保持对原则的警醒比应用了多少原则更重要。" }, { "title": "表达原则：让源代码成为一种逻辑线索", "url": "/posts/pie/", "categories": "Design Pattern", "tags": "表达原则", "date": "2022-04-07 14:30:20 +0000", "snippet": "维护代码是我们程序员非常重要的日常工作之一，每当遇见以下问题都会非常令人头疼： 接手维护项目，却发现文档缺失、代码无注释，加上维护人离职，基本只能靠猜来梳理代码逻辑； 代码风格过于抽象（命名过短、魔鬼数字、重名方法等），看不懂，也不敢轻易修改； 运行代码出现故障时，不打日志，不抛异常，导致定位问题需要耗费很长时间； 大段的if-else代码嵌套组合，调用逻辑复杂冗长，扩展性差，重构优化费时、费力。很明显，造成这些问题的原因都是代码可读性差，没能很好地串联起代码内在逻辑。可读性差的代码难以理解，不仅会造成诸多误解和麻烦，还会导致项目交付效率变低。虽然代码可读性是一种主观定义，但是它确实能保证研发人员快速准确理解代码真实含义。为了提高代码可读性，应该掌握快速提升代码可读性的原则：表达原则。提升源代码可读性的好处提升源代码可读性，主要会带来四大好处： 更易于维护。代码写好后，需要调试、运行以及修复 Bug。设计文档、需求文档和口头交流只能表达部分业务逻辑的意图，而代码则能直接反应出编程实现业务逻辑时的全部真实意图。可读性高的代码，能让阅读者在阅读时快速理解编写者的意图，即便逻辑复杂，也能在修改时准确地分析和理解，大大节省维护和修改代码的时间； 更易于重构。现在很多项目之所以难以重构，就是因为代码可读性太差。当无法理解一段代码时，就会跳过它，而整个系统都难以理解的话，可能就会选择重写而不是重构，因为重构必然会修改原有代码，这会引入一定的风险，一旦因为重构而导致故障，那么维护的人就要担责。所以说，可读性的高低在某种程度上决定了你重构意愿的大小； 更易于测试。代码在修改时需要反复调试，如果代码可读性很差，很多时候就需要写一些额外的 Mock 或测试接口来对原有代码进行测试，不仅浪费时间，还容易造成误读。可读性高的代码，参数与输出都更清晰，在测试时能更精准地找到对应逻辑和问题点； 更易于应用设计模式。设计模式除了在设计之初被使用外，其实更多时候都是在代码重构过程中被使用。在工作中，你会发现有的代码虽然写了很多嵌套的if-else，但命名和注释都写得很好，逻辑也很易读，在重构时就能通过设计模式很好地去优化。而有的代码虽然看上去很简洁，但使用了很多高级技巧或缩写命名，理解起来非常费时、费力，对于维护人员来说，自然不愿意考虑使用设计模式。总体来说，提升代码的可读性能够帮助我们更好地理解代码，只有理解了代码才能更好地维护代码，而本质上代码就是用来维护的——不断修改与发布；另外，还有一个重要的用处是，能帮助阅读代码的人快速找到代码的实现逻辑。表达原则：凸显代码内在逻辑虽说编写文档能够表达软件开发意图，但事实上，大多数人可能很讨厌写文档，这是因为大部分文档都与代码没有直接关系，并且随着代码的不断调试与修改，文档会变得越来越难以与最新的真实情况同步。另外，可能也没有太多时间阅读文档，需求上线、Bug 修复、多项目并发是现在程序员的日常现状。因为时间紧、任务重，可能只能边改代码边学习，这时一份逻辑清晰的代码才是你真正需要的。不过，很多时候你可能并不知道该怎么在代码中表达自己的意图，其实，有一个原则可以帮到你，那就是表达原则。表达原则（Program Intently and Expressively，简称 PIE），起源于敏捷编程，是指编程时应该有清晰的编程意图，并通过代码明确地表达出来。简单来说，表达原则的核心思想就是：代码即文档。也就是说，写代码时要像写设计文档一样帮助阅读者理解你想要表达的意图，要从程序设计者的角度跳出来，站在使用者的角度来写代码。可以换个角度想想，假如你是代码使用者，你希望看到什么样的代码？很明显，没有人想要看到这样的代码（来自网络的一段烂代码）：cName = InpList.get(0).replace(&quot;,&quot;, &quot;.&quot;);cCode = InpList.get(1).replace(&quot;,&quot;, &quot;.&quot;);cAlpha2 = InpList.get(2).replace(&quot;,&quot;, &quot;.&quot;);cAbreviation = InpList.get(3).replace(&quot;,&quot;, &quot;.&quot;);dYear = InpList.get(4).replace(&quot;,&quot;, &quot;.&quot;);dPoliticalCompatibility = InpList.get(5).replace(&quot;,&quot;, &quot;.&quot;);dRankPoliticalCompatibility = InpList.get(6).replace(&quot;,&quot;, &quot;.&quot;);dEconomicCompatibility = InpList.get(7).replace(&quot;,&quot;, &quot;.&quot;);dRankEconomicCompatibility = InpList.get(8).replace(&quot;,&quot;, &quot;.&quot;);dMilitaryCompatibility = InpList.get(9).replace(&quot;,&quot;, &quot;.&quot;);dRankMilitaryCompatibility = InpList.get(10).replace(&quot;,&quot;, &quot;.&quot;);dDemoScore = InpList.get(11).replace(&quot;,&quot;, &quot;.&quot;);dRankDemoScore = InpList.get(12).replace(&quot;,&quot;, &quot;.&quot;);dEnvironmentalCompatibility = InpList.get(13).replace(&quot;,&quot;, &quot;.&quot;);dRankEnvironmentalCompatibility = InpList.get(14).replace(&quot;,&quot;, &quot;.&quot;);dSumCompatibility = InpList.get(15).replace(&quot;,&quot;, &quot;.&quot;);dPoliticalUtility = InpList.get(17).replace(&quot;,&quot;, &quot;.&quot;);dRankPoliticalUtility = InpList.get(18).replace(&quot;,&quot;, &quot;.&quot;);dEconomicUtility = InpList.get(19).replace(&quot;,&quot;, &quot;.&quot;);dRankEconomicUtility = InpList.get(20).replace(&quot;,&quot;, &quot;.&quot;);dMilitaryUtility = InpList.get(21).replace(&quot;,&quot;, &quot;.&quot;);dRankMilitaryUtility = InpList.get(22).replace(&quot;,&quot;, &quot;.&quot;);dEnvironmentalUtility = InpList.get(23).replace(&quot;,&quot;, &quot;.&quot;);dRankEnvironmentalUtility = InpList.get(24).replace(&quot;,&quot;, &quot;.&quot;);dSumUtility = InpList.get(25).replace(&quot;,&quot;, &quot;.&quot;);dRankUtility = InpList.get(26).replace(&quot;,&quot;, &quot;.&quot;);dPoliticalScore = InpList.get(27).replace(&quot;,&quot;, &quot;.&quot;);dRankPoliticalScore = InpList.get(28).replace(&quot;,&quot;, &quot;.&quot;);dEconomicScore = InpList.get(29).replace(&quot;,&quot;, &quot;.&quot;);dRankEconomicScore = InpList.get(30).replace(&quot;,&quot;, &quot;.&quot;);dMilitaryScore = InpList.get(31).replace(&quot;,&quot;, &quot;.&quot;);dRankMilitaryScore = InpList.get(32).replace(&quot;,&quot;, &quot;.&quot;);dEnvironmentalScore = InpList.get(33).replace(&quot;,&quot;, &quot;.&quot;);dRankEnvironmentalScore = InpList.get(34).replace(&quot;,&quot;, &quot;.&quot;);dAggregate = InpList.get(35).replace(&quot;,&quot;, &quot;.&quot;);dRankAggregate = InpList.get(36).replace(&quot;,&quot;, &quot;.&quot;);而是，希望看到这样的代码（HttpClient 的某个代码片段）：/** * {@inheritDoc} */@Overridepublic CloseableHttpResponse execute( final HttpUriRequest request, final HttpContext context) throws IOException, ClientProtocolException { Args.notNull(request, &quot;HTTP request&quot;); return doExecute(determineTarget(request), request, context);}private static HttpHost determineTarget(final HttpUriRequest request) throws ClientProtocolException { // A null target may be acceptable if there is a default target. // Otherwise, the null target is detected in the director. HttpHost target = null; final URI requestURI = request.getURI(); if (requestURI.isAbsolute()) { target = URIUtils.extractHost(requestURI); if (target == null) { throw new ClientProtocolException(&quot;URI does not specify a valid host name: &quot; + requestURI); } } return target;}所以说，在开发代码时，应该更注重代码表达意图是否清晰，考虑使用一些方法和技巧，虽然会耗费一点时间，但是从整体来看，会节省很多沟通与解释时间，做到在真正的提升编码效率。写出有“逻辑线索”源代码要想写出可读性高的代码，可以从三个方面来入手。 代码表现形式：在命名（变量名、方法名、类名）、代码格式、注释等方面的改进。 控制流和逻辑：尽量分离控制流和逻辑，让代码变得更容易理解。 惯性思维：找出常犯的一些惯性思考方式并逐一改进。优化代码表现形式命名在编程中至关重要，无论是变量名、类名还是方法名，好的名字能快速准确地传达要表达的含义，而缩写、自定义名称会让代码变得难以理解。先来看一段代码：public class T { private Set&amp;lt;String&amp;gt; pns = new HashSet(); private int s = 0; private Boolean f(String n) {return pns.contains(n);} int getS() {return s;} int s(List&amp;lt;T&amp;gt; ts, String n) { for (T t :ts) if (t.f(n)) return t.getS(); return 0; } }这段代码到底实现了什么功能？估计没有人能回答出来。如果编写者不是我，我肯定也无法理解这段代码（即使是我，过一段时间后，再看这段代码也不一定知道写的是什么）。光凭看代码，几乎是无法理解这段代码的真实含义到底是什么的。实际上，这个类是获取球队比赛得分的，除了通过球队直接获得比赛得分外，还可以通过球队里的某个球员来查找对应得分，具体修改如下：/** * 获取球队比赛得分 */public class Team { // 保证名字不重复 private Set&amp;lt;String&amp;gt; playerNames = new HashSet&amp;lt;&amp;gt;(); // 默认为零 private int score = 0; /** * 判断是否包含球员 * @param playerName 球员名字 * @return */ private Boolean containsPlayer(String playerName) { return playerNames.contains(playerName); } /** * 知道队伍，直接获取分数 * @return 返回队伍分数值 */ public int getScore() { return score; } /** * 通过队员名字查找所属队伍分数 * @param teams 支持多个队伍 * @param playerName 队员名字 * @return 兜底为 0 分，不出现负分 */ int s(List&amp;lt;Team&amp;gt; teams, String playerName) { for (Team team: teams) { if (team.containsPlayer(playerName)) { return team.getScore(); } } return 0; }}从优化后的代码中，你就能直观地看到，“命名的优化加上注释的说明”一下子就让源代码的逻辑变得清晰起来，即便你没有学过编程，也能大致了解这段代码的逻辑和作用。改进控制流和逻辑还是直接从一个例子开始，具体代码如下：public List&amp;lt;User&amp;gt; getUsers(int id) { List&amp;lt;User&amp;gt; result = new ArrayList&amp;lt;&amp;gt;(); User user = getUserById(id); if (null != user) { Manager manager = user.getManager(); if (null != manager) { List&amp;lt;User&amp;gt; users = manager.getUsers(); if (null != users &amp;amp;&amp;amp; users.size() &amp;gt; 0) { for (User user1 : users) { if (user1.getAge() &amp;gt;= 35 &amp;amp;&amp;amp; &quot;MALE&quot;.equals(user1.getSex())) { result.add(user1); } } } else { System.out.println(&quot;获取员工列表失败&quot;); } } else { System.out.println(&quot;获取领导信息失败&quot;); } } else { System.out.println(&quot;获取员工信息失败&quot;); } return result;}这段代码的含义是：想要通过 id 来查询员工的信息，如果 id 找不到，就查询员工的领导，然后通过他领导下的员工信息来寻找，这时还需要判断员工年龄大于 35 岁且为男性。这是最常使用的逻辑实现方式，俗称箭头型代码，但是随着判断条件逐渐增多，嵌套就会增多。代码逻辑越多，你就越容易搞不清楚逻辑是什么，因为看到最内层的代码时，你已经忘记前面每一层的条件判断是什么了。那么，这样的代码应该如何优化呢？其实很简单，就是改变控制流，先判断会出现失败的条件，一旦出现优先推出。优化后的代码如下：public List&amp;lt;User&amp;gt; getStudents(int uid) { List&amp;lt;User&amp;gt; result = new ArrayList&amp;lt;&amp;gt;(); User user = getUserByUid(uid); if (null == user) { System.out.println(&quot;获取员工信息失败&quot;); return result; } Manager manager = user.getManager(); if (null == manager) { System.out.println(&quot;获取领导信息失败&quot;); return result; } List&amp;lt;User&amp;gt; users = manager.getUsers(); if (null == users || users.size() == 0) { System.out.println(&quot;获取员工列表失败&quot;); return result; } for (User user1 : users) { if (user1.getAge() &amp;gt; 35 &amp;amp;&amp;amp; &quot;MALE&quot;.equals(user1.getSex())) { result.add(user1); } } return result;}现在，代码逻辑是不是很清晰了？虽然这个快速失败方法很简单，但是非常有效。实际上，快速失败就是 KISS 原则一个很好的实践，这样能保证条件判断的逻辑简单清晰。只要 if 的嵌套超过三层，你就可以应用这个原则来改进控制流，让逻辑更清晰易懂。避免惯性思维除了改进表层和逻辑外，更应该尽量避免设计代码时的一些惯性思维，这里我总结出了“五个避免”： 要避免写一次性代码。一次性编码最大坏处在于，一旦需要修改，多处就得跟着修改，而多次修改又可能会出现遗漏的风险。一次性代码在越来越多的软件代码中出现，一个本质的原因就是多人协作开发的情况越来越多。由于编程是一件非标准化的事情，不同程序员可能对同一个逻辑的理解完全不同，而一旦每个人都只从自己的角度出发写一次性代码，那么同一个系统里的代码很快就会变得冗余与混乱； 要避免复制粘贴代码： 一方面，不同人的编码风格可能会有所不同，这会给阅读者在理解上造成一定的认知负担（需要来回切换判断标准）； 另一方面，还会带来未知 Bug 的风险。复制过来的代码，更多是关注输入和输出，一旦代码正常运行后，很少会去关注代码内部逻辑，直到出现问题后，再想去梳理逻辑反而变得更加困难（因为不知道详细实现逻辑）； 避免写超长代码。超长代码带来的最大问题是：在阅读代码时，函数方法之间跳转过多，思维很容易发生混乱，尤其对于一些命名相同但参数不同的方法，很容易出现修改错误的情况。 从编写者角度来看，写超长代码，可能是觉得在一个文件里维护代码比较方便； 对于阅读者来说，他可能并不知道你是如何对代码进行职责划分的，更多时候他都会以为一个类里都是一个职责，但实际上一旦出现多个职责，加上逻辑跳转很多，阅读者基本上是会放弃阅读的； 避免过度简化命名和表达式。在开发任务重的时候，通常会选用一些简化命名的方法，比如，num1、num2、num3 这类变量命名形式。虽然在写代码的时候，会记得这些变量含义，但是过一段时间后，如果没有注释或说明，几乎是不可能直接通过名字知道它们的作用的，还得借助上下文代码，这样不仅费时，而且还可能会出现理解错误的情况； 避免写“是什么”的注释。 代码的命名和结构如果能直接反映出来“是什么”的话，就不应该用注释去表达，因为看代码一眼就能明白，比如，获取用户信息的方法名——get 和 getFromUserInfo。 应该多写“为什么”的注释，比如，为什么要多加一个适配方法，原因可能是线上 xxx 问题引起，或临时修复的Bug，后续可能随 xxx 接口调整而废弃，等等。 在很多优秀开源框架中，都能看到作者会在 interface 接口上写很多“为什么”说明，就是为了快速抓住代码逻辑线索。 另外，写“为什么”注释还有一个好处，尤其是在早期快速迭代过程中，能给后来的维护者提供一个优化的切入点，不至于交接代码后让维护代码的人看不懂、不敢动。 总结表达原则核心思想：通过代码清晰地表达真实意图。虽然软件开发过程中会有诸多文档，比如，架构设计文档、流程文档、PRD 文档等，但这些文档并不足以帮助正确理解代码是如何运行和组织的，大多数时候只能通过阅读代码来掌握软件的真实运行情况。之所以应该把提高代码可读性作为第一要务，就是因为读代码的次数远比写代码的次数多，包括正在写的代码也是如此。三种主要改进办法： 表层改进，从命名、表达式、变量和注释入手去提升代码可读性； 改进代码逻辑，比如用策略模式替换嵌套过多的if-else 代码； 改进思维习惯，习惯的改善更多是需要日积月累的坚持。" }, { "title": "对象思维：面向对象编程的优势", "url": "/posts/object-thinking/", "categories": "Design Pattern", "tags": "对象思维", "date": "2022-04-05 03:30:20 +0000", "snippet": "一说到“面向对象编程”似乎感觉就是编程的全部，实际上它是 20 世纪 60 年代就已经出现的一门“古老”技术，在 2000 年以后，随着 Java 和 .NET 等编程语言的出现，才逐渐开始在企业软件开发中发挥重要作用。之所以后来面向对象编程越来越重要, 是因为面向对象编程是一门可以轻松编写高质量软件的综合技术。现在软件的复杂性已经从过去的底层复杂性（操作系统、编译器）转移到了更高的抽象层面（应用程序）： 底层操作系统和基础设施技术趋于稳定。现代软件重视可重用性、可维护性更胜于效率（性能），但并不是说性能不重要，而是因为基础设施，例如，操作系统、网络等变得越来越稳定，相反，上层软件的功能扩展需求则变得越来越多； 另一方面，人们期望通过理解高级抽象来快速认识计算机系统。对于一个用户来说，App 能提供什么样的功能体验比它到底是被如何被设计开发出来的更重要，但是矛盾也由此产生，没有运行良好的 App 程序，就无法提供体验更丰富的功能。实际上，面向对象技术的出现就是为了解决软件的大规模可扩展性问题。编程语言 VS 编程范式思考一个问题：现在 Java 8 以上已经提供了 Lambda 表达式进行函数式的编程，同时也能进行面向对象的编程，那么 Java 是属于什么类型的编程语言？可以使用哪些编程范式？（注意到这两个问题所用的措辞）什么是编程语言**编程语言，是一种标准化的通信方式，用来向计算机发出指令。 **比如，C 语言的 Hello World 代码示例：#include &amp;lt;stdio.h&amp;gt;int main(){ printf(&quot;Hello, World!&quot;); return 0;}再比如，Java 语言的 Hello World 代码示例：public class SayHelloWorld { public static void main(String[] argus){ System.out.println(&quot;Hello World!&quot;); }}如果只从上面两段代码来判断，只会得出它们最终是告诉计算机要在终端设备上打印 Hello World，但还是无法判断谁是面向对象编程语言。什么编程范式编程范式是一种根据编程语言的功能对编程语言进行分类的方法，它不针对具体的某种编程语言。编程语言和编程范式的关系根据功能重视的侧重点不同，编程范式通常可分为命令式和声明式两大类：命令式再往下又可以细分为面向过程编程范式、面向对象编程范式和并发处理范式，声明式又分为逻辑编程范式、函数编程和数据库编程范式。编程范式与编程语言的关系可参考下图：由此可知，编程语言 ≠ 编程范式。 换句话说，编程语言是对编程范式的一种工具技术上的支撑，一种语言可以适用多种编程范式。什么是面向对象编程面向对象编程是一种编程范式，是基于部分特定编程语言下的编程经验总结，代表了程序员在编码时应该如何构建和执行程序的一种方法集合。这样，开头问题的答案就比较清晰了，Java 是面向对象的编程语言，可以使用面向过程或面向对象的编程范式。面向对象编程优势面向对象编程是一种编程经验的抽象总结。除此以外，与之对应的编程范式还有泛型编程、函数式编程、过程式编程、响应式编程等。相比于其他编程范式，面向对象编程在解决软件扩展性和复用性上有非常大的优势。优势一：模块化更适合团队敏捷开发模块化编程思想最早出现于 1968 年，当时被当作编程语言本身的一种扩展来使用，真正支持模块化编程语言是 1975 年才出现，叫 Modula（现在很少有人知道），而后很长一段时间里，模块化编程没有得到重视，直到 C++ 和 Java 相继出现，模块化思想才逐渐发扬光大。众所周知，现代企业软件系统规模变得越来越庞大，有时可能需要几十、几百人组成的研发团队，耗费几个月甚至几年来开发一款软件产品。从时间和投入成本来看，这样大规模软件开发是一项巨大且复杂的工程，堪比修建一栋大楼。即使没有这么大规模，想要顺利推进软件开发并最终取得成功，也是非常不易的。所以，“敏捷开发实践与面向对象编程有什么关系呢？”面向对象编程所提倡的模块化编程，在很大程度上直接解决了开发团队之间的合作问题，也就是不同团队和个人可以通过编写程序模块来进行功能交互，这让整个团队的开发效率得到了真正的提升。在过去，我认为敏捷开发推行起来困难主要有三个原因： 单体软件应用修改成本很高； 组件重用性低，不同项目需要大量重复性开发； 代码耦合性高，逻辑难以理解，故障排查与修复效率低。面向对象编程解决上面三个问题，也是从三个方面： 单体软件会被称为单体应用代码地狱，最重要原因在于，随着代码量不断增加，软件功能逻辑会变得异常复杂，代码越难理解，维护代码人数越多。这使得代码修改成本非常高昂，因为每修改一次代码，很有可能会影响非常多代码。而面向对象技术会尽可能解耦复杂的逻辑，即使是单体应用，也会抽象更多模块，让功能更容易被理解，这在多人协同开发中尤其重要。试想几十个人在维护同一个系统代码时，看见功能划分清晰的模块，和看见一团糟代码的模块，心情一定是截然不同的； 如果没有组件化，在软件开发时会带来一个问题：很多基础功能都需要重新实现，比如，网络通信、操作 IO 等。因为没有重用组件，每一个软件项目都得自行开发实现，势必会造成巨大人力和时间浪费。现在，所熟知的类库、框架，最早起源于面向对象组件重用的思想，目的是提高多人协作的编程效率。也正是因为有了更多可复用组件，我们才不用从汇编语言开始编写，而是直接基于底层组件扩展更丰富的上层功能； 软件运行时，势必会发生故障。而发生故障不可怕，可怕的是发生故障以后很久都无法恢复。早年我曾亲身经历过为定位一个问题，排查一个超过 8 万行代码的模块时那种地狱般的场景，几十个开发和测试人员在一个办公室里轮班好几个通宵定位和测试问题，虽然最终找到了问题并修复，但是却错过了给客户演示的最佳时机，项目以失败告终。后来在复盘会上，所有人都提出应该划分更清晰的模块以及写更易读的代码来提升可维护性。 而在面向对象编程中，可以通过给对象分配职责来划分不同模块的功能，让各个模块的功能更聚焦在一个关注点上，这样当代码发生修改时，影响的范围几乎能很快被定位到。优势二：对象结构更能提升代码重用性、可读性面向对象有三大特性：封装、多态和继承。这个结构是之前很多编程语言所没有的，而且它解决了结构化语言面临的两大难题： 全局变量问题。全局变量是指程序中任意地方都能访问的变量，其最大问题是一处修改所有引用的地方都需要修改。结构化语言中通过局部变量和值传递结构，来避免使用全局变量。但是，局部变量是临时变量，在调用结束后就会消失，这会造成当需要在一段程序运行完成后，继续使用此信息时，则需要将值保存为全局变量； 可重用性范围问题。结构化语言中可重用范围是代码中的子代码（比如某个函数方法），如果离开当前代码，那么子代码就无法被再次使用。面向对象结构很好地避免了以上两种问题，类结构将相关的子程序（或函数）和全局变量汇总在一起，创建高一层级的软件组件；多态和继承则去除了冗余的公用子程序，让公用程序更好地被重用。这就让代码从复杂变得更简单、易懂，增加了代码的可读性。评价代码质量的好坏通常有三个维度：可读性、可测试性以及可维护性。其中，可读性是最重要的，只有写的代码可读性高，别人才更愿意维护你的代码；如果可读性低，大多数人的做法要么是重构，要么是重写。而重构则意味着要承担维护这部分代码的责任，如果不是迫不得已，一般没人愿意承担未知的风险，所以实际上对于难以阅读的代码，绝大部分人都宁愿选择重写而不是重构。所以说，为了让代码变得更加可读，面向对象编程具有不可替代的优势。优势三：组合和聚合思想让代码演进更重视组件化面向对象编程除了前面提到的两个优势外，还有一个更重要的优势：它提出的对象之间的组合和聚合关系，让代码在构建过程中更容易形成通用组件。什么是组件？简单来说就是封装了一个或多个程序代码的二进制文件，比如，Java 的 jar 包。那么，什么是组合、聚合关系？ 聚合关系表示整体由部分组成，但是整体和部分不是强依赖的，整体不存在了部分还是会存在。 组合关系和聚合不同，组合中整体和部分是强依赖的，整体不存在了部分也不存在了。组合、聚合本来是描述对象之间关系的一种设计方法，含义也简单清晰，但是随着面向对象编程技术的不断发展和程序员们的不断努力，现在已经出现了大量可重用的组件群，比如，类库、框架等。正是因为有了这些优秀的组件群，才让我们在软件开发时，不用每次都从零开始。比如说，Java 为了实现跨平台兼容性，使用 C++ 开发出了 JVM 这个非常重要的组件，当在不同操作系统下构建组件时，C 语言需要在不同的平台下进行编译构建，而 Java 则不需要，只需要一次编写然后在有 JVM 的环境中就可以编译运行了。除了不用“重复造轮子”外，组合/聚合思想另外一个优势是：充分利用众人智慧。最典型的例子就是 Linux 系统。对于开发者来说，使用开源框架同样也是组合/聚合思想的体现。比如，现在越来越多开发者都会优先选择开源框架，一方面可以快速复用已有能力，另一方面可以针对自身业务场景进行定制和修改。所以说，面向对象语言虽然没有面向过程语言的代码运行效率高，但通过牺牲部分性能换取跨平台的兼容性，也带来了更多优势。总结 编程语言是一种标准化的通信方式，用来向计算机发出指令；是对编程范式的一种工具技术上的支撑，一种语言可以适用多种编程范式； 编程范式是一种根据功能对编程语言进行分类的方式，但它并不针对某种编程语言。所以，常说的面向对象编程其实是一种编程范式。 面向对象编程有三大优势：模块化、对象结构和组合/聚合思想。它们的核心理念都是在提升代码的可扩展性、可重用性和可维护性。80% 的时间里代码都是在被阅读的，如果一段代码很难阅读，那么维护人员修复起来就会非常耗时耗力，而且难读的代码扩展性也非常差，任何的新增功能都有可能引入更多未知的问题。比如，我曾写过大量的一次性硬代码和过程式编码，从功能完成角度来看，没有任何问题，但是从扩展性和维护性的角度来看，真的非常难以维护，也给很多人带来过困扰。意识到这样的问题后，重新学习了面向对象编程，并不断地在新的项目中重新试着去写可读、可维护、可重用的代码。虽然刚开始代码量会比写硬编码时更多，直觉上也会很难受，但随着时间的推移，它的优势慢慢发挥出来后，项目就变得更易扩展和维护，周边所有同事的反馈也越来越好，这时我才明白：要想发挥面向对象编程的优势，遵循正确的方法很重要。" }, { "title": "分层思维：代码分层架构的必要性", "url": "/posts/layer/", "categories": "Design Pattern", "tags": "分层思维", "date": "2022-04-03 03:30:20 +0000", "snippet": "软件程序通常有两个层面的需求： 功能性需求，指的是一个程序能为用户做些什么，比如，文件上传、查询数据等； 非功能性需求，指的是除功能性需求以外的其他必要需求，比如，性能、安全性、容错与恢复、本地化、国际化等。事实上，非功能性需求所构建起来的正是软件架构。什么是软件架构？简单来说，就是软件的基本结构，包括三要素： 代码； 代码之间的关系； 两者各自的属性。如果把软件比作一座高楼，那么软件架构就是那个钢筋混凝土的框架，代码就是那个框架里的砖石，正是因为有了那个框架，才能让每一个代码都能很好地运行起来。由此可见，软件架构的重要性。其中，最为经典的软件架构就是分层架构，也就是将软件系统进行分层，现在几乎已经成为每个程序员最熟悉的思考模式之一。不过，分层架构越是流行，我们的设计就越容易僵化！！！代码分层架构要彻底理解代码分层架构，就得从软件部署分层架构说起。首先，先看一下常见的互联网软件部署分层架构，如下图所示：由上图可以看到，软件部署分层架构主要包括以下四个核心部分： 客户端层（Client）：调用方，比如浏览器或 App。 应用服务层的网页服务器（Web Server）：实现程序的运行逻辑，并从下层获取数据，返回给上层的客户端层。 应用服务层的缓存（Cache）：加速访问存储的数据。 数据层（DB）：存储数据。由此可以，软件分层架构是通过层来隔离不同的关注点（变化相似的地方），以此来解决不同需求变化的问题，使得这种变化可以被控制在一个层里。作为软件开发者，更关心的是应用程序里的分层架构。比如，下图展示的 MVC 分层架构：明显看到，MVC 分层架构是作用于程序本身的，程序作为一个整体被发布在服务器上运行使用。由此可以，代码分层架构就是将软件“元素”（代码）按照“层”（代码关系）的方式组织起来的一种结构。分层架构核心的原则是：当请求或数据从外部传递过来后，必须是从上一层传递给下一层。如下图，一个来自 View 层的数据，必须先通过 Controller 层、Model 层后，才能最终到达数据库层。“为什么不让 View 层的请求直接到达数据库呢？”这是因为会造成新代码耦合，增加代码复杂度。比如说，View 层直接调用 Model 层的组件，当 Model 层上的组件有变化时（比如， SQL 或逻辑修改），既会影响 Controller 层组件的使用，也会影响 View 层组件的使用（可参考下面的示意图）。所以，分层的本质就是为了让相似变化在各自的层内变化，而不造成层与层之间的相互影响。代码分层的作用代码分层架构主要是为了解决两个问题： 如何快速拆解功能问题？ 如何提升代码的可扩展性？快速拆解问题在软件开发中，一个功能需求问题通常都是笼统的复杂问题，我们一般都会将这个笼统的复杂问题拆分为多个层次的子问题来解决。比如，正在编写一段“通过 HTTP 向服务器发送字符串”的代码，如下所示：//创建HTTP连接URL url = new URL(&quot;http://xxx.test.com/sayHello&quot;);HttpURLConnection connection = (HttpURLConnection) url.openConnection();connection.connect();//发送数据OutputStream os = connection.getOutputStream();os.write(&quot;Hello World!&quot;.getBytes(&quot;UTF-8&quot;));//接收响应InputStream is = connection.getInputStream();BufferedReader br = new BufferedReader(new InputStreamReader(is, &quot;UTF-8&quot;));//……br.close();is.close();os.close();//关闭连接connection.disconnect();可以将上面的代码，简单地抽象成一个流程图，如下所示：这个流程图代表了对最初始问题的分层拆分：先创建 HTTP 连接，然后向服务器发送一串字符串，最后关闭 HTTP 连接。于是，原先的“如何通过 HTTP 向服务器发送字符串”的问题就变成了三个新层次的子问题： 如何创建 HTTP 连接？ 如何发送字符串？ 如何关闭连接？首先，在思考如何创建 HTTP 连接这个问题的过程中，会发现，要想通过 HTTP 发送消息，至少得打开 HTTP 连接，建立 HTTP 会话，并使用 TCP 协议，这样才能通过网络发送数据。接着，又发现，当成功解决了这个问题后，发送字符串和关闭 HTTP 连接还有更多的问题需要解决，于是，你开始一步一步地去分解……最后的分解结果如下图所示：当所有子问题都被成功解决以后，最初通过 HTTP 向服务器发送字符串的总问题也就得以解决了。这样，在不知不觉中就通过分层将一个复杂的大问题分解为多个容易解决的子层问题。而实际上，有的子层问题已经被前人解决过了，比如，如何使用 HTTP 协议来进行网络数据的通信。也就是说，最后真正需要关注的问题其实变少了。所以说，从功能性需求角度来看，代码分层是一种拆解复杂问题的好方法。提升代码可扩展性分层架构的出现，除了解决拆分复杂问题的困境外，还解决了代码可扩展性的问题。因为真实的系统数据一直在不断增加。比如说，一个电商网站的用户访问数会从一万个并发增长到十万个并发，或者从一百万增长到一千万。过去的单体架构之所以很难承载，是因为当需要扩展服务器和数据库功能时，一处的代码修改就会影响所有的功能。分层架构可以将复杂的逻辑切分为多个层，这样大问题就变成了多个小问题，而我们可以很方便地解决每个小问题。每个小问题更容易被抽象为一个组件，当组件功能需要扩充或替换时，修改代码的影响也被有效地控制在有限的范围内，这样组件自身的复用性也就提高了。除了提高代码组件之间的复用性外，分层架构还让我们更容易做服务的横向扩展。简单来说，横向扩展就是用多台配置较低的服务器共同提供服务，也就是集群部署服务方式。比如说，将 Model 层抽取出来作为通用的数据服务部署，这样既不影响其他业务层，也能在负载增加时，快速扩展服务的承载能力。代码分层架构的优点和缺点优点优点大致可总结为如下： 只用关注整个结构中的其中某一层的具体实现； 降低层与层之间的依赖； 很容易用新的实现来替换原有层次的实现； 有利于标准化的统一； 各层逻辑方便复用。总结来说，代码分层架构设计主要为了实现责任分离、解耦、组件复用和标准制定。如果不使用分层架构的话，代码逻辑肯定会紧紧依赖在一起，修改某一处必定影响其他很多处。从软件项目的角度看，这样会造成非常严重的影响。比如，一个上传功能需要存入下载链接到数据库，如果没有分层，那么当修改存储的路径或类型时，还得修改存储数据库的业务逻辑，想想就很麻烦。另外，层与层之间进行划分后，也提高了组件之间的复用性，层本身就是一种组件形式，通过统一的接口来与外界进行交互，而不再是按照功能上的依赖来进行交互。而统一的接口是模块之间相互约定的统一标准，只要按照标准来进行代码实现，就不会因为代码改动而影响接口的使用。缺点虽然代码分层有很多好处，但不可避免地也会有一些劣势。 开发成本变高：因为不同层分别承担各自的责任，如果是高层次新增功能，则需要多个低层增加代码，这样难免会增加开发成本。 性能降低：请求数据因为经过多层代码的处理，执行时长加长，性能会有所消耗。 代码复杂度增加：因为层与层之间存在强耦合，所以对于一些组合功能的调用，则需要增加很多层之间的调用。总结软件分层架构是通过层来隔离不同的关注点（变化相似的地方），以此来解决不同需求变化的问题，使得这种变化可以被控制在一个层里。代码分层架构的核心作用有两个： 对于功能性需求，将复杂问题分解为多个容易解决的子层问题； 对于非功能性需求，可以提升代码可扩展性。总结来说，代码分层架构是一种软件架构设计方法。 从软件的功能性需求角度看，分层是为了把较大的复杂问题拆分为多个较小的问题，在分散问题风险的同时，让问题更容易被解决，也就是我们常说的解耦。 从架构（非功能性需求）角度看，分层能提升代码可扩展性，帮助开发人员在相似的变化中修改代码。其实，复杂的设计概念和简单的代码之间存在一种平衡，这就是分层架构。 代码分层架构设计的思维模型是简化思维，本质是抽象与拆解。 代码分层架构设计的目的是将复杂问题拆分为更容易解决的小问题，降低实现难度。 代码分层架构设计的原则和方法是通用方法，可以应用到其他需要分层设计的地方。所以，分层架构从来不是目的，只是让软件变得更好的其中一种思维方法而已。除了分层架构外，其他架构设计模式： C/S模式： 优点： 能充分发挥客户端PC的处理能力，很多工作可以在客户端处理后再提交给服务器，所以 CS 客户端响应速度快； 操作界面漂亮、形式多样，可以充分满足客户自身的个性化要求； C/S 结构的管理信息系统具有较强的事务处理能力，能实现复杂的业务流程； 安全性能可以很容易保证，C/S一般面向相对固定的用户群，程序更加注重流程，它可以对权限进行多层次校验，提供了更安全的存取模式，对信息安全的控制能力很强。一般高度机密的信息系统采用C/S结构适宜。 缺点： 需要专门的客户端安装程序，分布功能弱，针对点多面广且不具备网络条件的用户群体，不能够实现快速部署安装和配置； 兼容性差，对于不同的开发工具，具有较大的局限性。若采用不同工具，需要重新改写程序； 开发、维护成本较高，需要具有一定专业水准的技术人员才能完成，发生一次升级，则所有客户端的程序都需要改变； 用户群固定。由于程序需要安装才可使用，因此不适合面向一些不可知的用户，所以适用面窄，通常用于局域网中。 主备模式（Master-slave pattern） 管道模式（Pipe-filter pattern） 代理模式（Broker pattern） 点对点模式（Peer-to-peer pattern） 事件总线模式（Event-bus pattern） MVC模式（Model-view-controller pattern） 黑板模式（Blackboard pattern） 解释器模式（Interpreter pattern）" }, { "title": "高效编程", "url": "/posts/efficient-programming/", "categories": "Design Pattern", "tags": "高效编程", "date": "2022-04-03 02:03:00 +0000", "snippet": "记得刚毕业刚刚参加工作时，有过一段时间这样的感觉：虽然学会一门编程语言后会写代码了，但是有时写出的代码可能并没有想象的那么好。比如： 只要代码没有经过测试，发布上线后总是会频繁发生故障； 每次修改完代码后，程序总会出现各种意外问题； 当别人读完自己写的代码后，会说“逻辑太复杂”、“代码看不懂”、“代码不够简洁”之类的话； 每隔一段时间，再看自己写的代码，发现不仅可读性差，而且很多逻辑连自己也看不懂； 接手离职同事留下的代码，却因为文档和注释太少，花了很长时间才看懂代码中的关键逻辑； 想在代码中新增功能，却不知道该先改动哪里，不知道改动的影响范围有多大； 再后来手头上必须完成的工作任务越积越多，编程的效率变得越来越低，甚至有时还会影响到项目交付。为什么自己编程效率不高呢？经过请求前辈经验和阅读相关书，才明白这是因为我把提升编程效率等同于提升编码速度了。部门大佬告诫我：成为一名真正高效编程者，除代码写得快以外，还得学会一些其他方法和技巧。高效编程我在刚学会编程时，认为：高效编程 = 高效写代码。因而开启了疯狂写代码模式，在不停地编码与调试中，度过了一年又一年，并以此沾沾自喜。这样坚持了很多年后，却慢慢发现自己的编程效率不仅没有提高，反而越来越慢。此时我意识到，整体编程效率之所以无法提升，是因为我一直都只是关注写代码的效率，这就给我带来了三个问题： 只关心代码是否正常运行，对最终是否满足用户需求不在意； 容易陷入代码细节而忽略整体，比如，系统设计、项目进度、与他人协作等； 不太关心可测试性、可维护性，以及简洁的高质量代码该怎么写。总之，正是因为太过于关注写代码，使我忘记了很多其他重要的事。比如：项目移交后，代码是否容易被别人维护，代码是否容易测试，代码是否健壮，等等。当从更广更高的维度，来审视写代码这件事时，写代码的效率只是整体编程效率的一部分，若只是盯着这一部分去提升或优化，很大程度上会顾此失彼，最后像我一样整体效率压根没有提高过。所以说，高效编程除了需要提升细节上的编程效率外，还需要时常跳出细节思维，从整体的工作流程上去思考与改进。因此总的来说，高效编程其实是一种高效的工作流。如何高效编程高效编程具备下面五个要素：$高效编程 = 原则 \\times 工具 \\times 编码 \\times 反馈 \\times 迭代$只有合理运用这些要素，才能真正地提升高效编程能力。这种能力，来自于两个方面： 来自编程的实践积累； . 来自各种技巧的合理使用。建立原则建立原则是高效编程的第一步。因为原则可以让你在编程时，不会轻易遗忘一些重要的事情，比如，项目进度、代码质量、会议效率等等。在过去编程实践中，踩过很多坑，结合他人的经验，总结出了三条有效原则： 问题到你（自己）为止: 比如这样的情形，当发现某人负责的功能模块出现问题时，去咨询对方，对方却对你不理不睬，然后回复一句：去找 XXX，这不是我的问题； 从自己的角度来看，遇见了问题只是想要咨询与讨论，但是对方给你的感觉却像是在推卸责任； 千万记住，无论是不是你（自己）的问题，你（自己）都应该尝试去终结这个问题； 这样做不仅能规避推卸责任的思维习惯，还能提升解决问题的实力。比如，不是你（自己）的问题，你（自己）会尝试和对方一起找到这个问题的来源；是你（自己）的问题，思考下一次应该怎么做才能不再出现类似的问题； 最后，会发现越是经常这样做，在别人心中的分量就会越重，因为问题一到你这里就终止了。 多读、多写代码:编程是一门重视实践的技术，不写代码就一定不能提升对代码细节的感受力。提升代码感受力，是提升编程效率的一种有效途径，有两个办法： 多读别人的代码:无论是优秀的开源框架的源代码，还是通篇 if-else 无注释业务代码，都应该多读，通过多读才能发现什么是优秀代码，什么是烂代码； 多写自己的代码:编程犹如跑马拉松，需要付出很多汗水，真实地训练，才有可能完成一次比赛。不要被一些原理讲解所误导，原理学习很重要，但是真实的编码同样重要，因为代码从编写到调试再到运行、发布、部署，会出现很多复杂的问题，解决这些问题同样对提升编程效率有很大的帮助。 打破砂锅问到底:任何时候，对于问题都要有“打破砂锅问到底”的精神具体提问的技巧，可以采用 5WHY 法。例如： Q：你在做什么？A：解决一个 Bug。Q：你为什么要解决这个 Bug？A：测试提了一个 Bug。Q：为什么测试会提这个 Bug？A：测试认为这是一个 Bug。Q：为什么测试会这么认为？A：……在这个不断提问的时候，也是发现问题本质的一个过程。而解决编程上的本质问题越多，越能反过来提升编程上的效率。打磨工具除了 IDE 编程工具和编程语言环境的使用外，要提升编程效率，应该需要一个组件实验环境和一个工具代码库。组件实验环境从 IDE 的安装、多种编程语言包到 Git 等基础工具的配置，作为一个编程者，一定非常熟悉。但是，在编程过程中，除了写代码外，还需要大量地阅读代码和实验新工具，比如，开源的数据库、MQ、缓存组件等。当在平时任务非常繁重的情况下，如果没有一个可以立即实验操作的环境，就很容易因为一点点麻烦而放弃新的尝试。而这背后其实隐藏了一个本质问题：长期做事时缺乏提前准备的习惯。编程效率看上去是当前某种执行效率，但是如果长期都没有提前准备的习惯，那么效率就会变得越来越低，因为很可能一直都在做低效重复的事情，一直没有新的突破。比如，工作十年使用的是第一年就学会的技能方法。所以，平时如果有空闲时，多尝试搭建一下新组件的实验环境，一方面可以熟悉组件特性，另一方面是培养自己编程上多准备的习惯。工具代码库虽然现在网络上确实有很多很好用的工具库，但是依然要搭建一个自己的工具库。这个工具库存储的可以是你工作中常用的自动化测试脚本、一段简练的代码片段、对某个工具的二次或三次封装等。更重要的是，在这个过程中，通过对工具的打磨，可以找到一些不变的代码特性，学习到优秀的代码设计应该怎么做。这样随着时间的推移，编程效率会随着工具效率的提升而变得更高。实践编码高效编程是一个不断实践的过程，虽然写代码不是全部，但不写代码就不是编程。“高效”的反面是“低效”，低效编程有如下三大特征： 靠运气编码.就是所谓的复制粘贴编码，或面向搜索编码. 这样的编码会造成思考停滞，虽说会让你在未来的几年里沉浸在每天敲代码的喜悦里，从短期看似乎是在高效编程，但把目光放长远点，这是典型的长期低效编程，一直在原地踏步。 因为运气有时靠不住。应该不断积累编程实力，最后用实力来解决编程问题。 重复硬编码。我经常对身边的人说，我不反对硬编码，但我反对很多次重复硬编码；硬编码有时是无奈的做法，比如，领导立即要一个都还没有上线的数据做报告，或者立即就要演示一个 Demo，特性都是紧急，甚至有可能做完了以后再也不会用到；如果一直这样重复硬编码，带来的后果会很严重，不仅会把系统搞崩，还会让维护代码的其他人非常痛苦。更重要的是，会忘记了真正的高效是抽象、封装和复用，最后被更多的编码任务压垮。 写 PPT/开会。这个好理解了，但是估计你会经常遇到或者曾经头疼过： PPT 可以作为总结时的有效材料，但是编程不是写 PPT，PPT 写得多，编码时间自然剩得少。提升编程能力，必须要多写代码，才能发现更多问题，然后再去总结归纳、写 PPT。千万不能本末倒置； 至于开会，本意是为了与更多的人沟通、对齐一致，但是开很多无效的会，会耽误提升编程效率，毕竟编程需要思考时间，同时还需要编写、调试、测试时间。 在实践编码这一步，做到高效编程，就是要避免低效编程，避免以上做法。及时反馈编程时只写代码是远远不够的，想要获得更高效率，还要及时反馈遇见的问题。可能认为，编程时需要更多的思考时间，即便遇见问题只要自己能解决就行，与人沟通会浪费编程的时间。沟通的确会在某种程度上降低写代码的效率，但是它会在另外的地方补足：团队效率。应该知道，现代大型软件产品的开发工作都是团队合作编程，很少有一个人单独完成的。（当然，也有大神人物独立完成的产品，但毕竟是少数）。如果开发完了自己的模块，却不及时反馈给团队其他成员，那么极大的可能是，团队其他成员也在等待着你的模块，你们相互等待，这样整体效率其实很低。当你及时反馈问题时，团队成员也能通过你的进度来及时调整他们的计划，团队整体的协作效率无形中便提高了。另一方面，在编码过程中你不断与他人沟通，不仅能让对方更容易理解你的代码，也能及时发现一些低级错误，降低 Bug 的修复成本。这也是为什么很多开发团队都会要求每日站立早会和平时多沟通的原因，就是为了及时地让大家做反馈，在开发早期就能发现并解决问题。其实，不管团队有没有要求，都应该更进一步地要求自己时常这样做，这会带来真正效率提升。迭代更新简单来说，每一次对过程的重复就被称为一次“迭代”，而每一次迭代得到的结果会被用来作为下一次迭代的初始值。迭代有如下三个关键特征。 每一个迭代都应该有输入、处理和输出。比如，你在负责设计某个数据中台的数据清洗服务中的一次功能优化时，输入可能是各种新的异构类型的数据，处理是做数据的过滤与保存，输出的是某种规范的结构化数据。这就是通常说的一次功能迭代； 记录版本。每一个迭代通常又叫一个版本。为什么要记录版本？因为版本要作为一个历史记录被反复查看； 不断更新。当你有了版本和迭代输出，一个迭代才能被更新，也就是在既有版本下对输入、处理、输出的整体优化。编程本身就是一个重复过程，可能经常在用迭代，也可能会在无意间忽略迭代的关键点——记录版本。在编程时，会面临一个问题：每一次的代码修改都会或多或少覆盖原有代码，久而久之，代码可能是最新的，功能看上去也是最全的，但想要回顾时，却发现历史版本太难寻找，比如，一直使用 1.0-SNAPSHOT 的版本号。如果没有历史版本，会很快忘记过去开发过哪些功能、没开发过哪些功能，虽然功能不断在迭代前进，但一年后你几乎就会忘记你曾经做过什么。正确的做法是：记录版本并且记录每一次关键修改信息。记录版本很容易理解，可以使用语义版本规范（主版本.次版本.修订号）或者时间版本（20200111.fix）等。**更重要的是记录每一次关键修改信息，这是下一次迭代更新的输入。 **这样做的好处在于，当你修改一个功能时，你能清楚地知道你新增或修改了哪些功能，而功能实现的背后其实是你如何实现这个功能的思路。比如说，你开发了一个上传功能，在 V1.0 版本只支持 .pdf 格式，在 V2.0 版本时你希望支持 .doc 和 .xlsx 格式，如果只是记录版本号，你最多是大概知道功能有什么变化，但如果你记录的是： V1.0.0，支持 .pdf 格式【实现：封装三方库 iText 】【0.5 天】【遇见 XXX 问题】【2020-01-11】 V2.0.0，计划支持 .doc 和 .xlsx 格式【实现：封装三方库 easyexcel】【1 天】这样的关键信息，可以不断扩充（甚至可以是一篇文档），成为编码结束后的一次简单总结。不要小看这次总结，在时间过去很久后，再看时你会发现，原来过去你写代码时是这样想的，用时居然这么久，而且还用了很老旧的库，等等。最重要的是，过去的经验没有被浪费，可以从之前信息中获得思考，更能减少重复犯低级错误的概率。总结 如果时常把编程干成这样：编程 = 写代码 + 写代码 + 写代码……那么一定要及时停下来想想，编程到底意味着什么； 编程 ≠ 写代码。 不要把有效时间，浪费在只写代码上，这样编程能力甚至个人成长都会变慢； 编程不是一个体力活，编程应该是一个综合的脑力活：编程 = 写代码 + 讨论 + 学习 + 反思； 在学习编程知识和编程技巧的同时，多和同事讨论编程，从项目中积累更多实践的经验，不断应用到下一次的编程中去； 每一次的编码实践都是你提升效率的好机会，别忘记及时反馈你遇到的问题，主动与他人分享你的实践想法； 只有把编码变成一次又一次的迭代，才能从短期的高效编程变成真正的长期高效编程。" }, { "title": "JVM 总结", "url": "/posts/common-problem/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-25 15:23:11 +0000", "snippet": "常见问题 对象在哪里分配 谈到了数组和对象是堆上分配，当学逃逸分析后，了解到并不完全是这样的。由于 JIT 的存在，如果发现某些对象没有逃逸出方法，那么就有可能被优化成了栈上分配。 CMS 是老年代垃圾回收器 初步印象是，但实际上不是。根据 CMS 的各个收集过程，它其实是一个涉及年轻代和老年代的综合性垃圾回收器。在很多文章和书籍的划分中，都将 CMS 划分为了老年代垃圾回收器，加上它主要作用于老年代，所以一般误认为是。 常量池问题 常量池的表述有些模糊，在此细化一下，注意我们指的是 Java 7 版本之后。 JVM 中有多个常量池： 字符串常量池，存放在堆上，也就是执行 intern 方法后存的地方，class 文件的静态常量池，如果是字符串，则也会被装到字符串常量池中。 运行时常量池，存放在方法区，属于元空间，是类加载后的一些存储区域，大多数是类中 constant_pool 的内容。 类文件常量池，也就是 constant_pool，这个是概念性的，并没有什么实际存储区域。 在平常的交流过程中，聊的最多的是字符串常量池，具体可参考官网。 ZGC 支持的堆上限 Java 13 增加到 16TB，Java 11 还是 4 TB，技术在发展，请保持关注。 年轻代提升阈值动态计算的描述 在第 06 课时中对于年轻代“动态对象年龄判定”的表述是错误的。 参考代码 share/gc/shared/ageTable.cpp 中的 compute_tenuring_threshold 函数，重新表述为：程序从年龄最小的对象开始累加，如果累加的对象大小，大于幸存区的一半，则将当前的对象 age 作为新的阈值，年龄大于此阈值的对象则直接进入老年代。 这里说的一半，是通过 TargetSurvivorRatio 参数进行设置的。 永久代 注意左半部分是 Java 8 版本之前的内存区域，右半部分是 Java 8 的内存区域，主要区别就在 Perm 区和 Metaspace 区。 Perm 区属于堆，独立控制大小，在 Java 8 中被移除了（JEP122），原来的方法区就在这里；Metaspace 是非堆，默认空间无上限，方法区移动到了这里。 JVM 有哪些内存区域？（JVM 的内存布局是什么？） JVM 包含堆、元空间、Java 虚拟机栈、本地方法栈、程序计数器等内存区域，其中，堆是占用内存最大的一块，如下图所示。 Java 的内存模型是什么？（JMM 是什么？） JVM 试图定义一种统一的内存模型，能将各种底层硬件以及操作系统的内存访问差异进行封装，使 Java 程序在不同硬件以及操作系统上都能达到相同的并发效果。它分为工作内存和主内存，线程无法对主存储器直接进行操作，如果一个线程要和另外一个线程通信，那么只能通过主存进行交换，如下图所示。 JVM 垃圾回收时如何确定垃圾？什么是 GC Roots？ JVM 采用的是可达性分析算法。JVM 是通过 GC Roots 来判定对象存活的，从 GC Roots 向下追溯、搜索，会产生一个叫做 Reference Chain 的链条。当一个对象不能和任何一个 GC Root 产生关系时，就判定为垃圾，如下图所示。 GC Roots 大体包括： 活动线程相关的各种引用，比如虚拟机栈中 栈帧里的引用； 类的静态变量引用； JNI 引用等。 注意：要想回答的更详细一些，请参照第 05 课时中的内容。 能够找到 Reference Chain 的对象，就一定会存活么？ 不一定，还要看 Reference 类型，弱引用在 GC 时会被回收，软引用在内存不足的时候会被回收，但如果没有 Reference Chain 对象时，就一定会被回收。 强引用、软引用、弱引用、虚引用是什么？ 普通的对象引用关系就是强引用。 软引用用于维护一些可有可无的对象。只有在内存不足时，系统则会回收软引用对象，如果回收了软引用对象之后仍然没有足够的内存，才会抛出内存溢出异常。 弱引用对象相比软引用来说，要更加无用一些，它拥有更短的生命周期，当 JVM 进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。 虚引用是一种形同虚设的引用，在现实场景中用的不是很多，它主要用来跟踪对象被垃圾回收的活动。 你说你做过 JVM 参数调优和参数配置，请问如何查看 JVM 系统默认值？ 使用 -XX:+PrintFlagsFinal 参数可以看到参数的默认值，这个默认值还和垃圾回收器有关，比如 UseAdaptiveSizePolicy。 你平时工作中用过的 JVM 常用基本配置参数有哪些？ 主要有 Xmx、Xms、Xmn、MetaspaceSize 等。 更加详细的可参照第 23 课时的参数总结，你只需要记忆 10 个左右即可，建议记忆 G1 相关的参数。面试时间有限，不会在这上面纠结，除非你表现的太嚣张了。 请你谈谈对 OOM 的认识 OOM 是非常严重的问题，除了程序计数器，其他内存区域都有溢出的风险。和我们平常工作最密切的，就是堆溢出，另外，元空间在加载的类非常多的情况下也会溢出，还有就是栈溢出，这个通常影响比较小。堆外也有溢出的可能，这个就比较难排查了。 你都有哪些手段用来排查内存溢出？ 这个话题很大，可以从实践环节中随便摘一个进行总结，下面举一个最普通的例子。 内存溢出包含很多种情况，我在平常工作中遇到最多的就是堆溢出。有一次线上遇到故障，重新启动后，使用 jstat 命令，发现 Old 区一直在增长。我使用 jmap 命令，导出了一份线上堆栈，然后使用 MAT 进行分析，通过对 GC Roots 的分析，发现了一个非常大的 HashMap 对象，这个原本是其他同事做缓存用的，但是一个无界缓存，造成了堆内存占用一直上升，后来，将这个缓存改成 guava 的 Cache，并设置了弱引用，故障就消失了。 GC 垃圾回收算法与垃圾收集器的关系？ 常用的垃圾回收算法有标记清除、标记整理、复制算法等，引用计数器也算是一种，但垃圾回收器不使用这种算法，因为有循环依赖的问题。 很多垃圾回收器都是分代回收的： 对于年轻代，主要有 Serial、ParNew 等垃圾回收器，回收过程主要使用复制算法； 老年代的回收算法有 Serial、CMS 等，主要使用标记清除、标记整理算法等。 我们线上使用较多的是 G1，也有年轻代和老年代的概念，不过它是一个整堆回收器，它的回收对象是小堆区 。 生产上如何配置垃圾收集器？ 首先是内存大小问题，基本上每一个内存区域我都会设置一个上限，来避免溢出问题，比如元空间。通常，堆空间我会设置成操作系统的 2/3，超过 8GB 的堆，优先选用 G1。 然后我会对 JVM 进行初步优化，比如根据老年代的对象提升速度，来调整年轻代和老年代之间的比例。 接下来是专项优化，判断的主要依据是系统容量、访问延迟、吞吐量等，我们的服务是高并发的，所以对 STW 的时间非常敏感。 我会通过记录详细的 GC 日志，来找到这个瓶颈点，借用 GCeasy 这样的日志分析工具，很容易定位到问题。 怎么查看服务器默认的垃圾回收器是哪一个？ 这通常会使用另外一个参数，即 -XX:+PrintCommandLineFlags，来打印所有的参数，包括使用的垃圾回收器。 假如生产环境 CPU 占用过高，请谈谈你的分析思路和定位。 首先，使用 top -H 命令获取占用 CPU 最高的线程，并将它转化为十六进制。 然后，使用 jstack 命令获取应用的栈信息，搜索这个十六进制，这样就能够方便地找到引起 CPU 占用过高的具体原因。 对于 JDK 自带的监控和性能分析工具用过哪些？ jps：用来显示 Java 进程； jstat：用来查看 GC； jmap：用来 dump 堆； jstack：用来 dump 栈； jhsdb：用来查看执行中的内存信息。 栈帧都有哪些数据？ 栈帧包含：局部变量表、操作数栈、动态连接、返回地址等。 JIT 是什么？ 为了提高热点代码的执行效率，在运行时，虚拟机将会把这些代码编译成与本地平台相关的机器码，并进行各种层次的优化，完成这个任务的编译器，就称为即时编译器（Just In Time Compiler），简称 JIT 编译器。 Java 的双亲委托机制是什么？ 双亲委托的意思是，除了顶层的启动类加载器以外，其余的类加载器，在加载之前，都会委派给它的父加载器进行加载，这样一层层向上传递，直到祖先们都无法胜任，它才会真正的加载，Java 默认是这种行为。 有哪些打破了双亲委托机制的案例？ Tomcat 可以加载自己目录下的 class 文件，并不会传递给父类的加载器； Java 的 SPI，发起者是 BootstrapClassLoader，BootstrapClassLoader 已经是最上层了，它直接获取了 AppClassLoader 进行驱动加载，和双亲委派是相反的。 简单描述一下（分代）垃圾回收的过程 分代回收器有两个分区：老生代和新生代，新生代默认的空间占总空间的 1/3，老生代的默认占比是 2/3。 新生代使用的是复制算法，新生代里有 3 个分区：Eden、To Survivor、From Survivor，它们的默认占比是 8:1:1。 当年轻代中的 Eden 区分配满的时候，就会触发年轻代的 GC（Minor GC），具体过程如下： 在 Eden 区执行了第一次 GC 之后，存活的对象会被移动到其中一个 Survivor 分区（以下简称 from）； Eden 区再次 GC，这时会采用复制算法，将 Eden 和 from 区一起清理，存活的对象会被复制到 to 区，接下来，只要清空 from 区就可以了。 CMS 分为哪几个阶段? 初始标记 并发标记 并发预清理 并发可取消的预清理 重新标记 并发清理 由于《深入理解 Java 虚拟机》一书的流行，面试时省略并发清理、并发可取消的预清理这两个阶段，一般也是没问题的。 CMS 都有哪些问题？ 内存碎片问题，Full GC 的整理阶段，会造成较长时间的停顿； 需要预留空间，用来分配收集阶段产生的“浮动垃圾”； 使用更多的 CPU 资源，在应用运行的同时进行堆扫描； 停顿时间是不可预期的。 你使用过 G1 垃圾回收器的哪几个重要参数？ 最重要的是 MaxGCPauseMillis，可以通过它设定 G1 的目标停顿时间，它会尽量去达成这个目标。G1HeapRegionSize 可以设置小堆区的大小，一般是 2 的次幂。InitiatingHeapOccupancyPercent 启动并发 GC 时的堆内存占用百分比，G1 用它来触发并发 GC 周期，基于整个堆的使用率，而不只是某一代内存的使用比例，默认是 45%。 GC 日志的 real、user、sys 是什么意思？ real 指的是从开始到结束所花费的时间，比如进程在等待 I/O 完成，这个阻塞时间也会被计算在内。 user 指的是进程在用户态（User Mode）所花费的时间，只统计本进程所使用的时间，是指多核。 sys 指的是进程在核心态（Kernel Mode）所花费的 CPU 时间量，即内核中的系统调用所花费的时间，只统计本进程所使用的时间。 什么情况会造成元空间溢出？ 元空间默认是没有上限的，不加限制比较危险。当应用中的 Java 类过多时，比如 Spring 等一些使用动态代理的框架生成了很多类，如果占用空间超出了我们的设定值，就会发生元空间溢出。 什么时候会造成堆外内存溢出？ 使用了 Unsafe 类申请内存，或者使用了 JNI 对内存进行操作，这部分内存是不受 JVM 控制的，不加限制使用的话，会很容易发生内存溢出。 SWAP 会影响性能么？ 当操作系统内存不足时，会将部分数据写入到 SWAP ，但是 SWAP 的性能是比较低的。如果应用的访问量较大，需要频繁申请和销毁内存，那么很容易发生卡顿。一般在高并发场景下，会禁用 SWAP。 有什么堆外内存的排查思路？ 进程占用的内存，可以使用 top 命令，看 RES 段占用的值，如果这个值大大超出我们设定的最大堆内存，则证明堆外内存占用了很大的区域。 使用 gdb 命令可以将物理内存 dump 下来，通常能看到里面的内容。更加复杂的分析可以使用 Perf 工具，或者谷歌开源的 GPerftools。那些申请内存最多的 native 函数，就很容易找到。 HashMap 中的 key，可以是普通对象么？有什么需要注意的地方？ Map 的 key 和 value 可以是任何类型，但要注意的是，一定要重写它的 equals 和 hashCode 方法，否则容易发生内存泄漏。 怎么看死锁的线程？ 通过 jstack 命令，可以获得线程的栈信息，死锁信息会在非常明显的位置（一般是最后）进行提示。 如何写一段简单的死锁代码？ 详情请见第 15 课时的 DeadLockDemo，笔试的话频率也很高。 invokedynamic 指令是干什么的？ invokedynamic 是 Java 7 版本之后新加入的字节码指令，使用它可以实现一些动态类型语言的功能。我们使用的 Lambda 表达式，在字节码上就是 invokedynamic 指令实现的，它的功能有点类似反射，但它是使用方法句柄实现的，执行效率更高。 volatile 关键字的原理是什么？有什么作用？ 使用了 volatile 关键字的变量，每当变量的值有变动的时候，都会将更改立即同步到主内存中；而如果某个线程想要使用这个变量，就先要从主存中刷新到工作内存，这样就确保了变量的可见性。 一般使用一个 volatile 修饰的 bool 变量，来控制线程的运行状态。 volatile boolean stop = false; void stop(){ this.stop = true; } void start(){ new Thread(()-&amp;gt;{ while (!stop){ //sth } }).start(); } 什么是方法内联？ 为了减少方法调用的开销，可以把一些短小的方法，比如 getter/setter，纳入到目标方法的调用范围之内，这样就少了一次方法调用，速度就能得到提升，这就是方法内联的概念。 对象是怎么从年轻代进入老年代的？ 在下面 4 种情况下，对象会从年轻代进入到老年代： 如果对象够老，则会通过提升（Promotion）的方式进入老年代，一般根据对象的年龄进行判断； 动态对象年龄判定，有的垃圾回收算法，比如 G1，并不要求 age 必须达到 15 才能晋升到老年代，它会使用一些动态的计算方法； 分配担保，当 Survivor 空间不够的时候，则需要依赖其他内存（指老年代）进行分配担保，这个时候，对象也会直接在老年代上分配； 超出某个大小的对象将直接在老年代上分配，不过这个值默认为 0，意思是全部首选 Eden 区进行分配。 safepoint 是什么？ 当发生 GC 时，用户线程必须全部停下来，才可以进行垃圾回收，这个状态我们可以认为 JVM 是安全的（safe），整个堆的状态是稳定的。 如果在 GC 前，有线程迟迟进入不了 safepoint，那么整个 JVM 都在等待这个阻塞的线程，造成了整体 GC 的时间变长。 MinorGC、MajorGC、FullGC 都什么时候发生？ MinorGC 在年轻代空间不足的时候发生，MajorGC 指的是老年代的 GC，出现 MajorGC 一般经常伴有 MinorGC。 FullGC 有三种情况：第一，当老年代无法再分配内存的时候；第二，元空间不足的时候；第三，显示调用 System.gc 的时候。另外，像 CMS 一类的垃圾回收器，在 MinorGC 出现 promotion failure 的时候也会发生 FullGC。 类加载有几个过程？ 加载、验证、准备、解析、初始化。 什么情况下会发生栈溢出？ 栈的大小可以通过 -Xss 参数进行设置，当递归层次太深的时候，则会发生栈溢出。 生产环境服务器变慢，请谈谈诊断思路和性能评估？ 第 11 课时和第 16 课时中的一些思路，能够祝你一臂之力。下图是第 11 课时的一张影响因素的全景图。 从各个层次分析代码优化的手段，如下图所示： " }, { "title": "JVM 的历史与展望", "url": "/posts/jvm-history/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-24 14:15:44 +0000", "snippet": "JVM 的历史众所周知，Java 目前被 Oracle 控制，它是从 Sun 公司手中收购的，HotSpot 最初也并非由 Sun 公司开发，是由一家名为 Longview Technologies 的小公司设计的，而且这款虚拟机一开始也不是为 Java 语言开发的。当时的 HotSpot，非常优秀，尤其是在 JIT 编译技术上，有一些超前的理念，于是 Sun 公司在 1997 年收购了 Longview Technologies，揽美人入怀。Sun 公司是一家对技术非常专情的公司，他们对 Java 语言进行了发扬光大，尤其是在 JVM 上，做了一些非常大胆的尝试和改进。9 年后，Sun 公司在 2006 年的 JavaOne 大会上，将 Java 语言开源，并在 GPL 协议下公开源码，在此基础上建立了 OpenJDK。GPL 协议的限制，是比较宽松的，这极大的促进了 Java 的发展，同时推动了 JVM 的发展。Sun 是一家非常有技术情怀的公司，最高市值曾超过 2000 亿美元。但是，最后却以 74 亿美元的价格被 Oracle 收购了，让人感叹不已。2010 年，HotSpot 进入了 Oracle 时代，这也是现在为什么要到 Oracle 官网上下载 J2SE 的原因。幸运的是，有 OpenJDK 这个凝聚了众多开源开发者心血的分支。从目前的情况来看，OpenJDK 与 Oracle 版本之间的差别越来越小，甚至一些超前的实验性特性，也会在 OpenJDK 上进行开发。对于使用者来说，这个差别并不大，因为 JVM 已经屏蔽了操作系统上的差异，而经常打交道的，是上层的 JRE 和 JDK。其它虚拟机由于 JVM 就是个规范，所以实现的方法很多。[完整的列表看这里]JVM 的版本非常之多，比较牛的公司都搞了自己的 JVM，但当时谁也没想到，话语权竟会到了 Oracle 手里。下面是几个比较典型： J9 VM 早些年工作的时候，有钱的公司喜欢买大型机，比如会买 WebLogic、WebSphere 等服务器。对于现在已经用惯了 Tomcat、Undertow 这些轻量级的 Web 服务器来说，这是一些很古老的名词了。 WebSphere 就是这样一个以“巨无霸”的形式存在，当年的中间件指的就是它，和现在的中间件完全不是一个概念。 WebSphere 是 IBM 的产品，开发语言是 Java。但是它运行时的 JVM，却是一个叫做 J9 的虚拟机，有非常多的 jar 包，由于引用了一些非常偏门的 API，却不能运行（现在应该好了很多）。 Zing VM Zing JVM 是 Azul 公司传统风格的产品，它在 HotSpot 上做了不少的定制及优化，主打低延迟、高实时服务器端 JDK 市场。它代表了一类商业化的定制，比如 JRockit，都比较贵。 IKVM LibGDX，相当于使用了 Java，最后却能跑在 .net 环境上，使用的方式是 IKVM 。它包含了一个使用 .net 语言实现的 Java 虚拟机，配合 Mono 能够完成 Java 和 .net 的交互，让人认识到语言之间的鸿沟是那么的渺小。 Dalvik Android 的 JVM，就是让 Google 吃官司的那个，从现在 Android 的流行度上也能看出来，Dalvik 优化的很好。 历史 1995 年 5 月 23 日，Sun 公司正式发布了 Java 语言和 HotJava 浏览器； 1996 年 1 月，Sun 公司发布了 Java 的第一个开发工具包（JDK 1.0）； 1996 年 4 月，10 个最主要的操作系统供应商申明将在其产品中嵌入 Java 技术，发展可真是迅雷不及掩耳； 1996 年 9 月，约 8.3 万个网页应用了 Java 技术来制作，这就是早年的互联网，即 Java Applet，真香； 1996 年 10 月，Sun 公司发布了 Java 平台第一个即时编译器（JIT），这一年很不平凡； 1997 年 2 月 18 日，JDK 1.1 面世，在随后的三周时间里，达到了 22 万次的下载量，PHP 甘拜下风； 1999 年 6 月，Sun 公司发布了第二代 Java 三大版本，即 J2SE、J2ME、J2EE，随之 Java2 版本发布； 2000 年 5 月 8 日，JDK 1.3 发布，四年升三版，不算过分哈； 2000 年 5 月 29 日，JDK 1.4 发布，获得 Apple 公司 Mac OS 的工业标准支持； 2001 年 9 月 24 日，Java EE 1.3 发布，注意是 EE，从此开始臃肿无比； 2002 年 2 月 26 日，J2SE 1.4 发布，自此 Java 的计算能力有了大幅度的提升，与 J2SE 1.3 相比，多了近 62% 的类与接口； 2004 年 9 月 30 日 18:00PM，J2SE 1.5 发布，1.5 正式更名为 Java SE 5.0； 2005 年 6 月，在 JavaOne 大会上，Sun 公司发布了 Java SE 6； 2009 年 4 月 20 日，Oracle 宣布收购 Sun，该交易的总价值约为 74 亿美元； 2010 年 Java 编程语言的创始人 James Gosling 从 Oracle 公司辞职，一朝天子一朝臣，国外也不例外； 2011 年 7 月 28 日，Oracle 公司终于发布了 Java 7，这次版本升级经过了将近 5 年时间； 2014 年 3 月 18 日，Oracle 公司发布了 Java 8，这次版本升级为 Java 带来了全新的 Lambda 表达式。小碎步越来越快，担心很快 2 位数都装不下 Java 的版本号了。目前 Java 的版本已经更新到 14 了，但市场主流使用的还是 JDK 8 版本。最近更新有些、现在认为理所当然的功能，在 Java 的早期版本是没有的。从 Java 7 说起，以下内容仅供参考，详细列表见 openjdk JEPJava 7Java 7 增加了以下新特性： try、catch 能够捕获多个异常； 新增 try-with-resources 语法； JSR341 脚本语言新规范； JSR203 更多的 NIO 相关函数 JSR292，第 17 课时提到的 InvokeDynamic 支持 JDBC 4.1 规范 文件操作的 Path 接口、DirectoryStream、Files、WatchService jcmd 命令 多线程 fork/join 框架 Java Mission ControlJava 8Java 8 也是一个重要的版本，在语法层面上有更大的改动，支持 Lamda 表达式，影响堪比 Java 5 的泛型支持： 支持 Lamda 表达式 支持集合的 stream 操作 提升了 HashMaps 的性能（红黑树） 提供了一系列线程安全的日期处理类 完全去掉了 Perm 区 Java 9Java 9 增加了以下新特性： JSR376 Java 平台模块系统 JEP261 模块系统 jlink 精简 JDK 大小 G1 成为默认垃圾回收器 CMS 垃圾回收器进入废弃倒计时 GC Log 参数完全改变，且不兼容 JEP110 支持 HTTP2，同时改进 HttpClient 的 API，支持异步模式 jshell 支持类似于 Python 的交互式模式 Java 10Java 10 增加了以下新特性： JEP304 垃圾回收器接口代码进行整改 JEP307 G1 在 FullGC 时采用并行收集方式 JEP313 移除 javah 命令 JEP317 重磅 JIT 编译器 Graal 进入实验阶段 Java 11Java 11 增加了以下新特性： JEP318 引入了 Epsilon 垃圾回收器，这个回收器什么都不干，适合短期任务 JEP320 移除了 JavaEE 和 CORBA Modules，应该要走轻量级路线 Flight Recorder 功能，类似 JMC 工具里的功能 JEP321 内置 httpclient 功能，java.net.http 包 JEP323 允许 lambda 表达式使用 var 变量 废弃了 -XX+AggressiveOpts 选项 引入了 ZGC，依然是实验性质 Java 12Java 12 增加了以下新特性： JEP189 先加入 ShenandoahGC JEP325 switch 可以使用表达式 JEP344 优化 G1 达成预定目标 优化 ZGC Java 13Java 13 增加了以下新特性： JEP354 yield 替代 break JEP355 加入了 Text Blocks，类似 Python 的多行文本 ZGC 的最大 heap 大小增大到 16TB 废弃 rmic Tool 并准备移除Java 14Java 14 增加了以下新特性： JEP343 打包工具引入 JEP345 实现了 NUMA-aware 的内存分配，以提升 G1 在大型机器上的性能 JEP359 引入了 preview 版本的 record 类型，可用于替换 lombok 的部分功能 JEP364 之前的 ZGC 只能在 Linux 上使用，现在 Mac 和 Windows 上也能使用 ZGC 了 JEP363 正式移除 CMS，一些优化参数，在 14 版本普及之后，将不复存在 OpenJDK 64-Bit Server VM warning: Ignoring option UseConcMarkSweepGC; support was removed in 14.0 现状2019 年 JVM 生态系统报告部分图示，参考自snyk这个网站生产环境中，主要用哪些 JDK可以看到 OracleJDK 和 OpenJDK 几乎统治了江湖，如果没有 IBM 那些捆绑销售的产品，份额只会更高。另外，使用 OpenJDK 的越来越多，差异也越来越小，在公有云、私有云等方面的竞争格局，深刻影响着在 OpenJDK 上的竞争格局；OpenJDK 很有可能被认为是一种退⽽求其次的选择。生产环境中，用哪个版本的 Java以 8 版本为主，当然还有 6 版本以下的，尝鲜的并不是很多，因为服务器环境的稳定性最重要。新版本升级在中国的宣传还是不够，如果很多企业看不到技术升级的红利，势必也会影响升级的积极性。应用程序的主要 JVM 语言是什么很多人反应 Kotlin 非常好用，我尝试着推广了一下，被喜欢 Groovy 的朋友鄙视了一番，目前还是以 Java 居多。有点规模的互联网公司，行事都会有些谨慎，虽然 JVM 做到了向下版本的兼容，但是有些性能问题还是不容忽视，尝鲜吃螃蟹的并不是很多。现在用的最多的，就是 Java 8 版本。如果你的服务器用的这个，那么用的最多的垃圾回收器就是 CMS，或者 G1。随着 ZGC 越来越稳定，CMS 终将会成为过去式。目前，最先进的垃圾回收器，叫做 ZGC，它有 3 个 flag： 支持 TB 级堆内存（最大 4T） 最大 GC 停顿 10ms 对吞吐量影响最大，不超过 15%每一个版本的发布，Java 都会对以下进行改进： 优化垃圾回收器，减少停顿，提高吞吐 语言语法层面的升级，这部分在最近的版本里最为明显 结构调整，减少运行环境的大小，模块化 废弃掉一些承诺要废弃的模块那么 JVM 将向何处发展呢？以目前来看，比较先进的技术，就是刚才提到的垃圾回收阶段的 ZGC ，能够显著的减少 STW 的问题；另外， GraalVM 是 Oracle 创建的一个研究项目，目标是完全替换 HotSpot，它是一个高性能的 JIT 编译器，接受 JVM 字节码，并生成机器代码。未来，会有更多的开发语言运行在 JVM 上，比如 Python、Ruby 等。Poject Loom 致力于在 JVM 层面，给予 Java 协程 （fibers）的功能，Java 程序的并发性能会上一个档次。Java 版本大部分是向下兼容的，能够做到这个兼容，是非常不容易的。但 Java 的特性越加越多，如果开发人员不能进行平滑的升级，会是一个非常严重的问题，JVM 也将会在这里花费非常大的精力。那 JVM 将聚焦在哪些方面呢？又有哪些挑战？我大体总结了几点： 内存管理依然是非常大的挑战，未来会有更厉害的垃圾回收器来支持更大的堆空间 多线程和协程，未来会加大对多核的利用，以及对轻量级线程的支持 性能，增加整个 JVM 的执行效率，这通常是多个模块协作的结果 对象管理和追踪，复杂的对象，有着复杂的生命周期，加上难以预料的内存申请方式，需要更精准的管理优化 可预测性及易用性，更少的优化参数，更高的性能 更多 JVM 监控工具，提供对 JVM 全方面的监控，跟踪对象，在线优化 多语言支持，支持除了 Java 语言之外的其他开发语言，能够运行在 JVM 上总结Java 9 之后，已经进入了快速发布阶段，大约每半年发布一次，Java 8 和 Java 11 是目前支持的 LTS 版本，它的功能变动也越来越多、越来越快。把握好 Java 发展的脉搏，加油 ！" }, { "title": "大型项目性能调优 —— JVM", "url": "/posts/jvm-performance-tuning/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-23 13:23:00 +0000", "snippet": "性能调优是一个非常大，并且非常模糊的话题。在大型的项目中，既有分布式的交互式调优问题，也有纯粹的单机调优问题。本文重点关注 JVM 的调优、故障或者性能瓶颈方面的问题排查！！！优化层次当一个系统出现问题的时候，一般不会想要立刻优化 JVM，会优先尝试从最高层次上进行问题的解决：解决最主要的瓶颈点。下面是一张关于优化层次的图，箭头表示优化时需要考虑的路径（也有例外）数据库优化数据库是最容易成为瓶颈的组件，研发会从 SQL 优化或者数据库本身去提高它的性能。如果瓶颈依然存在，则会考虑分库分表将数据打散，如果这样也没能解决问题，则可能会选择缓存组件进行优化。这个过程与本课时相关的知识点，可以使用 jstack 获取阻塞的执行栈，进行辅助分析。集群优化存储节点的问题解决后，计算节点也有可能发生问题。一个集群系统如果获得了水平扩容的能力，就会给下层的优化提供非常大的时间空间，这也是弹性扩容的魅力所在。我接触过一个服务，由最初的 3 个节点，扩容到最后的 200 多个节点，但由于人力问题，服务又没有什么新的需求，下层的优化就一直被搁置着。硬件升级水平扩容不总是有效的，原因在于单节点的计算量比较集中，或者 JVM 对内存的使用超出了宿主机的承载范围。在动手进行代码优化之前，我们会对节点的硬件配置进行升级。升级容易，降级难，降级需要依赖代码和调优层面的优化。代码优化出于成本的考虑，上面的这些问题，研发团队并不总是坐视不管。代码优化是提高性能最有效的方式，但需要收集一些数据，这个过程可能是服务治理，也有可能是代码流程优化。我在第 21 课时介绍的 JavaAgent 技术，会无侵入的收集一些 profile 信息，供我们进行决策。像 Sonar 这种质量监控工具，也可以在此过程中帮助到我们。并行优化并行优化的对象是这样一种接口，它占用的资源不多，计算量也不大，就是速度太慢。所以我们通常使用 ContDownLatch 对需要获取的数据进行并行处理，效果非常不错，比如在 200ms 内返回对 50 个耗时 100ms 的下层接口的调用。JVM 优化虽然对 JVM 进行优化，有时候会获得巨大的性能提升，但在 JVM 不发生问题时，我们一般不会想到它。原因就在于，相较于上面 5 层所达到的效果来说，它的优化效果有限。但在代码优化、并行优化、JVM 优化的过程中，JVM 的知识却起到了关键性的作用，是一些根本性的影响因素。操作系统优化操作系统优化是解决问题的杀手锏，比如像 HugePage、Luma、“CPU 亲和性”这种比较底层的优化。但就计算节点来说，对操作系统进行优化并不是很常见。运维在背后会做一些诸如文件句柄的调整、网络参数的修改，这对于我们来说就已经够用了。JVM 优化因为 JVM 一直处在变化之中，所以一些参数的配置并不总是有效的！！！有时候加入一个参数，“感觉上”运行速度加快了，但通过 -XX:+PrintFlagsFinal 来查看，却发现这个参数默认就是这样：比如 UseAdaptiveSizePolicy。因此，在不同的 JVM 版本上，不同的垃圾回收器上，要先看一下这个参数默认是什么，不要轻信他人的建议。java -XX:+PrintFlagsFinal -XX:+UseG1GC 2&amp;gt;&amp;amp;1 | grep UseAdaptiveSizePolicy内存区域大小内存调优垃圾回收器优化其他参数优化存疑优化GC 日志故障排查==有需求才需要优化，不要为了优化而优化。==一般来说，上面提到 JVM 参数，基本能够保证应用安全，如果想要更进一步、更专业的性能提升，就没有什么通用的法则了。打印详细的 GCLog，能够帮助了解到底是在哪一步骤发生了问题，然后才能对症下药。使用 gceasy.io 这样的线上工具，能够方便的分析到结果，但==一些偏门的 JVM 参数修改，还是需要进行详细的验证。==一次或者多次模拟性的压力测试是必要的，能够提前发现这些优化点！！！==JVM 故障涉及到内存问题和计算问题，其中内存问题占多数。==除了程序计数器，JVM 内存里划分每一个区域，都有溢出的可能： 最常见的就是堆溢出。使用 jmap 可以 dump 一份内存，然后使用 MAT 工具进行具体原因的分析； 对堆外内存的排查需要较高的技术水平。当发现进程占用的内存资源比使用 Xmx 设置得要多，不要忘这一环。使用 jstack 可以获取 JVM 的执行栈，并且能够看到线程的一些阻塞状态，这部分可以使用 arthas 进行瞬时态的获取，定位到瞬时故障。另外，一个完善的监控系统能够快速定位问题，包括操作系统的监控、JVM 的监控等。代码、JVM 优化和故障排查是一个持续优化的过程，只有更优、没有最优。如何在有限的项目时间内，最高效的完成工作，才是所需要的。总结JVM 的优化效果是有限的，但它是理论的基础，代码优化和参数优化都需要它的指导。同时，有非常多的工具能够定位到问题。偏门的优化参数可能有效，但不总是有效。实际上，从 CMS 到 G1，再到 ZGC，关于 GC 优化的配置参数也越来越少，协助排查问题的工具却越来越多。在大多数场景下，JVM 已经能够达到开箱即用的高性能效果，这是一个虚拟机所追求的最终目标。" }, { "title": "JIT 参数配置对程序的影响", "url": "/posts/jit/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-22 13:23:00 +0000", "snippet": "字节码指令，是 Java 能够跨平台的保证。程序运行时，这些指令按照顺序解释执行，显然这种解释执行方式是非常低效的，它需要把字节码先翻译成机器码，才能往下执行。另外，字节码是 Java 编译器做的一次初级优化，许多代码可以满足语法分析，但还有很大优化空间。所以，为了提高热点代码的执行效率，在运行时，虚拟机将会把这些代码编译成与本地平台相关的机器码，并进行各种层次的优化。完成这个任务的编译器，就称为即时编译器（Just In Time Compiler），简称 JIT 编译器。热点代码，就是那些被频繁调用的代码，比如调用次数很高或者在 for 循环里的那些代码。这些再次编译后的机器码会被缓存起来，以备下次使用，但对于那些执行次数很少的代码来说，这种编译动作就纯属浪费。参数-XX:ReservedCodeCacheSize，用来限制 CodeCache 的大小。也就是说，JIT 编译后的代码都会放在 CodeCache 里。如果这个空间不足，JIT 就无法继续编译，编译执行会变成解释执行，性能会降低一个数量级。同时，JIT 编译器会一直尝试去优化代码，从而造成了 CPU 占用上升。JITWatchJITWatch 是一个观察 JIT 执行过程的图形化工具。产生 JIT 日志有下面一段代码（没啥意义，写的很烂），在 test 函数中循环 cal 函数 1 千万次，在 cal 函数中，还有一些冗余的上锁操作和赋值操作，这些操作在解释执行的时候，会加重 JVM 的负担。public class JITDemo{ Integer a = 1000; public void setA(Integer a) { this.a = a; } public Integer getA(){ return this.a; } public Integer cal(int num) { sychronized (new Object()) { Integer a = getA(); int b = a * 10; b = a * 100; return b + num; } } public int test() { sychronized (new Object()) { int total = 0; int count = 10000000; for (int i = 0; i &amp;lt; count; i++) { total += cal(i); if (i % 1000 == 0 ) { System.out.println(i * 1000); } } return total; } } public static void main(String[] args) { JITDemo demo = new JITDemo(); int total = demo.test(); }}在上面代码执行的时候，加上一些参数，用来打印 JIT 最终生成的机器码，执行的命令如下：$JAVA_HOME_13/bin/java -server -XX:+UnlockDiagnosticVMOptions -XX:+TraceClassLoading -XX:+PrintAssembly -XX:+LogCompilation -XX:LogFile=jitdemo.log JITDemo执行的结果，会输入到 jitdemo.log 文件里。使用 JITWatch 单机 open log 按钮，打开生成的日志文件； 单击 config 按钮，加入要分析的源代码目录和字节码目录。确认后，单击 start 按钮进行分析； 在右侧找到 test 方法，聚焦光标后，弹出要分析的主要界面； 在同一个界面上，能够看到源代码、字节码、机器码的对应关系。在右上角，还有 C2/OSR/Level4 这样的字样，可以单击切换； 单击上图中的 Chain 按钮，会弹出一个依赖链界面，该界面显示了哪些方法已经被编译了、哪些被内联、哪些是通过普通的方法调用运行的； 使用 JITWatch 可以看到，调用了 1 千万次的 for 循环代码，已经被 C2 进行编译了。编译层次HotSpot 虚拟机包含多个即时编译器，有 C1、C2 和 Graal，采用的是分层编译的模式。使用 jstack 获得的线程信息，经常能看到它们的身影。实验性质的 Graal 可以通过追加 JVM 参数进行开启，命令行如下：$JAVA_HOME_13/bin/java -server -XX:+UnlockDiagnosticVMOptions -XX:+TraceClassLoading -XX:+PrintAssembly -XX:+LogCompilation -XX:+UnlockExperimentalVMOptions -XX:+UseJVMCICompiler -XX:LogFile=jitdemo.log JITDemo不同层次的编译器产生的效果、机器码会不同。仅关注 C1 和 C2 特点。JIT 编译方式有两种： 一种是编译方法； 另一种是编译循环。分层编译将 JVM 的执行状态分为了五个层次： 字节码的解释执行； 执行不带 profiling 的 C1 代码； profiling 指的是运行时程序执行状态数据，比如循环调用次数、方法调用次数、分支跳转次数、类型转换次数等。 JDK 中的 hprof 工具就是一种 profiler。 执行仅带方法调用次数，以及循环执行次数 profiling 的 C1 代码； 执行带所有 profiling 的 C1 代码； 执行 C2 代码。在不启用分层编译的情况下，当方法的调用次数和循环回边的次数总和，超过由参数 -XX:CompileThreshold 指定的阈值时，便会触发即时编译；当启用分层编译时，这个参数将会失效，会采用动态调整的方式进行。常见的优化方法有以下几种： 公共子表达式消除 数组范围检查消除 方法内联 逃逸分析方法内联方法调用的开销是比较大的，尤其是在调用量非常大的情况下。拿简单的 getter/setter 方法来说，这种方法在 Java 代码中大量存在，在访问的时候，需要创建相应的栈帧，访问到需要的字段后，再弹出栈帧，恢复原程序的执行。如果能够把这些对象的访问和操作，纳入到目标方法的调用范围之内，就少了一次方法调用，速度就能得到提升，这就是方法内联的概念。C2 编译器会在解析字节码的过程中完成方法内联。内联后的代码和调用方法的代码，会组成新的机器码，存放在 CodeCache 区域里。在 JDK 的源码里，有很多被 @ForceInline 注解的方法，这些方法会在执行的时候被强制进行内联；而被 @DontInline 注解的方法，则始终不会被内联，比如下面的一段代码。java.lang.ClassLoader 的 getClassLoader 方法将会被强制内联。@CallerSensitive@ForceInline // to ensure Reflection.getCallerClass optimizationpublic ClassLoader getClassLoader() { ClassLoader cl = getClassLoader0(); if (cl == null) return null; SecurityManager sm = System.getSecurityManager(); if (sm != null) { ClassLoader.checkClassLoaderPermission(cl, Reflection.getCallerClass()); } return cl;}方法内联的过程是非常智能的，内联后的代码，会按照一定规则进行再次优化。最终的机器码，在保证逻辑正确的前提下，可能和我们推理的完全不一样。在非常小的概率下，JIT 会出现 Bug，这时候可以关闭问题方法的内联，或者直接关闭 JIT 的优化，保持解释执行。实际上，这种 Bug 我从来没碰到过。-XX:CompileCommand=exclude,com/happymaya/Test,test上面的参数，表示 com.happymaya.Test 的 test 方法将不会进行 JIT 编译，一直解释执行。另外，C2 支持的内联层次不超过 9 层，太高的话，CodeCache 区域会被挤爆，这个阈值可以通过-XX:MaxInlineLevel 进行调整。相似的，编译后的代码超过一定大小也不会再内联，这个参数由 -XX:InlineSmallCode 进行调整。有非常多的参数，被用来控制对内联方法的选择，整体来说，短小精悍的小方法更容易被优化。这和在日常中的编码要求是一致的：代码块精简，逻辑清晰的代码，更容易获得优化的空间。使用 JITWatch 再看一下对于 getA() 方法的调用，将鼠标悬浮在字节码指令上，可以看到方法已经被内联了。逃逸分析逃逸分析（Escape Analysis）是目前 JVM 中比较前沿的优化技术。通过逃逸分析，JVM 能够分析出一个新的对象使用范围，从而决定是否要将这个对象分配到堆上。使用 -XX:+DoEscapeAnalysis 参数可以开启逃逸分析，逃逸分析现在是 JVM 的默认行为，这个参数可以忽略。JVM 判断新创建的对象是否逃逸的依据有： 对象被赋值给堆中对象的字段和类的静态变量； 对象被传进了不确定的代码中去运行。举个例子：代码 1：public Map fig() { Map map = new HashMap(); ... return}在代码 1 中，虽然 map 是一个局部变量，但是它通过 return 语句返回，其他外部方法可能会使用它，这就是方法逃逸。另外，如果被其他线程引用或者赋值，则成为线程逃逸。代码 2：public void fig() { Map map = new HashMap(); ...}代码 2，用完 Map 之后就直接销毁了，我们就可以说 map 对象没有逃逸。逃逸分析的好处： 同步省略，如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步； 栈上分配，如果一个对象在子程序中被分配，那么指向该对象的指针永远不会逃逸，对象有可能会被优化为栈分配。 分离对象或标量替换，有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分（或全部）可以不存储在内存，而是存储在 CPU 寄存器中。标量是指无法再分解的数据类型，比如原始数据类型及 reference 类型。再来看一下 JITWatch 对 synchronized 代码块的分析。根据提示，由于逃逸分析了解到新建的锁对象 Object 并没有逃逸出方法 cal，它将会在栈上直接分配。查看 C2 编译后的机器码，发现并没有同步代码相关的生成。这是因为 JIT 在分析之后，发现针对 new Object() 这个对象并没有发生线程竞争的情况，则会把这部分的同步直接给优化掉。我们在代码层次做了一些无用功，字节码无法发现它，而 JIT 智能地找到了它并进行了优化。因此，并不是所有的对象或者数组都会在堆上分配。由于 JIT 的存在，如果发现某些对象没有逃逸出方法，那么就有可能被优化成栈分配。intrinsic为什么 String 类的 indexOf 方法，比使用相同代码实现的方法，执行效率要高得多？在翻看 JDK 的源码时，能够看到很多地方使用了 HotSpotIntrinsicCandidate 注解。比如 StringBuffer 的 append 方法：@Override@HotSpotIntrinsicCandidatepublic synchronized StringBuffer append(char c) { toStringCache = null; super.append(c); return this;}被 @HotSpotIntrinsicCandidate 标注的方法，在 HotSpot 中都有一套高效的实现，该高效实现基于 CPU 指令，运行时，HotSpot 维护的高效实现会替代 JDK 的源码实现，从而获得更高的效率。上面的问题中，往下跟踪实现，可以发现 StringLatin1 类中的 indexOf 方法，同样适用了 HotSpotIntrinsicCandidate 注解，原因也就在于此。@HotSpotIntrinsicCandidatepublic static int indexOf(byte[] value, byte[] str) { if (str.length == 0) { return 0; } if (value.length == 0) { return -1; } return indexOf(value, value.length, str, str.length, 0);}@HotSpotIntrinsicCandidatepublic static int indexOf(byte[] value, int valueCount, byte[] str, int strCount, int fromIndex) { byte first = str[0];}JDK 中这种方法有接近 400 个，可以在 IDEA 中使用 Find Usages 找到它们。总结JIT 是现代 JVM 主要的优化点，能够显著地增加程序的执行效率，从解释执行到最高层次的 C2，一个数量级的性能提升也是有可能的。但即时编译的过程是非常缓慢的，耗时间也费空间，所以这些优化操作会和解释执行同时进行。一般，方法首先会被解释执行，然后被 3 层的 C1 编译，最后被 4 层的 C2 编译，这个过程也不是一蹴而就的。常用的优化手段有： 公共子表达式消除； 数组范围检查消除； 方法内联，通过将短小精悍的代码融入到调用方法的执行逻辑里，来减少方法调用上的开支； 逃逸分析等，通过分析变量的引用范围，对象可能会使用栈上分配的方式来减少 GC 的压力，或者使用标量替换来获取更多的优化。。这个过程的执行细节并不是那么“确定”，在不同的 JVM 中，甚至在不同的 HotSpot 版本中，效果也不尽相同。使用 JITWatch 工具，能够看到字节码和机器码的对应关系，以及执行过程中的一系列优化操作。这个工具的更多功能参考这个Wiki" }, { "title": "使用 Java Agent 修改字节码指令", "url": "/posts/java-agent-bytecode/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-21 13:23:00 +0000", "snippet": "Java 5 版本以后，JDK 有一个包叫做 instrument，可以实现一些非常酷的功能。一些 APM（Application Performance Management tools，最早是谷歌公开的论文提到的 Google Dapper） 工具，就是通过它来增强的。比如 Jrebel 酷炫的热部署功能（该工具能够明显的增加开发效率）。Java Agent这些工具的基础，就是 Java Agent 技术，可以利用它来构建一个附加的代理程序，用来协助检测性能，还可以替换一些现有功能，甚至修改 JDK 的一些类，有点像 JVM 级别的 AOP 功能。通常，Java 入口是一个 main 方法，而 Java Agent 的入口方法叫做 permain，表明实在 main 运行之前的一些操作。Java Agent 是一个这样的 Jar 包，定义了一个标准的入口方法，它不需要继承或者实现任何其他的类，属于无侵入的一种开发模式。 Java Agent 的入口方法为啥叫 permain，没有什么特别的理由，这个方法无论是第一次加载，还是每次加载新的 ClassLoader 加载，都会执行！！！可以在这个前置的方法里，对字节码进行一些修改，来增加功能或者改变代码的行为，这种方法没有侵入性，只需要在启动命令上加上 -javaagent 参数就可以了。Java 6 以后，可以通过 attach 的方式，动态的给运行中的程序设置加载代理类。其实，instrument 一共有两个 main 方法，一个是 premain，另一个是 agentmain，但在一个 JVM 中，只会调用一个；前者是 main 执行之前的修改，后者是控制类运行时的行为。它们区别是，agentmain 因为能够动态修改大部分代码，比较危险，限制会更大一些。用处获取统计信息在很多的 APM 产品里，比如 Pinponit、SKyWalking 等，是使用 Java Agent 对代码进行的增强！通过在方法执行前后动态加入的统计代码，来进行监控信息的收集；通过兼容 OpenTracing 协议，可以实现分布式链路的功能。原理类似于 AOP，最终以字节码的形式存在，性能的损失取决于自实现的代码逻辑。热部署通过自定义的 ClassLoader ，可以实现代码的热替换。使用 agentmain ，实现热部署功能会更加方便，通过 agentmain 获取到 Instrumentaion 以后，就可以对类进行动态重定义了。诊断配合 JVMTI 技术，可以 attach 到某个进程进行运行时的统计和调试，比如流行的 btrace 和 arthas，就是用的这种技术。练习构建一个 agent 程序，大体的步骤如下： 使用字节码增强工具，编写增强代码； 在 mainfest 中指定 Permain-Class/Agent-Class 属性； 使用参数加载或者使用 attacht 方式。编写 AgentJava Agent 最终的实现方式是一个 jar 包，使用 IDEA 创建一个默认的 Maven 工程就可以了。创建一个普通的 Java 类，添加 permain 或者 agentmain 方法，二者的参数完全一样。package com.happymaya.javaagent;import java.lang.instrument.Instrumentation;public class AgentApp { public static void premain(String agentOps, Instrumentation inst) { System.out.println(&quot;==============enter premain==============&quot;); System.out.println(agentOps); inst.addTransformer(new Agent()); } public static void agentmain(String agentOps, Instrumentation inst) { System.out.println(&quot;==============enter agentmain==============&quot;); }}编写 Transformer实际的代码逻辑需要实现 ClassFileTransformer 接口。假如要统计某个方法的执行时间，使用 JavaAssist 工具来增强字节码，通过以下代码实现： 获取 MainRun 类的字节码实例； 获取 hello 方法的字节码实例； 在方法前后，加入时间统计，首先定义变量 _begin ，然后追加要实现的代码由于借用的是 JavaAssist 完成字节码增强，因此不要忘记加入 Maven 依赖：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.javassist&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;javassist&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.24.1-GA&amp;lt;/version&amp;gt;&amp;lt;/dependency&amp;gt;字节码增强也可以使用 Cglib、ASM 等其他工具！！！MANIFEST.MF 文件编写的代码要让外界知晓，就是依靠 MANIFEST.MF 文件，具体路径在 src/main/resources/META-INF/MANIFEST.MF：Manifest-Version: 1.0Can-Redefine-Classes: trueCan-Retransform-Classes: truepremain-class: com.happymaya.javaagent.AgentAppagentmain-class: AgentAppmaven 打包会覆盖这个危机，因此需要为它指定一个：&amp;lt;build&amp;gt; &amp;lt;plugins&amp;gt; &amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;maven-jar-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;archive&amp;gt; &amp;lt;manifestFile&amp;gt;src/main/resources/META-INF/MANIFEST.MF&amp;lt;/manifestFile&amp;gt; &amp;lt;/archive&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;/plugin&amp;gt; &amp;lt;/plugins&amp;gt;&amp;lt;/build&amp;gt;然后，在命令行，执行 mvn install 安装到本地代码库，或者使用 mvn deploy 发布到私服上！MANIFEST.MF 完整参数清单：Premain-ClassAgent-ClassBoot-Class-PathCan-Redefine-ClassesCan-Retransform-ClassesCan-Set-Native-Method-Prefix使用使用方式取决于使用的是 permain 还是 agentmain，它们之前的区别，如下：premain直接在启动命令行中加入参数即可，在 jvm 启动时启动代理：java -javaagent:agent.jar MainRun在 IDEA 中，将参数添加到 jvm options 里。测试代码：package com.happymaya.javaagent.app;import java.util.HashMap;public class MainRun { public static void main(String[] args) { hello(&quot;world&quot;); } private static void hello(String name) { System.out.println(&quot;hello &quot; + name + new HashMap&amp;lt;&amp;gt;() + new java.lang.String(&quot;fixme&quot;).replace(&quot;i&quot;,&quot;a&quot;)); }}执行后，直接输出 hello world。通过增强以后，还额外的输出了执行时间，以及一些 debug 信息。其中，debug 信息在 main 方法执行之前输出。agentmain这种模式用在一些诊断工具上。使用 jdk/lib/tools.jar 中的工具类，可以动态的为运行中的程序加入一些功能。它的主要运行步骤如下： 获取机器上运行的所有 JVM 进程 ID； 选择要诊断的 jvm； 将 jvm 使用 attach 函数链接上； 使用 loadAgent 函数加载 agent，动态修改字节码； 卸载 jvm。代码如下：package com.happymaya.javaagent.app;import com.sun.tools.attach.VirtualMachine;import com.sun.tools.attach.VirtualMachineDescriptor;import java.util.List;public class JvmAttach { public static void main(String[] args) throws Exception { List&amp;lt;VirtualMachineDescriptor&amp;gt; list = VirtualMachine.list(); for (VirtualMachineDescriptor vmd : list) { if (vmd.displayName().endsWith(&quot;MainRun&quot;)) { VirtualMachine virtualMachine = VirtualMachine.attach(vmd.id()); virtualMachine.loadAgent(&quot;test.jar &quot;, &quot;...&quot;); //..... virtualMachine.detach(); } } }}这些代码虽然强大，但都是比较危险的。这也是 Btrace 这么多年不能流行的原因。相对而言，Arthas 显得友好并且安全得多！！！注意点：jar 包依赖方式 Agent 的 jar 包会以 fatjar 的方式提供，即将所有的依赖打包到一个大的 jar 包中。如果功能复杂、依赖多，这个 jar 包就会特别的大； 使用独立的 bom 文件维护这些依赖是另外一种方法。使用方自行管理依赖问题，但这通常会发生一些找不到 jar 包的错误，更糟糕的是，大多数在运行时才发现。类名称重复 不要使用和 jdk 及 instrument 包中相同的类名（包括包名），虽然有时能够侥幸过关，但也会陷入无法控制的异常中。做有限的功能 给系统动态的增加功能是非常酷的，但大多数情况下非常耗费性能。就像一些简单的诊断工具，会占用 1 核的 CPU，这是很平常的事情。ClassLoader 如果用的 JVM 比较旧，频繁地生成大量的代理类，会造成元空间的膨胀，容易发生内存占用问题； ClassLoader 有双亲委派机制，如果想要替换相应的类，一定要搞清楚它的类加载器应该用哪个，否则替换的类，是不生效的； 具体的调试方法，可以在 Java 进程启动时，加入 -verbose:class 参数，用来监视引用程序对类的加载。Arthas像 jstat 工具， jmap 等查看内存状态的工具 以及 jstack 这些工具，需要综合很多工具，对刚新手来说，很不友好。Arthas 就是使用 Java Agent 技术编写的一个工具，具体采用的方式，就是 attach 方式，它会无侵入的 attach 到具体的执行进程上，方便进行问题分析。可以像 debug 本地的 Java 代码一样，观测到方法执行的参数值，甚至做一些统计分析。这通常可以解决下面的问题： 哪个线程使用了最多的 CPU 运行中是否有死锁，是否有阻塞 如何监测一个方法哪里耗时最高 追加打印一些 debug 信息 监测 JVM 的实时运行状态Arthas 官方文档十分详细，地址是：https://arthas.aliyun.com/doc/。但无论工具如何强大，一些基础知识是需要牢固掌握的，否则，工具中出现的那些术语，也会让人一头雾水！！！！！工具常变，基础更加重要。还是要多花点时间在原始的排查方法上。" }, { "title": "字节码指令", "url": "/posts/Byte-code-instruction/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-20 13:23:00 +0000", "snippet": "通过 Java 语言中的一些常见特性，来看一下字节码的应用，由于 Java 特性非常多，仅总结一些经常遇到的特性。javap 是手中的利器，复杂的概念都可以在这里现出原形，并且对此产生深刻的印象。异常处理在 synchronized 生成的字节码中，其实包含两条 monitorexit 指令，是为了保证所有的异常条件，都能够退出。Java 字节码的异常处理机制，如下图所示：其中： Error 和 RuntimeException 是非检查型异常（Unchecked Exception），就是不需要 catch 语句去捕获的异常； 其他异常，则需要手动去处理。异常表在发生异常的时候，Java 就可以通过 Java 执行栈，来构造异常栈。回想一栈帧，获取这个异常栈只需要遍历一下它们就可以了。但是这种操作，比起常规操作，要昂贵的多。Java 的 Log 日志框架，通常会把所有错误信息打印到日志中，在异常非常多的情况下，会显著影响性能。看一生字节码： void doLock(); descriptor: ()V flags: Code: stack=2, locals=3, args_size=1 0: aload_0 1: getfield #7 // Field lock:Ljava/lang/Object; 4: dup 5: astore_1 6: monitorenter 7: getstatic #13 // Field java/lang/System.out:Ljava/io/PrintStream; 10: ldc #29 // String lock 12: invokevirtual #21 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 15: aload_1 16: monitorexit 17: goto 25 20: astore_2 21: aload_1 22: monitorexit 23: aload_2 24: athrow 25: return Exception table: from to target type 7 17 20 any 20 23 20 any 可以看到，编译后的字节码，带有一个叫 Exception table 的异常表，里面的每一行数据，都是一个异常处理器： from 指定字节码索引的开始位置; to 指定字节码索引的结束位置; target 异常处理的起始位置； type 异常类型也就是说，只要在 from 和 to 之间发生了异常，就会跳转到 target 所指定的位置。finally通们在做一些文件读取的时候，都会在 finally 代码块中关闭流，以避免内存的溢出。关于这个场景，分析一下下面这段代码的异常表。import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.io.InputStream;public class A { public void read() { InputStream in = null; try { in = new FileInputStream(&quot;A.java&quot;); } catch (FileNotFoundException e) { e.printStackTrace(); } finally { if (null != in) { try { in.close(); } catch (IOException e) { e.printStackTrace(); } } } }}上面的代码，捕获了一个 FileNotFoundException 异常，然后在 finally 中捕获了 IOException 异常。当分析字节码的时候，却发现了一个有意思的地方：IOException 足足出现了三次： Exception table: from to target type 17 21 24 Class java/io/IOException 2 12 32 Class java/io/FileNotFoundException 42 46 49 Class java/io/IOException 2 12 57 any 32 37 57 any 63 67 70 Class java/io/IOExceptionJava 编译器使用了一种比较傻的方式来组织 finally 的字节码，它分别在 try、catch 的正常执行路径上，复制一份 finally 代码，追加在 正常执行逻辑的后面；同时，再复制一份到其他异常执行逻辑的出口处。这也是下面这段方法不报错的原因，都可以在字节码中找到答案。public class B { public int read() { try { int a = 1 / 0; return a; } finally { return 1; } }}下面是上面程序的字节码，可以看到，异常之后，直接跳转到序号 8 了。 stack=2, locals=4, args_size=1 0: iconst_1 1: iconst_0 2: idiv 3: istore_1 4: iload_1 5: istore_2 6: iconst_1 7: ireturn 8: astore_3 9: iconst_1 10: ireturn Exception table: from to target type 0 6 8 any装箱拆箱在刚开始学习 Java 语言的的是，被自动装箱和拆箱搞得晕头转向。Java 中有 8 种基本类型，但鉴于 Java 面向对象的特点，它们同样有着对应的 8 个包装类型，比如 int 和 Integer，包装类型的值可以为 null，很多时候，它们都能够相互赋值。使用下面的代码从字节码层面上来观察一下：public class Box { public Integer cal() { Integer a = 1000; int b = a * 10; return b; }}上面是一段简单的代码，首先使用包装类型，构造了一个值为 1000 的数字，然后乘以 10 后返回，但是中间的计算过程，使用了普通类型 int。public java.lang.Integer cal(); descriptor: ()Ljava/lang/Integer; flags: ACC_PUBLIC Code: stack=2, locals=3, args_size=1 0: sipush 1000 3: invokestatic #7 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 6: astore_1 7: aload_1 8: invokevirtual #13 // Method java/lang/Integer.intValue:()I 11: bipush 10 13: imul 14: istore_2 15: iload_2 16: invokestatic #7 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 19: areturn通过观察字节码，发现赋值操作使用的是 Integer.valueOf 方法，在进行乘法运算的时候，调用了 Integer.intValue 方法来获取基本类型的值。在方法返回的时候，再次使用了 Integer.valueOf 方法对结果进行了包装。这就是 Java 中的自动装箱拆箱的底层实现。但这里有一个 Java 层面的陷阱问题，继续跟踪 Integer.valueOf 方法。 @HotSpotIntrinsicCandidate public static Integer valueOf(int i) { if (i &amp;gt;= IntegerCache.low &amp;amp;&amp;amp; i &amp;lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); }这个 IntegerCache，缓存了 low 和 high 之间的 Integer 对象，可以通过 -XX:AutoBoxCacheMax 来修改上限。下面是一道经典的面试题，请考虑一下运行代码后，会输出什么结果？public class BoxCacheError{ public static void main(String[] args) { Integer n1 = 123; Integer n2 = 123; Integer n3 = 128; Integer n4 = 128; System.out.println(n1 == n2); System.out.println(n3 == n4); }} 当我使用 java BoxCacheError 执行时，是 true,false； 当我加上参数 java -XX:AutoBoxCacheMax=256 BoxCacheError 执行时，结果是 true,ture，原因就在于此。 数组访问在访问一个数组长度的时候，直接使用它的属性 .length 就能获取，而在 Java 中却无法找到对于数组的定义。比如 int[] 这种类型，通过 getClass（getClass 是 Object 类中的方法）可以获取它的具体类型是 [I。其实，数组是 JVM 内置的一种对象类型，这个对象同样是继承的 Object 类。我们使用下面一段代码来观察一下数组的生成和访问。public class ArrayDemo { int getValue() { int[] arr = new int[]{ 1111, 2222, 3333, 4444 }; return arr[2]; } int getLength(int[] arr) { return arr.length; }}看一下 getValue 方法的字节码： int getValue(); descriptor: ()I flags: Code: stack=4, locals=2, args_size=1 0: iconst_4 1: newarray int 3: dup 4: iconst_0 5: sipush 1111 8: iastore 9: dup 10: iconst_1 11: sipush 2222 14: iastore 15: dup 16: iconst_2 17: sipush 3333 20: iastore 21: dup 22: iconst_3 23: sipush 4444 26: iastore 27: astore_1 28: aload_1 29: iconst_2 30: iaload 31: ireturn LineNumberTable: line 5: 0 line 8: 28可以看到，新建数组的代码，被编译成了 newarray 指令。数组里的初始内容，被顺序编译成了一系列指令放入： sipush 将一个短整型常量值推送至栈顶； iastore 将栈顶 int 型数值存入指定数组的指定索引位置。 为了支持多种类型，从操作数栈存储到数组，有更多的指令：bastore、castore、sastore、iastore、lastore、fastore、dastore、aastore。数组元素的访问，是通过第 28 ~ 30 行代码来实现的： aload_1 将第二个引用类型本地变量推送至栈顶，这里是生成的数组； iconst_2 将 int 型 2 推送至栈顶； iaload 将 int 型数组指定索引的值推送至栈顶。 值得注意的是，在这段代码运行期间，有可能会产生 ArrayIndexOutOfBoundsException，但由于它是一种非捕获型异常，我们不必为这种异常提供异常处理器。再看一下 getLength 的字节码，字节码如下： int getLength(int[]); descriptor: ([I)I flags: Code: stack=1, locals=2, args_size=2 0: aload_1 1: arraylength 2: ireturn LineNumberTable: line 12: 0可以看到，获取数组的长度，是由字节码指令 arraylength 来完成的。foreach无论是 Java 的数组，还是 List，都可以使用 foreach 语句进行遍历，比较典型的代码如下：import java.util.List;public class ForDemo { void loop(int[] arr) { for (int i : arr) { System.out.println(i); } } void loop(List&amp;lt;Integer&amp;gt; arr) { for (int i : arr) { System.out.println(i); } }}虽然在语言层面它们的表现形式是一致的，但实际实现的方法并不同。先看一下遍历数组的字节码： void loop(int[]); descriptor: ([I)V flags: Code: stack=2, locals=6, args_size=2 0: aload_1 1: astore_2 2: aload_2 3: arraylength 4: istore_3 5: iconst_0 6: istore 4 8: iload 4 10: iload_3 11: if_icmpge 34 14: aload_2 15: iload 4 17: iaload 18: istore 5 20: getstatic #7 // Field java/lang/System.out:Ljava/io/PrintStream; 23: iload 5 25: invokevirtual #13 // Method java/io/PrintStream.println:(I)V 28: iinc 4, 1 31: goto 8 34: return LineNumberTable: line 7: 0 line 8: 20 line 7: 28 line 10: 34 StackMapTable: number_of_entries = 2 frame_type = 254 /* append */ offset_delta = 8 locals = [ class &quot;[I&quot;, int, int ] frame_type = 248 /* chop */ offset_delta = 25可以很容易看到，它将代码解释成了传统的变量方式，即 for(int i;i&amp;lt;length;i++) 的形式。而 List 的字节码如下： void loop(java.util.List&amp;lt;java.lang.Integer&amp;gt;); descriptor: (Ljava/util/List;)V flags: Code: stack=2, locals=4, args_size=2 0: aload_1 1: invokeinterface #19, 1 // InterfaceMethod java/util/List.iterator:()Ljava/util/Iterator; 6: astore_2 7: aload_2 8: invokeinterface #25, 1 // InterfaceMethod java/util/Iterator.hasNext:()Z 13: ifeq 39 16: aload_2 17: invokeinterface #31, 1 // InterfaceMethod java/util/Iterator.next:()Ljava/lang/Object; 22: checkcast #35 // class java/lang/Integer 25: invokevirtual #37 // Method java/lang/Integer.intValue:()I 28: istore_3 29: getstatic #7 // Field java/lang/System.out:Ljava/io/PrintStream; 32: iload_3 33: invokevirtual #13 // Method java/io/PrintStream.println:(I)V 36: goto 7 39: return LineNumberTable: line 13: 0 line 14: 29 line 15: 36 line 16: 39 StackMapTable: number_of_entries = 2 frame_type = 252 /* append */ offset_delta = 7 locals = [ class java/util/Iterator ] frame_type = 250 /* chop */ offset_delta = 31 Signature: #52 // (Ljava/util/List&amp;lt;Ljava/lang/Integer;&amp;gt;;)V它实际是把 list 对象进行迭代并遍历的，在循环中，使用了 Iterator.next() 方法。使用 jd-gui 等反编译工具，可以看到实际生成的代码：void loop(List&amp;lt;Integer&amp;gt; paramList) { for (Iterator&amp;lt;Integer&amp;gt; iterator = paramList.iterator(); iterator.hasNext(); ) { int i = ((Integer)iterator.next()).intValue(); System.out.println(i); } }注解注解在 Java 中得到了广泛的应用，Spring 框架更是由于注解的存在而起死回生。注解在开发中的作用就是做数据约束和标准定义，可以将其理解成代码的规范标准，并帮助写出方便、快捷、简洁的代码。那么注解信息是存放在哪里的呢？使用两个 Java 文件来看一下其中的一种情况。MyAnnotation.javapublic @interface MyAnnotation {}AnnotationDemo@MyAnnotationpublic class AnnotationDemo { public void test(@MyAnnotation int a) {}}下字节码信息:{ public AnnotationDemo(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&amp;lt;init&amp;gt;&quot;:()V 4: return LineNumberTable: line 2: 0 public void test(int); descriptor: (I)V flags: ACC_PUBLIC Code: stack=0, locals=2, args_size=2 0: return LineNumberTable: line 6: 0 RuntimeInvisibleAnnotations: 0: #11() RuntimeInvisibleParameterAnnotations: 0: 0: #11()}SourceFile: &quot;AnnotationDemo.java&quot;RuntimeInvisibleAnnotations: 0: #11()可以看到，无论是类的注解，还是方法注解，都是由一个叫做 RuntimeInvisibleAnnotations 的结构来存储的，而参数的存储，是由 RuntimeInvisibleParameterAnotations 来保证的。总结所看到的字节码指令，可能洋洋洒洒几千行，看起来很吓人，但执行速度几乎都是纳秒级别的。Java 的无数框架，包括 JDK，也不会为了优化这种性能对代码进行限制。了解其原理，但不要舍本逐末，比如减少一次 Java 线程的上下文切换，就比你优化几千个装箱拆箱动作，来的更快捷一些。" }, { "title": "从字节码看并发编程的底层实现", "url": "/posts/bytecode-concurrency/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-19 13:23:00 +0000", "snippet": "如下图所示，Java 中的多线程： 第一类是 Thread 类。它有三种实现方式： 通过继承 Thread 覆盖它的 run 方法； 通过 Runnable 接口，实现它的 run 方法； 通过创建线程，就是通过线程池的方法去创建。 多线程除了增加任务的执行速度，同样也有共享变量的同步问题。传统的线程同步方式，是使用 synchronized 关键字，或者 wait、notify 方法等，使用 jstack 命令可以观测到各种线程的状态。在目前的并发编程中，使用 concurrent 包里的工具更多一些。线程模型JVM 的线程模型，以及它和操作系统进程之间的关系。如下图所示，对于 Hotspot 来说，每一个 Java 线程，都会映射到一条轻量级进程中（LWP，Light Weight Process）。轻量级进程是用户进程调用系统内核所提供的一套接口，实际上它还需要调用更加底层的内核线程（KLT，Kernel-Level Thread）。而具体的功能，比如创建、同步等，则需要进行系统调用。这些系统调用的操作，代价都比较高，需要在用户态（User Mode）和内核态（Kernel Mode）中来回切换，也就是常说的线程上下文切换（ CS，Context Switch）。使用 vmstat 命令能够方便地观测到这个数值。Java 在保证正确的前提下，要想高效并发，就要尽量减少上下文的切换。一般有下面几种做法来减少上下文的切换： CAS 算法，比如 Java 的 Atomic 类，如果使用 CAS 来更新数据，则不需要加锁； 减少锁粒度，多线程竞争会引起上下文的频繁切换，如果在处理数据的时候，能够将数据分段，即可减少竞争，Java 的 ConcurrentHashMap、LongAddr 等就是这样的思路； 协程，在单线程里实现多任务调度，并在单线程里支持多个任务之间的切换； 对加锁的对象进行智能判断，让操作更加轻量级。CAS 和无锁并发一般是建立在 concurrent 包里面的 AQS 模型之上，大多数属于 Java 语言层面上的知识点。本文在对其进行简单的描述后，会把重点放在普通锁的优化上。CASCAS（Compare And Swap，比较并替换）机制中使用了 3 个基本操作数： 内存地址 V； 旧的预期值 A ； 要修改的新值 B。更新一个变量时，只有当变量的预期值 A 和内存地址 V 当中的实际值相同时，才会将内存地址 V 对应的值修改为 B。如果修改不成功，CAS 将不断重试。拿 AtomicInteger 类来说，相关的代码如下：public final boolean compareAndSet(int expectedValue, int newValue) { return U.compareAndSetInt(this, VALUE, expectedValue, newValue);}可以看到，这个操作，是由 jdk.internal.misc.Unsafe 类进行操作的，而这是一个 native 方法：@HotSpotIntrinsicCandidatepublic final native boolean compareAndSetInt( Object o, long offset,int expected,int x);继续向下跟踪，在 Linux 机器上参照 os_cpu/linux_x86/atomic_linux_x86.hpp：template&amp;lt;&amp;gt;template&amp;lt;typename T&amp;gt;inline T Atomic::PlatformCmpxchg&amp;lt;4&amp;gt;::operator()( T exchange_value,T volatile* dest,T compare_value, atomic_memory_order /* order */) const { STATIC_ASSERT(4 == sizeof(T)); __asm__ volatile ( &quot;lock cmpxchgl %1,(%3)&quot; : &quot;=a&quot; (exchange_value) : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest) : &quot;cc&quot;, &quot;memory&quot;); return exchange_value;}可以看到，最底层的调用是汇编语言，而最重要的就是 cmpxchgl 指令，到这里没法再往下找代码了，也就是说 CAS 的原子性实际上是硬件 CPU 直接实现的。synchronized字节码synchronized 可以在是多线程中使用的最多的关键字了。先思考一个问题：在执行速度方面，是基于 CAS 的 Lock 效率高一些，还是同步关键字效率高一些？synchronized 关键字给代码或者方法上锁时，会有显示或者隐藏的上锁对象。当一个线程试图访问同步代码块时，它必须先得到锁，而在退出或抛出异常时必须释放锁： 给普通方法加锁时，上锁的对象是 this，如代码中的方法 m1 。 给静态方法加锁时，锁的是 class 对象，如代码中的方法 m2 。 给代码块加锁时，可以指定一个具体的对象。关于对象对锁的争夺，依然拿前面的一张图来看一下这个过程：下面看一段简单的代码，并观测一下它的字节码：public class SynchronizeDemo { synchronized void m1() { System.out.println(&quot;m1&quot;); } static synchronized void m2() { System.out.println(&quot;m2&quot;); } final Object lock = new Object(); void doLock() { synchronized (lock) { System.out.println(&quot;lock&quot;); } }}下面是普通方法 m1 的字节码： synchronized void m1(); descriptor: ()V flags: ACC_SYNCHRONIZED Code: stack=2, locals=1, args_size=1 0: getstatic #13 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #19 // String m1 5: invokevirtual #21 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 6: 0 line 7: static synchronized void m2(); descriptor: ()V flags: ACC_STATIC, ACC_SYNCHRONIZED Code: stack=2, locals=0, args_size=0 0: getstatic #13 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #27 // String m2 5: invokevirtual #21 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 10: 0 line 11: 8 void doLock(); descriptor: ()V flags: Code: stack=2, locals=3, args_size=1 0: aload_0 1: getfield #7 // Field lock:Ljava/lang/Object; 4: dup 5: astore_1 6: monitorenter 7: getstatic #13 // Field java/lang/System.out:Ljava/io/PrintStream; 10: ldc #29 // String lock 12: invokevirtual #21 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 15: aload_1 16: monitorexit 17: goto 25 20: astore_2 21: aload_1 22: monitorexit 23: aload_2 24: athrow 25: return Exception table: from to target type 7 17 20 any 20 23 20 any 可以看到，在字节码的体现上，它只给方法加了一个 flag：ACC_SYNCHRONIZED。静态方法 m2 和 m1 区别不大，只不过 flags 上多了一个参数：ACC_STATIC。相比较起来，doLock 方法就麻烦了一些，其中出现了 monitorenter 和 monitorexit 等字节码指令。很多人都认为，synchronized 是一种悲观锁、一种重量级锁；而基于 CAS 的 AQS 是一种乐观锁，这种理解并不全对。JDK1.6 之后，JVM 对同步关键字进行了很多的优化，这把锁有了不同的状态，大多数情况下的效率，已经和 concurrent 包下的 Lock 不相上下了，甚至更高。对象内存布局说到 synchronized 加锁原理，就不得不先说 Java 对象在内存中的布局，Java 对象内存布局如下图所示:各部分的含义： Mark Word：用来存储 hashCode、GC 分代年龄、锁类型标记、偏向锁线程 ID、CAS 锁指向线程 LockRecord 的指针等，synconized 锁的机制与这里密切相关，这有点像 TCP/IP 中的协议头； Class Pointer：用来存储对象指向它的类元数据指针、JVM 就是通过它来确定是哪个 Class 的实例； Instance Data：存储的是对象真正有效的信息，比如对象中所有字段的内容； Padding：HostSpot 规定对象的起始地址必须是 8 字节的整数倍，这是为了高效读取对象而做的一种“对齐”操作。可重入锁synchronized 是一把可重入锁。因此，在一个线程使用 synchronized 方法时可以调用该对象的另一个 synchronized 方法，即一个线程得到一个对象锁后再次请求该对象锁，是可以永远拿到锁的。Java 中线程获得对象锁的操作是以线程而不是以调用为单位的。synchronized 锁的对象头的 Mark Work 中会记录该锁的线程持有者和计数器。当一个线程请求成功后，JVM 会记下持有锁的线程，并将计数器计为 1 。此时如果有其他线程请求该锁，则必须等待。而该持有锁的线程如果再次请求这个锁，就可以再次拿到这个锁，同时计数器会递增。当线程退出一个 synchronized 方法/块时，计数器会递减，如果计数器为 0 则释放该锁。锁升级根据使用情况，锁升级大体可以按照下面的路径：偏向锁→轻量级锁→重量级锁，锁只能升级不能降级，所以一旦锁升级为重量级锁，就只能依靠操作系统进行调度。我们再看一下 Mark Word 的结构。其中，Biased 有 1 bit 大小，Tag 有 2 bit 大小，锁升级就是通过 Thread Id、Biased、Tag 这三个变量值来判断的。偏向锁偏向锁，其实是一把偏心锁（一般不这么描述）。在 JVM 中，当只有一个线程使用了锁的情况下，偏向锁才能够保证更高的效率。当第 1 个线程第一次访问同步块时，会先检测对象头 Mark Word 中的标志位（Tag）是否为 01，以此来判断此时对象锁是否处于无锁状态或者偏向锁状态（匿名偏向锁）。这也是锁默认的状态，线程一旦获取了这把锁，就会把自己的线程 ID 写到 Mark Word 中，在其他线程来获取这把锁之前，该线程都处于偏向锁状态。轻量级锁当下一个线程参与到偏向锁竞争时，会先判断 Mark Word 中保存的线程 ID 是否与这个线程 ID 相等，如果不相等，则会立即撤销偏向锁，升级为轻量级锁。轻量级锁的获取是怎么进行的呢？它们使用的是自旋方式。参与竞争的每个线程，会在自己的线程栈中生成一个 LockRecord ( LR )，然后每个线程通过 CAS（自旋）的操作将锁对象头中的 Mark Work 设置为指向自己的 LR 指针，哪个线程设置成功，就意味着哪个线程获得锁。在这种情况下，JVM 不会依赖内核进行线程调度。 LR 就是 LoackRecord，在当前线程栈上分配。对象可以有指针指向它。打开下面的文件： https://www.oracle.com/technetwork/java/biasedlocking-oopsla2006-wp-149958.pdf 可详细了解当锁处于轻量级锁的状态时，就不能够再通过简单的对比 Tag 值进行判断了，每次对锁的获取，都需要通过自旋的操作。当然，自旋也是面向不存在锁竞争的场景，比如一个线程运行完了，另外一个线程去获取这把锁。但如果自旋失败达到一定的次数（JVM 自动管理）时，就会膨胀为重量级锁。重量级锁重量级锁即为我们对 synchronized 的直观认识，在这种情况下，线程会挂起，进入到操作系统内核态，等待操作系统的调度，然后再映射回用户态。系统调用是昂贵的，重量级锁的名称也由此而来。如果系统的共享变量竞争非常激烈，那么锁会迅速膨胀到重量级锁，这些优化也就名存实亡了。如果并发非常严重，则可以通过参数 -XX:-UseBiasedLocking 禁用偏向锁。这种方法在理论上会有一些性能提升，但实际上并不确定。因为，synchronized 在 JDK，包括一些框架代码中的应用是非常广泛的。在一些不需要同步的场景中，即使加上了 synchronized 关键字，由于锁升级的原因，效率也不会太差。下面这张图展示了三种锁的状态和 Mark Word 值的变化：" }, { "title": "JMM 与 JVM", "url": "/posts/jmm/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-18 13:23:00 +0000", "snippet": "有一个问题经常被问到，那就是 Java 的内存模型，它已经成为了面试中的标配，是非常具有原理性的一个知识点。但是，有不少人把它和 JVM 的内存布局搞混了，以至于答非所问。这个现象在一些工作多年的程序员中非常普遍，主要是因为 JMM 与多线程有关，而且相对于底层而言，很多人平常的工作就是 CRUD，很难接触到这方面的知识。概念JVM 的内存布局，可以认为是 JVM 的数据存储模型；对于 JVM 运行时模型，还有一个和多线程相关，并且非常容易搞混的概念 —— Java 内存模型（JMM，Java Memory Model）JVM 内存布局中的 Java 虚拟机栈，是和线程相关的，字节码指令其实就是靠操作栈来完成的。用一小段代码，观察以下这个执行引擎的一些特点：import java.util.stream.IntStream;public class JMMDemo { int value = 0; void add() { value++; } public static void main(String[] args) throws InterruptedException { final int count = 100000; final JMMDemo demo = new JMMDemo(); Thread t1 = new Thread(() -&amp;gt; IntStream.range(0, count).forEach(i -&amp;gt; demo.add())); Thread t2 = new Thread(() -&amp;gt; IntStream.range(0, count).forEach(i -&amp;gt; demo.add())); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(demo.value); }}上面的代码没有任何同步块，每个线程单独运行后，都会对 value 加 10 万，但执行一次之后，大概率不会输出 20 万。深层次的原因，使用 javap 命令从字节码层面找一下： void add(); descriptor: ()V flags: Code: stack=3, locals=1, args_size=1 0: aload_0 1: dup 2: getfield #7 // Field value:I 5: iconst_1 6: iadd 7: putfield #7 // Field value:I 10: return LineNumberTable: line 10: 0 line 11: 10着重看一下 add 方法，可以看到一个简单的 i++ 操作，竟然有这么多的字节码，而它们都是傻乎乎按照“顺序执行”的。当它自己执行的时候不会有什么问题，但是如果放在多线程环境中，执行顺序就变得不可预料了。上图展示了这个乱序的过程。线程 A 和线程 B“并发”执行相同的代码块 add，执行的顺序如图中的标号，它们在线程中是有序的（1、2、5 或者 3、4、6），但整体顺序是不可预测的。线程 A 和 B 各自执行了一次加 1 操作，但在这种场景中，线程 B 的 putfield 指令直接覆盖了线程 A 的值，最终 value 的结果是 101。上面的示例仅仅是字节码层面上的，更加复杂的是，CPU 和内存之间同样存在一致性问题。很多人认为 CPU 是一个计算组件，并没有数据一致性的问题。但事实上，由于内存的发展速度跟不上 CPU 的更新，在 CPU 和内存之间，存在着多层的高速缓存。原因就是由于多核所引起的，这些高速缓存，往往会有多层。如果一个线程的时间片跨越了多个 CPU，那么同样存在同步的问题。另外，在执行过程中，CPU 可能会对输入的代码进行乱序执行优化，Java 虚拟机的即时编译器也有类似的指令重排序优化。整个函数的执行步骤就分的更加细致，看起来非常的碎片化（比字节码指令要细很多）。不管是字节码的原因，还是硬件的原因，在粗粒度上简化来看，比较浅显且明显的因素，那就是线程 add 方法的操作并不是原子性的。为了解决这个问题，可以在 add 方法上添加 synchronized 关键字，它不仅保证了内存上的同步，而且还保证了 CPU 的同步。这个时候，各个线程只能排队进入 add 方法，也能够得到期望的结果 102。synchronized void add() { value++;}到这里，Java 的内存模型就呼之欲出了。==JMM 是一个抽象的概念，它描述了一系列的规则或者规范，用来解决多线程的共享变量问题，比如 volatile、synchronized 等关键字就是围绕 JMM 的语法。这里所说的变量，包括实例字段、静态字段，但不包括局部变量和方法参数，因为后者是线程私有的，不存在竞争问题。==JVM 试图定义一种统一的内存模型，能将各种底层硬件，以及操作系统的内存访问差异进行封装，使 Java 程序在不同硬件及操作系统上都能达到相同的并发效果。JMM 的结构JMM 分为主存储器（Main Memory）和工作存储器（Working Memory）两种。 主存储器是实例位置所在的区域，所有的实例都存在于主存储器内。比如，实例所拥有的字段即位于主存储器内，主存储器是所有的线程所共享的。 工作存储器是线程所拥有的作业区，每个线程都有其专用的工作存储器。工作存储器存有主存储器中必要部分的拷贝，称之为工作拷贝（Working Copy）。在这个模型中，线程无法对主存储器直接进行操作。如下图，线程 A 想要和线程 B 通信，只能通过主存进行交换。那这些内存区域都是在哪存储的呢？如果非要有个对应的话，你可以认为主存中的内容是 Java 堆中的对象，而工作内存对应的是虚拟机栈中的内容。但实际上，主内存也可能存在于高速缓存，或者 CPU 的寄存器上；工作内存也可能存在于硬件内存中，我们不用太纠结具体的存储位置。8 个 Action操作类型为了支持 JMM，Java 定义了 8 种原子操作（Action），用来控制主存与工作内存之间的交互: read（读取）作用于主内存，它把变量从主内存传动到线程的工作内存中，供后面的 load 动作使用。 load（载入）作用于工作内存，它把 read 操作的值放入到工作内存中的变量副本中; store（存储）作用于工作内存，它把工作内存中的一个变量传送给主内存中，以备随后的 write 操作使用; write （写入）作用于主内存，它把 store 传送值放到主内存中的变量中; use（使用）作用于工作内存，它把工作内存中的值传递给执行引擎，每当虚拟机遇到一个需要使用这个变量的指令时，将会执行这个动作; assign（赋值）作用于工作内存，它把从执行引擎获取的值赋值给工作内存中的变量，每当虚拟机遇到一个给变量赋值的指令时，执行该操作; lock（锁定）作用于主内存，把变量标记为线程独占状态； unlock（解锁）作用于主内存，它将释放独占状态。如上图所示，把一个变量从主内存复制到工作内存，就要顺序执行 read 和 load；而把变量从工作内存同步回主内存，就要顺序执行 store 和 write 操作。三大特征 原子性 JMM 保证了 read、load、assign、use、store 和 write 六个操作具有原子性，可以认为除了 long 和 double 类型以外，对其他基本数据类型所对应的内存单元的访问读写都是原子的。 如果想要一个颗粒度更大的原子性保证，就可以使用 lock 和 unlock 这两个操作。 可见性 可见性是指当一个线程修改了共享变量的值，其他线程也能立即感知到这种变化。 从前面的图中可以看到，要保证这种效果，需要经历多次操作。一个线程对变量的修改，需要先同步给主内存，赶在另外一个线程的读取之前刷新变量值。 volatile、synchronized、final 和锁，都是保证可见性的方式。 着重提一下 volatile，因为它的特点最显著。使用了 volatile 关键字的变量，每当变量的值有变动时，都会把更改立即同步到主内存中；而如果某个线程想要使用这个变量，则先要从主存中刷新到工作内存上，这样就确保了变量的可见性。 而锁和同步关键字就比较好理解一些，它是把更多个操作强制转化为原子化的过程。由于只有一把锁，变量的可见性就更容易保证。 有序性 Java 程序很有意思，从上面的 add 操作可以看出，如果在线程中观察，则所有的操作都是有序的；而如果在另一个线程中观察，则所有的操作都是无序的。 除了多线程这种无序性的观测，无序的产生还来源于指令重排。 指令重排序是 JVM 为了优化指令，来提高程序运行效率的，在不影响单线程程序执行结果的前提下，按照一定的规则进行指令优化。在某些情况下，这种优化会带来一些执行的逻辑问题，在并发执行的情况下，按照不同的逻辑会得到不同的结果。 可以看一下 Java 语言中默认的一些“有序”行为，也就是先行发生（happens-before）原则，这些可能在写代码的时候没有感知，因为它是一种默认行为。 先行发生是一个非常重要的概念，如果操作 A 先行发生于操作 B，那么操作 A 产生的影响能够被操作 B 感知到。 下面的原则是《Java 并发编程实践》这本书中对一些法则的描述： 程序次序：一个线程内，按照代码顺序，写在前面的操作先行发生于写在后面的操作； 监视器锁定：unLock 操作先行发生于后面对同一个锁的 lock 操作； volatile：对一个变量的写操作先行发生于后面对这个变量的读操作； 传递规则：如果操作 A 先行发生于操作 B，而操作 B 又先行发生于操作 C，则可以得出操作 A 先行发生于操作 C； 线程启动：对线程 start() 的操作先行发生于线程内的任何操作； 线程中断：对线程 interrupt() 的调用先行发生于线程代码中检测到中断事件的发生，可以通过 Thread.interrupted() 方法检测是否发生中断； 线程终结规则：线程中的所有操作先行发生于检测到线程终止，可以通过 Thread.join()、Thread.isAlive() 的返回值检测线程是否已经终止； 对象终结规则：一个对象的初始化完成先行发生于它的 finalize() 方法的开始。 内存屏障上面提到这么多规则和特性，是靠什么保证的呢？内存屏障（Memory Barrier）用于控制在特定条件下的重排序和内存可见性问题。JMM 内存屏障可分为： 读屏障 写屏障。Java 的内存屏障实际上也是上述两种的组合，完成一系列的屏障和数据同步功能。Java 编译器在生成字节码时，会在执行指令序列的适当位置插入内存屏障来限制处理器的重排序。Load-Load Barriers保证 load1 数据的装载优先于 load2 以及所有后续装载指令的装载。对于 Load Barrier 来说，在指令前插入 Load Barrier，可以让高速缓存中的数据失效，强制重新从主内存加载数据。load1LoadLoadload2Load-Store Barriers保证 load1 数据装载优先于 store2 以及后续的存储指令刷新到内存。store1StoreStorestoreStore-Load Barriers在 Load2 及后续所有读取操作执行前，保证 Store1 的写入对所有处理器可见。这条内存屏障指令是一个全能型的屏障，它同时具有其他 3 条屏障的效果，而且它的开销也是四种屏障中最大的一个。store1StoreLoadload2总结JMM 可以说是 Java 并发的基础，它的定义将直接影响多线程实现的机制。 ps：JMM 保证了 read、load、assign、use、store 和 write 六个操作具有原子性，可以认为除了 long 和 double 类型以外，对其他基本数据类型所对应的内存单元的访问读写都是原子的。long 和double 没有原子性？ 目前大多数机器是64位的，可以认为是原子的。这是因为，在32位操作系统上对64位的数据的读写要分两步完成，每一步取32位数据。随着时间推移，这种知识点会越来越冷。https://www.infoq.cn/article/java-memory-model-1" }, { "title": "从字节码看方法调用的底层实现", "url": "/posts/bytecode-methodcall/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-17 13:23:00 +0000", "snippet": "字节码结构基本结构class 文件结构的资料，大体结构如下： magic：魔法数，用于标识当前 class 的文件格式，JVM 可据此判断该文件是否可以被解析，目前固定为 0xCAFEBABE； minor_version：副版本号； major_version：主版本号； 这两个版本号用来标识编译时的 JDK 版本，常见的一个异常比如 Unsupported major.minor version 52.0 就是因为运行时的 JDK 版本低于编译时的 JDK 版本（52 是 Java 8 的主版本号）； constant_pool_count：常量池计数器，等于常量池中的成员数加 1； constant_pool：常量池，是一种表结构，包含 class 文件结构和子结构中引用的所有字符串常量，类或者接口名，字段名和其他常量； access_flags：表示某个类或者接口的访问权限和属性； this_class：类索引，该值必须是对常量池中某个常量的一个有效索引值，该索引处的成员必须是一个 CONSTANT_Class_info 类型的结构体，表示这个 class 文件所定义的类和接口； super_class：父类索引； interfaces_count：接口计数器，表示当前类或者接口直接继承接口的数量； interfaces：接口表，是一个表结构，成员同 this_class，是对常量池中 CONSTANT_Class_info 类型的一个有效索引值； fields_count：字段计数器，当前 class 文件所有字段的数量； fields：字段表，是一个表结构，表中每个成员必须是 filed_info 数据结构，用于表示当前类或者接口的某个字段的完整描述，但它不包含从父类或者父接口继承的字段； methods_count：方法计数器，表示当前类方法表的成员个数； methods：方法表，是一个表结构，表中每个成员必须是 method_info 数据结构，用于表示当前类或者接口的某个方法的完整描述； attributes_count：属性计数器，表示当前 class 文件 attributes 属性表的成员个数； attributes：属性表，是一个表结构，表中每个成员必须是 attribute_info 数据结构，这里的属性是对 class 文件本身，方法或者字段的补充描述，比如 SourceFile 属性用于表示 class 文件的源代码文件名。 实际观测使用一个小工具 asmtools ，学习字节码会节省很多的时间。提前准备好的 jar 包地址：执行下面的命令，将看到类的 JCOD 语法（整个 .class 用容器的方式来表示，可以很清楚表示类文件的结构）结果：java -jar asmtools-7.0.jar jdec LambdaDemo.class输出的结果类似于下面的结构，与字节码组成是一一对应的。class LambdaDemo {  0xCAFEBABE;  0; // minor version  52; // version  [] { // Constant Pool    ; // first element is empty    Method #8 #25; // #1    InvokeDynamic 0s #30; // #2    InterfaceMethod #31 #32; // #3    Field #33 #34; // #4    String #35; // #5    Method #36 #37; // #6    class #38; // #7    class #39; // #8    Utf8 &quot;&amp;lt;init&amp;gt;&quot;; // #9    Utf8 &quot;()V&quot;; // #10    Utf8 &quot;Code&quot;; // #11了解了类的文件组织方式，下面我们来看一下，类文件在加载到内存中以后，是一个怎样的表现形式。内存表示准备以下代码，使用 javac -g InvokeDemo.java 进行编译，然后使用 java 命令执行。程序将阻塞在 sleep 函数上，来看一下它的内存分布：interface I { default void infMethod() { } void inf();}abstract class Abs { abstract void abs();}public class InvokeDemo extends Abs implements I { static void staticMethod() { } private void privateMethod() { } public void publicMethod() { } @Override public void inf() { } @Override void abs() { } public static void main(String[] args) throws Exception{ InvokeDemo demo = new InvokeDemo(); InvokeDemo.staticMethod(); demo.abs(); ((Abs) demo).abs(); demo.inf(); ((I) demo).inf(); demo.privateMethod(); demo.publicMethod(); demo.infMethod(); ((I) demo).infMethod(); Thread.sleep(Integer.MAX_VALUE); }}为了更加明显的看到这个过程，使用 jhsdb 工具，这是在 Java 9 之后 JDK 先加入的调试工具，可以在命令行中使用 jhsdb hsdb 来启动它。注意，要加载相应的进程时，必须确保是同一个版本的应用进程，否则会产生报错。attach 启动 Java 进程后，可以在 Class Browser 菜单中查看加载的所有类信息。我们在搜索框中输入 InvokeDemo，找到要查看的类。@ 符号后面的，就是具体的内存地址，我们可以复制一个，然后在 Inspector 视图中查看具体的属性，可以大体认为这就是类在方法区的具体存储。在 Inspector 视图中，我们找到方法相关的属性 _methods，可惜它无法点开，也无法查看。接下来使用命令行来检查这个数组里面的值。打开菜单中的 Console，然后输入 examine 命令，可以看到这个数组里的内容，对应的地址就是 Class 视图中的方法地址。examine 0x000000010e650570/10可以在 Inspect 视图中看到方法所对应的内存信息，这确实是一个 Method 方法的表示。相比较起来，对象就简单了，它只需要保存一个到达 Class 对象的指针即可。我们需要先从对象视图中进入，然后找到它，一步步进入 Inspect 视图。由以上的这些分析，可以得出下面这张图。执行引擎想要运行某个对象的方法，需要先在栈上找到这个对象的引用，然后再通过对象的指针，找到相应的方法字节码。方法调用指令关于方法的调用，Java 共提供了 5 个指令，来调用不同类型的函数： invokestatic 用来调用静态方法； invokevirtual 用于调用非私有实例方法，比如 public 和 protected，大多数方法调用属于这一种； invokeinterface 和上面这条指令类似，不过作用于接口类； invokespecial 用于调用私有实例方法、构造器及 super 关键字等； invokedynamic 用于调用动态方法。依然使用上面的代码片段来看一下前四个指令的使用场景。代码中包含一个接口 I、一个抽象类 Abs、一个实现和继承了两者类的 InvokeDemo。回想一下类加载机制，在 class 文件被加载到方法区以后，就完成了从符号引用到具体地址的转换过程。看一下编译后的 main 方法字节码，尤其需要注意的是对于接口方法的调用。使用实例对象直接调用，和强制转化成接口调用，所调用的字节码指令分别是 invokevirtual 和 invokeinterface，它们是有所不同的。public static void main(java.lang.String[]);    descriptor: ([Ljava/lang/String;)V    flags: ACC_PUBLIC, ACC_STATIC    Code:      stack=2, locals=2, args_size=1         0: new           #2                  // class InvokeDemo         3: dup         4: invokespecial #3                  // Method &quot;&amp;lt;init&amp;gt;&quot;:()V         7: astore_1         8: invokestatic  #4                  // Method staticMethod:()V        11: aload_1        12: invokevirtual #5                  // Method abs:()V        15: aload_1        16: invokevirtual #6                  // Method Abs.abs:()V        19: aload_1        20: invokevirtual #7                  // Method inf:()V        23: aload_1        24: invokeinterface #8,  1            // InterfaceMethod I.inf:()V        29: aload_1        30: invokespecial #9                  // Method privateMethod:()V        33: aload_1        34: invokevirtual #10                 // Method publicMethod:()V        37: aload_1        38: invokevirtual #11                 // Method infMethod:()V        41: aload_1        42: invokeinterface #12,  1           // InterfaceMethod I.infMethod:()V        47: return另外还有一点，和想象中的不同，大多数普通方法调用，使用的是 invokevirtual 指令，它其实和 invokeinterface 是一类的，都属于虚方法调用。很多时候，JVM 需要根据调用者的动态类型，来确定调用的目标方法，这就是动态绑定的过程。invokevirtual 指令有多态查找的机制，该指令运行时，解析过程如下： 找到操作数栈顶的第一个元素所指向的对象实际类型，记做 c； 如果在类型 c 中找到与常量中的描述符和简单名称都相符的方法，则进行访问权限校验，如果通过则返回这个方法直接引用，查找过程结束，不通过则返回 java.lang.IllegalAccessError； 否则，按照继承关系从下往上依次对 c 的各个父类进行第二步的搜索和验证过程； 如果始终没找到合适的方法，则抛出 java.lang.AbstractMethodError 异常，这就是 Java 语言中方法重写的本质。相对比，invokestatic 指令加上 invokespecial 指令，就属于静态绑定过程所以静态绑定，指的是能够直接识别目标方法的情况，而动态绑定指的是需要在运行过程中根据调用者的类型来确定目标方法的情况。可以想象，相对于静态绑定的方法调用来说，动态绑定的调用会更加耗时一些。由于方法的调用非常的频繁，JVM 对动态调用的代码进行了比较多的优化，比如使用方法表来加快对具体方法的寻址，以及使用更快的缓冲区来直接寻址（ 内联缓存）。invokedynamic有时候在写一些 Python 脚本或者 JS 脚本时，特别羡慕这些动态语言。如果把查找目标方法的决定权，从虚拟机转嫁给用户代码，就会有更高的自由度。之所以单独把 invokedynamic 抽离出来介绍，是因为它比较复杂。和反射类似，它用于一些动态的调用场景，但它和反射有着本质的不同，效率也比反射要高得多。这个指令通常在 Lambda 语法中出现，来看一下一小段代码：public class LambdaDemo { public static void main(String[] args) { Runnable r = () -&amp;gt; System.out.println(&quot;Hello Lambda&quot;); r.run(); }}使用 javap -p -v 命令可以在 main 方法中看到 invokedynamic 指令： public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=1, locals=2, args_size=1 0: invokedynamic #7, 0 // InvokeDynamic #0:run:()Ljava/lang/Runnable; 5: astore_1 6: aload_1 7: invokeinterface #11, 1 // InterfaceMethod java/lang/Runnable.run:()V 12: return另外，在 javap 的输出中找到了一些奇怪的东西：BootstrapMethods: 0: #39 invokestatic java/lang/invoke/LambdaMetafactory.metafactory:(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodHandle;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite; Method arguments: #46 ()V #47 invokestatic jvm/LambdaDemo.lambda$main$0:()V #46 ()VBootstrapMethods 属性在 Java 1.7 以后才有，位于类文件的属性列表中，这个属性用于保存 invokedynamic 指令引用的引导方法限定符。和上面介绍的四个指令不同，invokedynamic 并没有确切的接受对象，取而代之的，是一个叫 CallSite 的对象。其实，invokedynamic 指令的底层，是使用方法句柄（MethodHandle）来实现的。方法句柄是一个能够被执行的引用，它可以指向静态方法和实例方法，以及虚构的 get 和 set 方法，从 IDE 中可以看到这些函数。句柄类型（MethodType）是对方法的具体描述，配合方法名称，能够定位到一类函数。访问方法句柄和调用原来的指令基本一致，但它的调用异常，包括一些权限检查，在运行时才能被发现。下面这段代码，可以完成一些动态语言的特性，通过方法名称和传入的对象主体，进行不同的调用，而 Bike 和 Man 类，可以没有任何关系。import java.lang.invoke.MethodHandle;import java.lang.invoke.MethodHandles;import java.lang.invoke.MethodType;public class MethodHandleDemo { static class Bike { String sound() { return &quot;ding ding&quot;; } } static class Animal { String sound() { return &quot;wow wow&quot;; } } static class Man extends Animal { @Override String sound() { return &quot;hou hou&quot;; } } String sound(Object o) throws Throwable { MethodHandles.Lookup lookup = MethodHandles.lookup(); MethodType methodType = MethodType.methodType(String.class); MethodHandle methodHandle = lookup.findVirtual(o.getClass(), &quot;sound&quot;, methodType); String obj = (String) methodHandle.invoke(o); return obj; } public static void main(String[] args) throws Throwable { String str = new MethodHandleDemo().sound(new Bike()); System.out.println(str); str = new MethodHandleDemo().sound(new Animal()); System.out.println(str); str = new MethodHandleDemo().sound(new Man()); System.out.println(str); }}可以看到 Lambda 语言实际上是通过方法句柄来完成的，在调用链上自然也多了一些调用步骤，那么在性能上，是否就意味着 Lambda 性能低呢？对于大部分“非捕获”的 Lambda 表达式来说，JIT 编译器的逃逸分析能够优化这部分差异，性能和传统方式无异；但对于“捕获型”的表达式来说，则需要通过方法句柄，不断地生成适配器，性能自然就低了很多（不过和便捷性相比，一丁点性能损失是可接受的）。除了 Lambda 表达式，还没有其他的方式来产生 invokedynamic 指令。但可以使用一些外部的字节码修改工具，比如 ASM，来生成一些带有这个指令的字节码，这通常能够完成一些非常酷的功能，比如完成一门弱类型检查的 JVM-Base 语言。" }, { "title": "分库分表后应用崩溃后的总结", "url": "/posts/subdb-optimization/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-16 13:23:00 +0000", "snippet": "假设有一个用户表，想要通过用户名来查询某个用户，一句简单的 SQL 语句即可：select * from user where fullname = &quot;xxx&quot; and other=&quot;other&quot;;为了达到动态拼接的效果，这句 SQL 语句被一位同事进行了如下修改。他的本意是，当 fullname 或者 other 传入为空的时候，动态去掉这些查询条件。这种写法，在 MyBaits 的配置文件中，也非常常见。List&amp;lt;User&amp;gt; query(String fullname, String other) { StringBuilder sb = new StringBuilder(&quot;select * from user where 1=1 &quot;); if (!StringUtils.isEmpty(fullname)) { sb.append(&quot; and fullname=&quot;); sb.append(&quot; \\&quot;&quot; + fullname + &quot;\\&quot;&quot;); } if (!StringUtils.isEmpty(other)) { sb.append(&quot; and other=&quot;); sb.append(&quot; \\&quot;&quot; + other + &quot;\\&quot;&quot;); } String sql = sb.toString(); ...}大多数情况下，这种写法是没有问题的，因为结果集合是可以控制的。但随着系统的运行，用户表的记录越来越多，当传入的 fullname 和 other 全部为空时，悲剧的事情发生了，SQL 被拼接成了如下的语句：select * from user where 1=1数据库中的所有记录，都会被查询出来，载入到 JVM 的内存中。由于数据库记录实在太多，直接把内存给撑爆了。在工作中，由于这种原因引起的内存溢出，发生的频率非常高。通常的解决方式是强行加入分页功能，或者对一些必填的参数进行校验，但不总是有效。因为上面的示例仅展示了一个非常简单的 SQL 语句，而在实际工作中，这个 SQL 语句会非常长，每个条件对结果集的影响也会非常大，在进行数据筛选的时候，一定要小心。内存使用问题拿一个最简单的 Spring Boot 应用来说： 请求会通过 Controller 层来接收数据， 然后 Service 层会进行一些逻辑的封装； 数据通过 Dao 层的 ORM 比如 JPA 或者 MyBatis 等，来调用底层的 JDBC 接口进行实际的数据获取。通常情况下，JVM 对这种数据获取方式，表现都是非常温和的。挨个看一下每一层可能出现的一些不正常的内存使用问题（仅限 JVM 相关问题），以便对平常工作中的性能分析和性能优化有一个整体的思路。首先，提到一种可能，那就是类似于 Fastjson 工具所产生的 bug，这类问题只能通过升级依赖的包来解决，属于一种极端案例。具体可参考这里Controller 层Controller 层用于接收前端查询参数，然后构造查询结果。现在很多项目都采用前后端分离架构，所以 Controller 层的方法，一般使用 @ResponseBody 注解，把查询的结果，解析成 JSON 数据返回。这在数据集非常大的情况下，会占用很多内存资源。假如结果集在解析成 JSON 之前，占用的内存是 10MB，那么在解析过程中，有可能会使用 20M 或者更多的内存去做这个工作。如果结果集有非常深的嵌套层次，或者引用了另外一个占用内存很大，且对于本次请求无意义的对象（比如非常大的 byte[] 对象），那这些序列化工具会让问题变得更加严重。因此，对于一般的服务，保持结果集的精简，是非常有必要的，这也是 DTO（Data Transfer Object）存在的必要。如果你的项目，返回的结果结构比较复杂，对结果集进行一次转换是非常有必要的。互联网环境不怕小结果集的高并发请求，却非常恐惧大结果集的耗时请求，这是其中一方面的原因。Service 层Service 层用于处理具体的业务，更加贴合业务的功能需求。一个 Service，可能会被多个 Controller 层所使用，也可能会使用多个 dao 结构的查询结果进行计算、拼装。Service 的问题主要是对底层资源的不合理使用。举个例子，有一回在一次代码 review 中，发现了下面让人无语的逻辑：// 错误代码示例int getUserSize() { List&amp;lt;User&amp;gt; users = dao.getAllUser(); return null == users ? 0 : users.sizes();}这种代码，其实在一些现存的项目里大量存在，只不过由于项目规模和工期的原因，被隐藏了起来，成为内存问题的定时炸弹。Service 层的另外一个问题就是，职责不清、代码混乱，以至于在发生故障的时候，让人无从下手。这种情况就更加常见了，比如使用了 Map 作为函数的入参，或者把多个接口的请求返回放在一个 Java 类中。// 错误代码示例Object exec(Map&amp;lt;String, Object&amp;gt; params){ String q = getString(params, &quot;q&quot;); if(q.equals(&quot;insertToa&quot;)){ String q1 = getString(params, &quot;q1&quot;); String q2 = getString(params, &quot;q2&quot;); // do A } else if(q.equals(&quot;getResources&quot;)){ String q3 = getString(params,&quot;q3&quot;); // do B } ... return null;}这种代码使用了万能参数和万能返回值，exec 函数会被几十个上百个接口调用，进行逻辑的分发。这种将逻辑揉在一起的代码块，当发生问题时，即使使用了 Jstack，也无法发现具体的调用关系，在平常的开发中，应该严格禁止。ORM 层ORM 层可能是发生内存问题最多的地方，除了开始提到的 SQL 拼接问题，大多数是由于对这些 ORM 工具使用不当而引起的。举个例子： 在 JPA 中，如果加了一对多或者多对多的映射关系，而又没有开启懒加载、级联查询的时候就容易造成深层次的检索，内存的开销就超出了我们的期望，造成过度使用； 另外，JPA 可以通过使用缓存来减少 SQL 的查询，它默认开启了一级缓存，也就是 EntityManager 层的缓存（会话或事务缓存），如果你的事务非常的大，它会缓存很多不需要的数据； JPA 还可以通过一定的配置来完成二级缓存，也就是全局缓存，造成更多的内存占用； 一般，项目中用到缓存的地方，要特别小心。除了容易造成数据不一致之外，对堆内内存的使用也要格外关注。如果使用量过多，很容易造成频繁 GC，甚至内存溢出。 JPA 比起 MyBatis 等 ORM 拥有更多的特性，看起来容易使用，但精通门槛却比较高。这并不代表 MyBatis 就没有内存问题，在这些 ORM 框架之中，存在着非常多的类型转换、数据拷贝。举个例子： 有一个批量导入服务，在 MyBatis 执行批量插入的时候，竟然产生了内存溢出，按道理这种插入操作是不会引起额外内存占用的，最后通过源码追踪到了问题； 这是因为 MyBatis 循环处理 batch 的时候，操作对象是数组，而在接口定义的时候，使用的是 List；当传入一个非常大的 List 时，它需要调用 List 的 toArray 方法将列表转换成数组（浅拷贝）； 在最后的拼装阶段，使用了 StringBuilder 来拼接最终的 SQL，所以实际使用的内存要比 List 多很多。事实证明，不论是插入操作还是查询动作，只要涉及的数据集非常大，就容易出现问题。由于项目中众多框架的引入，想要分析这些具体的内存占用，就变得非常困难。保持小批量操作和结果集的干净，是一个非常好的习惯。分库分表内存溢出分库分表组件如果数据库的记录非常多，达到千万或者亿级别，对于一个传统的 RDBMS 来说，最通用的解决方式就是分库分表。这也是海量数据的互联网公司必须面临的一个问题。根据切入的层次，数据库中间件一般分为： 编码层； 框架层； 驱动层； 代理层； 实现层 5 大类。典型的框架有驱动层的 sharding-jdbc 和代理层的 MyCat。 MyCat 是一个独立部署的 Java 服务，它模拟了一个 MySQL 进行请求的处理，对于应用来说使用是透明的； 而 sharding-jdbc 实际上是一个数据库驱动，或者说是一个 DataSource，它是作为 jar 包直接嵌入在客户端应用的，所以它的行为会直接影响到主应用。这里所要说的分库分表组件，就是 sharding-jdbc。不管是普通 Spring 环境，还是 Spring Boot 环境，经过一系列配置之后，都可以像下面这种方式来使用 sharding-jdbc，应用层并不知晓底层实现的细节：@Autowiredprivate DataSource dataSource;我们有一个线上订单应用，由于数据量过多的原因，进行了分库分表。但是在某些条件下，却经常发生内存溢出。分库分表的内存溢出一个最典型的内存溢出场景，就是在订单查询中使用了深分页，并且在查询的时候没有使用“切分键”。使用前面介绍的一些工具，比如 MAT、Jstack，最终追踪到是由于 sharding-jdbc 内部实现所引起的。这个过程也是比较好理解的，如图所示，订单数据被存放在两个库中。如果没有提供切分键，查询语句就会被分发到所有的数据库中，这里的查询语句是 limit 10、offset 1000，最终结果只需要返回 10 条记录，但是数据库中间件要完成这种计算，则需要 (1000+10)*2=2020 条记录来完成这个计算过程。如果 offset 的值过大，使用的内存就会暴涨。虽然 sharding-jdbc 使用归并算法进行了一些优化，但在实际场景中，深分页仍然引起了内存和性能问题。下面这一句简单的 SQL 语句，会产生严重的后果：select * from order order by updateTime desc limit 10 offset 1000;这种在中间节点进行归并聚合的操作，在分布式框架中非常常见。比如在 ElasticSearch 中，就存在相似的数据获取逻辑，不加限制的深分页，同样会造成 ES 的内存问题。另外一种情况，就是在进行一些复杂查询的时候，发现分页失效了，每次都是取出全部的数据。最后根据 Jstack，定位到具体的执行逻辑，发现分页被重写了。 private void appendLimitRowCount( final sqlBuilder, final RowCountToken rowCountToken, final int count, final List&amp;lt;SQLToken&amp;gt; sqlTokens, final boolean isRewrite ) { SelectStatement selectStatement = (SelectStatement)sqlStatement; Limit limit = selectStatement.getLimit(); if (!isRewrite) { sqlBuilder.appendLiterals(String.valueOf(rowCountToken.getRowCount())); } else if ((!selectStatement.getGroupByItems().isEmpty()||!selectStatement.getAggregationSelectItems().isEmpty()) &amp;amp;&amp;amp; !selectStatement.isSameGroupByAndOrderByItems()) { sqlBuilder.appendLiterals(String.valueOf(Integer.MAX_VALUE)); } else { sqlBuilder.appendLiterals( String.valueOf(limit.isNeedRewriteRowCount() ? rowCountToken.getRowCount() + limit.getOffsetValue() : rowCountToken.getRowCount())); } int beginPosition = rowCountToken.getBeginPosition() + String.valueOf(rowCountToken.getRowCount()).length(); appendRest(sqlBuilder, count, sqlTokens, beginPosition); }如上代码，在进入一些复杂的条件判断时（参照 SQLRewriteEngine.java），分页被重置为 Integer.MAX_VALUE。总结以 Spring Boot 项目常见的分层结构，总结了每一层可能会引起的内存问题，把结论归结为一点，那就是：保持输入集或者结果集的简洁。一次性获取非常多的数据，会让中间过程变得非常不可控。一个驱动层的数据库中间件，以及对内存使用的一些问题。很多程序员把这些耗时又耗内存的操作，写了非常复杂的 SQL 语句，然后扔给最底层的数据库去解决，这种情况大多数认为换汤不换药，不过是把具体的问题冲突，转移到另一个场景而已。 ps：深分页： 分页举例：有一亿条数据，要看 orderby 的第 5 千万条数据后的 10 条数据。" }, { "title": "高死亡率的报表系统的优化之路", "url": "/posts/db-optimization/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-15 13:23:00 +0000", "snippet": "传统观念上的报表系统，可能访问量不是特别多，点击一个查询按钮，后台 SQL 语句的执行需要等数秒。如果使用 jstack 来查看执行线程，会发现大多数线程都阻塞在数据库的 I/O 上。上面这种是非常传统的报表。还有一种类似于大屏监控一类的实时报表，这种报表的并发量也是比较可观的，但由于它的结果集都比较小，所以可以像对待一个高并发系统一样对待它，问题不是很大。本文所总结的就是传统观念上的报表。除了处理时间比较长以外，报表系统每次处理的结果集，普遍都比较大，这给 JVM 造成了非常大的压力。。有一个报表系统，频繁发生内存溢出，在高峰期间使用时，还会频繁的发生拒绝服务，这是不可忍受的。服务背景本次要优化的服务是一个 SaaS 服务，使用 Spring Boot 编写，采用的是 CMS 垃圾回收器。如下图所示，有些接口会从 MySQL 中获取数据，有些则从 MongoDB 中获取数据，涉及的结果集合都比较大。由于有些结果集的字段不是太全，因此需要对结果集合进行循环，可通过 HttpClient 调用其他服务的接口进行数据填充。也许你会认为某些数据可能会被复用，于是使用 Guava 做了 JVM 内缓存。大体的服务依赖可以抽象成下面的图：初步排查，JVM 的资源太少。当接口 A 每次进行报表计算时，都要涉及几百兆的内存，而且在内存里驻留很长时间，同时有些计算非常耗 CPU，特别的“吃”资源。而我们分配给 JVM 的内存只有 3 GB，在多人访问这些接口的时候，内存就不够用了，进而发生了 OOM。在这种情况下，即使连最简单的报表都不能用了。没办法，只有升级机器。把机器配置升级到 4core8g，给 JVM 分配 6GB 的内存，这样 OOM 问题就消失了。但随之而来的是频繁的 GC 问题和超长的 GC 时间，平均 GC 时间竟然有 5 秒多。初步优化我们前面算过，6GB 大小的内存，年轻代大约是 2GB，在高峰期，每几秒钟则需要进行一次 MinorGC。报表系统和高并发系统不太一样，它的对象，存活时长大得多，并不能仅仅通过增加年轻代来解决；而且，如果增加了年轻代，那么必然减少了老年代的大小，由于 CMS 的碎片和浮动垃圾问题，我们可用的空间就更少了。虽然服务能够满足目前的需求，但还有一些不太确定的风险。第一，了解到程序中有很多缓存数据和静态统计数据，为了减少 MinorGC 的次数，通过分析 GC 日志打印的对象年龄分布，把 MaxTenuringThreshold 参数调整到了 3（请根据你自己的应用情况设置）。这个参数是让年轻代的这些对象，赶紧回到老年代去，不要老呆在年轻代里。第二，我们的 GC 时间比较长，就一块开了参数 CMSScavengeBeforeRemark，使得在 CMS remark 前，先执行一次 Minor GC 将新生代清掉。同时配合上个参数，其效果还是比较好的，一方面，对象很快晋升到了老年代，另一方面，年轻代的对象在这种情况下是有限的，在整个 MajorGC 中占的时间也有限。第三，由于缓存的使用，有大量的弱引用，拿一次长达 10 秒的 GC 来说。我们发现在 GC 日志里，处理 weak refs 的时间较长，达到了 4.5 秒。2020-01-28T12:13:32.876+0800: 526569.947: [weak refs processing, 4.5240649 secs]所以加入了参数 ParallelRefProcEnabled 来并行处理 Reference，以加快处理速度，缩短耗时。同时还加入了其他一些优化参数，比如通过调整触发 GC 的参数来进行优化。-Xmx6g -Xms6g -XX:MaxTenuringThreshold=3 -XX:+AlwaysPreTouch -XX:+ParallelRefProcEnabled -XX:+CMSScavengeBeforeRemark -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseCMSInitiatingOccupancyOnly -XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M优化之后，效果不错，但并不是特别明显。经过评估，针对高峰时期的情况进行调研，我们决定再次提升机器性能，改用 8core16g 的机器。但是，这会带来另外一个问题。高性能的机器带来了非常大的服务吞吐量，通过 jstat 进行监控，能够看到年轻代的分配速率明显提高，但随之而来的 MinorGC 时长却变的不可控，有时候会超过 1 秒。累积的请求造成了更加严重的后果。这是由于堆空间明显加大造成的回收时间加长。为了获取较小的停顿时间，我们在堆上采用了 G1 垃圾回收器，把它的目标设定在 200ms。G1 是一款非常优秀的垃圾收集器，不仅适合堆内存大的应用，同时也简化了调优的工作。通过主要的参数初始和最大堆空间、以及最大容忍的 GC 暂停目标，就能得到不错的性能。所以为了照顾大对象的生成，我们把小堆区的大小修改为 16 M。修改之后，虽然 GC 更加频繁了一些，但是停顿时间都比较小，应用的运行较为平滑。-Xmx12g -Xms12g -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 -XX:G1HeapRegionSize=16m -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m这个时候，任务来了：业务部门发力，预计客户增长量增长 10 ~ 100 倍，报表系统需要评估其可行性，以便进行资源协调。可问题是，这个“千疮百孔”的报表系统，稍微一压测，就宕机，那如何应对十倍百倍的压力呢？使用 MAT 分析堆快照，发现很多地方可以通过代码优化，那些占用内存特别多的对象，都是我们需要优化的。代码优化我们使用扩容硬件的方式，暂时缓解了 JVM 的问题，但是根本问题并没有触及到。为了减少内存的占用，肯定要清理无用的信息。通过对代码的仔细分析，首先要改造的就是 SQL 查询语句。很多接口，其实并不需要把数据库的每个字段都查询出来，当你在计算和解析的时候，它们会不知不觉地“吃掉”你的内存。所以我们只需要获取所需的数据就够了，也就是把 select * 这种方式修改为具体的查询字段，对于报表系统来说这种优化尤其明显。再一个就是 Cache 问题，通过排查代码，会发现一些命中率特别低，占用内存又特别大的对象，放到了 JVM 内的 Cache 中，造成了无用的浪费。解决方式，就是把 Guava 的 Cache 引用级别改成弱引用（WeakKeys），尽量去掉无用的应用缓存。对于某些使用特别频繁的小 key，使用分布式的 Redis 进行改造即可。为了找到更多影响因子大的问题，我们部署了独立的环境，然后部署了 JVM 监控。在回放某个问题请求后，观察 JVM 的响应，通过这种方式，发现了更多的优化可能。报表系统使用了 POI 组件进行导入导出功能的开发，结果客户在没有限制的情况下上传、下载了条数非常多的文件，直接让堆内存飙升。为了解决这种情况，我们在导入功能加入了文件大小的限制，强制客户进行拆分；在下载的时候指定范围，严禁跨度非常大的请求。在完成代码改造之后，再把机器配置降级回 4core8g，依然采用 G1 垃圾回收器，再也没有发生 OOM 的问题了，GC 问题也得到了明显的缓解。拒绝服务问题上面解决的是 JVM 的内存问题，可以看到除了优化 JVM 参数、升级机器配置以外，代码修改带来的优化效果更加明显，但这个报表服务还有一个严重的问题。刚开始我们提到过，由于没有微服务体系，有些数据需要使用 HttpClient 来获取进行补全。提供数据的服务有的响应时间可能会很长，也有可能会造成服务整体的阻塞。如上图所示，接口 A 通过 HttpClient 访问服务 2，响应 100ms 后返回；接口 B 访问服务 3，耗时 2 秒。HttpClient 本身是有一个最大连接数限制的，如果服务 3 迟迟不返回，就会造成 HttpClient 的连接数达到上限，最上层的 Tomcat 线程也会一直阻塞在这里，进而连响应速度比较快的接口 A 也无法正常提供服务。这是出现频率非常高的的一类故障，在工作中你会大概率遇见。概括来讲，就是同一服务，由于一个耗时非常长的接口，进而引起了整体的服务不可用。这个时候，通过 jstack 打印栈信息，会发现大多数竟然阻塞在了接口 A 上，而不是耗时更长的接口 B。这是一种错觉，其实是因为接口 A 的速度比较快，在问题发生点进入了更多的请求，它们全部都阻塞住了。证据本身具有非常强的迷惑性。由于这种问题发生的频率很高，排查起来又比较困难，我这里专门做了一个小工程，用于还原解决这种问题的一个方式，参见 report-demo 工程。demo 模拟了两个使用同一个 HttpClient 的接口。如下图所示，fast 接口用来访问百度，很快就能返回；slow 接口访问谷歌，由于众所周知的原因，会阻塞直到超时，大约 10 s。使用 wrk 工具对这两个接口发起压测。wrk -t10 -c200 -d300s http://127.0.0.1:8084/slowwrk -t10 -c200 -d300s http://127.0.0.1:8084/fast此时访问一个简单的接口，耗时竟然能够达到 20 秒。time curl http://localhost:8084/statfast648,slow:1curl http://localhost:8084/stat 0.01s user 0.01s system 0% cpu 20.937 total使用 jstack 工具 dump 堆栈。首先使用 jps 命令找到进程号，然后把结果重定向到文件（可以参考 10271.jstack 文件）。过滤一下 nio 关键字，可以查看 tomcat 相关的线程，足足有 200 个，这和 Spring Boot 默认的 maxThreads 个数不谋而合。更要命的是，有大多数线程，都处于 BLOCKED 状态，说明线程等待资源超时。cat 10271.jstack |grep http-nio-80 -A 3使用脚本分析，发现有大量的线程阻塞在 fast 方法上。我们上面也说过，这是一个假象，可能你到了这一步，会心生存疑，以至于无法再向下分析。$ cat 10271.jstack | grep fast | wc -l 137$ cat 10271.jstack | grep slow | wc -l 63分析栈信息，你可能会直接查找 locked 关键字，如下图所示，但是这样的方法一般没什么用，我们需要做更多的统计。注意下图中有一个处于 BLOCKED 状态的线程，它阻塞在对锁的获取上（wating to lock）。大体浏览一下 DUMP 文件，会发现多处这种状态的线程，可以使用如下脚本进行统计。cat 10271.tdump| grep &quot;waiting to lock &quot; | awk &#39;{print $5}&#39; | sort | uniq -c | sort -k1 -r 26 &amp;lt;0x0000000782e1b590&amp;gt; 18 &amp;lt;0x0000000787b00448&amp;gt; 16 &amp;lt;0x0000000787b38128&amp;gt; 10 &amp;lt;0x0000000787b14558&amp;gt; 8 &amp;lt;0x0000000787b25060&amp;gt; 4 &amp;lt;0x0000000787b2da18&amp;gt; 4 &amp;lt;0x0000000787b00020&amp;gt; 2 &amp;lt;0x0000000787b6e8e8&amp;gt; 2 &amp;lt;0x0000000787b03328&amp;gt; 2 &amp;lt;0x0000000782e8a660&amp;gt; 1 &amp;lt;0x0000000787b6ab18&amp;gt; 1 &amp;lt;0x0000000787b2ae00&amp;gt; 1 &amp;lt;0x0000000787b0d6c0&amp;gt; 1 &amp;lt;0x0000000787b073b8&amp;gt; 1 &amp;lt;0x0000000782fbcdf8&amp;gt; 1 &amp;lt;0x0000000782e11200&amp;gt; 1 &amp;lt;0x0000000782dfdae0&amp;gt;我们找到给 0x0000000782e1b590 上锁的执行栈，可以发现全部卡在了 HttpClient 的读操作上。在实际场景中，可以看下排行比较靠前的几个锁地址，找一下共性。返回头去再看一下代码。我们发现 HttpClient 是共用了一个连接池，当连接数超过 100 的时候，就会阻塞等待。它的连接超时时间是 10 秒，这和 slow 接口的耗时不相上下。 private final static HttpConnectionManager httpConnectionManager = new SimpleHttpConnectionManager(true); static { HttpConnectionManagerParams params = new HttpConnectionManagerParams(); params.setMaxTotalConnections(100); params.setConnectionTimeout(1000 * 10); params.setSoTimeout(defaultTimeout); httpConnectionManager.setParams(params); }slow 接口和 fast 接口同时在争抢这些连接，让它时刻处在饱满的状态，进而让 tomcat 的线程等待、占满，造成服务不可用。问题找到了，解决方式就简单多了。望 slow 接口在阻塞的时候，并不影响 fast 接口的运行。这就可以对某一类接口进行限流，或者对不重要的接口进行熔断处理，这里不再深入讲（具体可参考 Spring Boot 的限流熔断处理）。现实情况是，对于一个运行的系统，并不知道是 slow 接口慢还是 fast 接口慢，这就需要加入一些额外的日志信息进行排查。当然，如果有一个监控系统能够看到这些数据是再好不过了。项目中的 HttpClientUtil2 文件，是改造后的一个版本。除了调大了连接数，它还使用了多线程版本的连接管理器（MultiThreadedHttpConnectionManager），这个管理器根据请求的 host 进行划分，每个 host 的最大连接数不超过 20。还提供了 getConnectionsInPool 函数，用于查看当前连接池的统计信息。采用这些辅助的手段，可以快速找到问题服务，这是典型的情况。由于其他应用的服务水平低而引起的连锁反应，一般的做法是熔断、限流等，在此不多做介绍了。jstack 产生的信息为了观测一些状态，我上传了几个 Java 类，你可以实际运行一下，然后使用 jstack 来看一下它的状态。waiting on condition示例参见 SleepDemo.java。public class SleepDemo { public static void main(String[] args) { new Thread(()-&amp;gt;{ try { Thread.sleep(Integer.MAX_VALUE); } catch (InterruptedException e) { e.printStackTrace(); } },&quot;sleep-demo&quot;).start(); }}这个状态出现在线程等待某个条件的发生，来把自己唤醒，或者调用了 sleep 函数，常见的情况就是等待网络读写，或者等待数据 I/O。如果发现大多数线程都处于这种状态，证明后面的资源遇到了瓶颈。此时线程状态大致分为以下两种： java.lang.Thread.State: WAITING (parking)：一直等待条件发生； java.lang.Thread.State: TIMED_WAITING (parking 或 sleeping)：定时的，即使条件不触发，也将定时唤醒。&quot;sleep-demo&quot; #12 prio=5 os_prio=31 cpu=0.23ms elapsed=87.49s tid=0x00007fc7a7965000 nid=0x6003 waiting on condition [0x000070000756d000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(java.base@13.0.1/Native Method) at SleepDemo.lambda$main$0(SleepDemo.java:5) at SleepDemo$$Lambda$16/0x0000000800b45040.run(Unknown Source) at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)值的注意的是，Java 中的可重入锁，也会让线程进入这种状态，但通常带有 parking 字样，parking 指线程处于挂起中，要注意区别。代码可参见 LockDemo.javaimport java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class LockDemo { public static void main(String[] args) { Lock lock = new ReentrantLock(); lock.lock(); new Thread(() -&amp;gt; { try { lock.lock(); } finally { lock.unlock(); } }, &quot;lock-demo&quot;).start(); }}堆栈代码如下：&quot;lock-demo&quot; #12 prio=5 os_prio=31 cpu=0.78ms elapsed=14.62s tid=0x00007ffc0b949000 nid=0x9f03 waiting on condition  [0x0000700005826000]   java.lang.Thread.State: WAITING (parking)    at jdk.internal.misc.Unsafe.park(java.base@13.0.1/Native Method)    - parking to wait for  &amp;lt;0x0000000787cf0dd8&amp;gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)    at java.util.concurrent.locks.LockSupport.park(java.base@13.0.1/LockSupport.java:194)    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(java.base@13.0.1/AbstractQueuedSynchronizer.java:885)    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(java.base@13.0.1/AbstractQueuedSynchronizer.java:917)    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(java.base@13.0.1/AbstractQueuedSynchronizer.java:1240)    at java.util.concurrent.locks.ReentrantLock.lock(java.base@13.0.1/ReentrantLock.java:267)    at LockDemo.lambda$main$0(LockDemo.java:11)    at LockDemo$$Lambda$14/0x0000000800b44840.run(Unknown Source)    at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)waiting for monitor entry上面提到的 HttpClient 例子，就是大部分处于这种状态，线程都是 BLOCKED 的。这意味着它们都在等待进入一个临界区，需要重点关注。&quot;http-nio-8084-exec-120&quot; #143 daemon prio=5 os_prio=31 cpu=122.86ms elapsed=317.88s tid=0x00007fedd8381000 nid=0x1af03 waiting for monitor entry  [0x00007000150e1000]   java.lang.Thread.State: BLOCKED (on object monitor)    at java.io.BufferedInputStream.read(java.base@13.0.1/BufferedInputStream.java:263)    - waiting to lock &amp;lt;0x0000000782e1b590&amp;gt; (a java.io.BufferedInputStream)    at org.apache.commons.httpclient.HttpParser.readRawLine(HttpParser.java:78)    at org.apache.commons.httpclient.HttpParser.readLine(HttpParser.java:106)    at org.apache.commons.httpclient.HttpConnection.readLine(HttpConnection.java:1116)    at org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1973)    at org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1735)in Object.wait()示例代码参见 WaitDemo.java：public class WaitDemo { public static void main(String[] args) throws Exception { Object o = new Object(); new Thread(() -&amp;gt; { try { synchronized (o) { o.wait(); } } catch (InterruptedException e) { e.printStackTrace(); } }, &quot;wait-demo&quot;).start(); Thread.sleep(1000); synchronized (o) { o.wait(); } }}说明在获得了监视器之后，又调用了 java.lang.Object.wait() 方法。关于这部分的原理，可以参见一张经典的图。每个监视器（Monitor）在某个时刻，只能被一个线程拥有，该线程就是“Active Thread”，而其他线程都是“Waiting Thread”，分别在两个队列“Entry Set”和“Wait Set”里面等候。在“Entry Set”中等待的线程状态是“Waiting for monitor entry”，而在“Wait Set”中等待的线程状态是“in Object.wait()”。&quot;wait-demo&quot; #12 prio=5 os_prio=31 cpu=0.14ms elapsed=12.58s tid=0x00007fb66609e000 nid=0x6103 in Object.wait()  [0x000070000f2bd000]   java.lang.Thread.State: WAITING (on object monitor)    at java.lang.Object.wait(java.base@13.0.1/Native Method)    - waiting on &amp;lt;0x0000000787b48300&amp;gt; (a java.lang.Object)    at java.lang.Object.wait(java.base@13.0.1/Object.java:326)    at WaitDemo.lambda$main$0(WaitDemo.java:7)    - locked &amp;lt;0x0000000787b48300&amp;gt; (a java.lang.Object)    at WaitDemo$$Lambda$14/0x0000000800b44840.run(Unknown Source)    at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)死锁public class DeadLockDemo { public static void main(String[] args) { Object object1 = new Object(); Object object2 = new Object(); Thread t1 = new Thread(() -&amp;gt; { synchronized (object1) { try { Thread.sleep(200); } catch (InterruptedException e) { e.printStackTrace(); } synchronized (object2) { } } }, &quot;deadlock-demo-1&quot;); t1.start(); Thread t2 = new Thread(() -&amp;gt; { synchronized (object2) { synchronized (object1) { } } }, &quot;deadlock-demo-2&quot;); t2.start(); }}死锁属于比较严重的一种情况，jstack 会以明显的信息进行提示。Found one Java-level deadlock:=============================&quot;deadlock-demo-1&quot;:  waiting to lock monitor 0x00007fe5e406f500 (object 0x0000000787cecd78, a java.lang.Object),  which is held by &quot;deadlock-demo-2&quot;&quot;deadlock-demo-2&quot;:  waiting to lock monitor 0x00007fe5e406d500 (object 0x0000000787cecd68, a java.lang.Object),  which is held by &quot;deadlock-demo-1&quot;Java stack information for the threads listed above:===================================================&quot;deadlock-demo-1&quot;:    at DeadLockDemo.lambda$main$0(DeadLockDemo.java:13)    - waiting to lock &amp;lt;0x0000000787cecd78&amp;gt; (a java.lang.Object)    - locked &amp;lt;0x0000000787cecd68&amp;gt; (a java.lang.Object)    at DeadLockDemo$$Lambda$14/0x0000000800b44c40.run(Unknown Source)    at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)&quot;deadlock-demo-2&quot;:    at DeadLockDemo.lambda$main$1(DeadLockDemo.java:21)    - waiting to lock &amp;lt;0x0000000787cecd68&amp;gt; (a java.lang.Object)    - locked &amp;lt;0x0000000787cecd78&amp;gt; (a java.lang.Object)    at DeadLockDemo$$Lambda$16/0x0000000800b45040.run(Unknown Source)    at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)Found 1 deadlock当然，关于线程的 dump，也有一些线上分析工具可以使用。下图是 fastthread 的一个分析结果，但也需要你先了解这些情况发生的意义总结本文以一个处处有问题的报表系统，并逐步解决了它的 OOM 问题，同时定位到了拒绝服务的原因。在研发资源不足的时候，简单粗暴的进行了硬件升级，并切换到了更加优秀的 G1 垃圾回收器，还通过代码手段进行了问题的根本解决： 缩减查询的字段，减少常驻内存的数据； 去掉不必要的、命中率低的堆内缓存，改为分布式缓存； 从产品层面限制了单次请求对内存的无限制使用。在这个过程中，使用 MAT 分析堆数据进行问题代码定位，帮了大忙。代码优化的手段是最有效的，改造完毕后，可以节省更多的硬件资源。事实上，使用了 G1 垃圾回收器之后，那些乱七八糟的调优参数越来越少用了。接下来，使用 jstack 分析了一个出现频率非常非常高的问题，主要是不同速度的接口在同一应用中的资源竞争问题，我们发现一些成熟的微服务框架，都会对这些资源进行限制和隔离。最后，以 4 个简单的示例，展示了 jstack 输出内容的一些意义。" }, { "title": "GC 监控与调优", "url": "/posts/gc-monitor-optimization/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-14 13:23:00 +0000", "snippet": "对于问题的排查，现在都有哪些资源： GC 日志，能够反映每次 GC 的具体状况，可根据这些信息调整一些参数及容量； 问题发生点的堆快照，能够在线下找到具体内存泄漏的原因； 问题发生点的堆栈信息，能够定位到当前正在运行的业务，以及一些死锁问题； 操作系统监控，比如 CPU 资源、内存、网络、I/O 等，能够看到问题发生前后整个操作系统的资源状况； 服务监控，比如服务的访问量、响应时间等，可以评估故障堆服务的影响面，或者找到一些突增的流量来源； JVM 各个区的内存变化、GC 变化、耗时等监控，能够帮我们了解到 JVM 在整个故障周期的时间跨度上，到底发生了什么。在实践课时优化和问题排查是一个综合的过程。故障相关信息越多越好，哪怕是同事不经意间透露的一次压测信息，都能够帮助你快速找到问题的根本。本文将以一个实际的监控解决方案，来看一下监控数据是怎么收集和分析的。使用的工具主要集中在 Telegraf、InfluxDB 和 Grafana 上，如果你在用其他的监控工具，思路也是类似的。监控指标在前面的一些示例代码中，会看到如下的 JMX 代码片段：static void memPrint() { for (MemoryPoolMXBean memoryPoolMXBean : ManagementFactory.getMemoryPoolMXBeans()) { System.out.println(memoryPoolMXBean.getName() + &quot;vcommitted:&quot; + memoryPoolMXBean.getUsage().getCommitted() + &quot; used:&quot; + memoryPoolMXBean.getUsage().getUsed()); }}这就是 JMX 的作用。除了使用代码，通过 jmc 工具也可以简单地看一下它们的值（前面提到的 VisualVM 通过安装插件，也可以看到这些信息）。新版本的 JDK 不再包含 jmc 这个工具，可以在这里自行下载。如下图所示，可以看到一个 Java 进程的资源概览，包括内存、CPU、线程等。下图是切换到 MBean 选项卡之后的截图，可以看到图中展示的 Metaspace 详细信息。jmc 还是一个性能分析平台，可以录制、收集正在运行的 Java 程序的诊断数据和概要分析数据，但还是那句话，线上环境可能没有条件让我们使用一些图形化分析工具，相对比 Arthas 这样的命令行工具就比较吃香。比如，下图就是一个典型的互联网架构图，真正的服务器可能是一群 docker 实例，如果自己的机器想要访问 JVM 的宿主机器，则需要配置一些复杂的安全策略和权限开通。图像化的工具在平常的工作中不是非常有用，而且，由于性能损耗和安全性的考虑，也不会让研发主动去通过 JMX 连接这些机器。所以面试的时候如果你一直在提一些图形化工具，面试官只能无奈的笑笑，这个话题也无法进行下去了。在必要的情况下，JMX 还可以通过加上一些参数，进行远程访问。-Djava.rmi.server.hostname=127.0.0.1-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=14000 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false无论是哪种方式，每个内存区域，都有四个值：init、used、committed 和 max，下图展示了它们之间的大小关系。以堆内存大小来说： -Xmx 就是 max -Xms 就是 init committed 指的是当前可用的内存大小，它的大小包括已经使用的内存 used 指的是实际被使用的内存大小，它的值总是小于 committed如果在启动的时候，指定了 -Xmx = -Xms，也就是初始值和最大值是一样的，可以看到这四个值，只有 used 是变动的。Jolokia单独看这些 JMX 的瞬时监控值，是没有什么用的，需要使用程序收集起来并进行分析。但是 JMX 的客户端 API 使用起来非常的不方便，Jolokia 就是一个将 JMX 转换成 HTTP 的适配器，方便了 JMX 的使用。Jokokia 可以通过 jar 包和 agent 的方式启动，在一些框架中，比如 Spring Boot 中，很容易进行集成。访问 http://start.spring.io，生成一个普通的 Spring Boot 项目。直接在 pom 文件里加入 jolokia 的依赖。&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.jolokia&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;jolokia-core&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;在 application.yml 中简单地加入一点配置，就可以通过 HTTP 接口访问 JMX 的内容了。management: endpoints: web: exposure: include: jolokia也可以直接下载仓库中的 monitor-demo 项目，启动后访问 8084 端口，即可获取 JMX 的 json 数据。访问链接 /demo 之后，会使用 guava 持续产生内存缓存。接下来，收集这个项目的 JMX 数据。url:http://localhost:8084/actuator/jolokia/listJVM 监控搭建JVM 监控的整体架构图：JVM 的各种内存信息，会通过 JMX 接口进行暴露；Jolokia 组件负责把 JMX 信息翻译成容易读取的 HTTP 请求。telegraf 组件作为一个通用的监控 agent，和 JVM 进程部署在同一台机器上，通过访问转化后的 HTTP 接口，以固定的频率拉取监控信息；然后把这些信息存放到 influxdb 时序数据库中；最后，通过高颜值的 Grafana 展示组件，设计 JVM 监控图表。整个监控组件是可以热拔插的，并不会影响原有服务。监控部分也是可以复用的，比如 telegraf 就可以很容易的进行操作系统监控。influxdbinfluxdb 是一个性能和压缩比非常高的时序数据库，在中小型公司非常流行，点击这里可获取 influxdb。在 CentOS 环境中，可以使用下面的命令下载。wget -c https://dl.influxdata.com/influxdb/releases/influxdb-1.7.9_linux_amd64.tar.gztar xvfz influxdb-1.7.9_linux_amd64.tar.gz解压后，然后使用 nohup 进行启动。nohup ./influxd &amp;amp;InfluxDB 将在 8086 端口进行监听。TelegrafTelegraf 是一个监控数据收集工具，支持非常丰富的监控类型，其中就包含内置的 Jolokia 收集器。接下来，下载并安装 Telegraf：wget -c https://dl.influxdata.com/telegraf/releases/telegraf-1.13.1-1.x86_64.rpmsudo yum localinstall telegraf-1.13.1-1.x86_64.rpmTelegraf 通过 jolokia 配置收集数据相对简单，比如下面就是收集堆内存使用状况的一段配置。[[inputs.jolokia2_agent.metric]] name = &quot;jvm&quot; field_prefix = &quot;Memory_&quot; mbean = &quot;java.lang:type=Memory&quot; paths = [&quot;HeapMemoryUsage&quot;, &quot;NonHeapMemoryUsage&quot;, &quot;ObjectPendingFinalizationCount&quot;]设计这个配置文件的主要难点在于对 JVM 各个内存分区的理解。由于配置文件比较长，可以参考仓库中的 jvm.conf 和 sys.conf，你可以把这两个文件，复制到 /etc/telegraf/telegraf.d/ 目录下面，然后执行 systemctl restart telegraf 重启 telegraf。grafanagrafana 是一个颜值非常高的监控展示组件，支持非常多的数据源类型，对 influxdb 的集成度也比较高，可通过以下地址进行下载：https://grafana.com/grafana/downloadwget -c https://dl.grafana.com/oss/release/grafana-6.5.3.linux-amd64.tar.gztar -zxvf grafana-6.5.3.linux-amd64.tar.gz下面是我已经做好的一张针对于 CMS 垃圾回收器的监控图，可以导入 grafana-jvm-influxdb.json 文件进行测试。在导入之前，还需要创建一个数据源，选择 influxdb，填入 db 的地址即可。集成把Spring Boot 项目打包（见仓库），然后上传到服务器上去执行。打包方式：mvn package -Dmaven.tesk.skip=true执行方式（自行替换日志方面配置）：mkdir /tmp/logsnohup java -XX:+UseConcMarkSweepGC -Xmx512M -Xms512M -Djava.rmi.server.hostname=192.168.99.101 -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=14000 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintTenuringDistribution -Xloggc:/tmp/logs/gc_%p.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/logs -XX:ErrorFile=/tmp/logs/hs_error_pid%p.log -XX:-OmitStackTraceInFastThrow -jar monitor-demo-0.0.1-SNAPSHOT.jar 2&amp;gt;&amp;amp;1 &amp;amp;请将 IP 地址改成自己服务器的实际 IP 地址，这样就可以使用 jmc 或者 VisualVM 等工具进行连接了。确保 Telegraf、InfluxDB、Grafana 已经启动，这样，Java 进程的 JVM 相关数据，将会以 10 秒一次的频率进行收集，我们可以选择 Grafana 的时间轴，来查看实时的或者历史的监控曲线。这类监控信息，可以保存长达 1 ~ 2 年，也就是说非常久远的问题，也依然能够被追溯到。如果你想要对 JVM 尽可能地进行调优，就要时刻关注这些监控图。举一个例子：发现有一个线上服务，运行一段时间以后，CPU 升高、程序执行变慢，登录相应的服务器进行分析，发现 C2 编译线程一直处在高耗 CPU 的情况。但是我们无法解决这个问题，一度以为是 JVM 的 Bug。通过分析 CPU 的监控图和 JVM 每个内存分区的曲线，发现 CodeCache 相应的曲线，在增加到 32MB 之后，就变成了一条直线，同时 CPU 的使用也开始增加。通过检查启动参数和其他配置，最终发现一个开发环境的 JVM 参数被一位想要练手的同学给修改了，他本意是想要通过参数 “-XX:ReservedCodeCacheSize” 来限制 CodeCache 的大小，这个参数被误推送到了线上环境。JVM 通过 JIT 编译器来增加程序的执行效率，JIT 编译后的代码，都会放在 CodeCache 里。如果这个空间不足，JIT 则无法继续编译，编译执行会变成解释执行，性能会降低一个数量级。同时，JIT 编译器会一直尝试去优化代码，造成了 CPU 的占用上升。由于收集了这些分区的监控信息，所以很容易就发现了问题的相关性，这些判断也会反向支持我们的分析，而不仅仅是靠猜测。代码清单 sys.conf 操作系统监控数据收集配置文件，Telegraf 使用。 jvm.conf JVM 监控配置文件，Telegraf 使用。 grafana-jvm-influxdb.json JVM 监控面板，Grafana 使用。 monitor-demo 被收集的 Spring Boot 项目。" }, { "title": "堆外内存排查", "url": "/posts/outside-of-heap-checked/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-13 13:23:00 +0000", "snippet": "Metaspace 属于堆外内存，但由于它是单独管理的，所以排查起来没什么难度。平常可能见到的使用堆外内存的场景还有下面这些： JNI 或者 JNA 程序，直接操纵了本地内存，比如一些加密库； 使用了 Java 的 Unsafe 类，做了一些本地内存的操作； Netty 的直接内存（Direct Memory），底层会调用操作系统的 malloc 函数。使用堆外内存可以调用一些功能完备的库函数，而且减轻了 GC 的压力。这些代码，有可能是你了解的人写的，也有可能隐藏在第三方的 jar 包里。虽然有一些好处，但是问题排查起来通常会比较的困难。虽然 MaxDirectMemorySize 参数可以控制直接内存的申请。其实，通过这个参数，仍然限制不住所有堆外内存的使用，它只是限制了使用 DirectByteBuffer 的内存申请。很多时候（比如直接使用了 sun.misc.Unsafe 类），堆外内存会一直增长，直到机器物理内存爆满，被 oom killer。public class UnSafeDemo { public static final int _1MB = 1024 * 1024; public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException { Field field = UnSafeDemo.class.getDeclaredField(&quot;theUnsafe&quot;); field.setAccessible(true); Unsafe unsafe = (Unsafe) field.get(null); for (;;) { unsafe.allocateMemory(_1MB); } } }上面这段代码，就会持续申请堆外内存，但它返回的是 long 类型的地址句柄，所以堆内内存的使用会很少。使用下面的命令去限制堆内和直接内存的使用，结果发现程序占用的操作系统内存在一直上升，这两个参数在这种场景下没有任何效果。这段程序搞死了我的机器很多次，运行的时候要小心。java -XX:MaxDirectMemorySize=10M -Xmx10M UnsafeDemo这种情况困扰许久，因为使用一些 JDK 提供的工具，根本无法发现这部门内存的使用。我们需要一些更加底层的工具来发现这些游离的内存分配。其实，很多内存和性能问题，都逃不过下面要介绍的这些工具的联合分析。本课时将会结合一个实际的例子，来看一下一个堆外内存的溢出情况，了解常见的套路。现象我们有一个服务，非常的奇怪，在某个版本之后，占用的内存开始增长，直到虚拟机分配的内存上限，但是并不会 OOM。如果开启了 SWAP，会发现这个应用也会毫不犹豫的将它吞掉，有多少吞多少。说它的内存增长，是通过 top 命令去观察的，看它的 RES 列的数值；反之，如果使用 jmap 命令去看内存占用，得到的只是堆的大小，只能看到一小块可怜的空间。使用 ps 也能看到相同的效果。观测到，除了虚拟内存比较高，达到了 17GB 以外，实际使用的内存 RSS 也夸张的达到了 7 GB，远远超过了 -Xmx 的设定。ps -p 75 -o rss,vsz RSS VSZ 7152568 17485844使用 jps 查看启动参数，发现分配了大约 3GB 的堆内存。实际内存使用超出了最大内存设定的一倍还多，这明显是不正常的，肯定是使用了堆外内存。模拟程序为了能够使用这些工具实际观测这个内存泄漏的过程，我这里准备了一份小程序。程序将会持续的使用 Java 的 Zip 函数进行压缩和解压，这种操作在一些对传输性能较高的的场景经常会用到。程序将会申请 1kb 的随机字符串，然后持续解压。为了避免让操作系统陷入假死状态，我们每次都会判断操作系统内存使用率，在达到 60% 的时候，我们将挂起程序；通过访问 8888 端口，将会把内存阈值提高到 85%。我们将分析这两个处于相对静态的虚拟快照。package jvm;import com.sun.management.OperatingSystemMXBean;import com.sun.net.httpserver.HttpContext;import com.sun.net.httpserver.HttpServer;import java.io.*;import java.lang.management.ManagementFactory;import java.net.InetSocketAddress;import java.util.Random;import java.util.concurrent.ThreadLocalRandom;import java.util.zip.GZIPInputStream;import java.util.zip.GZIPOutputStream;public class LeakExample { private static final OperatingSystemMXBean osmxb = (OperatingSystemMXBean) ManagementFactory.getOperatingSystemMXBean(); private static volatile int RADIO = 60; public static String randomString(int strLength) { Random rnd = ThreadLocalRandom.current(); StringBuilder ret = new StringBuilder(); for (int i = 0; i &amp;lt; strLength; i++) { boolean isChar = (rnd.nextInt(2) % 2 == 0); if (isChar) { int choice = rnd.nextInt(2) % 2 == 0 ? 65 : 97; ret.append((char) (choice + rnd.nextInt(26))); } else { ret.append(rnd.nextInt(10)); } } return ret.toString(); } public static int copy(InputStream input, OutputStream output) throws IOException { long count = copyLarge(input, output); return count &amp;gt; 2147483647L ? -1 : (int) count; } public static long copyLarge(InputStream input, OutputStream output) throws IOException { byte[] buffer = new byte[4096]; long count = 0L; int n; for (; -1 != (n = input.read(buffer)); count += n) { output.write(buffer, 0, n); } return count; } public static String decompress(byte[] input) throws Exception { ByteArrayOutputStream out = new ByteArrayOutputStream(); copy(new GZIPInputStream(new ByteArrayInputStream(input)), out); return out.toString(); } public static byte[] compress(String str) throws Exception { ByteArrayOutputStream bos = new ByteArrayOutputStream(); GZIPOutputStream gzip = new GZIPOutputStream(bos); try { gzip.write(str.getBytes()); gzip.finish(); byte[] b = bos.toByteArray(); return b; } finally { try { gzip.close(); } catch (Exception ex) { } try { bos.close(); } catch (Exception ex) { } } } public static int memoryLoad() { double totalVirtualMemory = osmxb.getTotalMemorySize(); double freePhysicalMemorySize = osmxb.getFreeMemorySize(); double value = freePhysicalMemorySize / totalVirtualMemory; int percentMemoryLoad = (int) ((1 - value) * 100); return percentMemoryLoad; } public static void main(String[] args) throws Exception { HttpServer server = HttpServer.create(new InetSocketAddress(8888), 0); HttpContext context = server.createContext(&quot;/&quot;); context.setHandler(exchange -&amp;gt; { try { RADIO = 85; String response = &quot;OK!&quot;; exchange.sendResponseHeaders(200, response.getBytes().length); OutputStream os = exchange.getResponseBody(); os.write(response.getBytes()); os.close(); } catch (Exception ex) { } }); server.start(); //1kb int BLOCK_SIZE = 1024; String str = randomString(BLOCK_SIZE / Byte.SIZE); byte[] bytes = compress(str); for (; ; ) { int percent = memoryLoad(); if (percent &amp;gt; RADIO) { Thread.sleep(1000); } else { decompress(bytes); Thread.sleep(1); } } }}程序将使用下面的命令行进行启动。为了简化问题，这里省略了一些无关的配置。java -Xmx1G -Xmn1G -XX:+AlwaysPreTouch -XX:MaxMetaspaceSize=10M -XX:MaxDirectMemorySize=10M -XX:NativeMemoryTracking=detail LeakExampleNMT首先介绍一下上面的几个 JVM 参数，分别使用 Xmx、MaxMetaspaceSize、MaxDirectMemorySize 这三个参数限制了堆、元空间、直接内存的大小。然后，使用 AlwaysPreTouch 参数。其实，通过参数指定了 JVM 大小，只有在 JVM 真正使用的时候，才会分配给它。这个参数，在 JVM 启动的时候，就把它所有的内存在操作系统分配了。在堆比较大的时候，会加大启动时间，但在这个场景中，我们为了减少内存动态分配的影响，把这个值设置为 True。接下来的 NativeMemoryTracking，是用来追踪 Native 内存的使用情况。通过在启动参数上加入 -XX:NativeMemoryTracking=detail 就可以启用。使用 jcmd 命令，就可查看内存分配。jcmd $pid VM.native_memory summary在一台 4GB 的虚拟机上使用上面的命令。启动程序之后，发现进程使用的内存迅速升到 2.4GB。# jcmd 2154 VM.native_memory summary2154:Native Memory Tracking:Total: reserved=2370381KB, committed=1071413KB- Java Heap (reserved=1048576KB, committed=1048576KB) (mmap: reserved=1048576KB, committed=1048576KB) - Class (reserved=1056899KB, committed=4995KB) (classes #432) (malloc=131KB #328) (mmap: reserved=1056768KB, committed=4864KB)- Thread (reserved=10305KB, committed=10305KB) (thread #11) (stack: reserved=10260KB, committed=10260KB) (malloc=34KB #52) (arena=12KB #18)- Code (reserved=249744KB, committed=2680KB) (malloc=144KB #502) (mmap: reserved=249600KB, committed=2536KB)- GC (reserved=2063KB, committed=2063KB) (malloc=7KB #80) (mmap: reserved=2056KB, committed=2056KB)- Compiler (reserved=138KB, committed=138KB) (malloc=8KB #38) (arena=131KB #5)- Internal (reserved=789KB, committed=789KB (malloc=757KB #1272) (mmap: reserved=32KB, committed=32KB)- Symbol (reserved=1535KB, committed=1535KB) (malloc=983KB #114) (arena=552KB #1)- Native Memory Tracking (reserved=159KB, committed=159KB) (malloc=99KB #1399) (tracking overhead=60KB)可惜的是，这个名字让人振奋的工具并不能如它描述的一样，看到我们这种泄漏的场景。下图这点小小的空间，是不能和 2GB 的内存占用相比的。NMT 能看到堆内内存、Code 区域或者使用 unsafe.allocateMemory 和 DirectByteBuffer 申请的堆外内存，虽然是个好工具但问题并不能解决。使用 jmap 工具，dump 一份堆快照，然后使用 MAT 分析，依然不能找到这部分内存。pmap像是 EhCache 这种缓存框架，提供了多种策略，可以设定将数据存储在非堆上，我们就是要排查这些影响因素。如果能够在代码里看到这种可能性最大的代码块，是最好的。为了进一步分析问题，使用 pmap 命令查看进程的内存分配，通过 RSS 升序序排列。结果发现除了地址 00000000c0000000 上分配的 1GB 堆以外（也就是我们的堆内存），还有数量非常多的 64M 一块的内存段，还有巨量小的物理内存块映射到不同的虚拟内存段上。但到现在为止，我们不知道里面的内容是什么，是通过什么产生的。pmap -x 2154 | sort -n -k3通过 Google，找到以下资料 Linux glibc &amp;gt;= 2.10 (RHEL 6) malloc may show excessive virtual memory usage) 。文章指出造成应用程序大量申请 64M 大内存块的原因是由 Glibc 的一个版本升级引起的，通过 export MALLOC_ARENA_MAX=4 可以解决 VSZ 占用过高的问题。虽然这也是一个问题，但却不是我们想要的，因为我们增长的是物理内存，而不是虚拟内存，程序在这一方面表现是正常的。gdb非常好奇 64M 或者其他小内存块中是什么内容，接下来可以通过 gdb 工具将其 dump 出来。读取 /proc 目录下的 maps 文件，能精准地知晓目前进程的内存分布。以下脚本通过传入进程 id，能够将所关联的内存全部 dump 到文件中。注意，这个命令会影响服务，要慎用。pid=$1;grep rw-p /proc/$pid/maps | sed -n &#39;s/^\\([0-9a-f]*\\)-\\([0-9a-f]*\\) .*$/\\1 \\2/p&#39; | while read start stop; do gdb --batch --pid $pid -ex &quot;dump memory $1-$start-$stop.dump 0x$start 0x$stop&quot;; done这个命令十分霸道，甚至把加载到内存中的 class 文件、堆文件一块给 dump 下来。这是机器的原始内存，大多数文件打不开。更多时候，只需要 dump 一部分内存就可以。再次提醒操作会影响服务，注意 dump 的内存块大小，线上一定要慎用。复制 pman 的一块 64M 内存，比如 00007f2d70000000，然后去掉前面的 0，使用下面代码得到内存块的开始和结束地址。at /proc/2154/maps | grep 7f2d700000007f2d6fff1000-7f2d70000000 ---p 00000000 00:00 0 7f2d70000000-7f2d73ffc000 rw-p 00000000 00:00 0接下来就 dump 这 64MB 的内存。gdb --batch --pid 2154 -ex &quot;dump memory a.dump 0x7f2d70000000 0x7f2d73ffc000&quot;使用 du 命令查看具体的内存块大小，不多不少正好 64M。# du -h a.dump64M a.dump是时候查看里面的内容了，使用 strings 命令可以看到内存块里一些可以打印的内容。# strings -10 a.dump0R4f1Qej1ty5GT8V1R8no6T44564wz499E6Y582q2R9h8CC175GJ3yeJ1Q3P5Vt757Mcf6378kM36hxZ5U8uhg2A26T5l7f68719WQK6vZ2BOdH9lH5C7838qf1...等等？这些内容不应该在堆里面么？为何还会使用额外的内存进行分配？那么还有什么地方在分配堆外内存呢？这种情况，只可能是 native 程序对堆外内存的操作。perf神器 perf，除了能够进行一些性能分析，它还能帮助我们找到相应的 native 调用。这么突出的堆外内存使用问题，肯定能找到相应的调用函数。使用 perf record -g -p 2154 开启监控栈函数调用，然后访问服务器的 8888 端口，这将会把内存使用的阈值增加到 85%，我们的程序会逐渐把这部分内存占满，你可以手工观察这个过程。perf 运行一段时间后 Ctrl+C 结束，会生成一个文件 perf.data。执行 perf report -i perf.data 查看报告。如图，一般第三方 JNI 程序，或者 JDK 内的模块，都会调用相应的本地函数，在 Linux 上，这些函数库的后缀都是 so。依次浏览用的可疑资源，发现了“libzip.so”，还发现了不少相关的调用。搜索 zip（输入 / 进入搜索模式），结果如下：查看 JDK 代码，发现 bzip 大量使用了 native 方法。也就是说，有大量内存的申请和销毁，是在堆外发生的。进程调用了Java_java_util_zip_Inflater_inflatBytes() 申请了内存，却没有调用 Deflater 释放内存。与 pmap 内存地址相比对，确实是 zip 在搞鬼。gperftoolsgoogle 还有一个类似的、非常好用的工具，叫做 gperftools，我们主要用到它的 Heap Profiler，功能更加强大。它的启动方式有点特别，安装成功之后，只需要输出两个环境变量即可。mkdir -p /opt/test export LD_PRELOAD=/usr/lib64/libtcmalloc.so export HEAPPROFILE=/opt/test/heap在同一个终端，再次启动我们的应用程序，可以看到内存申请动作都被记录到了 opt 目录下的 test 目录。接下来，就可以使用 pprof 命令分析这些文件。cd /opt/testpprof -text *heap | head -n 200使用这个工具，能够一眼追踪到申请内存最多的函数。Java_java_util_zip_Inflater_init 这个函数立马就被发现了。Total: 25205.3 MB 20559.2 81.6% 81.6% 20559.2 81.6% inflateBackEnd 4487.3 17.8% 99.4% 4487.3 17.8% inflateInit2_ 75.7 0.3% 99.7% 75.7 0.3% os::malloc@8bbaa0 70.3 0.3% 99.9% 4557.6 18.1% Java_java_util_zip_Inflater_init 7.1 0.0% 100.0% 7.1 0.0% readCEN 3.9 0.0% 100.0% 3.9 0.0% init 1.1 0.0% 100.0% 1.1 0.0% os::malloc@8bb8d0 0.2 0.0% 100.0% 0.2 0.0% _dl_new_object 0.1 0.0% 100.0% 0.1 0.0% __GI__dl_allocate_tls 0.1 0.0% 100.0% 0.1 0.0% _nl_intern_locale_data 0.0 0.0% 100.0% 0.0 0.0% _dl_check_map_versions 0.0 0.0% 100.0% 0.0 0.0% __GI___strdup 0.0 0.0% 100.0% 0.1 0.0% _dl_map_object_deps 0.0 0.0% 100.0% 0.0 0.0% nss_parse_service_list 0.0 0.0% 100.0% 0.0 0.0% __new_exitfn 0.0 0.0% 100.0% 0.0 0.0% getpwuid 0.0 0.0% 100.0% 0.0 0.0% expand_dynamic_string_token解决这就是模拟内存泄漏的整个过程，到此问题就解决了。GZIPInputStream 使用 Inflater 申请堆外内存、Deflater 释放内存，调用 close() 方法来主动释放。如果忘记关闭，Inflater 对象的生命会延续到下一次 GC，有一点类似堆内的弱引用。在此过程中，堆外内存会一直增长。把 decompress 函数改成如下代码，重新编译代码后观察，问题解决。public static String decompress(byte[] input) throws Exception { ByteArrayOutputStream out = new ByteArrayOutputStream(); GZIPInputStream gzip = new GZIPInputStream(new ByteArrayInputStream(input)); try { copy(gzip, out); return out.toString(); } finally { try { gzip.close(); } catch (Exception ex) {} try { out.close(); } catch (Exception ex) {} }}总结本课时使用了非常多的工具和命令来进行堆外内存的排查，可以看到，除了使用 jmap 获取堆内内存，还对堆外内存的获取也有不少办法。现在，可以把堆外内存进行更加细致地划分了： 元空间属于堆外内存，主要是方法区和常量池的存储之地，使用参数MaxMetaspaceSize可以限制它的大小，也能观测到它的使用； 直接内存主要是通过 DirectByteBuffer 申请的内存，可以使用参数MaxDirectMemorySize来限制它的大小（参考第 10 课时）。 其他堆外内存，主要是指使用了 Unsafe 或者其他 JNI 手段直接直接申请的内存。这种情况，就没有任何参数能够阻挡它们，要么靠它自己去释放一些内存，要么等待操作系统对它的审判了; 还有一种情况，和内存的使用无关，但是也会造成内存不正常使用，那就是使用了 Process 接口，直接调用了外部的应用程序，这些程序对操作系统的内存使用一般是不可预知的。堆外内存的泄漏是非常严重的，它的排查难度高、影响大，甚至会造成宿主机的死亡。在排查内存问题时，不要忘了这一环。" }, { "title": "利用 MAT 找到问题发生的根本原因", "url": "/posts/mat/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-12 13:23:00 +0000", "snippet": "我们知道，在存储用户输入的密码时，会使用一些 hash 算法对密码进行加工，比如 SHA-1。这些信息同样不允许在日志输出里出现，必须做脱敏处理，但是对于一个拥有系统权限的攻击者来说，这些防护依然是不够的。攻击者可能会直接从内存中获取明文数据，尤其是对于 Java 来说，由于提供了 jmap 这一类非常方便的工具，可以把整个堆内存的数据 dump 下来。比如，“我的世界”这一类使用 Java 开发的游戏，会比其他语言的游戏更加容易破解一些，所以在 JVM 中，如果把密码存储为 char 数组，其安全性会稍微高一些。这是一把双刃剑，在保证安全的前提下，可以借助一些外部的分析工具，我们方便的找到问题根本。有两种方式来获取内存的快照： 通过配置一些参数，可以在发生 OOM 的时候，被动 dump 一份堆栈信息，这是一种； 通过 jmap 主动去获取内存的快照。jmap 命令在 Java 9 之后，使用 jhsdb 命令替代，它们在用法上，区别不大。注意，这些命令本身会占用操作系统的资源，在某些情况下会造成服务响应缓慢，所以不要频繁执行。jmap -dump:format=b,file=heap.bin 37340jhsdb jmap --binaryheap --pid 37340工具 MAT有很多工具能够分析这份内存快照。在前面已多次提到 VisualVm 这个工具，它同样可以加载和分析这份 dump 数据，虽然比较“寒碜”。专业的事情要有专业的工具来做，一款专业的开源分析工具，即 MAT。MAT 工具是基于 Eclipse 平台开发的，本身是一个 Java 程序，所以如果堆快照比较大的话，则需要一台内存比较大的分析机器，并给 MAT 本身加大初始内存，这个可以修改安装目录中的 MemoryAnalyzer.ini 文件。来看一下 MAT 工具的截图，主要的功能都体现在工具栏上了。其中，默认的启动界面，展示了占用内存最高的一些对象，并有一些常用的快捷方式。通常，发生内存泄漏的对象，会在快照中占用比较大的比重，分析这些比较大的对象，是切入问题的第一步。点击对象，可以浏览对象的引用关系，这是一个非常有用的功能： outgoing references 对象的引出 incoming references 对象的引入path to GC Roots 这是快速分析的一个常用功能，显示和 GC Roots 之间的路径。另外一个比较重要的概念，就是浅堆（Shallow Heap）和深堆（Retained Heap），在 MAT 上经常看到这两个数值。浅堆代表了对象本身的内存占用，包括对象自身的内存占用，以及“为了引用”其他对象所占用的内存。深堆是一个统计结果，会循环计算引用的具体对象所占用的内存。但是深堆和“对象大小”有一点不同，深堆指的是一个对象被垃圾回收后，能够释放的内存大小，这些被释放的对象集合，叫做保留集（Retained Set）。如上图所示，A 对象浅堆大小 1 KB，B 对象 2 KB，C 对象 100 KB。A 对象同时引用了 B 对象和 C 对象，但由于 C 对象也被 D 引用，所以 A 对象的深堆大小为 3 KB（1 KB + 2 KB）。A 对象大小（1 KB + 2 KB + 100 KB）&amp;gt; A 对象深堆 &amp;gt; A 对象浅堆。栗子public class Objects4MAT { static class A4MAT { B4MAT b4MAT = new B4MAT(); } static class B4MAT { C4MAT c4MAT = new C4MAT(); } static class C4MAT { List&amp;lt;String&amp;gt; list = new ArrayList&amp;lt;&amp;gt;(); } static class DominatorTreeDemo1 { DominatorTreeDemo2 dominatorTreeDemo2; public void setValue(DominatorTreeDemo2 value) { this.dominatorTreeDemo2 = value; } } static class DominatorTreeDemo2 { DominatorTreeDemo1 dominatorTreeDemo1; public void setValue(DominatorTreeDemo1 value) { this.dominatorTreeDemo1 = value; } } static class Holder { DominatorTreeDemo1 demo1 = new DominatorTreeDemo1(); DominatorTreeDemo2 demo2 = new DominatorTreeDemo2(); Holder() { demo1.setValue(demo2); demo2.setValue(demo1); } private boolean aBoolean =false; private char aChar = &#39;\\0&#39;; private short aShort = 1; private int anInt = 1; private long aLong = 1L; private float aFloat = 1.0F; private double aDouble = 1.0D; private Double aDouble_2 = 1.00; private int[] ints = new int[2]; private String string = &quot;1234&quot;; } Runnable runnable = () -&amp;gt; { Map&amp;lt;String, A4MAT&amp;gt; map = new HashMap&amp;lt;&amp;gt;(); IntStream.range(0, 100).forEach( i -&amp;gt; { byte[] bytes = new byte[1024 * 1024]; String str = new String(bytes).replace(&#39;\\0&#39;, (char)i); A4MAT a4MAT = new A4MAT(); a4MAT.b4MAT.c4MAT.list.add(str); map.put(i + &quot;&quot;, a4MAT); }); Holder holder = new Holder(); try { // sleep forever, retain the memory Thread.sleep(Integer.MAX_VALUE); } catch (InterruptedException e) { e.printStackTrace(); } }; void startHugeThread() throws Exception { new Thread(runnable, &quot;huge-thread&quot;).start(); } public static void main(String[] args) throws Exception { Objects4MAT objects4MAT = new Objects4MAT(); objects4MAT.startHugeThread(); }}以一段代码示例 Objects4MAT，来具体看一下 MAT 工具的使用。代码创建了一个新的线程 “huge-thread”，并建立了一个引用的层级关系，总的内存大约占用 100 MB。同时，demo1 和 demo2 展示了一个循环引用的关系。最后，使用 sleep 函数，让线程永久阻塞住，此时整个堆处于一个相对“静止”的状态。如果是在本地启动的示例代码，则可以使用 Accquire 的方式来获取堆快照。内存泄漏检测如果问题特别突出，则可以通过 Find Leaks 菜单快速找出问题。如下图所示，展示了名称叫做 huge-thread 的线程，持有了超过 96% 的对象，数据被一个 HashMap 所持有。对于特别明显的内存泄漏，在这里能够帮助我们迅速定位，但通常内存泄漏问题会比较隐蔽，我们需要更加复杂的分析。支配树视图支配树视图对数据进行了归类，体现了对象之间的依赖关系。如图，我们通常会根据“深堆”进行倒序排序，可以很容易的看到占用内存比较高的几个对象，点击前面的箭头，即可一层层展开支配关系。图中显示的是其中的 1 MB 数据，从左侧的 inspector 视图，可以看到这 1 MB 的 byte 数组具体内容。从支配树视图同样能够找到我们创建的两个循环依赖，但它们并没有显示这个过程。支配树视图的概念有一点点复杂，我们只需要了解这个概念即可。如上图，左边是引用关系，右边是支配树视图。可以看到 A、B、C 被当作是“虚拟”的根，支配关系是可传递的，因为 C 支配 E，E 支配 G，所以 C 也支配 G。另外，到对象 C 的路径中，可以经过 A，也可以经过 B，因此对象 C 的直接支配者也是根对象。同理，对象 E 是 H 的支配者。我们再来看看比较特殊的 D 和 F。对象 F 与对象 D 相互引用，因为到对象 F 的所有路径必然经过对象 D，因此，对象 D 是对象 F 的直接支配者。可以看到支配树视图并不一定总是能看到对象的真实应用关系，但对我们分析问题的影响并不是很大。这个视图是非常好用的，甚至可以根据 package 进行归类，对目标类的查找也是非常快捷的。编译下面这段代码，可以展开视图，实际观测一下支配树，这和上面介绍的是一致的。package jvm;public class DorminatorTreeDemo { static class A { C c; byte[] data = new byte[1024 * 1024 * 2]; } static class B { C c; byte[] data = new byte[1024 * 1024 * 3]; } static class C { D d; E e; byte[] data = new byte[1024 * 1024 * 5]; } static class D { F f; byte[] data = new byte[1024 * 1024 * 7]; } static class E { G g; byte[] data = new byte[1024 * 1024 * 11]; } static class F { D d; H h; byte[] data = new byte[1024 * 1024 * 13]; } static class G { H h; byte[] data = new byte[1024 * 1024 * 17]; } static class H { byte[] data = new byte[1024 * 1024 * 19]; } A makeRef(A a, B b) { C c = new C(); D d = new D(); E e = new E(); F f = new F(); G g = new G(); H h = new H(); a.c = c; b.c = c; c.e = e; c.d = d; d.f = f; e.g = g; f.d = d; f.h = h; g.h = h; return a; } static A a = new A(); static B b = new B(); public static void main(String[] args) throws Exception { new DorminatorTreeDemo().makeRef(a, b); Thread.sleep(Integer.MAX_VALUE); }}线程视图想要看具体的引用关系，可以通过线程视图。我们在第 5 讲，就已经了解了线程其实是可以作为 GC Roots 的。如图展示了线程内对象的引用关系，以及方法调用关系，相对比 jstack 获取的栈 dump，我们能够更加清晰地看到内存中具体的数据。如下图，找到了 huge-thread，依次展开找到 holder 对象，可以看到循环依赖已经陷入了无限循环的状态。这在查看一些 Java 对象的时候，经常发生，不要感到奇怪。柱状图视图返回头来再看一下柱状图视图，可以看到除了对象的大小，还有类的实例个数。结合 MAT 提供的不同显示方式，往往能够直接定位问题。也可以通过正则过滤一些信息，我们在这里输入 MAT，过滤猜测的、可能出现问题的类，可以看到，创建的这些自定义对象，不多不少正好一百个。右键点击类，然后选择 incoming，这会列出所有的引用关系。再次选择某个引用关系，然后选择菜单“Path To GC Roots”，即可显示到 GC Roots 的全路径。通常在排查内存泄漏的时候，会选择排除虚弱软等引用。使用这种方式，即可在引用之间进行跳转，方便的找到所需要的信息。再介绍一个比较高级的功能。对于堆的快照，其实是一个“瞬时态”，有时候仅仅分析这个瞬时状态，并不一定能确定问题，这就需要对两个或者多个快照进行对比，来确定一个增长趋势。可以将代码中的 100 改成 10 或其他数字，再次 dump 一份快照进行比较。如图，通过分析某类对象的增长，即可辅助问题定位。OQLMAT 支持一种类似于 SQL 的查询语言 OQL（Object Query Language），这个查询语言 VisualVM 工具也支持。以下是几个例子。查询 A4MAT 对象：SELECT * FROM Objects4MAT$A4MAT正则查询 MAT 结尾的对象：SELECT * FROM &quot;.*MAT&quot;查询 String 类的 char 数组：SELECT OBJECTS s.value FROM java.lang.String s SELECT OBJECTS mat.b4MAT FROM Objects4MAT$A4MAT mat根据内存地址查找对象：select * from 0x55a034c8使用 INSTANCEOF 关键字，查找所有子类：SELECT * FROM INSTANCEOF java.util.AbstractCollection查询长度大于 1000 的 byte 数组：SELECT * FROM byte[] s WHERE s.@length&amp;gt;1000查询包含 java 字样的所有字符串：SELECT * FROM java.lang.String s WHERE toString(s) LIKE &quot;.*java.*&quot;查找所有深堆大小大于 1 万的对象：SELECT * FROM INSTANCEOF java.lang.Object o WHERE o.@retainedHeapSize&amp;gt;10000如果你忘记这些属性的名称的话，MAT 是可以自动补全的。OQL 有比较多的语法和用法，若想深入了解，可参考这里。一般，使用上面这些简单的查询语句就够用了。OQL 还有一个好处，就是可以分享。如果你和同事同时在分析一个大堆，不用告诉他先点哪一步、再点哪一步，共享给他一个 OQL 语句就可以了。如下图，MAT 贴心的提供了复制 OQL 的功能，但是用在其他快照上，不会起作用，因为它复制的是如下的内容。总结在 Java 9 以前的版本中，有一个工具 jhat，可以以 html 的方式显示堆栈信息，但和 VisualVm 一样，都太过于简陋，推荐使用 MAT 工具。把问题设定为内存泄漏，但其实 OOM 或者频繁 GC 不一定就是内存泄漏，它也可能是由于某次或者某批请求频繁而创建了大量对象，所以一些严重的、频繁的 GC 问题也能在这里找到原因。有些情况下，占用内存最多的对象，并不一定是引起内存泄漏问题的元凶，但我们也有一个比较通用的分析过程。并不是所有的堆都值得分析的，在做这个耗时的分析之前，需要有个依据。比如，经过初步调优之后，GC 的停顿时间还是较长，则需要找到频繁 GC 的原因；再比如，发现了内存泄漏，需要找到是谁在搞鬼。首先，高度关注快照载入后的初始分析，占用内存高的 topN 对象，大概率是问题产生者。对照自己的代码，首先要分析的，就是产生这些大对象的逻辑。举几个实际发生的例子。有一个 Spring Boot 应用，由于启用了 Swagger 文档生成器，但是由于它的 API 关系非常复杂，嵌套层次又非常深（每次要产生几百 M 的文档！），结果请求几次之后产生了内存溢出，这在 MAT 上就能够一眼定位到问题；而另外一个应用，在读取数据库的时候使用了分页，但是 pageSize 并没有做一些范围检查，结果在请求一个较大分页的时候，使用 fastjson 对获取的数据进行加工，直接 OOM。如果不能通过大对象发现问题，则需要对快照进行深入分析。使用柱状图和支配树视图，配合引入引出和各种排序，能够对内存的使用进行整体的摸底。由于我们能够看到内存中的具体数据，排查一些异常数据就容易得多。可以在程序运行的不同时间点，获取多份内存快照，对比之后问题会更加容易发现。我们还是用一个例子来看。有一个应用，使用了 Kafka 消息队列，开了一般大小的消费缓冲区，Kafka 会复用这个缓冲区，按理说不应该有内存问题，但是应用却频繁发生 GC。通过对比请求高峰和低峰期间的内存快照，发现有工程师把消费数据放入了另外一个 “内存队列”，写了一些画蛇添足的代码，结果在业务高峰期一股脑把数据加载到了内存中。上面这些问题通过分析业务代码，也不难发现其关联性。问题如果非常隐蔽，则需要使用 OQL 等语言，对问题一一排查、确认。可以看到，上手 MAT 工具是有一定门槛的，除了其操作模式，还需要对理论知识有深入的理解，比如 GC Roots、各种引用级别等。在很多场景，MAT 并不仅仅用于内存泄漏的排查。由于能够看到内存上的具体数据，在排查一些难度非常高的 bug 时，MAT 也有用武之地。比如，因为某些脏数据，引起了程序的执行异常，此时，想要找到它们，不要忘了 MAT 这个老朋友。" }, { "title": "解决内存泄漏的思路", "url": "/posts/memory-leakage/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-11 04:13:00 +0000", "snippet": "当一个系统在发生 OOM 的时候，这种行为经常让我感到非常困惑。因为 JVM 是运行在操作系统之上的，操作系统的一些限制，会严重影响 JVM 的行为。故障排查是一个综合性的技术问题，在日常工作中要一定要增加自己的知识广度。多总结、多思考、多记录！现在的互联网服务，一般都做了负载均衡。如果一个实例发生了问题，一定不要着急去重启。万能的重启会暂时缓解问题，但如果不保留现场，可能就错失了解决问题的根本，担心的事情还会到来。所以，当实例发生问题的时候： 第一步是隔离；隔离，就是把这台机器从请求列表里摘除；比如把 nginx 相关的权重设成零。在微服务中，也有相应的隔离机制， 第二步才是问题排查。GC 引起 CPU 飙升我们有个线上应用，单节点在运行一段时间后，CPU 的使用会飙升，一旦飙升，一般怀疑某个业务逻辑的计算量太大，或者是触发了死循环（比如著名的 HashMap 高并发引起的死循环），但排查到最后其实是 GC 的问题。在 Linux 上，分析哪个线程引起的 CPU 问题，通常有一个固定的步骤，如下： top，使用 top 命令，查找到使用 CPU 最多的某个进程，记录它的 pid。使用 Shift + P 快捷键可以按 CPU 的使用率进行排序； top -Hp $pid，再次使用 top 命令，加 -H 参数，查看某个进程中使用 CPU 最多的某个线程，记录线程的 ID； printf %x $tid，使用 printf 函数，将十进制的 tid 转化成十六进制； jstack $pid &amp;gt; $pid.log，使用 jstack 命令，查看 Java 进程的线程栈； less $pid.log；使用 less 命令查看生成的文件，并查找刚才转化的十六进制 tid，找到发生问题的线程上下文。在 jstack 日志中找到了 CPU 使用最多的几个线程。可以看到问题发生的根源，是堆已经满了，但是又没有发生 OOM，于是 GC 进程就一直在那里回收，回收的效果又非常一般，造成 CPU 升高应用假死。接下来，就是具体问题排查，需要把内存 dump 一份下来，使用 MAT 等工具分析具体原因！现场保留这个过程是繁杂而冗长的，需要记忆很多内容。现场保留可以使用自动化方式将必要的信息保存下来，在线上系统会保留信息有：瞬时态和历史态这里引入了工作中经常使用的两个名词：瞬时态和历史态。 瞬时态是指当时发生的、快照类型的元素； 历史态是指按照频率抓取的，有固定监控项的资源变动图。有很多信息，比如 CPU、系统内存等，瞬时态的价值就不如历史态来的直观一些。因为瞬时状态无法体现一个趋势性问题（比如斜率、求导等），而这些信息的获取一般依靠监控系统的协作。但对于 lsof、heap 等，这种没有时间序列概念的混杂信息，体积都比较大，无法进入监控系统产生有用价值，就只能通过瞬时态进行分析。在这种情况下，瞬时态的价值反而更大一些。常见的堆快照，就属于瞬时状态。问题不是凭空产生的，在分析时，一般要收集系统的整体变更集合，比如代码变更、网络变更，甚至数据量的变化。保留信息系统当前网络连接ss -antp &amp;gt; $DUMP_DIR/ss.dump 2&amp;gt;&amp;amp;1ss 命令将系统的所有网络连接输出到 ss.dump 文件中。使用 ss 命令而不是 netstat 的原因，是因为 netstat 在网络连接非常多的情况下，执行非常缓慢。后续的处理，通过查看各种网络连接状态的梳理，来排查 TIME_WAIT 或者 CLOSE_WAIT，或者其他连接过高的问题，非常有用。线上有个系统更新之后，监控到 CLOSE_WAIT 的状态突增，最后整个 JVM 都无法响应。CLOSE_WAIT 状态的产生一般都是代码问题，使用 jstack 最终定位到是因为 HttpClient 的不当使用而引起的，多个连接不完全主动关闭。网络状态统计netstat -s &amp;gt; $DUMP_DIR/netstat-s.dump 2&amp;gt;&amp;amp;1此命令将网络统计状态输出到 netstat-s.dump 文件中。它能够按照各个协议进行统计输出，对把握当时整个网络状态，有非常大的作用。sar -n DEV 1 2 &amp;gt; $DUMP_DIR/sar-traffic.dump 2&amp;gt;&amp;amp;1上面这个命令，会使用 sar 输出当前的网络流量。在一些速度非常高的模块上，比如 Redis、Kafka，就经常发生跑满网卡的情况。如果你的 Java 程序和它们在一起运行，资源则会被挤占，表现形式就是网络通信非常缓慢。进程资源lsof -p $PID &amp;gt; $DUMP_DIR/lsof-$PID.dump非常强大的命令，通过查看进程，能看到打开了哪些文件，这是一个神器，可以以进程的维度来查看整个资源的使用情况，包括每条网络连接、每个打开的文件句柄。同时，也可以很容易的看到连接到了哪些服务器、使用了哪些资源。这个命令在资源非常多的情况下，输出稍慢，耐心等待！！！CPU 资源mpstat &amp;gt; $DUMP_DIR/mpstat.dump 2&amp;gt;&amp;amp;1vmstat 1 3 &amp;gt; $DUMP_DIR/vmstat.dump 2&amp;gt;&amp;amp;1sar -p ALL &amp;gt; $DUMP_DIR/sar-cpu.dump 2&amp;gt;&amp;amp;1uptime &amp;gt; $DUMP_DIR/uptime.dump 2&amp;gt;&amp;amp;1输出当前系统的 CPU 和负载，便于事后排查。这几个命令的功能，有不少重合。注意选择！！！I/O 资源iostat -x &amp;gt; $DUMP_DIR/iostat.dump 2&amp;gt;&amp;amp;1以计算为主的服务节点，I/O 资源会比较正常，但有时也会发生问题，比如日志输出过多，或者磁盘问题等。此命令可以输出每块磁盘的基本性能信息，用来排查 I/O 问题。GC 日志分磁盘问题，就可以使用这个命令去发现。内存问题free -h &amp;gt; $DUMP_DIR/free.dump 2&amp;gt;&amp;amp;1free 命令能够大体展现操作系统的内存概况，这是故障排查中一个非常重要的点，比如 SWAP 影响了 GC，SLAB 区挤占了 JVM 的内存。其他全局ps -ef &amp;gt; $DUMP_DIR/ps.dump 2&amp;gt;&amp;amp;1dmesg &amp;gt; $DUMP_DIR/dmesg.dump 2&amp;gt;&amp;amp;1sysctl -a &amp;gt; $DUMP_DIR/sysctl.dump 2&amp;gt;&amp;amp;1dmesg 是许多静悄悄死掉的服务留下的最后一点线索。当然，ps 作为执行频率最高的一个命令，它当时的输出信息，也必然有一些可以参考的价值。另外，由于内核的配置参数，会对系统和 JVM 产生影响，所以也输出了一份。进程快照，最后的遗言（jinfo）${JDK_BIN}jinfo $PID &amp;gt; $DUMP_DIR/jinfo.dump 2&amp;gt;&amp;amp;1此命令将输出 Java 的基本进程信息，包括环境变量和参数配置，可以查看是否因为一些错误的配置造成了 JVM 问题。dump 堆信息${JDK_BIN}jstat -gcutil $PID &amp;gt; $DUMP_DIR/jstat-gcutil.dump 2&amp;gt;&amp;amp;1${JDK_BIN}jstat -gccapacity $PID &amp;gt; $DUMP_DIR/jstat-gccapacity.dump 2&amp;gt;&amp;amp;1将输出当前的 gc 信息。一般，基本能大体看出一个端倪，如果不能，可将借助 jmap 来进行分析。堆信息${JDK_BIN}jmap $PID &amp;gt; $DUMP_DIR/jmap.dump 2&amp;gt;&amp;amp;1${JDK_BIN}jmap -heap $PID &amp;gt; $DUMP_DIR/jmap-heap.dump 2&amp;gt;&amp;amp;1${JDK_BIN}jmap -histo $PID &amp;gt; $DUMP_DIR/jmap-histo.dump 2&amp;gt;&amp;amp;1${JDK_BIN}jmap -dump:format=b,file=$DUMP_DIR/heap.bin $PID &amp;gt; /dev/null 2&amp;gt;&amp;amp;1jmap 将会得到当前 Java 进程的 dump 信息。如上所示，其实最有用的就是第 4 个命令，但是前面三个能够让你初步对系统概况进行大体判断。因为，第 4 个命令产生的文件，一般都非常的大。而且，需要下载下来，导入 MAT 这样的工具进行深入分析，才能获取结果。这是分析内存泄漏一个必经的过程。JVM 执行栈${JDK_BIN}jstack $PID &amp;gt; $DUMP_DIR/jstack.dump 2&amp;gt;&amp;amp;1jstack 将会获取当时的执行栈。一般会多次取值，我们这里取一次即可。这些信息非常有用，能够还原 Java 进程中的线程情况。top -Hp $PID -b -n 1 -c &amp;gt; $DUMP_DIR/top-$PID.dump 2&amp;gt;&amp;amp;1为了能够得到更加精细的信息，我们使用 top 命令，来获取进程中所有线程的 CPU 信息，这样，就可以看到资源到底耗费在什么地方了。高级替补kill -3 $PID有时候，jstack 并不能够运行，有很多原因，比如 Java 进程几乎不响应了等之类的情况。尝试向进程发送 kill -3 信号，这个信号将会打印 jstack 的 trace 信息到日志文件中，是 jstack 的一个替补方案。对于 jmap 无法执行的问题，也有替补，那就是 GDB 组件中的 gcore，将会生成一个 core 文件。我们可以使用如下的命令去生成 dump：${JDK_BIN}jhsdb jmap --exe ${JDK}java --core $DUMP_DIR/core --binaryheapjmap 命令，它在 9 版本里被干掉了，取而代之的是 jhsdb，可以像下面的命令一样使用：jhsdb jmap --heap --pid 37340jhsdb jmap --pid 37288jhsdb jmap --histo --pid 37340jhsdb jmap --binaryheap --pid 37340 heap 参数能够看到大体的内存布局，以及每一个年代中的内存使用情况。这内存布局，以及在 VisualVM 中看到的 没有什么不同。但由于它是命令行，所以使用更加广泛。 histo 能够大概的看到系统中每一种类型占用的空间大小，用于初步判断问题。比如某个对象 instances 数量很小，但占用的空间很大，这就说明存在大对象。但它也只能看大概的问题，要找到具体原因，还是要 dump 出当前 live 的对象。内存泄漏的现象jmap 命令，它在 9 版本里被干掉了，取而代之的是 jhsdb，你可以像下面的命令一样使用。jhsdb jmap --heap --pid 37340jhsdb jmap --pid 37288jhsdb jmap --histo --pid 37340jhsdb jmap --binaryheap --pid 37340heap 参数能够帮我们看到大体的内存布局，以及每一个年代中的内存使用情况。这和我们前面介绍的内存布局，以及在 VisualVM 中看到的 没有什么不同。但由于它是命令行，所以使用更加广泛。histo 能够大概的看到系统中每一种类型占用的空间大小，用于初步判断问题。比如某个对象 instances 数量很小，但占用的空间很大，这就说明存在大对象。但它也只能看大概的问题，要找到具体原因，还是要 dump 出当前 live 的对象。一般内存溢出，表现形式就是 Old 区的占用持续上升，即使经过了多轮 GC 也没有明显改善。内存泄漏的根本就是，有些对象并没有切断和 GC Roots 的关系，可通过一些工具，能够看到它们的联系。一个卡顿实例有一个关于服务的某个实例，经常发生服务卡顿。由于服务的并发量是比较高的，所以表现也非常明显。每多停顿 1 秒钟，几万用户的请求就会感到延迟。经过统计、类比了此服务其他实例的 CPU、内存、网络、I/O 资源，区别并不是很大，所以一度怀疑是机器硬件的问题。接着对比了节点的 GC 日志，发现无论是 Minor GC，还是 Major GC，这个节点所花费的时间，都比其他实例长得多。通过仔细观察，发现在 GC 发生的时候，vmstat 的 si、so 飙升的非常严重，这和其他实例有着明显的不同。使用 free 命令再次确认，发现 SWAP 分区，使用的比例非常高，引起的具体原因是什么呢？更详细的操作系统内存分布，从 /proc/meminfo 文件中可以看到具体的逻辑内存块大小，有多达 40 项的内存信息，这些信息都可以通过遍历 /proc 目录的一些文件获取。注意到 slabtop 命令显示的有一些异常，dentry（目录高速缓冲）占用非常高。问题最终定位到是运维小伙伴执行了一句命令：find / | grep &quot;x&quot;他是想找一个叫做 x 的文件，看看在哪台服务器上，结果，这些老服务器由于文件太多，扫描后这些文件信息都缓存到了 slab 区上。而服务器开了 swap，操作系统发现物理内存占满后，并没有立即释放 cache，导致每次 GC 都要和硬盘打一次交道。解决方式就是关闭 SWAP 分区。swap 是很多性能场景的万恶之源，建议禁用。当你的应用真正高并发了，SWAP 绝对能让你体验到它魔鬼性的一面：进程倒是死不了了，但 GC 时间长的却让人无法忍受。内存泄漏内存溢出和内存泄漏的区别： 内存溢出是一个结果，原因有内存空间不足、配置错误等因素。 内存泄漏是一个原因。原因不再被使用的对象、没有被回收、没有及时切断与 GC Roots 的联系，很大程度上是一些错误的编程方式，或者过多的无用对象创建引起的。 举个例子，使用了 HashMap 做缓存，但是并没有设置超时时间或者 LRU 策略，造成了放入 Map 对象的数据越来越多，而产生了内存泄漏； 还有一个经常发生的内存泄漏的例子，也是由于 HashMap 产生的。代码如下： package cn.happymaya.jvmpractise.oom; import java.util.HashMap;import java.util.Map;import java.util.Objects; // leak examolepublic class HashMapLeakDemo { // 由于没有重写 Key 类的 hashCode 和 equals 方法 // 造成了放入 HashMap 的所有对象都无法被取出来，它们和外界失联了。所以下面的代码结果是 null。 public static class Key { String title; public Key(String title) { this.title = title; } // @Override// public boolean equals(Object o) {// if (this == o) return true;// if (o == null || getClass() != o.getClass()) return false;// Key key = (Key) o;// return Objects.equals(title, key.title);// }//// @Override// public int hashCode() {// return Objects.hash(title);// } } public static void main(String[] args) { Map&amp;lt;Key, Integer&amp;gt; map = new HashMap&amp;lt;&amp;gt;(); map.put(new Key(&quot;1&quot;), 1); map.put(new Key(&quot;2&quot;), 2); map.put(new Key(&quot;3&quot;), 2); Integer integer = map.get(new Key(&quot;2&quot;)); System.out.println(integer); }} 由于没有重写 Key 类的 hashCode 和 equals 方法，造成了放入 HashMap 的所有对象都无法被取出来，它们和外界失联了。所以代码结果是 null。 即使提供了 equals 方法和 hashCode 方法，也要非常小心，尽量避免使用自定义的对象作为 Key。如下： ​ 关于文件处理器的应用，在读取或者写入一些文件之后，由于发生了一些异常，close 方法又没有放在 finally 块里面，造成了文件句柄的泄漏。由于文件处理十分频繁，产生了严重的内存泄漏问题。 另外，对 Java API 的一些不当使用，也会造成内存泄漏。有喜欢使用 String 的 intern 方法，但如果字符串本身是一个非常长的字符串，而且创建之后不再被使用，则会造成内存泄漏。 import java.util.UUID; public class InternDemo { static String getLongStr() { StringBuilder sb = new StringBuilder(); for (int i = 0; i &amp;lt; 100000; i++) { sb.append(UUID.randomUUID().toString()); } return sb.toString(); } public static void main(String[] args) { while (true) { getLongStr().intern(); } }} " }, { "title": "模拟 JVM 内存溢出场景", "url": "/posts/memory-overfolw/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-10 04:13:00 +0000", "snippet": "模拟 JVM 内存溢出之前，先思考下面几个问题： 老年代溢出为什么那么可怕？ 元空间也有溢出？怎么优化？ 如何配置栈大小？并且避免栈溢出？ 进程突然死掉，没有留下任何信息！如何进行排查？年轻代由于有老年代的担保，一般在内存占满的时候，没什么问题。但老年代满了就比较严重了，它没有其他的空间用来做担保，只能 OOM 了，也就是发生 Out Of Memery Error。JVM 会在这种情况下直接停止工作，是非常严重的后果！！！OOM 一般是内存泄漏引起的，表现在 GC 日志里，一般情况下就是 GC 的时间变长了，而且每次回收的效果都非常一般。GC 后，堆内存的实际占用呈上升趋势。本次，我模拟了三种溢出场景，同时使用 VisualVM 工具 进行观测。虽然 VisualVM 工具非常好用，但一般生产环境都没有这样的条件，所以大概率使用不了。新版本 JDK 把这个工具单独抽离了出去，需要自行下载。需要注意，下载安装完成之后，需要在插件选项中勾选 Visual GC 下载，它将可视化内存布局。堆溢出模拟测试代码：package cn.happymaya.jvmpractise.oom;import com.sun.net.httpserver.HttpContext;import com.sun.net.httpserver.HttpExchange;import com.sun.net.httpserver.HttpServer;import java.io.OutputStream;import java.lang.management.ManagementFactory;import java.lang.management.MemoryPoolMXBean;import java.net.InetSocketAddress;import java.util.ArrayList;import java.util.List;public class OOMTest { public static final int _1MB = 1024 * 1024; static List&amp;lt;byte[]&amp;gt; byteList = new ArrayList&amp;lt;&amp;gt;(); private static void oom(HttpExchange exchange) { try { String response = &quot;oom begin!&quot;; exchange.sendResponseHeaders(200, response.getBytes().length); OutputStream os = exchange.getResponseBody(); os.write(response.getBytes()); os.close(); } catch (Exception ignored) { } for (int i = 0; ; i++) { byte[] bytes = new byte[_1MB]; byteList.add(bytes); System.out.println(i + &quot;MB&quot;); memPrint(); try { Thread.sleep(1000); } catch (Exception ignored) { } } } static void memPrint() { for (MemoryPoolMXBean memoryPoolMXBean : ManagementFactory.getMemoryPoolMXBeans()) { System.out.println(memoryPoolMXBean.getName() + &quot; committed:&quot; + memoryPoolMXBean.getUsage().getCommitted() + &quot; used:&quot; + memoryPoolMXBean.getUsage().getUsed()); } } private static void srv() throws Exception { HttpServer server = HttpServer.create(new InetSocketAddress(8888), 0); HttpContext context = server.createContext(&quot;/&quot;); context.setHandler(OOMTest::oom); server.start(); } public static void main(String[] args) throws Exception{ srv(); }}这份代码开放了一个 HTTP 接口，当你触发它之后，将每秒钟生成 10 MB 的数据。由于它和 GC Roots 的强关联性，每次都不能被回收。程序通过 JMX，将在每一秒创建数据之后，输出一些内存区域的占用情况。然后通过访问 http://localhost:8888 触发后，它将一直运行，直到堆溢出。使用 CMS 收集器进行垃圾回收，可以看到如下的信息：java -Xmx20m -Xmn4m -XX:+UseConcMarkSweepGC -verbose:gc -Xlog:gc,gc+ref=debug,gc+heap=debug,gc+age=trace:file=/tmp/logs/gc_%p.log:tags,uptime,time,level -Xlog:safepoint:file=/tmp/logs/safepoint_%p.log:tags,uptime,time,level -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/logs -XX:ErrorFile=/tmp/logs/hs_error_pid%p.log -XX:-OmitStackTraceInFastThrow OOMTest输出结果如下：0MBCode Cache committed:3932160 used:3900288Metaspace committed:11010048 used:10336696Compressed Class Space committed:1310720 used:1206608PS Eden Space committed:65536000 used:34114968PS Survivor Space committed:10485760 used:0PS Old Gen committed:173539328 used:01MBCode Cache committed:3932160 used:3920128Metaspace committed:11010048 used:10341432Compressed Class Space committed:1310720 used:1206608PS Eden Space committed:65536000 used:40668616PS Survivor Space committed:10485760 used:0PS Old Gen committed:173539328 used:02MBCode Cache committed:3997696 used:3929408Metaspace committed:11010048 used:10347888Compressed Class Space committed:1310720 used:1206608PS Eden Space committed:65536000 used:45911512PS Survivor Space committed:10485760 used:0PS Old Gen committed:173539328 used:03MBCode Cache committed:3997696 used:3944960Metaspace committed:11010048 used:10351712Compressed Class Space committed:1310720 used:1206608PS Eden Space committed:65536000 used:51154408PS Survivor Space committed:10485760 used:0PS Old Gen committed:173539328 used:0·············538MBCode Cache committed:6750208 used:4756352Metaspace committed:12189696 used:11501024Compressed Class Space committed:1441792 used:1274968PS Eden Space committed:74448896 used:70764656PS Survivor Space committed:524288 used:0PS Old Gen committed:2767192064 used:2762065824539MBCode Cache committed:6750208 used:4756352Metaspace committed:12189696 used:11501024Compressed Class Space committed:1441792 used:1274968PS Eden Space committed:74448896 used:74448896PS Survivor Space committed:524288 used:0PS Old Gen committed:2767192064 used:2762048248540MBCode Cache committed:6750208 used:4758400Metaspace committed:12189696 used:11496328Compressed Class Space committed:1441792 used:1272248PS Eden Space committed:74448896 used:74315072PS Survivor Space committed:524288 used:0PS Old Gen committed:2767192064 used:2766792176Exception in thread &quot;Thread-2&quot; java.lang.OutOfMemoryError: Java heap space at cn.happymaya.jvmpractise.oom.OOMTest.oom(OOMTest.java:29) at cn.happymaya.jvmpractise.oom.OOMTest$$Lambda$1/777874839.handle(Unknown Source) at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79) at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83) at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82) at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:700) at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79) at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:672) at sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:158) at sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:431) at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:396) at java.lang.Thread.run(Thread.java:748)Profiler Agent: Connection with agent closedProfiler Agent: JNI OnLoad Initializing...Profiler Agent: JNI OnLoad Initialized successfullyProcess finished with exit code 0最后 JVM 在一阵疯狂的 GC 日志输出后，进程停止了。在现实情况中，JVM 在停止工作之前，很多会垂死挣扎一段时间，这个时候，GC 线程会造成 CPU 飙升，但其实它已经不能工作了。VisualVM 的截图展示了这个溢出结果。可以看到 Eden 区刚开始还是运行平稳的，内存泄漏之后就开始疯狂回收（其实是提升），老年代内存一直增长，直到 OOM。很多参数会影响对象的分配行为，但不是非常必要，一般不去主动调整它们。为了观察这些参数的默认值，通常使用 -XX:+PrintFlagsFinal 参数，输出一些设置信息：java -XX:+PrintFlagsFinal 2 &amp;gt; &amp;amp;1 | grep SurvivorRatio输入如下信息：uintx SurvivorRatio = 8 {product} {default}Java13 输出了几百个参数和默认值，通过修改一些参数来观测一些不同的行为： NewRatio 默认值为 2，表示年轻代是老年代的 1/2。追加参数 “-XX:NewRatio=1”，可以把年轻代和老年代的空间大小调成一样大。在实践中，使用 -Xmn 来设置一个固定值。注意，这两个参数不要用在 G1 垃圾回收器中。 SurvivorRatio 默认值为 8。表示伊甸区和幸存区的比例。在上面的例子中，Eden 的内存大小为：0.8*4MB。S 分区不到 1MB，根本存不下 1MB 数据。 MaxTenuringThreshold 这个值在 CMS 下默认为 6，G1 下默认为 15。这是因为 G1 存在动态阈值计算。这个值和我们前面提到的对象提升有关，如果你想要对象尽量长的时间存在于年轻代，则在 CMS 中，可以把它调整到 15。 java -XX:+PrintFlagsFinal -XX:+UseConcMarkSweepGC 2&amp;gt;&amp;amp;1 | grep MaxTenuringThreshold java -XX:+PrintFlagsFinal -XX:+UseG1GC 2&amp;gt;&amp;amp;1 | grep MaxTenuringThreshold PretenureSizeThreshold 这个参数默认值是 0，意味着所有的对象年轻代优先分配。我们把这个值调小一点，再观测 JVM 的行为。追加参数 -XX:PretenureSizeThreshold=1024，可以看到 VisualVm 中老年代的区域增长。 TargetSurvivorRatio 默认值为 50。在动态计算对象提升阈值的时候使用。计算时，会从年龄最小的对象开始累加，如果累加的对象大小大于幸存区的一半，则将当前的对象 age 作为新的阈值，年龄大于此阈值的对象直接进入老年代。工作中不建议调整这个值，如果要调，则调成比 50 大的值。 UseAdaptiveSizePolicy ，因为它和 CMS 不兼容，所以 CMS 下默认为 false，但 G1 下默认为 true。这是一个非常智能的参数，它是用来自适应调整空间大小的参数。它会在每次 GC 之后，重新计算 Eden、From、To 的大小。在 Java 8 的一些配置中会见到这个参数，但其实在 CMS 和 G1 中是不需要显式设置的。 值的注意的是，Java 8 默认垃圾回收器是 Parallel Scavenge，它的这个参数是默认开启的，有可能会发生把幸存区自动调小的可能，造成一些问题，显式的设置 SurvivorRatio 可以解决这个问题。 使用下面的命令，切换成 G1 的效果：java -Xmx20m -XX:+UseG1GC -verbose:gc -Xlog:gc,gc+ref=debug,gc+heap=debug,gc+age=trace:file=/tmp/logs/gc_%p.log:tags,uptime,time,level -Xlog:safepoint:file=/tmp/logs/safepoint_%p.log:tags,uptime,time,level -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/logs -XX:ErrorFile=/tmp/logs/hs_error_pid%p.log -XX:-OmitStackTraceInFastThrow OOMTest可以通过-XX:G1HeapRegionSize=&amp;lt;N&amp;gt;M命令调整小堆区的大小。元空间溢出堆一般都是指定大小的，但元空间不是。因此元空间发生内存溢出会更加严重，会造成操作系统的内存溢出。在使用的时候，它设置一个上限 for safe。元空间溢出主要是由于加载的类太多或者动态生成的类太多。下面是一段模拟代码。通过访问 http://localhost:8888 触发后，它将会发生元空间溢出。package cn.happymaya.jvmpractise.oom;import com.sun.net.httpserver.HttpContext;import com.sun.net.httpserver.HttpExchange;import com.sun.net.httpserver.HttpServer;import java.io.OutputStream;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.net.InetSocketAddress;import java.net.URL;import java.net.URLClassLoader;import java.util.HashMap;import java.util.Map;public class MetaspaceOOMTest { public interface Facade { void m(String input); } public static class FacadeImpl implements Facade { @Override public void m(String name) { } } public static class MetaspaceFacadeInvocationHandler implements InvocationHandler { private Object impl; public MetaspaceFacadeInvocationHandler(Object impl) { this.impl = impl; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return method.invoke(impl, args); } } private static Map&amp;lt;String, Facade&amp;gt; classLeakingMap = new HashMap&amp;lt;String, Facade&amp;gt;(); private static void oom(HttpExchange exchange) { try { String response = &quot;oom begin!&quot;; exchange.sendResponseHeaders(200, response.getBytes().length); OutputStream os = exchange.getResponseBody(); os.write(response.getBytes()); os.close(); } catch (Exception ex) { } try { for (int i = 0; ; i++) { String jar = &quot;file:&quot; + i + &quot;.jar&quot;; URL[] urls = new URL[]{new URL(jar)}; URLClassLoader newClassLoader = new URLClassLoader(urls); Facade t = (Facade) Proxy.newProxyInstance(newClassLoader, new Class&amp;lt;?&amp;gt;[]{Facade.class}, new MetaspaceFacadeInvocationHandler(new FacadeImpl())); classLeakingMap.put(jar, t); } } catch (Exception ignored) { } } private static void srv() throws Exception { HttpServer server = HttpServer.create(new InetSocketAddress(8888), 0); HttpContext context = server.createContext(&quot;/&quot;); context.setHandler(MetaspaceOOMTest::oom); server.start(); } public static void main(String[] args) throws Exception { srv(); }}上面的代码，通过 Java 代理，不断生成新的 Class。使用命令启动这个类：java -Xmx20m -Xmn4m -XX:+UseG1GC -verbose:gc -Xlog:gc,gc+ref=debug,gc+heap=debug,gc+age=trace:file=/tmp/logs/gc_%p.log:tags,uptime,time,level -Xlog:safepoint:file=/tmp/logs/safepoint_%p.log:tags,uptime,time,level -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/logs -XX:ErrorFile=/tmp/logs/hs_error_pid%p.log -XX:-OmitStackTraceInFastThrow -XX:MetaspaceSize=16M -XX:MaxMetaspaceSize=16M MetaspaceOOMTest在启动的时候，限制 Metaspace 空间大小为 16MB。可以看到运行一小会之后，Metaspace 会发生内存溢出。假如把堆 Metaspace 的限制给去掉，会更可怕。它占用的内存会一直增长。堆外内存溢出严格来说，上面的 Metaspace 也是属于堆外内存的。但是我们这里的堆外内存指的是 Java 应用程序通过直接方式从操作系统中申请的内存。所以严格来说，这里是指直接内存。程序将通过 ByteBuffer 的 allocateDirect 方法每 1 秒钟申请 1MB 的直接内存。不要忘了通过链接触发这个过程。package cn.happymaya.jvmpractise.oom;import com.sun.net.httpserver.HttpContext;import com.sun.net.httpserver.HttpExchange;import com.sun.net.httpserver.HttpServer;import java.io.OutputStream;import java.lang.management.ManagementFactory;import java.lang.management.MemoryPoolMXBean;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.util.ArrayList;import java.util.List; public class OffHeapOOMTest { public static final int _1MB = 1024 * 1024; static List&amp;lt;ByteBuffer&amp;gt; byteList = new ArrayList&amp;lt;&amp;gt;(); private static void oom(HttpExchange exchange) { try { String response = &quot;oom begin!&quot;; exchange.sendResponseHeaders(200, response.getBytes().length); OutputStream os = exchange.getResponseBody(); os.write(response.getBytes()); os.close(); } catch (Exception ex) { } for (int i = 0; ; i++) { ByteBuffer buffer = ByteBuffer.allocateDirect(_1MB); byteList.add(buffer); System.out.println(i + &quot;MB&quot;); memPrint(); try { Thread.sleep(1000); } catch (Exception e) { } } } private static void srv() throws Exception { HttpServer server = HttpServer.create(new InetSocketAddress(8888), 0); HttpContext context = server.createContext(&quot;/&quot;); context.setHandler(OffHeapOOMTest::oom); server.start(); } public static void main(String[] args) throws Exception { srv(); } static void memPrint() { for (MemoryPoolMXBean memoryPoolMXBean : ManagementFactory.getMemoryPoolMXBeans()) { System.out.println(memoryPoolMXBean.getName() + &quot; committed:&quot; + memoryPoolMXBean.getUsage().getCommitted() + &quot; used:&quot; + memoryPoolMXBean.getUsage().getUsed()); } }}但是，使用 VisualVM 看不到这个过程，使用 JMX 的 API 同样也看不到。通过 top 或者操作系统的监控工具，能够看到内存占用的明显增长。为了限制这些危险的内存申请，如果确定在自己的程序中用到了大量的 JNI 和 JNA 操作，要显式的设置 MaxDirectMemorySize 参数。启动命令：java -XX:MaxDirectMemorySize=10M -Xmx10M OffHeapOOMTest以下是程序运行一段时间抛出的错误：Exception in thread &quot;Thread-2&quot; java.lang.OutOfMemoryError: Direct buffer memory at java.nio.Bits.reserveMemory(Bits.java:694) at java.nio.DirectByteBuffer.&amp;lt;init&amp;gt;(DirectByteBuffer.java:123) at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) at OffHeapOOMTest.oom(OffHeapOOMTest.java:27) at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79) at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83) at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82) at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675) at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79) at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647) at sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:158) at sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:431) at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:396) at java.lang.Thread.run(Thread.java:748)栈溢出栈溢出指的就是这里的数据太多造成的泄漏。通过 -Xss 参数可以设置它的大小。比如：# 设置栈大小为 128K-Xss128K由于每个线程都有一个虚拟机栈。线程的开销也是要占用内存的。如果系统中的线程数量过多，那么占用内存的大小也是非常可观的。栈溢出不会造成 JVM 进程死亡，危害“相对较小”。下面是一个简单的模拟栈溢出的代码，只需要递归调用就可以了：package cn.happymaya.jvmpractise.oom;public class StackOverflowTest { static int count = 0; static void a() { System.out.println(count); count++; b(); } static void b() { System.out.println(count); count++; a(); } public static void main(String[] args) throws Exception { a(); }}运行后，程序直接报错：*** java.lang.instrument ASSERTION FAILED ***: &quot;!errorOutstanding&quot; with message transform method call failed at JPLISAgent.c line: 844*** java.lang.instrument ASSERTION FAILED ***: &quot;!errorOutstanding&quot; with message transform method call failed at JPLISAgent.c line: 844Exception in thread &quot;main&quot; java.lang.StackOverflowError at java.io.PrintStream.write(PrintStream.java:526) at java.io.PrintStream.print(PrintStream.java:597) at java.io.PrintStream.println(PrintStream.java:736) at cn.happymaya.jvmpractise.oom.StackOverflowTest.a(StackOverflowTest.java:6) at cn.happymaya.jvmpractise.oom.StackOverflowTest.b(StackOverflowTest.java:13) at cn.happymaya.jvmpractise.oom.StackOverflowTest.a(StackOverflowTest.java:8)如果应用经常发生这种情况，可以试着调大这个值。但一般都是因为程序错误引起的，最好检查一下自己的代码。进程异常退出上面这几种溢出场景，都有明确的原因和报错，排查起来也是非常容易的。但是还有一类应用，死亡的时候，静悄悄的，什么都没留下。这是趣味性和技巧性非常突出的一个问题。通过执行 dmesg 命令，大概率会看到进程崩溃信息躺在那里。为了能看到发生的时间，我习惯性加上参数 T（dmesg -T）。这个现象，和 Linux 的内存管理有关。由于 Linux 系统采用的是虚拟内存分配方式，JVM 的代码、库、堆和栈的使用都会消耗内存，但是申请出来的内存，只要没真正 access 过，是不算的，因为没有真正为之分配物理页面。随着使用内存越用越多。第一层防护墙就是 SWAP；当 SWAP 也用的差不多了，会尝试释放 cache；当这两者资源都耗尽，杀手就出现了。oom-killer 会在系统内存耗尽的情况下跳出来，选择性的干掉一些进程以求释放一点内存。所以这时的 Java 进程，是操作系统“主动”终结的，JVM 连发表遗言的机会都没有。这个信息，只能在操作系统日志里查找。要解决这种问题，首先不能太贪婪。比如一共 8GB 的机器，你把整整 7.5GB 都分配给了 JVM。当操作系统内存不足时，你的 JVM 就可能成为 oom-killer 的猎物。相对于被动终结，还有一种主动求死的方式。比如：直接调用 System.exit() 函数。这个函数危险得很，它将强制终止应用，而且什么都不会留下。应该扫描的代码，确保这样的逻辑不会存在。还有一种最初级最常见还经常发生的，会造成应用程序意外死亡的情况，那就是对 Java 程序错误的启动方式。使用 XShell 登陆之后，调用java com.cn.AA &amp;amp;命令进行启动。这样调用还算有点意识，在最后使用了“&amp;amp;”号，以期望进程在后台运行。但可惜的是，很多情况下，随着 XShell Tab 页的关闭，或者等待超时，后面的 Java 进程就随着一块停止了，很让人困惑。正确的启动方式，就是使用 nohup 关键字：nohup java com.cn.AA &amp;amp;，或者阻塞在其他更加长命的进程里（比如docker）。进程这种静悄悄的死亡方式，会给问题排查带来更多的困难。在发生问题时，要确保留下了足够的证据，来支持接下来的分析。不能喊一句“出事啦”，然后就陷入无从下手的尴尬境地。通常，在关闭服务的时候，会使用“kill -15”，而不是“kill -9”，以便让服务在临死之前喘口气。信号 9 和 15 的区别，是面试经常问的一个问题，也是一种非常有效的手段。" }, { "title": "解决 GC 问题的思路", "url": "/posts/gc-5/", "categories": "Java, JVM", "tags": "JVM, GC", "date": "2021-08-09 14:13:00 +0000", "snippet": "想要下手解决 GC 问题，首先需要掌握下面三种问题： 使用 jstat 命令查看 JVM 的 GC 情况； 面对海量 GC 日志参数，快速抓住问题根源！ 掌握的日志分析工具。工欲善其事，必先利其器。优化手段，包括代码优化、扩容、参数优化，甚至估算，都需要一些支撑信息加以判断。对于 JVM 来说，一种情况是 GC 时间过长，会影响用户的体验，这个时候就需要调整某些 JVM 参数、观察日志。另外一种情况就比较严重了，发生了 OOM，或者操作系统的内存溢出。服务直接宕机，需要寻找背后的原因。这时，GC 日志能够帮找到问题的根源。GC 日志输出最近几年 Java 的版本更新速度是很快的，JVM 的参数配置其实变化也很大。就拿 GC 日志这一块来说，Java 9 几乎是推翻重来。网络上的一些文章，把这些参数写的乱七八糟，根本不能投入生产。如果碰到不能被识别的参数，先确认一下自己的 Java 版本。在事故出现的时候，通常并不是那么温柔。在半夜里就能接到报警电话，这是因为很多定时任务都设定在夜深人静的时候执行。这个时候，再去看 jstat 已经来不及了，需要保留现场。这个便是看门狗的工作，看门狗可以通过设置一些 JVM 参数进行配置。下面命令行：在 JDK8 中的使用#!/bin/shLOG_DIR=&quot;/tmp/logs&quot;JAVA_OPT_LOG=&quot; -verbose:gc&quot;JAVA_OPT_LOG=&quot;${JAVA_OPT_LOG} -XX:+PrintGCDetails&quot;JAVA_OPT_LOG=&quot;${JAVA_OPT_LOG} -XX:+PrintGCDateStamps&quot;JAVA_OPT_LOG=&quot;${JAVA_OPT_LOG} -XX:+PrintGCApplicationStoppedTime&quot;JAVA_OPT_LOG=&quot;${JAVA_OPT_LOG} -XX:+PrintTenuringDistribution&quot;JAVA_OPT_LOG=&quot;${JAVA_OPT_LOG} -Xloggc:${LOG_DIR}/gc_%p.log&quot;JAVA_OPT_OOM=&quot; -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${LOG_DIR} -XX:ErrorFile=${LOG_DIR}/hs_error_pid%p.log &quot;JAVA_OPT=&quot;${JAVA_OPT_LOG} ${JAVA_OPT_OOM}&quot;JAVA_OPT=&quot;${JAVA_OPT} -XX:-OmitStackTraceInFastThrow&quot;合成一行，如下：-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintTenuringDistribution -Xloggc:/tmp/logs/gc_%p.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/logs -XX:ErrorFile=/tmp/logs/hs_error_pid%p.log -XX:-OmitStackTraceInFastThrow这些参数的含义如下表： 参数 意义 -verbose:gc 打印 GC 日志 PrintGCDetails 打印详细 GC 日志 PrintGCDateStamps 系统时间，更加可读，PrintGCTimeStamps 是 JVM 启动时间 PrintGCApplicationStoppedTime 打印 STW 时间 PrintTenuringDistribution 打印对象年龄分布，对调优 MaxTenuringThreshold 参数帮助很大 loggc 将以上 GC 内容输出到文件中 OOM 时的参数： 参数 意义 HeapDumpOnOutOfMemoryError OOM 时 Dump 信息，非常有用 HeapDumpPath Dump 文件保存路径 ErrorFile 错误日志存放路径 最后的 OmitStackTraceInFastThrow ，是 JVM 用来缩简日志输出的。开启这个参数之后，如果多次发生了空指针异常，将会打印以下信息：java.lang.NullPointerExceptionjava.lang.NullPointerExceptionjava.lang.NullPointerExceptionjava.lang.NullPointerException在实际生产中，这个参数是默认开启的，这样导致有时候排查问题非常不方便（很多研发对此无能为力），这里把它关闭，但这样它会输出所有的异常堆栈，日志会多很多！！！GC 日志的意义 表示 GC 发生的时间，一般使用可读的方式打印； 表示日志表明是 G1 的“转移暂停: 混合模式”，停顿了约 223ms； 表明由 8 个 Worker 线程并行执行，消耗了 214ms； 表示 Diff 越小越好，说明每个工作线程的速度都很均匀； 表示外部根区扫描，外部根是堆外区。JNI 引用，JVM 系统目录，Classloaders 等； 表示更新 RSet 的时间信息； 表示该任务主要是对 CSet 中存活对象进行转移（复制）； 表示花在 GC 之外的工作线程的时间； 表示并行阶段的 GC 总时间； 表示其他清理活动； 表示收集结果统计； 表示时间花费统计。GC 日志描述了垃圾回收器过程中的几乎每一个阶段。但即使了解了这些数值的意义，在分析问题时，也会感到吃力，可以借助图形化的分析工具进行分析。尤其注意的是最后一行日志，需要详细描述。可以看到 GC 花费的时间，竟然有 3 个数值。如果你手有 Linux 机器，可以执行以下命令：time ls /可以看到一段命令的执行，有三种纬度的时间统计： real 实际花费的时间，指的是从开始到结束所花费的时间。比如进程在等待 I/O 完成，这个阻塞时间也会被计算在内； user 指的是进程在用户态（User Mode）所花费的时间，只统计本进程所使用的时间，注意是指多核； sys 指的是进程在核心态（Kernel Mode）花费的 CPU 时间量，指的是内核中的系统调用所花费的时间，只统计本进程所使用的时间。在上面的 GC 日志中，real &amp;lt; user + sys，因为使用了多核进行垃圾收集，所以实际发生的时间比 (user + sys) 少很多。在多核机器上，这很常见。[Times: user=1.64 sys=0.00, real=0.23 secs]下面是一个串行垃圾收集器收集的 GC 时间的示例。由于串行垃圾收集器始终仅使用一个线程，因此实际使用的时间等于用户和系统时间的总和：[Times: user=0.29 sys=0.00, real=0.29 secs]统计 GC的以哪个时间为准呢？一般来说，用户只关心系统停顿了多少秒，对实际的影响时间非常感兴趣。至于背后是怎么实现的，是多核还是单核，是用户态还是内核态，它们都不关心。所以直接使用 real 字段。GC日志可视化肉眼可见的这些日志信息，让人非常头晕，尤其是日志文件特别大的时候。所幸现在有一些在线分析平台，可以帮助我们分析这个过程。最常用的 gceasy 。以下是一个使用了 G1 垃圾回收器，堆内存为 6GB 的服务，运行 5 天的 GC 日志。 堆信息 关键信息 从图中可以看到一些性能的关键信息： 吞吐量：98.6%（一般超过 95% 就 ok 了）； 最大延迟：230ms，平均延迟：42.8ms； 延迟要看服务的接受程度，比如 SLA 定义 50ms 返回数据，上面的最大延迟就会有一点问题。本服务接近 99% 的停顿在 100ms 以下，可以说算是非常优秀了。 在看这些信息的时候，一定要结合宿主服务器的监控去看。比如 GC 发生期间，CPU 会突然出现尖锋，就证明 GC 对 CPU 资源使用的有点多。但多数情况下，如果吞吐量和延迟在可接受的范围内，这些对 CPU 的超额使用是可以忍受的。 交互式图表 可以对有问题的区域进行放大查看，图中表示垃圾回收后的空间释放，可以看到效果是比较好的。 G1 的时间耗时 如图展示了 GC 的每个阶段花费的时间。可以看到平均耗时最长的阶段，就是 Concurrent Mark 阶段，但由于是并发的，影响并不大。随着时间的推移，YoungGC 竟然达到了 136485 次。运行 5 天，光花在 GC 上的时间就有 2 个多小时，还是比较可观的。 其他 如图所示，整个 JVM 创建了 100 多 T 的数据，其中有 2.4TB 被 promoted 到老年代。 另外，还有一些 safepoint 的信息等，你可以自行探索。 那到底什么样的数据才是有问题的呢？gceasy 提供了几个案例。比如下面这个就是停顿时间明显超长的 GC 问题。 下面这个是典型的内存泄漏。 上面这些问题都是非常明显的。但大多数情况下，问题是偶发的。从基本的衡量指标，就能考量到整体的服务水准。如果这些都没有问题，就要看曲线的尖峰。一般来说，任何不平滑的曲线，都是值得怀疑的，那就需要看一下当时的业务情况具体是什么样子的。是用户请求突增引起的，还是执行了一个批量的定时任务，再或者查询了大批量的数据，这要和一些服务的监控一起看才能定位出根本问题。只靠 GC 来定位问题是比较困难的，我们只需要知道它有问题就可以了。后面，会介绍更多的支持工具进行问题的排解。另外，GCViewer 这个工具也是常用的，可以下载到本地，以 jar 包的方式运行。在一些极端情况下，也可以使用脚本简单过滤一下。比如下面行命令，就是筛选停顿超过 100ms 的 GC 日志和它的行数（G1）。# grep -n real gc.log | awk -F&quot;=| &quot; &#39;{ if($8&amp;gt;0.1){ print }}&#39;1975: [Times: user=2.03 sys=0.93, real=0.75 secs]2915: [Times: user=1.82 sys=0.65, real=0.64 secs]16492: [Times: user=0.47 sys=0.89, real=0.35 secs]16627: [Times: user=0.71 sys=0.76, real=0.39 secs]16801: [Times: user=1.41 sys=0.48, real=0.49 secs]17045: [Times: user=0.35 sys=1.25, real=0.41 secs]jstat上面的可视化工具，必须经历导出、上传、分析三个阶段，这种速度太慢了。有没有可以实时看堆内存的工具？你可能会第一时间想到 jstat 命令。第一次接触这个命令，我也是很迷惑的，主要是输出的字段太多，不了解什么意义。但其实了解内存区域划分和堆划分之后，再看这些名词就非常简单了。拿 -gcutil 参数来说明一下。jstat -gcutil $pid 1000只需要提供一个 Java 进程的 ID，然后指定间隔时间（毫秒）就 OK 了。S0 S1 E O M CCS YGC YGCT FGC FGCT GCT0.00 0.00 72.03 0.35 54.12 55.72 11122 16.019 0 0.000 16.0190.00 0.00 95.39 0.35 54.12 55.72 11123 16.024 0 0.000 16.0240.00 0.00 25.32 0.35 54.12 55.72 11125 16.025 0 0.000 16.0250.00 0.00 37.00 0.35 54.12 55.72 11126 16.028 0 0.000 16.0280.00 0.00 60.35 0.35 54.12 55.72 11127 16.028 0 0.000 16.028可以看到，E 其实是 Eden 的缩写，S0 对应的是 Surivor0，S1 对应的是 Surivor1，O 代表的是 Old，而 M 代表的是 Metaspace。YGC 代表的是年轻代的回收次数，YGC T对应的是年轻代的回收耗时。那么 FGC 肯定代表的是 Full GC 的次数。你在看日志的时候，一定要注意其中的规律。-gcutil 位置的参数可以有很多种。最常用的有 gc、gcutil、gccause、gcnew 等，其他的了解一下即可。 gc: 显示和 GC 相关的 堆信息； gcutil: 显示 垃圾回收信息； gccause: 显示垃圾回收 的相关信息（同 -gcutil），同时显示 最后一次 或 当前 正在发生的垃圾回收的 诱因； gcnew: 显示 新生代 信息； gccapacity: 显示 各个代 的 容量 以及 使用情况； gcmetacapacity: 显示 元空间 metaspace 的大小； gcnewcapacity: 显示 新生代大小 和 使用情况； gcold: 显示 老年代 和 永久代 的信息； gcoldcapacity: 显示 老年代 的大小； printcompilation: 输出 JIT 编译 的方法信息； class: 显示 类加载 ClassLoader 的相关信息； compiler: 显示 JIT 编译 的相关信息；如果 GC 问题特别明显，通过 jstat 可以快速发现。我们在启动命令行中加上参数 -t，可以输出从程序启动到现在的时间。如果 FGC 和启动时间的比值太大，就证明系统的吞吐量比较小，GC 花费的时间太多了。另外，如果老年代在 Full GC 之后，没有明显的下降，那可能内存已经达到了瓶颈，或者有内存泄漏问题。下面这行命令，就追加了 GC 时间的增量和 GC 时间比率两列。jstat -gcutil -t 90542 1000 | awk &#39;BEGIN{pre=0}{if(NR&amp;gt;1) {print $0 &quot;\\t&quot; ($12-pre) &quot;\\t&quot; $12*100/$1 ; pre=$12 } else { print $0 &quot;\\tGCT_INC\\tRate&quot;} }&#39; Timestamp S0 S1 E O M CCS YGC YGCT FGC FGCT GCT GCT_INC Rate 18.7 0.00 100.00 6.02 1.45 84.81 76.09 1 0.002 0 0.000 0.002 0.002 0.0106952 19.7 0.00 100.00 6.02 1.45 84.81 76.09 1 0.002 0 0.000 0.002 0 0.0101523​GC 日志也会搞鬼ElasticSearch 的速度是非常快的，为了压榨它的性能，对磁盘的读写几乎是全速的。它在后台做了很多 Merge 动作，将小块的索引合并成大块的索引。还有 TransLog 等预写动作，都是 I/O 大户。使用 iostat -x 1 可以看到具体的 I/O 使用状况。问题是，我们有一套 ES 集群，在访问高峰时，有多个 ES 节点发生了严重的 STW 问题。有的节点竟停顿了足足有 7~8 秒。[Times: user=0.42 sys=0.03, real=7.62 secs]从日志可以看到在 GC 时用户态只停顿了 420ms，但真实的停顿时间却有 7.62 秒。盘点一下资源，唯一超额利用的可能就是 I/O 资源了（%util 保持在 90 以上），GC 可能在等待 I/O。通过搜索，发现已经有人出现过这个问题，这里直接说原因和结果。原因就在于，写 GC 日志的 write 动作，是统计在 STW 的时间里的。在我们的场景中，由于 ES 的索引数据，和 GC 日志放在了一个磁盘，GC 时写日志的动作，就和写数据文件的动作产生了资源争用。解决方式也是比较容易的，把 ES 的日志文件，单独放在一块普通 HDD 磁盘上就可以了。 Linux生产环境上，最常用的一套“Sed“技巧 Linux生产环境上，最常用的一套“AWK“技巧" }, { "title": "大流量高并发场景下的估算和调优", "url": "/posts/gc-4/", "categories": "Java, JVM", "tags": "JVM, GC", "date": "2021-08-08 08:33:00 +0000", "snippet": "垃圾回收器一般使用默认参数，就可以比较好的运行。如果用错了某些参数，后果可能会比较严重！！！曾经看到，一个小伙伴想要验证某个刚刚学到的优化参数，结果引起了线上 GC 的严重问题。如果应用程序目前已经满足了需求，就不要随便动这些参数了。另外，优化代码获得的性能提升，远远大于参数调整所获得的性能提升，莫要纯粹为了调参数而走了弯路。考量指标： 系统容量（Capacity）； 延迟（Latency）； 吞吐量（Throughput）系统容量比如，领导要求每个月的运维费用不能超过 x 万，那就决定了机器最多是 2C4G 的。举个极端的例子。假设内存是无限大的，那么无论是存活对象，还是垃圾对象，都不需要额外的计算和回收，只需要往里放就可以了。这样，就没有什么吞吐量和延迟的概念了。但这毕竟是做梦。越是资源限制比较严格的系统，对它的优化就会越明显。通常在一个资源相对宽松的环境下优化的参数，平移到另外一个限制资源的环境下，并不是最优解。吞吐量 &amp;amp; 延迟假如有一个面包店，其首要目标是卖出更多的面包，因为赚钱来说是最要紧的。为了让客人更快买到面包，引进了很多先进的设备，使得制作面包的间隔减少到 30 分钟，一批面包可以有 100 个。工人师傅是拿工资的，并不想和你一样加班。按照一天 8 小时工作制，每天就可以制作 8 x 2 x1 00 = 1600 个面包。但是依旧不满意，因为每天的客人都很多，需求大约是 2000 个面包。只好再引进更加先进的设备，这种设备可以一次做出 200 个面包，一天可以做 2000~3000 个面包，但是每运行一段时间就需要冷却一会儿。原来每个客人最多等 30 分钟就可以拿到面包，现在有的客人需要等待 40 分钟。客人通常受不了这么长的等待时间，第二天就不来了。考虑到营业目标，就可以抽象出两个概念： 吞吐量，也就是每天制作的面包数量； 延迟，也就是等待的时间，涉及影响顾客的满意度。吞吐量大不代表响应能力高，吞吐量一般这么描述：在一个时间段内完成了多少个事务操作；在一个小时之内完成了多少批量操作。响应能力是以最大的延迟时间来判断的，比如：一个桌面按钮对一个触发事件响应有多快；需要多长时间返回一个网页；查询一行 SQL 需要多长时间，等等。这两个目标，在有限的资源下，通常不能够同时达到，需要做一些权衡。选择垃圾回收器 如果堆大小不是很大（比如 100MB），选择串行收集器一般是效率最高的。参数：-XX:+UseSerialGC； 如果应用运行在单核的机器上，或者虚拟机核数只有 1C，选择串行收集器依然是合适的，这时候启用一些并行收集器没有任何收益。参数：-XX:+UseSerialGC； 如果应用是“吞吐量”优先的，并且对较长时间的停顿没有什么特别的要求。选择并行收集器是比较好的。参数：-XX:+UseParallelGC； 如果应用对“响应时间”要求较高，想要较少的停顿。甚至 1 秒的停顿都会引起大量的请求失败，那么选择 G1、ZGC、CMS 都是合理的。虽然这些收集器的 GC 停顿通常都比较短，但它需要一些额外的资源去处理这些工作，通常吞吐量会低一些。参数：-XX:+UseConcMarkSweepGC、-XX:+UseG1GC、-XX:+UseZGC 等。从上面这些出发点来看，平常的 Web 服务器，都是对响应性要求非常高的。选择性其实就集中在 CMS、G1、ZGC 上。而对于某些定时任务，使用并行收集器，是一个比较好的选择。大流量应用特点这是一类对延迟非常敏感的系统。吞吐量一般可以通过堆机器解决。如果一项业务有价值，客户很喜欢，那亿级流量很容易就能达到了。假如某个接口一天有 10 亿次请求，每秒的峰值大概也就 5~6 w/秒，虽然不算是很大，但也不算小。最直接的影响就是：可能你发个版，几万用户的请求就抖一抖。一般达到这种量级的系统，承接请求的都不是一台服务器，接口都会要求快速响应，一般不会超过 100ms。这种系统，一般都是社交、电商、游戏、支付场景等，要求的是短、平、快。长时间停顿会堆积海量的请求，所以在停顿发生的时候，表现会特别明显。要考量这些系统，有很多指标。 每秒处理的事务数量（TPS）； 平均响应时间（AVG）； TP 值，比如 TP90 代表有 90% 的请求响应时间小于 x 毫秒。可以看出来，它和 JVM 的某些指标很像。尤其是 TP 值，最能代表系统中到底有多少长尾请求，这部分请求才是影响系统稳定性的元凶。大多数情况下，GC 增加，长尾请求的数量也会增加。我们的目标，就是减少这些停顿。本文假定使用的是 CMS 垃圾回收器。估算在《编程珠玑》第七章里，将估算看作程序员的一项非常重要的技能。这是一种化繁为简的能力，不要求极度精确，但对问题的分析有着巨大的帮助。拿一个简单的 Feed 业务来说。查询用户在社交网站上发送的帖子，还需要查询第一页的留言（大概是 15 条），它们共同组成了每次查询后的实体。public class Feed { private User user; private List&amp;lt;Comment&amp;gt; commentList; private String content;}这种类型的数据结构，一般返回体都比较大，大概会有几 KB 到几十 KB 不等。以此就可以对这些数据进行以大体估算。具体的数据来源可以看日志，也可以分析线上的请求。假设接口每天有 10 亿次请求，如果每次请求的大小有 20KB（很容易达到），那么一天的流量就有 18TB 之巨。假如高峰请求 6w/s，部署了 10 台机器，那么每个 JVM 的流量就可以达到 120MB/s，这个速度算是比较快的了。如果实在不知道怎么去算这个数字，那就按照峰值的 2 倍进行准备，一般都是 OK 的。调优问题是这样的，机器是 4C8GB 的，分配给了 JVM 1024 * 8GB / 3 * 2= 5460 MB （经验值，独占式应用一般使用操作系统内存的 2/3，留下的 1/3 给堆外内存和操作系统。 ）的空间。那么年轻代大小就有 5460 MB / 3 = 1820 MB。进而可以推断出，Eden 区的大小约 1456MB，那么大约只需要 12 秒，就会发生一次 Minor GC。不仅如此，每隔半个小时，会发生一次 Major GC。不管是年轻代还是老年代，这个 GC 频率都有点频繁了。Survivor 区大小，大约是 182MB 左右，如果稍微有点流量偏移，或者流量突增，再或者和其他接口共用了 JVM，那么这个 Survivor 区就已经装不下 Minor GC 后的内容了。总有一部分超出的容量，需要老年代来补齐。这些垃圾信息就要保存更长时间，直到老年代空间不足。可以发现，用户请求完这些信息之后，很快它们就会变成垃圾。所以每次 MinorGC 之后，剩下的对象都很少。也就是说，流量虽然很多，但大多数都在年轻代就销毁了。如果加大年轻代的大小，由于 GC 的时间受到活跃对象数的影响，回收时间并不会增加太多。如果把一半空间给年轻代。也就是下面的配置：-XX:+UseConcMarkSweepGC -Xmx5460M -Xms5460M -Xmn2730M重新估算一下，发现 Minor GC 的间隔，由 12 秒提高到了 18 秒。线上观察：[ParNew: 2292326K‐&amp;gt;243160K(2795520K), 0.1021743 secs]3264966K‐&amp;gt;10880154K(1215800K), 0.1021417 secs][Times: user=0.52 sys=0.02, real=0.2 secs]Minor GC 有所改善，但是并没有显著的提升。相比较而言，Major GC 的间隔却增加到了 3 小时，是一个非常大的性能优化。这就是在容量限制下的初步调优方案。此种场景，我以更加激进一些，调大年轻代（顺便调大了幸存区），让对象在年轻代停留的时间更长一些，有更多的 buffer 空间。这样 Minor GC 间隔又可以提高到 23 秒。参数配置：-XX:+UseConcMarkSweepGC -Xmx5460M -Xms5460M -Xmn3460M一切看起来很美好，但还是有一个瑕疵。问题如下：由于每秒的请求都非常大，如果应用重启或者更新，流量瞬间打过来，JVM 还没预热完毕，这时候就会有大量的用户请求超时、失败。为了解决这种问题，通常会逐步的把新发布的机器进行放量预热。比如第一秒 100 请求，第二秒 200 请求，第三秒 5000 请求。大型的应用都会有这个预热过程。如图所示，负载均衡器负责服务的放量，server4 将在 6 秒之后流量正常流通。但是奇怪的是，每次重启大约 20 多秒以后，就会发生一次诡异的 Full GC。注意是 Full GC，而不是老年代的 Major GC，也不是年轻代的 Minor GC。事实上，经过观察，此时年轻代和老年代的空间还有很大一部分，那 Full GC 是怎么产生的呢？一般，Full GC 都是在老年代空间不足的时候执行。但不要忘了，还有一个区域叫作 Metaspace，它的容量是没有上限的，但是每当它扩容时，就会发生 Full GC。使用下面的命令可以看到它的默认值：java -XX:+PrintFlagsFinal 2&amp;gt;&amp;amp;1 | grep Meta默认值如下：size_t MetaspaceSize = 21807104 {pd product} {default}size_t MaxMetaspaceSize = 18446744073709547520 {product} {default}可以看到 MetaspaceSize 的大小大约是 20MB。这个初始值太小了。现在很多类库，包括 Spring，都会大量生成一些动态类，20MB 很容易就超了，可以试着调大这个数值。按照经验，一般调整成 256MB 就足够了。同时，为了避免无限制使用造成操作系统内存溢出，我们同时设置它的上限。配置参数如下：-XX:+UseConcMarkSweepGC -Xmx5460M -Xms5460M -Xmn3460M -XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M经观察，启动后停顿消失。这种方式通常是行之有效的，但也可以通过扩容机器内存或者扩容机器数量的办法，显著地降低 GC 频率。这些都是在估算容量后的优化手段。把部分机器升级到 8C16GB 的机器，使用如下的参数:-XX:+UseConcMarkSweepGC -Xmx10920M -Xms10920M -Xmn5460M -XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M相比较其他实例，系统运行的特别棒，系统平均 1 分钟左右发生一次 MinorGC，老年代观察了一天才发生 GC，响应水平明显提高。这是一种非常简单粗暴的手段，但是有效。对 JVM 的优化，不仅仅是优化参数本身。我们目的是解决问题，寻求多种有用手段。总结如果没有明显的内存泄漏问题和严重的性能问题，专门调优一些 JVM 参数是非常没有必要的，优化空间也比较小。因此，一般优化的思路有一个重要的顺序： 程序优化，效果通常非常大； 扩容，如果金钱的成本比较小，不要和自己过不去； 参数调优，在成本、吞吐量、延迟之间找一个平衡点。本文主要是在第三点的基础上，一步一步地增加 GC 的间隔，达到更好的效果。还可以再加一些原则用以辅助完成优化： 一个长时间的压测是必要的，通常我们使用 JMeter 工具； 如果线上有多个节点，可以把优化在其中几个节点上生效。等优化真正有效果之后再全面推进； 优化过程和目标之间可能是循环的，结果和目标不匹配，要推翻重来。业务场景是高并发的。对象诞生的快，死亡的也快，对年轻代的利用直接影响了整个堆的垃圾收集： 足够大的年轻代，会增加系统的吞吐，但不会增加 GC 的负担； 容量足够的 Survivor 区，能够让对象尽可能的留在年轻代，减少对象的晋升，进而减少 Major GC。还有一个元空间引起的 Full GC 的过程，这在高并发的场景下影响会格外突出，尤其是对于使用了大量动态类的应用来说。通过调大它的初始值，可以解决这个问题。 对于 nginx,可以使用 lua脚本。最笨的办法，手动改动 nginx upstream 的 weight 权重。 对 SpringCloud来说，可以重写 Robbin 的负载均衡策略。 自研框架也是，主要是在负载均衡层面进行控制。 关于预热算法，可以参考 Guava 的RateLimiter，它是线性增长的算法。" }, { "title": "垃圾回收 - CMS 垃圾收集器", "url": "/posts/gc-3/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-07 13:23:00 +0000", "snippet": "垃圾回收 - G1 G1 的回收原理是什么？为什么 G1 比传统 GC 回收性能好？ 为什么 G1 如此完美仍然会有 ZGC？先简单回忆一下 CMS 垃圾收集器的一个极端场景（而且是经常发生的场景）。在发生 Minor GC 时，由于 Survivor 区已经放不下了，多出的对象只能提升（promotion）到老年代。但是此时老年代因为空间碎片的缘故，会发生 concurrent mode failure 的错误。这个时候，就需要降级为 Serail Old 垃圾回收器进行收集。这就是比 concurrent mode failure 更加严重的 promotion failed 问题。一次简单的 Major GC，竟然能演化成耗时最长的 Full GC。最要命的是，这个停顿时间是不可预知的。有没有一种办法，能够首先定义一个停顿时间，然后反向推算收集内容呢？就像是领导在年初制定 KPI 一样，分配的任务多就多干些，分配的任务少就少干点。很久之前就有领导教导过我，如果你列的目标太大，看起来无法完成，不要怕。有一个叫作里程碑的名词，可以让我们以小跑的姿态，完成一次马拉松。G1 的思路说起来也类似，它不要求每次都把垃圾清理的干干净净，它只是努力做它认为对的事情。我们要求 G1，在任意 1 秒的时间内，停顿不得超过 10ms，这就是在给它制定 KPI。G1 会尽量达成这个目标，它能够推算出本次要收集的大体区域，以增量的方式完成收集。这也是使用 G1 垃圾回收器不得不设置的一个参数：-XX:MaxGCPauseMillis=10为什么叫 G1G1 的目标是用来干掉 CMS 的，它同样是一款软实时垃圾回收器。相比 CMS，G1 的使用更加人性化。比如： CMS 垃圾回收器的相关参数有 72 个； 而 G1 的参数只有 26 个。G1 的全称是 Garbage­First GC，为了达成上面制定的 KPI，它和前面介绍的垃圾回收器，在对堆的划分上有一些不同。其他的回收器，都是对某个年代的整体收集，收集时间上自然不好控制。G1 把堆切成了很多份，把每一份当作一个小目标，部分上目标很容易达成。那又有问题：G1 有年轻代和老年代的区分吗？如图所示，G1 也是有 Eden 区和 Survivor 区的概念的，只不过它们在内存上不是连续的，而是由一小份一小份组成的。这一小份区域的大小是固定的，名字叫作小堆区（Region）。小堆区可以是 Eden 区，也可以是 Survivor 区，还可以是 Old 区。所以 G1 的年轻代和老年代的概念都是逻辑上的。每一块 Region，大小都是一致的，它的数值是在 1M 到 32M 字节之间的一个 2 的幂值数。但假如我的对象太大，一个 Region 放不下了怎么办？注意图中有一块面积很大的黄色区域，它的名字叫作 Humongous Region，大小超过 Region 50% 的对象，将会在这里分配。Region 的大小，可以通过参数进行设置：-XX:G1HeapRegionSize=M那么，回收的时候，到底回收哪些小堆区呢？是随机的么？这当然不是。事实上，垃圾最多的小堆区，会被优先收集。这就是 G1 名字的由来。G1 的垃圾回收过程在逻辑上，G1 分为年轻代和老年代，但它的年轻代和老年代比例，并不是那么“固定”，为了达到 MaxGCPauseMillis 所规定的效果，G1 会自动调整两者之间的比例。如果你强行使用 -Xmn 或者 -XX:NewRatio 去设定它们的比例的话，我们给 G1 设定的这个目标将会失效。G1 的回收过程主要分为 3 类：（1）G1“年轻代”的垃圾回收，同样叫 Minor GC，这个过程和我们前面描述的类似，发生时机就是 Eden 区满的时候。（2）老年代的垃圾收集，严格上来说其实不算是收集，它是一个“并发标记”的过程，顺便清理了一点点对象。（3）真正的清理，发生在“混合模式”，它不止清理年轻代，还会将老年代的一部分区域进行清理。在 GC 日志里，这个过程描述特别有意思，（1）的过程，叫作 [GC pause (G1 Evacuation Pause) (young)，而（2）的过程，叫作 [GC pause (G1 Evacuation Pause) (mixed)。Evacuation 是转移的意思，和 Copy 的意思有点类似。这三种模式之间的间隔也是不固定的。比如，1 次 Minor GC 后，发生了一次并发标记，接着发生了 9 次 Mixed GC。RSetRSet 是一个空间换时间的数据结构。就像一个叫作卡表（Card Table）的数据结构，用来解决跨代引用的问题。RSet 的功能与此类似，它的全称是 Remembered Set，用于记录和维护 Region 之间的对象引用关系。但 RSet 与 Card Table 有些不同的地方。Card Table 是一种 points-out（我引用了谁的对象）的结构。而 RSet 记录了其他 Region 中的对象引用本 Region 中对象的关系，属于 points-into 结构（谁引用了我的对象），有点倒排索引的味道。你可以把 RSet 理解成一个 Hash，key 是引用的 Region 地址，value 是引用它的对象的卡页集合。有了这个数据结构，在回收某个 Region 的时候，就不必对整个堆内存的对象进行扫描了。它使得部分收集成为了可能。对于年轻代的 Region，它的 RSet 只保存了来自老年代的引用，这是因为年轻代的回收是针对所有年轻代 Region 的，没必要画蛇添足。所以说年轻代 Region 的 RSet 有可能是空的。而对于老年代的 Region 来说，它的 RSet 也只会保存老年代对它的引用。这是因为老年代回收之前，会先对年轻代进行回收。这时，Eden 区变空了，而在回收过程中会扫描 Survivor 分区，所以也没必要保存来自年轻代的引用。RSet 通常会占用很大的空间，大约 5% 或者更高。不仅仅是空间方面，很多计算开销也是比较大的。事实上，为了维护 RSet，程序运行的过程中，写入某个字段就会产生一个 post-write barrier 。为了减少这个开销，将内容放入 RSet 的过程是异步的，而且经过了很多的优化：Write Barrier 把脏卡信息存放到本地缓冲区（local buffer），有专门的 GC 线程负责收集，并将相关信息传给被引用 Region 的 RSet。参数 -XX:G1ConcRefinementThreads 或者 -XX:ParallelGCThreads 可以控制这个异步的过程。如果并发优化线程跟不上缓冲区的速度，就会在用户进程上完成。具体回收过程G1 还有一个 CSet 的概念。这个就比较好理解了，它的全称是 Collection Set，即收集集合，保存一次 GC 中将执行垃圾回收的区间（Region）。GC 是在 CSet 中的所有存活数据（Live Data）都会被转移。年轻代回收年轻代回收是一个 STW 的过程，它的跨代引用使用 RSet 数据结构来追溯，会一次性回收掉年轻代的所有 Region。JVM 启动时，G1 会先准备好 Eden 区，程序在运行过程中不断创建对象到 Eden 区，当所有的 Eden 区都满了，G1 会启动一次年轻代垃圾回收过程。年轻代的收集包括下面的回收阶段： 扫描根根 可以看作是我们前面介绍的 GC Roots，加上 RSet 记录的其他 Region 的外部引用。 更新 RS 处理 dirty card queue 中的卡页，更新 RSet。此阶段完成后，RSet 可以准确的反映老年代对所在的内存分段中对象的引用。可以看作是第一步的补充。 处理 RS 识别被老年代对象指向的 Eden 中的对象，这些被指向的 Eden 中的对象被认为是存活的对象。 复制对象 没错，收集算法依然使用的是 Copy 算法。 在这个阶段，对象树被遍历，Eden 区内存段中存活的对象会被复制到 Survivor 区中空的 Region。这个过程和其他垃圾回收算法一样，包括对象的年龄和晋升，无需做过多介绍。 处理引用 处理 Soft、Weak、Phantom、Final、JNI Weak 等引用。结束收集。 它的大体示意图如下所示。并发标记（Concurrent Marking）当整个堆内存使用达到一定比例（默认是 45%），并发标记阶段就会被启动。这个比例也是可以调整的，通过参数 -XX:InitiatingHeapOccupancyPercent 进行配置。Concurrent Marking 是为 Mixed GC 提供标记服务的，并不是一次 GC 过程的一个必须环节。这个过程和 CMS 垃圾回收器的回收过程非常类似，你可以类比 CMS 的回收过程看一下。具体标记过程如下： 初始标记（Initial Mark） 这个过程共用了 Minor GC 的暂停，这是因为它们可以复用 root scan 操作。虽然是 STW 的，但是时间通常非常短。 Root 区扫描（Root Region Scan） 并发标记（ Concurrent Mark） 这个阶段从 GC Roots 开始对 heap 中的对象标记，标记线程与应用程序线程并行执行，并且收集各个 Region 的存活对象信息。 重新标记（Remaking） 和 CMS 类似，也是 STW 的。标记那些在并发标记阶段发生变化的对象。 清理阶段（Cleanup） 这个过程不需要 STW。如果发现 Region 里全是垃圾，在这个阶段会立马被清除掉。不全是垃圾的 Region，并不会被立马处理，它会在 Mixed GC 阶段，进行收集。 了解 CMS 垃圾回收器后，上面这个过程就比较好理解。但是还有一个疑问需要稍微提一下。如果在并发标记阶段，又有新的对象变化，该怎么办？这是由算法 SATB 保证的。SATB 的全称是 Snapshot At The Beginning，它作用是保证在并发标记阶段的正确性。这个快照是逻辑上的，主要是有几个指针，将 Region 分成个多个区段。如图所示，并发标记期间分配的对象，都会在 next TAMS 和 top 之间。混合回收（Mixed GC）能并发清理老年代中的整个整个的小堆区是一种最优情形。混合收集过程，不只清理年轻代，还会将一部分老年代区域也加入到 CSet 中。通过 Concurrent Marking 阶段，我们已经统计了老年代的垃圾占比。在 Minor GC 之后，如果判断这个占比达到了某个阈值，下次就会触发 Mixed GC。这个阈值，由 -XX:G1HeapWastePercent 参数进行设置（默认是堆大小的 5%）。因为这种情况下， GC 会花费很多的时间但是回收到的内存却很少。所以这个参数也是可以调整 Mixed GC 的频率的。还有参数 G1MixedGCCountTarget，用于控制一次并发标记之后，最多执行 Mixed GC 的次数。ZGC在系统切换到 G1 垃圾回收器之后，线上发生的严重 GC 问题已经非常少了？这归功于 G1 的预测模型和它创新的分区模式。但预测模型也会有失效的时候，它并不是总如我们期望的那样运行，尤其是你给它定下一个苛刻的目标之后。另外，如果应用的内存非常吃紧，对内存进行部分回收根本不够，始终要进行整个 Heap 的回收，那么 G1 要做的工作量就一点也不会比其他垃圾回收器少，而且因为本身算法复杂了，还可能比其他回收器要差。所以垃圾回收器本身的优化和升级，从来都没有停止过。最新的 ZGC 垃圾回收器，就有 3 个令人振奋的 Flag： 停顿时间不会超过 10ms； 停顿时间不会随着堆的增大而增大（不管多大的堆都能保持在 10ms 以下）； 可支持几百 M，甚至几 T 的堆大小（最大支持 4T）。在 ZGC 中，连逻辑上的年轻代和老年代也去掉了，只分为一块块的 page，每次进行 GC 时，都会对 page 进行压缩操作，所以没有碎片问题。ZGC 还能感知 NUMA 架构，提高内存的访问速度。与传统的收集算法相比，ZGC 直接在对象的引用指针上做文章，用来标识对象的状态，所以它只能用在 64 位的机器上。现在在线上使用 ZGC 的还非常少。即使是用，也只能在 Linux 平台上使用。等待它的普及，还需要一段时间。总结相对于 CMS，G1 有了更可靠的驾驭度。而且有 RSet 和 SATB 等算法的支撑，Remark 阶段更加高效。G1 最重要的概念，其实就是 Region。它采用分而治之，部分收集的思想，尽力达到我们给它设定的停顿目标。G1 的垃圾回收过程分为三种，其中，并发标记阶段，为更加复杂的 Mixed GC 阶段做足了准备。以下是一个线上运行系统的 JVM 参数样例：JAVA_OPTS=&quot;$JAVA_OPTS -XX:NewRatio=2 -XX:G1HeapRegionSize=8m -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m -XX:MaxTenuringThreshold=10 -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintReferenceGC -XX:+PrintAdaptiveSizePolicy -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=6 -XX:GCLogFileSize=32m -Xloggc:./var/run/gc.log.$(date +%Y%m%d%H%M) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./var/run/heap-dump.hprof -Dfile.encoding=UTF-8 -Dcom.sun.management.jmxremote -Dcom.sun.management. jmxremote.port=${JMX_PORT:-0} -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false&quot;" }, { "title": "垃圾回收 - CMS 垃圾收集器", "url": "/posts/gc-2-2/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-06 13:23:00 +0000", "snippet": "首先熟悉几个概念： Minor GC：发生在年轻代的 GC。 Major GC：发生在老年代的 GC。 Full GC：全堆垃圾回收。比如 Metaspace 区引起年轻代和老年代的回收。CMS 垃圾收集器全称是：主要并发标记清除垃圾收集器，Mostly Concurrent Mark and Sweep Garbage Collector。是一种以获取最短回收停顿时间为目标的收集器。避免在老年代 GC 时出现长时间的卡顿，但它并不是一个老年代收集器。如果不希望有较长时间的停顿，同时 CPU 资源也比较丰富，使用 CMS 是比较合适的。目前很大一部分的 Java 应用集中在互联网网站或者基于浏览器的 B/S 系统的服务端上，这类应用通常都会较为关注服务的响应速度，希望系统停顿时间尽可能短，以给用户带来良好的交互体验， CMS 收集器就非常符合这类应用的需求。从名字（包含 “Mark Sweep” ）上就可以看出，CMS 收集是基于标记-清除算法实现的。它在年轻代使用复制算法，而对老年代使用标记-清除算法。可以看到，在老年代阶段，比起 Mark-Sweep，它多了一个并发字样。CMS 使用的是 Sweep 而不是 Compact，所以它的主要问题是碎片化。随着 JVM 的长时间运行，碎片化会越来越严重，只有通过 Full GC 才能完成整理。为什么 CMS 能够获得更小的停顿时间呢？主要是因为它把最耗时的一些操作，做成了和应用线程并行。CMS 回收过程CMS 垃圾收集器的的回收过程分为四个步骤： 初始标记（CMS Initial Mark） 并发标记（CMS Concurrent Mark） 重新标记（CMS remark） 并发清除（CMS Concurrent Sweep）初始标记初始标记阶段，只标记直接关联 GC root 的对象，不用向下追溯。因为最耗时的就在 tracing 阶段，这样就极大地缩短了初始标记时间。这个过程是 STW 的，但由于只是标记第一层，所以速度是很快的。注意，这里除了要标记相关的 GC Roots 之外，还要标记年轻代中对象的引用，这也是 CMS 老年代回收，依然要扫描新生代的原因。并发标记在初始标记的基础上，进行并发标记。这一步骤主要是 tracinng 的过程，用于标记所有可达的对象。这个过程会持续比较长的时间，但却可以和用户线程并行。在这个阶段的执行过程中，可能会产生很多变化： 有些对象，从新生代晋升到了老年代； 有些对象，直接分配到了老年代； 老年代或者新生代的对象引用发生了变化。在这个阶段受到影响的老年代对象所对应的卡页，会被标记为 dirty（卡片标记），用于后续重新标记阶段的扫描。并发预清理（Concurrent Preclean）并发预清理也是不需要 STW 的，目的是为了让重新标记阶段的 STW 尽可能短。这个时候，老年代中被标记为 dirty 的卡页中的对象，就会被重新标记，然后清除掉 dirty 的状态。由于这个阶段也是可以并发的，在执行过程中引用关系依然会发生一些变化。可以假定这个清理动作是第一次清理。所以重新标记阶段，有可能还会有处于 dirty 状态的卡页。并发可取消的预清理（Concurrent Abortable Preclean）因为重新标记需要 STW 的，所以会有很多次预清理动作。并发可取消的预清理，顾名思义，在满足某些条件的时候，可以终止，比如迭代次数、有用工作量、消耗的系统时间等。这个阶段是可选的。换句话说，这个阶段是“并发预清理”阶段的一种优化。这个阶段的第一个意图，是避免回扫年轻代的大量对象；另外一个意图，就是当满足最终标记的条件时，自动退出。标记动作需要扫描年轻代的。如果年轻代的对象太多，肯定会严重影响标记的时间。如果在此之前能够进行一次 Minor GC，情况会不会变得好了许多？CMS 提供了参数 CMSScavengeBeforeRemark，可以在进入重新标记之前强制进行一次 Minor GC。记住一件事情，GC 的停顿是不分什么年轻代老年代的。设置了上面的参数，可能会在一个比较长的 Minor GC 之后，紧跟着一个 CMS 的 Remark，它们都是 STW 的。这部分有非常多的配置参数。但是一般都不会去改动。最终标记（Final Remark）通常 CMS 会尝试在年轻代尽可能空的情况下运行 Final Remark 阶段，以免接连多次发生 STW 事件。这是 CMS 垃圾回收阶段的第二次 STW 阶段，目标是完成老年代中所有存活对象的标记。我们前面多轮的 preclean 阶段，一直在和应用线程玩追赶游戏，有可能跟不上引用的变化速度。本轮的标记动作就需要 STW 来处理这些情况如果预处理阶段做的不够好，会显著增加本阶段的 STW 时间。你可以看到，CMS 垃圾回收器把回收过程分了多个部分，而影响最大的不是 STW 阶段本身，而是它之前的预处理动作。并发清除（Concurrent Sweep）此阶段用户线程被重新激活，目标是删掉不可达的对象，并回收它们的空间。由于 CMS 并发清理阶段用户线程还在运行中，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS 无法在当次 GC 中处理掉它们，只好留待下一次 GC 时再清理掉。这一部分垃圾就称为“浮动垃圾”。并发重置（Concurrent Reset）此阶段与应用程序并发执行，重置 CMS 算法相关的内部数据，为下一次 GC 循环做准备。内存碎片由于 CMS 在执行过程中，用户线程还需要运行，那就需要保证有充足的内存空间供用户使用。如果等到老年代空间快满了，再开启这个回收过程，用户线程可能会产生“Concurrent Mode Failure”的错误，这时会临时启用 Serial Old 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了（STW）。这部分空间预留，一般在 30% 左右即可，能用的大概只有 70%。参数 -XX:CMSInitiatingOccupancyFraction 用来配置这个比例（记得要首先开启参数UseCMSInitiatingOccupancyOnly ）。也就是说，当老年代的使用率达到 70%，就会触发 GC 了。如果系统老年代增长不是太快，可以调高这个参数，降低内存回收的次数。其实，这个比率非常不好设置。一般在堆大小小于 2GB 的时候，都不会考虑 CMS 垃圾回收器。另外，CMS 对老年代回收的时候，并没有内存的整理阶段。这就造成程序在长时间运行之后，碎片太多。如果申请一个稍大的对象，就会引起分配失败。CMS 提供了两个参数来解决这个问题 UseCMSCompactAtFullCollection（默认开启），表示在要进行 Full GC 的时候，进行内存碎片整理。内存整理的过程是无法并发的，所以停顿时间会变长; CMSFullGCsBeforeCompaction，每隔多少次不压缩的 Full GC 后，执行一次带压缩的 Full GC。默认值为 0，表示每次进入 Full GC 时都进行碎片整理。所以，预留空间加上内存的碎片，使用 CMS 垃圾回收器的老年代，留给我们的空间就不是太多，这也是 CMS 的一个弱点。总结CMS 垃圾回收器分为四个阶段： 初始标记 并发标记 重新标记 并发清理CMS 中都会有哪些停顿（STW）： 初始标记，这部分的停顿时间较短； Minor GC（可选），在预处理阶段对年轻代的回收，停顿由年轻代决定； 重新标记，由于 preclaen 阶段的介入，这部分停顿也较短； Serial-Old 收集老年代的停顿，主要发生在预留空间不足的情况下，时间会持续很长； Full GC，永久代空间耗尽时的操作，由于会有整理阶段，持续时间较长。在发生 GC 问题时，你一定要明确发生在哪个阶段，然后对症下药。gclog 通常能够非常详细的表现这个过程。我们再来看一下 CMS 的 trade-off。优势：低延迟，尤其对于大堆来说。大部分垃圾回收过程并发执行。劣势： 内存碎片问题。Full GC 的整理阶段，会造成较长时间的停顿。 需要预留空间，用来分配收集阶段产生的“浮动垃圾”。 使用更多的 CPU 资源，在应用运行的同时进行堆扫描。CMS 是一种高度可配置的复杂算法，因此给 JDK 中的 GC 代码库带来了很多复杂性。由于 G1 和 ZGC 的产生，CMS 已经在被废弃的路上。但是，目前仍然有大部分应用是运行在 Java8 及以下的版本之上，针对它的优化，还是要持续很长一段时间。" }, { "title": "从栈帧看字节码在 JVM 中进行流转", "url": "/posts/stackframe-bytecode/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-04 15:23:00 +0000", "snippet": " 怎么查看字节码文件？ 字节码文件长什么样子？ 对象初始化之后，具体的字节码又是怎么执行的？工具javapjavap 是 JDK 自带的反解析工具。它的作用是将 .class 字节码文件解析成可读的文件格式。在使用 javap 时我一般会添加 -v 参数，尽量多打印一些信息。同时，我也会使用 -p 参数，打印一些私有的字段和方法。使用起来大概是这样：javap -p -v HelloWord在 StackOverflow 上有一个非常有意思的问题：在某个类中增加一行注释之后，为什么两次生成的 .class 文件，它们的 MD5 是不一样的？这是因为在 javac 中可以指定一些额外的内容输出到字节码。经常用的有： javac -g:lines ，强制生成 LineNumberTable。 javac -g:vars ，强制生成 LocalVariableTable。 javac -g ，生成所有的 debug 信息。jclasslibjclasslib 是一个图形化的工具，能够更加直观的查看字节码中的内容。它还分门别类的对类中的各个部分进行了整理，非常的人性化。同时，它还提供了 Idea 的插件，可以从 plugins 中搜索到它。jclasslib 的下载地址：https://github.com/ingokegel/jclasslib类加载和对象创建的时机首先，写一个最简单的 Java 程序 A.java。它有一个公共方法 test，还有一个静态成员变量和动态成员变量：package cn.happymaya.four;class B { private int a = 1234; private long c = 1111; public long test(long num) { long ret = this.a + num + c; return ret; }}public class A { private B b = new B(); public static void main(String[] args) { A a = new A(); long num = 4321; long ret = a.b.test(num); System.out.println(ret); }}类的初始化发生在类加载阶段，除了常用的 new ，还有下面一些方式： 使用 Class 的 newInstance 方法； 使用 Constructor 类的 newInstance 方法。 反序列化； 使用 Object 的 clone 方法。其中，最后两种方式没有调用到构造函数！当虚拟机遇到一条 new 指令时： 首先检查这个指令的参数能否在常量池中定位一个符号引用； 然后检查这个符号引用的类字节码是否被加载、解析和初始化。如果没有，将执行对应的类加载过程。拿上面代码来说，执行 A 代码，在调用 private B b = new B() 时，就会触发 B 类的加载：如上图，A 和 B 会被加载到元空间的方法区，进入到 main 方法后，就会交给执行引擎（Execution engine）执行。这个执行过程是在栈上完成的，其中有几个重要的区域：包括虚拟机栈（Virtual Machine Stacks）、程序计数器（Program counter register， 针对于字节码指令的，主要线程切换的时候使用）等。查看字节码命令行查看字节码 使用命令javac -g:lines -g:vars A.java编译源代码 A.java。在 idea 里面，直接将参数追加在 VM options 里面，这会强制生成 LineNumberTable 和 LocalVariableTable； 使用 javap 命令查看 A 和 B 的字节码， javap -p -v Ajavap -p -v B 这个命令，会输出：行号、本地变量表信息、反编译汇编代码、当前类用到的常量池等信息 javap 中的如下字样： &amp;lt;1&amp;gt; 1: invokespecial #1 // Method java/lang/Object.&quot;&amp;lt;init&amp;gt;&quot;:()V 可以看到：对象的初始化，首先调用了 Object 类的初始化方法。注意这里是，而不是 。 &amp;lt;2&amp;gt; #7 = Fieldref #8.#9 // cn/happymaya/four/B.a:I 它其实直接拼接了 #8 和 #9 的内容： #8 = Class #10 // cn/happymaya/four/B #9 = NameAndType #11:#12 // a:I ... #11 = Utf8 a #12 = Utf8 I &amp;lt;3&amp;gt; :I 这样特殊字符，也是有意义的，大体包括： B 基本类型 byte C 基本类型 char D 基本类型 double F 基本类型 float I 基本类型 int J 基本类型 long S 基本类型 short Z 基本类型 boolean V 特殊类型 void L 对象类型，以分号结尾，如 Ljava/lang/Object; [Ljava/lang/String; 数组类型，每一位使用一个前置的”[“字符来描述 在注意下 code 区域，有非常多的二进制指令。跟汇编语言有一定的相似性。但这些二进制指令，操作系统不认识，它们只是提供给 JVM 运行的原材料。 可视化查看字节码使用更加直观的工具 jclasslib，来查看字节码中的具体内容了。以 B.class 文件为例，来查看它的内容： 首先，看到 Constant Pool（常量池），这些内容，存放于 Metaspace 区域，属于非堆。 常量池包含：.class 文件常量池、运行时常量池、String 常量池等部分。大多是一些静态内容。 接下来，可以看到两个默认的 &amp;lt;init&amp;gt; 和 &amp;lt;cinit&amp;gt; 方法。以下截图是 test 方法的 code 区域，比命令行版的更加直观。 继续往下看，看到了 LocalVariableTable 的三个变量。其中，slot 0 指向的是 this 关键字。该属性的作用是描述帧栈中局部变量与源码中定义的变量之间的关系。如果没有这些信息，那么在 IDE 中引用这个方法时，将无法获取到方法名，取而代之的则是 arg0 这样的变量名。 本地变量表的 slot 是可以复用的。注意一个有意思的地方，index 的最大值为 3，证明了本地变量表同时最多能够存放 4 个变量（示例代码的最大值。如果创建了上千个变量，最大值达到1k都有可能）。 另外，观察到还有 LineNumberTable 等选项。该属性的作用是描述源码行号与字节码行号（字节码偏移量，偏移量是字节单位，不是指令的个数，包含操作数等额外数量）之间的对应关系，有了这些信息，在 debug 时，就能够获取到发生异常的源代码行号。 test 函数执行过程Code 区域test 函数同时使用了成员变量 a、静态变量 C，以及输入参数 num。此时说的函数执行，内存其实就是在虚拟机栈上分配的。下面这些内容，就是 test 方法的字节码。 public long test(long); descriptor: (J)J flags: ACC_PUBLIC Code: stack=4, locals=5, args_size=2 0: aload_0 1: getfield #7 // Field a:I 4: i2l 5: lload_1 6: ladd 7: aload_0 8: getfield #15 // Field c:J 11: ladd 12: lstore_3 13: lload_3 14: lreturn LineNumberTable: line 8: 0 line 9: 13 LocalVariableTable: Start Length Slot Name Signature 0 15 0 this Lcn/happymaya/four/B; 0 15 1 num J 13 2 3 ret J比较重要的 3 三个数值： stack ，它此时的数值为 4，表明了 test 方法的最大操作数栈深度为 4。JVM 运行时，会根据这个数值，来分配栈帧中操作栈的深度。 locals 变量存储了局部变量的存储空间。它的单位是 Slot（槽），可以被重用。其中存放的内容，包括： this 方法参数 异常处理器的参数 方法体中定义的局部变量 args_size 。它指的是方法的参数个数，因为每个方法都有一个隐藏参数 this，所以这里的数字是 2。字节码执行过程main 线程会拥有两个主要的运行时区域：Java 虚拟机栈和程序计数器。其中，虚拟机栈中的每一项内容叫作栈帧，栈帧中包含四项内容：局部变量报表、操作数栈、动态链接和完成出口。字节码指令，就是靠操作这些数据结构运行的！！！0:aload_0把第 1 个引用型局部变量推到操作数栈，这里的意思是把 this 装载到了操作数栈中。对于 static 方法，aload_0 表示对方法的第一个参数的操作。1: getfield #2 // Field a:I将栈顶指定的对象的第 2 个实例域（Field）的值，压入栈顶。#7 就是指的是成员变量 a。#7 = Fieldref #8.#9 // cn/happymaya/four/B.a:I#8 = Class #10 // cn/happymaya/four/B#9 = NameAndType #11:#12 // a:Ii2l将栈顶 int 类型的数据转化为 long 类型，这涉及隐式类型转换了，图中的信息没有变动。iload_1将第一个局部变量入栈。也就是我们的参数 num。这里的 l 表示 long，同样用于局部变量装载。你会看到这个位置的局部变量，一开始就已经有值了。ladd把栈顶两个 long 型数值出栈后相加，并将结果入栈。getsatic #3根据偏移获取静态属性的值，并把这个值 push 到操作数栈上。ladd再次执行 ladd。lstore_3把栈顶 long 型数值存入第 4 个局部变量。上面的图，slot 为 4，索引为 3 的就是 ret 变量。lload_3正好与上面相反。上面是变量存入，现在要做的，就是把这个变量 ret，压入虚拟机栈中。Ireturn从当前方法返回 long。到此为止，函数就完成了相加动作，执行成功了。JVM 提供非常丰富的字节码指令。详细的字节码指令列表，参考以下网址：https://docs.oracle.com/javase/specs/jvms/se8/html/jvms-6.html注意：上面的第 8 步，首先把变量存放到了变量报表，然后又拿出这个值，把它入栈。为什么会有这种多此一举的操作？原因就在于我们定义了 ret 变量。JVM 不知道后面还会不会用到这个变量，所以只好傻瓜式的顺序执行。为了看到这些差异。可以把程序稍微改动一下，直接返回这个值：public long test(long num) { return this.a + num + C;}再次看下，对应的字节码指令简单了很多！！！0: aload_01: getfield #2 // Field a:I4: i2l5: lload_16: ladd7: getstatic #3 // Field C:J10: ladd11: lreturn尽管如此，以后编写程序时，是不是要尽量少的定义成员变量？这是没有必要的。栈的操作复杂度是 O(1)，对程序性能几乎没有影响。平常的代码编写，还是以可读性作为首要任务。总结字节码的执行流程这样理解： 字节码在Java虚拟机栈中被执行，每一项内容都可以看作是一个栈帧； 栈帧的结构包括局部变量表、操作数栈、链接、返回地址。这时候就很明了了，栈帧的执行流程就是字节码的执行流程了； 类中变量会被解析到局部变量表，然后对操作数栈进行入栈出栈的操作，在此期间有可能引用到动态或静态链接，最后把计算结果的引用地址返回。" }, { "title": "类的加载机制", "url": "/posts/classloader/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-03 14:23:00 +0000", "snippet": "类的加载机制JVM 的类加载机制和 Java 的类加载机制类似，但 JVM 的类加载过程稍有些复杂。JVM 通过加载 .class 文件，能够将其中的字节码解析成操作系统机器码。那这些文件是怎么加载进来的呢？又有哪些约定？类加载过程并不是说，把一个文件修改成 .class 后缀，就能够被 JVM 识别。类的加载过程非常复杂，主要有这几个过程： 加载； 验证； 准备； 解析； 初始化如图所示。大多数情况下，类会按照图中给出的顺序进行加载。加载主要作用是将外部的 .class 文件，加载到 Java 的方法区内（想一下内存区域图）。加载阶段主要是找到并加载类的二进制数据，比如从 jar 包里或者 war 包里找到它们。验证肯定不能任何 .class 文件都能加载，那样太不安全了，容易受到恶意代码的攻击。验证阶段在虚拟机整个类加载过程中占了很大一部分，不符合规范的将抛出 java.lang.VerifyError 错误。像一些低版本的 JVM，是无法加载一些高版本的类库的，就是在这个阶段完成的。准备从这部分开始，将为一些类变量分配内存，并将其初始化为默认值。此时，实例对象还没有分配内存，所以这些动作是在方法区上进行的。一道面试题。下面两段代码，A 将会输出 0，而 B 将无法通过编译：package cn.happymaya.three;public class A { static int a; public static void main(String[] args) { System.out.println(a); }}public class B { public static void main(String[] args) { int a; System.out.println(a); }}之所以有这样的不同，这是因为局部变量不像类变量那样存在准备阶段。类变量有两次赋初始值的过程： 一次在准备阶段，赋予初始值（也可以是指定值）； 另外一次在初始化阶段，赋予程序员定义的值。因此，即使程序员没有为类变量赋值也没有关系，它仍然有一个默认的初始值。但局部变量就不一样了，如果没有给它赋初始值，是不能使用的。解析解析在类加载中是非常非常重要的一环，是将符号引用替换为直接引用的过程（非常的拗口，其实理解起来也非常的简单）。 符号引用，是一种定义，可以是任何字面上的含义； 直接引用，是直接指向目标的指针、相对偏移量。直接引用的对象都存在于内存中，可以把通讯录里的女友手机号码，类比为符号引用，把面对面和你吃饭的人，类比为直接引用。解析阶段负责把整个类激活，串成一个可以找到彼此的网，过程不可谓不重要。这个阶段做的工作呢大体可以分为： 类或接口的解析 类方法解析 接口方法解析 字段解析几个经常发生的异常，就与这个阶段有关： java.lang.NoSuchFieldError，根据继承关系从下往上，找不到相关字段时的报错。 java.lang.IllegalAccessError，字段或者方法，访问权限不具备时的错误。 java.lang.NoSuchMethodError，找不到相关方法时的错误。 解析过程保证了相互引用的完整性，把继承与组合推进到运行时。初始化如果前面的流程一切顺利的话，接下来该初始化成员变量了，到了这一步，才真正开始执行一些字节码。一道面试题：public class C { static int a = 0; static { System.out.println(&quot;there is static block&quot;); a = 1; b = 1; } static int b = 0; public static void main(String[] args) { System.out.println(a); System.out.println(b); }}结果是 1 0。a 和 b 唯一的区别就是它们的 static 代码块的位置。这就引出一个规则：static 语句块，只能访问到定义在 static 语句块之前的变量。所以下面的代码是无法通过编译的。static { b = b + 1;}static int b = 0;第二个规则：JVM 会保证在子类的初始化方法执行之前，父类的初始化方法已经执行完毕。所以，JVM 第一个被执行的类初始化方法一定是 java.lang.Object。也意味着父类中定义的 static 语句块要优先于子类的。&amp;lt;clinit&amp;gt;与&amp;lt;init&amp;gt;不得不再说一个面试题： 方法和 方法有什么区别（明白类的初始化和对象的初始化之间的差别）？package cn.happymaya.three;public class D { static { System.out.println(&quot;i&#39;m class D : static block&quot;); } public D() { System.out.println(&quot;i&#39;m class D : No args constructor&quot;); }}public class E extends D{ static { System.out.println(&quot;i&#39;m class E : static block : class D is My Father&quot;); } public E() { System.out.println(&quot;i&#39;m class E : No args constructor : class D is My Father&quot;); } public static void main(String[] args) { D d = new E(); d = new E(); }}输出结果如下：i&#39;m class D : static blocki&#39;m class E : static block : class D is My Fatheri&#39;m class D : No args constructori&#39;m class E : No args constructor : class D is My Fatheri&#39;m class D : No args constructori&#39;m class E : No args constructor : class D is My Father看下这张图：static 字段和 static 代码块，是属于类的，在类加载的初始化阶段就已经被执行。类信息会被存放在方法区，在同一个类加载器下，这些信息有一份就够了，所以上面的 static 代码块只会执行一次，它对应的是 &amp;lt;clinit&amp;gt; 方法。而对象初始化就不一样了。通常，在 new 一个新对象的时候，都会调用它的构造方法，就是 &amp;lt;init&amp;gt;，用来初始化对象的属性。每次新建对象的时候，都会执行。类加载器整个类加载过程任务非常繁重，虽然这活儿很累，但总得有人干。类加载器做的就是上面 5 个步骤的事。如果在项目代码里，写一个 java.lang 的包，然后改写 String 类的一些行为，编译后，发现并不能生效。JRE 的类当然不能轻易被覆盖，否则会被别有用心的人利用，这就太危险了。类加载器有着严格的等级制度的保证这个过程的安全性！！！几个类加载器 Bootstrap ClassLoader，根加载器，加载器中的大 Boss，任何类的加载行为，都要经它过问，C++ 编写的，随着 JVM 启动。 它的作用是加载核心类库，也就是 rt.jar、resources.jar、charsets.jar 等。当然这些 jar 包的路径是可以指定的，-Xbootclasspath 参数可以完成指定操作。 Extention ClassLoader，扩展类加载器，是个 Java 类，继承自 URLClassLoader。 主要用于加载 lib/ext 目录下的 jar 包和 .class 文件。同样的，通过系统变量 java.ext.dirs 可以指定这个目录。 App ClassLoader，这是我们写 Java 类的默认加载器，也叫作 System ClassLoader。 一般用来加载 classpath 下的其他所有 jar 包和 .class 文件，我们写的代码，会首先尝试使用这个类加载器进行加载。 Custom ClassLoader，自定义加载器。 支持一些个性化的扩展功能。 双亲委派机制双亲委派机制的意思是：除了顶层的启动类加载器以外，其余的类加载器，在加载之前，都会委派给它的父加载器进行加载。这样一层层向上传递，直到祖先们都无法胜任，它才会真正的加载。打个比方。有一个家族，都是一些听话的孩子。孙子想要买一块棒棒糖，最终都要经过爷爷过问，如果力所能及，爷爷就直接帮孙子买了。有个问题：“类加载的双亲委派机制，双亲在哪里？明明都是单亲？”用一张图来理解：可以看到，除了启动类加载器，每一个加载器都有一个 parent，并没有所谓的双亲。但是由于翻译的问题，这个叫法已经非常普遍了，一定要注意背后的差别。翻阅 JDK 代码的 ClassLoader#loadClass 方法，来看一下具体的加载过程： 首先，使用 parent 尝试进行类加载； 然后，parent 失败后才轮到自己。这个方法是可以被覆盖的，也就是双亲委派机制不一定生效。public Class&amp;lt;?&amp;gt; loadClass(String name) throws ClassNotFoundException { return loadClass(name, false);}protected Class&amp;lt;?&amp;gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // First, check if the class has already been loaded Class&amp;lt;?&amp;gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) { // 首先使用 parent 尝试进行类加载 c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found // from the non-null parent class loader } // parent 失败后才轮到自己 if (c == null) { // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; }}注意到，这个方法是可以被覆盖的，也就是双亲委派机制并不一定生效。这个模型的好处在于： Java 类有了一种优先级的层次划分关系。比如 Object 类，这个毫无疑问应该交给最上层的加载器进行加载，即使是覆盖了它，最终也是由系统默认的加载器进行加载的。 如果没有双亲委派模型，就会出现很多个不同的 Object 类，应用程序会一片混乱。一些自定义加载器打破双亲委派机制的一些案例。为了支持一些自定义加载类多功能的需求，Java 设计者作出了一些妥协。案例一：tomcattomcat 通过 war 包进行应用的发布，它其实是违反了双亲委派机制原则的。tomcat 类加载器的层次结构：对于一些需要加载的非基础类，由一个叫作 WebApp ClassLoader 的类加载器优先加载。等它加载不到的时候，再交给上层的 ClassLoader 进行加载。这个加载器用来隔绝不同应用的 .class 文件，比如两个应用，可能会依赖同一个第三方的不同版本，它们是相互没有影响的。如何在同一个 JVM 里，运行着不兼容的两个版本，当然是需要自定义加载器才能完成的事。那么 tomcat 是怎么打破双亲委派机制的呢：看图中的 WebApp ClassLoader，它加载自己目录下的 .class 文件，并不会传递给父类的加载器。但是，它却可以使用 SharedClassLoader 所加载的类，实现了共享和分离的功能。假设自己写一个 ArrayList，放在应用目录里，tomcat 依然不会加载。它只是自定义的加载器顺序不同，但对于顶层来说，还是一样的。案例二：SPIJava 中有一个 SPI 机制，全称是 Service Provider Interface，是 Java 提供的一套用来被第三方实现或者扩展的 API，它可以用来启用框架扩展和替换组件。拿常用的数据库驱动加载来说，在使用 JDBC 写程序之前，通常会调用下面这行代码，用于加载所需要的驱动类。Class.forName(&quot;com.mysql.cj.jdbc.Driver&quot;)这只是一种初始化模式，通过 static 代码块显式地声明了驱动对象，然后把这些信息，保存到底层的一个 List 中。但是删除了 Class.forName 这一行代码，也能加载到正确的驱动类，什么都不需要做，非常的神奇，它是怎么做到的呢 翻开 MySQL 的驱动代码，发现了一个奇怪的文件。之所以能够发生这样神奇的事情，就是在这里实现的。 路径：mysql-connector-java-8.0.15.jar!/META-INF/services/java.sql.Driver 里面的内容是：com.mysql.cj.jdbc.Driver 通过在 META-INF/services 目录下，创建一个以接口全限定名为命名的文件（内容为实现类的全限定名），即可自动加载这一种实现，这就是 SPI。 SPI 实际上是“基于接口的编程＋策略模式＋配置文件”组合实现的动态加载机制，主要使用 java.util.ServiceLoader 类进行动态装载。这种方式，同样打破了双亲委派的机制。 DriverManager 类和 ServiceLoader 类都是属于 rt.jar 的。它们的类加载器是 Bootstrap ClassLoader，也就是最上层的那个； 具体的数据库驱动，却属于业务代码，这个启动类加载器是无法加载的。这就比较尴尬了，虽然凡事都要祖先过问，但祖先没有能力去做这件事情，怎么办？ 通过代码发现 Java 玩了个魔术，它把当前的类加载器，设置成了线程的上下文类加载器。那么，对于一个刚刚启动的应用程序来说，它当前的加载器是谁呢？也就是说，启动 main 方法的那个加载器，到底是哪一个？ 继续跟踪代码，找到 Launcher 类，就是 jre 中用于启动入口函数 main 的类。在 Launcher 中找到以下代码； 到此为止，事情就比较明朗了，当前线程上下文的类加载器，是应用程序类加载器。使用它来加载第三方驱动，是没有什么问题的。案例三：OSGIOSGI 曾经非常流行，Eclipse 就使用 OSGi 作为插件系统的基础。OSGI 是服务平台的规范，旨在用于需要长运行时间、动态更新和对运行环境破坏最小的系统。OSGI 规范定义了很多关于包生命周期，以及基础架构和绑定包的交互方式。这些规则，通过使用特殊 Java 类加载器来强制执行，比较霸道。比如，在一般 Java 应用程序中，classpath 中的所有类都对所有其他类可见，这是毋庸置疑的。但是，OSGi 类加载器基于 OSGI 规范和每个绑定包的 manifest.mf 文件中指定的选项，来限制这些类的交互，这就让编程风格变得非常的怪异。不难想象，这种与直觉相违背的加载方式，肯定是由专用的类加载器来实现的。随着 jigsaw 的发展（旨在为 Java SE 平台设计、实现一个标准的模块系统），个人认为，现在的 OSGI ，意义已经不是很大了。OSGI 是一个庞大的话题，你只需要知道，有这么一个复杂的东西，实现了模块化，每个模块可以独立安装、启动、停止、卸载，就可以了。Java 的类加载器，可以玩出这么多花样。替换 JDK 的类如何替换 JDK 中的类？当 Java 的原生 API 不能满足需求时，比如修改 HashMap 类，就必须要使用到 Java 的 endorsed 技术。 需要将自己的 HashMap 类，打包成一个 jar 包，然后放到-Djava.endorsed.dirs 指定的目录中。 注意类名和包名，应该和 JDK 自带的是一样的。但是，java.lang 包下面的类除外，因为这些都是特殊保护的。因为双亲委派机制，无法直接在应用中替换 JDK 的原生类的。但是，有时候又不得不进行一下增强、替换，比如想要调试一段代码，或者比 Java 团队早发现了一个 Bug。所以，Java 提供了 endorsed 技术，用于替换这些类。这个目录下的 jar 包，会比 rt.jar 中的文件，优先级更高，可以被最先加载到。总结 一个 Java 类的加载，经过了加载、验证、准备、解析、初始化几个过程，每一个过程都划清了各自负责的事情； Java 自带的三个类加载器。main 方法的线程上下文加载器，其实是 Application ClassLoader； 一般情况下，类加载是遵循双亲委派机制的。这个双亲很有问题有很多打破这个规则的情况。类加载器通过开放的 API，让加载过程更加灵活； 无论是远程存储字节码，还是将字节码进行加密，这都是业务需求。要做这些，我们实现一个新的类加载器就可以了。" }, { "title": "垃圾回收 - 垃圾回收算法", "url": "/posts/gc-2-1/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-02 14:23:00 +0000", "snippet": "之所以 Java 不用“手动管理”内存回收，代码写起来很顺畅。是因为 JVM 是有专门的线程在做这件事情。当内存空间达到一定条件时，会自动触发。这个过程就叫作 GC，负责 GC 的组件，就叫作垃圾回收器。JVM 规范没有规定垃圾回收器怎么实现，它只需要保证不要把正在使用的对象给回收掉就可以。在现在的服务器环境中，经常被使用的垃圾回收器有 CMS 和 G1，但 JVM 还有其他几个常见的垃圾回收器。按照语义上的意思，垃圾回收，首先就需要找到这些垃圾，然后回收掉。但是 GC 过程正好相反，它是先找到活跃的对象，然后把其他不活跃的对象判定为垃圾，然后删除。所以垃圾回收只与活跃的对象有关，和堆的大小无关。标记（Mark）垃圾回收的第一步，就是找出活跃的对象。GC Roots 遍历所有的可达对象，这个过程，就叫做标记。如图所示，圆圈代表的是对象。绿色的代表 GC Roots，红色的代表可以追溯到的对象。可以看到标记之后，仍然有多个灰色的圆圈，它们都是被回收的对象。清除（Sweep）清除阶段就是把未被标记的对象回收掉。但是这种简单的清除方式，有一个明显的弊端，那就是碎片问题。比如申请了 1k、2k、3k、4k、5k 的内存：由于某种原因 ，2k 和 4k 的内存，不再使用，就需要交给垃圾回收器回收：这个时候，应该有足足 6k 的空闲空间。接下来，打算申请另外一个 5k 的空间，结果系统告诉我内存不足了。系统运行时间越长，这种碎片就越多。在很久之前使用 Windows 系统时，有一个非常有用的功能，就是内存整理和磁盘整理，运行之后有可能会显著提高系统性能。这个出发点是一样的。复制（Copy）解决碎片问题没有银弹，只有老老实实的进行内存整理。有一个比较好的思路可以完成这个整理过程，就是提供一个对等的内存空间，将存活的对象复制过去，然后清除原内存空间。在程序设计中，一般遇到扩缩容或者碎片整理问题时，复制算法都是非常有效的。比如：HashMap 的扩容，Redis 的 rehash 也是类似的。整个过程如图所示：这种方式看似非常完美的，解决了碎片问题。但是，它的弊端也非常明显。它浪费了几乎一半的内存空间来做这个事情，如果资源本来就很有限，这就是一种无法容忍的浪费。整理（Compact）其实，不用分配一个对等的额外空间，也是可以完成内存的整理工作。可以把内存想象成一个非常大的数组，根据随机的 index 删除了一些数据。那么对整个数组的清理，其实是不需要另外一个数组来进行支持的，使用程序就可以实现。它的主要思路，就是移动所有存活的对象，且按照内存地址顺序依次排列，然后将末端内存地址以后的内存全部回收。可以用一个理想的算法来看一下这个过程。last = 0for( i = 0;i &amp;lt; mems.length; i++){ if(mems[i] != null){ mems[last++] = mems[i] changeReference(mems[last]) }}clear(mems,last,mems.length)分代目前，JVM 的垃圾回收器，都是对几种朴素算法的发扬光大，它们的特点如下. 标记 - 清除（Mark-Sweep）算法 最基础的收集算法。因为后续的收集算法都是以其作为基础，对其缺点进行改进得到的 缺点 执行效率不稳定 如果 Java 堆中包含大量对象，并且其中大部分是需要被回收的，此时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随着对象数量增长而降低。 内存空间的碎片化问题 标记、清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存，而不得不提前触发另一次垃圾回收动作。 标记 - 复制（Mark-Copy）算法 所有算法里面效率最高； 缺点是会造成一定的空间浪费。 标记 - 整理（Mark-Compact）算法 效率比前者要差，但是没有空间浪费，也消除了内存碎片问题。 可见，没有最优的算法，只有最合适的算法。JVM 是计算节点，而不是存储节点。最理想的情况。就是对象在用完之后，它的生命周期立马结束。而哪些被频繁访问的资源，则希望它能够常驻在内存里。研究表明，大部分对象，分为两类： 大部分对象的生命周期都很短； 其他对象则可能会存活很长时间。大部分死的快，其他活的长。这个假设被称之为：弱代假设（Weak generational hypothesis）。从上图中可以看到，大部分对象是朝生夕灭，其他的则活的很久。现在的垃圾回收器，都会在物理上或者逻辑上，把这两类对象进行区分： 把死的快的对象所占的区域，叫作年轻代（Young generation）； 把其他活的长的对象所占的区域，叫作老年代（Old generation）。老年代在有些地方也会叫作 Tenured Generation。年轻代年轻代使用的垃圾回收算法是复制算法。因为年轻代发生 GC 后，只会有非常少的对象存活，复制这部分对象是非常高效的。因为复制算法会造成一定的空间浪费，所以年轻代中间也会分很多区域：如图所示，年轻代分为： 一个伊甸园空间（Eden ） 两个幸存者空间（Survivor ） to Servivor0 from Servivor1 当年轻代中的 Eden 区分配满的时候，就会触发年轻代的 GC（Minor GC）。具体过程如下： 在 Eden 区执行了第一次 GC 之后，存活的对象会被移动到其中一个 Survivor 分区（以下简称 from）； Eden 区再次 GC，这时会采用复制算法，将 Eden 和 from 区一起清理。存活的对象会被复制到 to 区； 接下来，只需要清空 from 区就可以了。所以在这个过程中，总会有一个 Survivor 分区是空置的。Eden、from、to 的默认比例是 8:1:1，所以只会造成 10% 的空间浪费。这个比例，是由参数 -XX:SurvivorRatio 进行配置的（默认为 8）。还有一个点会经常提到，就是 TLAB。TLAB 的全称是 Thread Local Allocation Buffer，JVM 默认给每个线程开辟一个 buffer 区域，用来加速对象分配。这个 buffer 就放在 Eden 区中。这个道理和 Java 语言中的 ThreadLocal 类似，避免了对公共区的操作，以及一些锁竞争。对象的分配优先在 TLAB上 分配，但 TLAB 通常都很小，所以对象相对比较大的时候，会在 Eden 区的共享区域进行分配。TLAB 是一种优化技术，类似的优化还有对象的栈上分配（这可以引出逃逸分析的话题，默认开启）。这属于非常细节的优化。老年代老年代一般使用： “标记-清除”； “标记-整理”算法。因为老年代的对象存活率一般是比较高的，空间又比较大，拷贝起来并不划算，还不如采取就地收集的方式。对象进入老年代有多种途径： 提升（Promotion） 如果对象够老，会通过“提升”进入老年代。关于对象老不老，是通过它的年龄（age）来判断的。每当发生一次 Minor GC，存活下来的对象年龄都会加 1。直到达到一定的阈值，就会把这些“老顽固”给提升到老年代。这些对象如果变的不可达，直到老年代发生 GC 的时候，才会被清理掉。 这个阈值，可以通过参数 ‐XX:+MaxTenuringThreshold 进行配置，最大值是 15，因为它是用 4bit 存储的（所以网络上那些要把这个值调的很大的文章，是没有什么根据的）。 分配担保 看一下年轻代的图，每次存活的对象，都会放入其中一个幸存区，这个区域默认的比例是 10%。 但是无法保证每次存活的对象都小于 10%，当 Survivor 空间不够，就需要依赖其他内存（指老年代）进行分配担保。这个时候，对象也会直接在老年代上分配。 大对象直接在老年代分配 超出某个大小的对象将直接在老年代分配。这个值是通过参数 -XX:PretenureSizeThreshold 进行配置的。默认为 0，意思是全部首选 Eden 区进行分配。 动态对象年龄判定 有的垃圾回收算法，并不要求 age 必须达到 15 才能晋升到老年代，它会使用一些动态的计算方法。比如，如果幸存区中相同年龄对象大小的和，大于幸存区的一半，大于或等于 age 的对象将会直接进入老年代。 这些动态判定一般不受外部控制，我们知道有这么回事就可以了。通过下图可以看一下一个对象的分配逻辑。 卡片标记（card marking）对象的引用关系是一个巨大的网状。有的对象可能在 Eden 区，有的可能在老年代，那么这种跨代的引用是如何处理的呢？由于 Minor GC 是单独发生的，如果一个老年代的对象引用了它，如何确保能够让年轻代的对象存活呢？对于是、否的判断，通常都会用 Bitmap（位图）和布隆过滤器来加快搜索的速度。JVM 也是用了类似的方法。其实，老年代是被分成众多的卡页（card page）的（一般数量是 2 的次幂）。卡表（Card Table）就是用于标记卡页状态的一个集合，每个卡表项对应一个卡页。如果年轻代有对象分配，而且老年代有对象指向这个新对象， 那么这个老年代对象所对应内存的卡页，就会标识为 dirty，卡表只需要非常小的存储空间就可以保留这些状态。垃圾回收时，就可以先读这个卡表，进行快速判断。HotSpot 垃圾回收器HotSpot 的几个垃圾回收器，每种回收器都有各自的特点。在平常的 GC 优化时，一定要搞清楚现在用的是哪种垃圾回收器。在此之前，把上面的分代垃圾回收整理成一张大图，在介绍下面的收集器时，你可以对应一下它们的位置。年轻代垃圾回收器 Serial 垃圾收集器 处理 GC 的只有一条线程，并且在垃圾回收的过程中暂停一切用户线程。 这可以说是最简单的垃圾回收器，但千万别以为它没有用武之地。因为简单，所以高效，它通常用在客户端应用上。因为客户端应用不会频繁创建很多对象，用户也不会感觉出明显的卡顿。相反，它使用的资源更少，也更轻量级。 ParNew 垃圾收集器 ParNew 是 Serial 的多线程版本。由多条 GC 线程并行地进行垃圾清理。清理过程依然要停止用户线程。 ParNew 追求“低停顿时间”，与 Serial 唯一区别就是使用了多线程进行垃圾收集，在多 CPU 环境下性能比 Serial 会有一定程度的提升；但线程切换需要额外的开销，因此在单 CPU 环境中表现不如 Serial。 Parallel Scavenge 垃圾收集器 另一个多线程版本的垃圾回收器。它与 ParNew 的主要区别是： Parallel Scavenge：追求 CPU 吞吐量，能够在较短时间内完成指定任务，适合没有交互的后台计算。弱交互强计算； ParNew：追求降低用户停顿时间，适合交互式应用。强交互弱计算。 老年代垃圾收集器 Serial Old 垃圾收集器 与年轻代的 Serial 垃圾收集器对应，都是单线程版本，同样适合客户端使用。 年轻代的 Serial，使用复制算法。 老年代的 Old Serial，使用标记-整理算法。 Parallel Old Parallel Old 收集器是 Parallel Scavenge 的老年代版本，追求 CPU 吞吐量。 CMS 垃圾收集器 CMS（Concurrent Mark Sweep）收集器是以获取最短 GC 停顿时间为目标的收集器，它在垃圾收集时使得用户线程和 GC 线程能够并发执行，因此在垃圾收集过程中用户也不会感到明显的卡顿。我们会在后面的课时详细介绍它。 长期来看，CMS 垃圾回收器，是要被 G1 等垃圾回收器替换掉的。在 Java8 之后，使用它将会抛出一个警告。 Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release. 配置参数除了上面几个垃圾回收器，还有 G1、ZGC 等更加高级的垃圾回收器，它们都有专门的配置参数来使其生效。通过 -XX:+PrintCommandLineFlags 参数，可以查看当前 Java 版本默认使用的垃圾回收器。例如：java -XX:+PrintCommandLineFlags -version -XX:G1ConcRefinementThreads=4 -XX:GCDrainStackTargetSize=64 -XX:InitialHeapSize=134217728 -XX:MaxHeapSize=2147483648 -XX:MinHeapSize=6815736 -XX:+PrintCommandLineFlags -XX:ReservedCodeCacheSize=251658240 -XX:+SegmentedCodeCache -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseG1GC输出结果：java version &quot;13.0.1&quot; 2019-10-15Java(TM) SE Runtime Environment (build 13.0.1+9)Java HotSpot(TM) 64-Bit Server VM (build 13.0.1+9, mixed mode, sharing)一些配置参数： XX:+UseSerialGC，年轻代和老年代都用串行收集器 -XX:+UseParNewGC，年轻代使用 ParNew，老年代使用 Serial Old -XX:+UseParallelGC，年轻代使用 ParallerGC，老年代使用 Serial Old -XX:+UseParallelOldGC， 新生代和老年代都使用并行收集器 -XX:+UseConcMarkSweepGC，表示年轻代使用 ParNew，老年代的用 CMS -XX:+UseG1GC， 使用 G1垃圾回收器 -XX:+UseZGC， 使用 ZGC 垃圾回收器请看下图。它们的关系还是比较复杂的。尤其注意 -XX:+UseParNewGC 这个参数，已经在 Java9 中就被抛弃了。很多程序（比如 ES）会报这个错误，不要感到奇怪。有这么多垃圾回收器和参数，到底用什么？在什么地方优化？目前，虽然 Java 的版本比较高，但是使用最多的还是 Java8。从 Java8 升级到高版本的 Java 体系，是有一定成本的，所以 CMS 垃圾回收器还会持续一段时间。线上使用最多的垃圾回收器，就有 CMS 和 G1，以及 Java8 默认的 Parallel Scavenge。 CMS 的设置参数：-XX:+UseConcMarkSweepGC。 Java8 的默认参数：-XX:+UseParallelGC。 Java13 的默认参数：-XX:+UseG1GC。STW如果在垃圾回收的时候（不管是标记还是整理复制），又有新的对象进入怎么办？为了保证程序不会乱套，最好的办法就是暂停用户的一切线程。也就是在这段时间，不能 new 对象的，只能等待。表现在 JVM 上就是短暂的卡顿，什么都干不了。这个头疼的现象，就叫作 Stop the world。简称 STW。标记阶段，大多数是要 STW 的。如果不暂停用户进程，在标记对象的时候，有可能有其他用户线程会产生一些新的对象和引用，造成混乱。现在的垃圾回收器，都会尽量去减少这个过程。但即使是最先进的 ZGC，也会有短暂的 STW 过程。我们要做的就是在现有基础设施上，尽量减少 GC 停顿。栗子：某个高并发服务的峰值流量是 10 万次/秒，后面有 10 台负载均衡的机器，那么每台机器平均下来需要 1w/s。假如某台机器在这段时间内发生了 STW，持续了 1 秒，那么本来需要 10ms 就可以返回的 1 万个请求，需要至少等待 1 秒钟。在用户那里的表现，就是系统发生了卡顿。如果 GC 非常的频繁，这种卡顿就会特别的明显，严重影响用户体验。虽然说 Java 为我们提供了非常棒的自动内存管理机制，但也不能滥用，因为它是有 STW 硬伤的。" }, { "title": "JVM 内存管理", "url": "/posts/jvm-memory/", "categories": "Java, JVM", "tags": "JVM", "date": "2021-08-02 14:23:00 +0000", "snippet": "随着 Java 的发展，内存布局一直在调整之中。比如，Java 8 及之后的版本，彻底移除了持久代，而使用 Metaspace 来进行替代。这也表示着 -XX:PermSize 和 -XX:MaxPermSize 等参数调优，已经没有了意义。但大体上，比较重要的内存区域是固定的。 数据私有（线程相关） 程序计数器（Program counter register） Java 虚拟机栈（Java Virtual Machine Stacks） 本地方法区（Native Method Stacks） 数据共有 元空间（Metaspace），新划分的区域，包含“方法区” 堆（Heap），占用内存最大的一块区域 执行引擎（Execution engine），负责执行被加载类方法中的字节码指令，在线程切换时的恢复依靠的是程序计数器。 JVM 的内存划分与多线程是息息相关的。像程序中运行时用到的栈，以及本地方法栈，它们的维度都是线程。 本地内存包含元数据区和一些直接内存。虚拟机栈栈是什么样的数据结构？你可以想象一下子弹上膛的这个过程，后进的子弹最先射出，最上面的子弹就相当于栈顶。我们在上面提到，Java 虚拟机栈是基于线程的。哪怕你只有一个 main() 方法，也是以线程的方式运行的。在线程的生命周期中，参与计算的数据会频繁地入栈和出栈，栈的生命周期是和线程一样的。栈里的每条数据，就是栈帧。在每个 Java 方法被调用的时候，都会创建一个栈帧，并入栈。一旦完成相应的调用，则出栈。所有的栈帧都出栈后，线程也就结束了。每个栈帧，都包含四个区域： 局部变量表 操作数栈 动态连接 返回地址我们的应用程序，就是在不断操作这些内存空间中完成的。本地方法栈是和虚拟机栈非常相似的一个区域，它服务的对象是 native 方法。你甚至可以认为虚拟机栈和本地方法栈是同一个区域，这并不影响我们对 JVM 的了解。这里有一个比较特殊的数据类型叫作 returnAdress。因为这种类型只存在于字节码层面，所以我们平常打交道的比较少。对于 JVM 来说，程序就是存储在方法区的字节码指令，而 returnAddress 类型的值就是指向特定指令内存地址的指针。这部分有两个比较有意思的内容，面试中说出来会让面试官眼前一亮。 这里有一个两层的栈。第一层是栈帧，对应着方法；第二层是方法的执行，对应着操作数。注意千万不要搞混了。 你可以看到，所有的字节码指令，其实都会抽象成对栈的入栈出栈操作。执行引擎只需要傻瓜式的按顺序执行，就可以保证它的正确性。 这一点很神奇，也是基础。我们接下来从线程角度看一下里面的内容。程序计数器程序在线程之间进行切换，凭什么能够知道这个线程已经执行到什么地方呢？既然是线程，就代表它在获取 CPU 时间片上，是不可预知的，需要有一个地方，对线程正在运行的点位进行缓冲记录，以便在获取 CPU 时间片时能够快速恢复。就好比你停下手中的工作，倒了杯茶，然后如何继续之前的工作？程序计数器是一块较小的内存空间，它的作用可以看作是当前线程所执行的字节码的行号指示器。这里面存的，就是当前线程执行的进度。下面这张图，能够加深大家对这个过程的理解。可以看到，程序计数器也是因为线程而产生的，与虚拟机栈配合完成计算操作。程序计数器还存储了当前正在运行的流程，包括正在执行的指令、跳转、分支、循环、异常处理等。我们可以看一下程序计数器里面的具体内容。下面这张图，就是使用 javap 命令输出的字节码。大家可以看到在每个 opcode 前面，都有一个序号。就是图中红框中的偏移地址，你可以认为它们是程序计数器的内容。堆堆是 JVM 上最大的内存区域，申请的几乎所有的对象，都是在这里存储的。我们常说的垃圾回收，操作的对象就是堆。堆空间一般是程序启动时，就申请了，但是并不一定会全部使用。随着对象的频繁创建，堆空间占用的越来越多，就需要不定期的对不再使用的对象进行回收。这个在 Java 中，就叫作 GC（Garbage Collection）。由于对象的大小不一，在长时间运行后，堆空间会被许多细小的碎片占满，造成空间浪费。所以，仅仅销毁对象是不够的，还需要堆空间整理。这个过程非常的复杂。那一个对象创建的时候，到底是在堆上分配，还是在栈上分配呢？这和两个方面有关：对象的类型和在 Java 类中存在的位置。Java 的对象可以分为基本数据类型和普通对象。对于普通对象来说，JVM 会首先在堆上创建对象，然后在其他地方使用的其实是它的引用。比如，把这个引用保存在虚拟机栈的局部变量表中。对于基本数据类型来说（byte、short、int、long、float、double、char)，有两种情况。我们上面提到，每个线程拥有一个虚拟机栈。当你在方法体内声明了基本数据类型的对象，它就会在栈上直接分配。其他情况，都是在堆上分配。注意，像 int[] 数组这样的内容，是在堆上分配的。数组并不是基本数据类型。这就是 JVM 的基本的内存分配策略。而堆是所有线程共享的，如果是多个线程访问，会涉及数据同步问题。这同样是个大话题，我们在这里先留下一个悬念。元空间关于元空间，我们还是以一个非常高频的面试题开始：“为什么有 Metaspace 区域？它有什么问题？”说到这里，你应该回想一下类与对象的区别。对象是一个活生生的个体，可以参与到程序的运行中；类更像是一个模版，定义了一系列属性和操作。那么你可以设想一下。我们前面生成的 A.class，是放在 JVM 的哪个区域的？想要问答这个问题，就不得不提下 Java 的历史。在 Java 8 之前，这些类的信息是放在一个叫 Perm 区的内存里面的。更早版本，甚至 String.intern 相关的运行时常量池也放在这里。这个区域有大小限制，很容易造成 JVM 内存溢出，从而造成 JVM 崩溃。Perm 区在 Java 8 中已经被彻底废除，取而代之的是 Metaspace。原来的 Perm 区是在堆上的，现在的元空间是在非堆上的，这是背景。关于它们的对比，可以看下这张图。然后，元空间的好处也是它的坏处。使用非堆可以使用操作系统的内存，JVM 不会再出现方法区的内存溢出；但是，无限制的使用会造成操作系统的死亡。所以，一般也会使用参数 -XX:MaxMetaspaceSize 来控制大小。方法区，作为一个概念，依然存在。它的物理存储的容器，就是 Metaspace。我们将在后面的课时中，再次遇到它。现在，你只需要了解到，这个区域存储的内容，包括：类的信息、常量池、方法数据、方法代码就可以了。总结好了，到这里本课时的基本内容就讲完了，针对这块的内容在面试中还经常会遇到下面这两个问题。我们常说的字符串常量，存放在哪呢？由于常量池，在 Java 7 之后，放到了堆中，我们创建的字符串，将会在堆上分配。堆、非堆、本地内存，有什么关系？关于它们的关系，我们可以看一张图。在我的感觉里，堆是软绵绵的，松散而有弹性；而非堆是冰冷生硬的，内存非常紧凑。大家都知道，JVM 在运行时，会从操作系统申请大块的堆内内存，进行数据的存储。但是，堆外内存也就是申请后操作系统剩余的内存，也会有部分受到 JVM 的控制。比较典型的就是一些 native 关键词修饰的方法，以及对内存的申请和处理。在 Linux 机器上，使用 top 或者 ps 命令，在大多数情况下，能够看到 RSS 段（实际的内存占用），是大于给 JVM 分配的堆内存的。如果你申请了一台系统内存为 2GB 的主机，可能 JVM 能用的就只有 1GB，这便是一个限制。JVM 的运行时区域是栈，而存储区域是堆。很多变量，其实在编译期就已经固定了。.class 文件的字节码，由于助记符的作用，理解起来并不是那么吃力，我们将在课程最后几个课时，从字节码层面看一下多线程的特性。" }, { "title": "典型的分布式缓存系统", "url": "/posts/cache-redis-15/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2021-07-23 13:33:22 +0000", "snippet": "分布式 Redis 服务以微博内的 分布式 Redis 服务系统为例，总结一个典型的分布式缓存系统的组成：微博的 Redis 服务内部也称为 RedisService。RedisService 的整体架构如图所示。主要分为Proxy、存储、集群管理、配置中心、Graphite，5 个部分： RedisService 中的 Proxy 是无状态多租户模型，每个 Proxy 下可以挂载不同的业务存储，通过端口进行业务区分； 存储基于 Redis 开发，但在集群数据存储时，只保留了基本的存储功能，支持定制的迁移功能，但存储内部无状态，不存储 key-slot 映射关系； 配置中心用于记录及分发各种元数据，如存储 Proxy 的 IP、端口、配置等，在发生变化时，订阅者可以及时感知； Graphite 系统用于记录并展现系统、业务，组件以及实例等的状态数据； ClusterManager 用于日常运维管理，业务 SLA 监控，报警等。同时 ClusterManager 会整合 Proxy、Redis 后端存储以及配置中心，对业务数据进行集群管理多租户 ProxyRedisService 中的 Proxy 无任何状态，所有 Proxy 实例的启动参数相同。但 Proxy 启动前，clusterManager 会在配置中心设置该实例的业务及存储配置信息，Proxy 启动后，到配置中心通过自己的 IP 来获取并订阅配置，然后进行初始化。Proxy 与后端 Redis 存储采用长连接，当 Client 并发发送请求到 Proxy 后，Proxy 会将请求进行打包，并发地以 pipeline 的方式批量访问后端，以提升请求效率。对于多租户 Proxy，由于不同业务的存储位置可能不同，因此对每个请求需要进行业务区分，一般有 2 种方式进行区分： 方案 1，按照 key 的 namespace 前缀进行业务区分，比如 Client 分别请求 user、graph、feed 业务下的 key k1，业务 Client 分别构建 {user}k1、{graph}k1、{feed}k1，然后发送给 Proxy，Proxy 解析 key 前缀确定 key 对应的业务； 方案 2，对每个业务分配一个业务端口，不同业务访问自己的端口，Proxy 会根据端口确定业务类型。这种类型不需要解析 key 前缀，不需要重构请求，性能更为高效。但需要为业务配置端口，增加管理成本，实践上，由于业务 Redis 资源一般会采用不同端口，所以业务 Proxy 可以采用业务资源分片的最小端口来作为业务端口标志。Redis 数据存储RedisService 中的 Redis 存储基于 Redis 5.0 扩展，内部称 wredis，wredis 不存储 key-slot 映射，只记录当前实例中存储的 slot 的 key 计数。wredis 处理任何收到的操作命令，而数据分片访问的正确性由访问端确保。在每日低峰时段，clusterManager 对 Redis 存储进行扫描，发现 slot 存储是否存在异常。因为微博中有大量的小 value key，如果集群中增加 key-slot 映射，会大大增大存储成本，通过消除 key-slot 映射等相关优化，部分业务可以减少 20% 以上的存储容量。wredis 支持 slot 的同步迁移及异步迁移。同时支持热升级，可以毫秒级完成组件升级。wredis 也支持全增量复制，支持微博内部扩展的多种数据结构。热升级、全增量复制、数据结构扩展等，在之前的课时中有介绍，具体可以参考之前讲的“Redis 功能扩展”课时的内容。配置中心 configService微博的配置中心，内部称为 configService，是微博内部配置元数据管理的基础组件。configService 自身也是多 IDC 部署的，配置信息通过多版本数据结构存储，支持版本回溯。同时配置数据可以通过 merkle hash 树进行快速一致性验证。RedisService 中的所有业务、资源、Proxy 的配置都存储在 configService 中，由 cluster 写入并变更，Proxy、业务 Client 获取并订阅所需的配置数据。configService 在配置节点发生变更时，会只对节点进行事件通知，订阅者无需获取全量数据，可以大大减轻配置变更后的获取开销。ClusterManager 是一个运维后台。主要用于运维工作，如后端资源、Proxy 的实例部署，配置变更，版本升级等。也用于数据的集群管理，clusterManager 内部会存储业务数据的集群映射，并在必要时进行数据迁移和故障转移。迁移采用 slot 方式，可以根据负载进行迁移流量控制，同时会探测集群内的节点状态，如在 wredis 的 master 异常后，从 slave 中选择一个新的master，并重建主从关系。clusterManager 还支持业务访问的 Proxy 域名管理，监控集群节点的实例状态，监控业务的 SLA 指标，对异常进行报警，以便运维及时进行处理。集群数据同步RedisService 中的数据存储在多个区域，每个区域都有多个 IDC。部署方式是核心内网加公有云的方式。使用公有云，主要是由微博的业务特点决定的，在突发事件或热点事件发生时，很容易形成流量洪峰，读写 TPS 大幅增加，利用公有云可以快速、低成本的扩展系统，大幅增加系统处理能力。根据业务特点，wredis 被分为缓存和存储类型。对于 Redis 缓存主要通过消息总线进行驱动更新，而对于 Redis 存储则采用主从复制更新。更新方式不同，主要是因为 Redis 作为缓存类型的业务数据，在不同区或者不同 IDC 的热点数据不同，如果采用主从复制，部署从库的 IDC，会出现热数据无法进入缓存，同时冷数据无法淘汰的问题，因为从库的淘汰也要依赖主库进行。而对于 Redis 作存储的业务场景，由于缓存存放全量数据，直接采用主从复制进行数据一致性保障，这样最便捷。" }, { "title": "从容应对亿级QPS访问，Redis 还缺少什么!", "url": "/posts/cache-redis-14/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2021-07-19 13:33:22 +0000", "snippet": "众所周知，Redis 在线上实际运行时，面对海量数据、高并发访问，会遇到不少问题，需要进行针对性扩展及优化。本课时，我会结合微博在使用 Redis 中遇到的问题，来分析如何在生产环境下对 Redis 进行扩展改造，以应对百万级 QPS。功能扩展对于线上较大流量的业务，单个 Redis 实例的内存占用很容易达到数 G 的容量，对应的 aof 会占用数十 G 的空间。即便每天流量低峰时间，对 Redis 进行 rewriteaof，减少数据冗余，但由于业务数据多，写操作多，aof 文件仍然会达到 10G 以上。此时，在 Redis 需要升级版本或修复 bug 时，如果直接重启变更，由于需要数据恢复，这个过程需要近 10 分钟的时间，时间过长，会严重影响系统的可用性。面对这种问题，可以对 Redis 扩展热升级功能，从而在毫秒级完成升级操作，完全不影响业务访问。热升级方案如下，首先构建一个 Redis 壳程序，将 redisServer 的所有属性（包括redisDb、client等）保存为全局变量。然后将 Redis 的处理逻辑代码全部封装到动态连接库 so 文件中。Redis 第一次启动，从磁盘加载恢复数据，在后续升级时，通过指令，壳程序重新加载 Redis 新的 so 文件，即可完成功能升级，毫秒级完成 Redis 的版本升级。而且整个过程中，所有 Client 连接仍然保留，在升级成功后，原有 Client 可以继续进行读写操作，整个过程对业务完全透明。​在 Redis 使用中，也经常会遇到一些特殊业务场景，是当前 Redis 的数据结构无法很好满足的。此时可以对 Redis 进行定制化扩展。可以根据业务数据特点，扩展新的数据结构，甚至扩展新的 Redis 存储模型，来提升 Redis 的内存效率和处理性能。在微博中，有个业务类型是关注列表。关注列表存储的是一个用户所有关注的用户 uid。关注列表可以用来验证关注关系，也可以用关注列表，进一步获取所有关注人的微博列表等。由于用户数量过于庞大，存储关注列表的 Redis 是作为一个缓存使用的，即不活跃的关注列表会很快被踢出 Redis。在再次需要这个用户的关注列表时，重新从 DB 加载，并写回 Redis。关注列表的元素全部 long，最初使用 set 存储，回种 set 时，使用 sadd 进行批量添加。线上发现，对于关注数比较多的关注列表，比如关注数有数千上万个用户，需要 sadd 上成千上万个 uid，即便分几次进行批量添加，每次也会消耗较多时间，数据回种效率较低，而且会导致 Redis 卡顿。另外，用 set 存关注列表，内存效率也比较低。于是，我们对 Redis 扩展了 longset 数据结构。longset 本质上是一个 long 型的一维开放数组。可以采用 double-hash 进行寻址。从 DB 加载到用户的关注列表，准备写入 Redis 前。Client 首先根据关注的 uid 列表，构建成 long 数组的二进制格式，然后通过扩展的 lsset 指令写入 Redis。Redis 接收到指令后，直接将 Client 发来的二进制格式的 long 数组作为 value 值进行存储。longset 中的 long 数组，采用 double-hash 进行寻址，即对每个 long 值采用 2 个哈希函数计算，然后按 (h1 + n*h2)% 数组长度 的方式，确定 long 值的位置。n 从 0 开始计算，如果出现哈希冲突，即计算的哈希位置，已经有其他元素，则 n 加 1，继续向前推进计算，最大计算次数是数组的长度。在向 longset 数据结构不断增加 long 值元素的过程中，当数组的填充率超过阀值，Redis 则返回 longset 过满的异常。此时 Client 会根据最新全量数据，构建一个容量加倍的一维 long 数组，再次 lsset 回 Redis 中。在移动社交平台中，庞大的用户群体，相互之间会关注、订阅，用户自己会持续分享各种状态，另外这些状体数据会被其他用户阅读、评论、扩散及点赞。这样，在用户维度，就有关注数、粉丝数、各种状态行为数，然后用户每发表的一条 feed、状态，还有阅读数、评论数、转发数、表态数等。一方面会有海量 key 需要进行计数，另外一方面，一个 key 会有 N 个计数。在日常访问中，一次查询，不仅需要查询大量的 key，而且对每个 key 需要查询多个计数。以微博为例，历史计数高达千亿级，而且随着每日新增数亿条 feed 记录，每条记录会产生 4~8 种计数，如果采用 Redis 的计数，仅仅单副本存储，历史数据需要占用 5~6T 以上的内存，每日新增 50G 以上，如果再考虑多 IDC、每个 IDC 部署 1 主多从，占用内存还要再提升一个数量级。由于微博计数，所有的 key 都是随时间递增的 long 型值，于是我们改造了 Redis 的存储结构。首先采用 cdb 分段存储计数器，通过预先分配的内存数组 Table 存储计数，并且采用 double hash 解决冲突，避免 Redis 实现中的大量指针开销。 然后，通过 Schema 策略支持多列，一个 key id 对应的多个计数可以作为一条计数记录，还支持动态增减计数列，每列的计数内存使用精简到 bit。而且，由于 feed 计数冷热区分明显，我们进行冷热数据分离存储方案，根据时间维度，近期的热数据放在内存，之前的冷数据放在磁盘， 降低机器成本。关于计数器服务的扩展，后面的案例分析课时，我会进一步深入介绍改造方案。完全增量复制于是，微博整合 Redis 的 rdb 和 aof 策略，构建了完全增量复制方案。在完全增量方案中，aof 文件不再只有一个，而是按后缀 id 进行递增，如 aof.00001、aof.00002，当 aof 文件超过阀值，则创建下一个 id 加 1 的文件，从而滚动存储最新的写指令。在 bgsave 构建 rdb 时，rdb 文件除了记录当前的内存数据快照，还会记录 rdb 构建时间，对应 aof 文件的 id 及位置。这样 rdb 文件和其记录 aof 文件位置之后的写指令，就构成一份完整的最新数据记录。主从复制时，master 通过独立的复制线程向 slave 同步数据。每个 slave 会创建一个复制线程。第一次复制是全量复制，之后的复制，不管 slave 断开复制连接有多久，只要 aof 文件没有被删除，都是增量复制。第一次全量复制时，复制线程首先将 rdb 发给 slave，然后再将 rdb 记录的 aof 文件位置之后的所有数据，也发送给 slave，即可完成。整个过程不用重新构建 rdb。后续同步时，slave 首先传递之前复制的 aof 文件的 id 及位置。master 的复制线程根据这个信息，读取对应 aof 文件位置之后的所有内容，发送给 slave，即可完成数据同步。由于整个复制过程，master 在独立复制线程中进行，所以复制过程不影响用户的正常请求。为了减轻 master 的复制压力，全增量复制方案仍然支持 slave 嵌套，即可以在 slave 后继续挂载多个 slave，从而把复制压力分散到多个不同的 Redis 实例。集群管理前面讲到，Redis-Cluster 的数据存储和集群逻辑耦合，代码逻辑复杂易错，存储 slot 和 key 的映射需要额外占用较多内存，对小 value 业务影响特别明显，而且迁移效率低，迁移大 value 容易导致阻塞，另外，Cluster 复制只支持 slave 挂在 master 下，无法支持 需要较多slave、读 TPS 特别大的业务场景。除此之外，Redis 当前还只是个存储组件，线上运行中，集群管理、日常维护、状态监控报警等这些功能，要么没有支持，要么支持不便。因此我们也基于 Redis 构建了集群存储体系。首先将 Redis 的集群功能剥离到独立系统，Redis 只关注存储，不再维护 slot 等相关的信息。通过新构建的 clusterManager 组件，负责 slot 维护，数据迁移，服务状态管理。Redis 集群访问可以由 proxy 或 smart client 进行。对性能特别敏感的业务，可以通过 smart client 访问，避免访问多一跳。而一般业务，可以通过 Proxy 访问 Redis。业务资源的部署、Proxy 的访问，都通过配置中心进行获取及协调。clusterManager 向配置中心注册业务资源部署，并持续探测服务状态，根据服务状态进行故障转移，切主、上下线 slave 等。proxy 和 smart client 从配置中心获取配置信息，并持续订阅服务状态的变化。" }, { "title": "构建一个高性能、易扩展的Redis集群", "url": "/posts/cache-redis-13/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2021-07-15 16:33:22 +0000", "snippet": "复制功能可以 N 倍提升 Redis 节点的读性能，而集群则可以通过分布式方案来 N 倍提升 Redis 的写性能。除了提升性能之外，Redis 集群还可以提供更大的容量，提升资源系统的可用性。Redis 集群的分布式方案主要有 3 种。分别是 Client 端分区方案，Proxy 分区方案，以及原生的 Redis Cluster 分区方案。Client 端分区Client 端分区方案就是由 Client 决定数据被存储到哪个 Redis 分片，或者由哪个 Redis 分片来获取数据。它的核心思想是通过哈希算法将不同的 key 映射到固定的 Redis 分片节点上。对于单个 key 请求，Client 直接对 key 进行哈希后，确定 Redis 分片，然后进行请求。而对于一个请求附带多个 key 的场景，Client 会首先将这些 key 按哈希分片进行分类，从而将一个请求分拆为多个请求，然后再分别请求不同的哈希分片节点。Client 通过哈希算法将数据进行分布，一般采用的哈希算法是取模哈希、一致性哈希和区间分布哈希。前两种哈希算法之前的课程已有详细分析，此处不在赘述。对于区间分布哈希，实际是一种取模哈希的变种，取模哈希是哈希并取模计算后，按哈希值来分配存储节点，而区间哈希是在哈希计算后，将哈希划分为多个区间，然后将这些区间分配给存储节点。如哈希后分 1024 个哈希点，然后将 0~511 作为分片 1，将 512~1023 作为分片 2。对于 Client 端分区，由于 Redis 集群有多个 master 分片，同时每个 master 下挂载多个 slave，每个 Redis 节点都有独立的 IP 和端口。如果 master 异常需要切换 master，或读压力过大需要扩展新的 slave，这些都会涉及集群存储节点的变更，需要 Client 端做连接切换。为了避免 Client 频繁变更 IP 列表，可以采用 DNS 的方式来管理集群的主从。对 Redis 集群的每个分片的主和从均采用不同 DNS 域名。Client 通过域名解析的方式获取域名下的所有 IP，然后来访问集群节点。由于每个分片 master 下有多个 slave，Client 需要在多个 slave 之间做负载均衡。可以按照权重建立与 slave 之间的连接，然后访问时，轮询使用这些连接依次访问，即可实现按权重访问 slave 节点。在 DNS 访问模式下，Client 需要异步定时探测主从域名，如果发现 IP 变更，及时与新节点建立连接，并关闭老连接。这样在主库故障需要切换时，或者从库需要增加减少时，任何分片的主从变化，只需运维或管理进程改一下 DNS 下的 IP 列表，业务 Client 端不需要做任何配置变更，即可正常切换访问。​Client 端分区方案的优点在于分区逻辑简单，配置简单，Client 节点之间和 Redis 节点之间均无需协调，灵活性强。而且 Client 直接访问对应 Redis 节点，没有额外环节，性能高效。但该方案扩展不便。在 Redis 端，只能成倍扩展，或者预先分配足够多的分片。在 Client 端，每次分片后，业务端需要修改分发逻辑，并进行重启。Proxy 端分区Proxy 端分区方案是指 Client 发送请求给 Proxy 请求代理组件，Proxy 解析 Client 请求，并将请求分发到正确的 Redis 节点，然后等待 Redis 响应，最后再将结果返回给 Client 端。如果一个请求包含多个 key，Proxy 需要将请求的多个 key，按分片逻辑分拆为多个请求，然后分别请求不同的 Redis 分片，接下来等待Redis响应，在所有的分拆响应到达后，再进行聚合组装，最后返回给 Client。在整个处理过程中，Proxy 代理首先要负责接受请求并解析，然后还要对 key 进行哈希计算及请求路由，最后还要将结果进行读取、解析及组装。如果系统运行中，主从变更或发生扩缩容，也只需由 Proxy 变更完成，业务 Client 端基本不受影响。​常见的 Proxy 端分区方案有 2 种，第一种是基于 Twemproxy 的简单分区方案，第二种是基于Codis 的可平滑数据迁移的分区方案。Twemproxy 是 Twitter 开源的一个组件，支持 Redis 和 Memcached 协议访问的代理组件。在讲分布式 Memecached 实战时，我曾经详细介绍了它的原理和实现架构，此处不再赘述。总体而言，Twemproxy 实现简单、稳定性高，在一些访问量不大且很少发生扩缩的业务场景中，可以很好的满足需要。但由于 Twemproxy 是单进程单线程模型的，对包含多个 key 的 mutli 请求，由于需要分拆请求，然后再等待聚合，处理性能较低。而且，在后端 Redis 资源扩缩容，即增加或减少分片时，需要修改配置并重启，无法做到平滑扩缩。而且 Twemproxy 方案默认只有一个代理组件，无管理后端，各种运维变更不够便利。而 Codis 是一个较为成熟的分布式 Redis 解决方案。对于业务 Client 访问，连接 Codis-proxy 和连接单个 Redis 几乎没有区别。Codis 底层除了会自动解析分发请求之外，还可以在线进行数据迁移，使用非常方便。Codis 系统主要由 Codis-server、Codis-proxy、Codis-dashboard、Zookeeper 等组成。 Codis-server 是 Codis 的存储组件，它是基于 Redis 的扩展，增加了 slot 支持和数据迁移功能，所有数据存储在预分配的 1024 个 slot 中，可以按 slot 进行同步或异步数据迁移； Codis-proxy 处理 Client 请求，解析业务请求，并路由给后端的 Codis-server group。Codis 的每个 server group 相当于一个 Redis 分片，由 1 个 master 和 N 个从库组成； Zookeeper 用于存储元数据，如 Proxy 的节点，以及数据访问的路由表。除了 Zookeeper，Codis 也支持 etcd 等其他组件，用于元数据的存储和通知； Codis-dashboard 是 Codis 的管理后台，可用于管理数据节点、Proxy 节点的加入或删除，还可用于执行数据迁移等操作。Dashboard 的各项变更指令通过 Zookeeper 进行分发； Codis 提供了功能较为丰富的管理后台，可以方便的对整个集群进行监控及运维。Proxy 端分区方案的优势，是 Client 访问逻辑和 Redis 分布逻辑解耦，业务访问便捷简单。在资源发生变更或扩缩容时，只用修改数量有限的 Proxy 即可，数量庞大的业务 Client 端不用做调整。但 Proxy 端分区的方案，访问时请求需要经过 Proxy 中转，访问多跳了一级，性能会存在损耗，一般损耗会达到 5~15% 左右。另外多了一个代理层，整个系统架构也会更复杂。Redis Cluster 分区Redis 社区版在 3.0 后开始引入 Cluster 策略，一般称之为 Redis-Cluster 方案。Redis-Cluster 按 slot 进行数据的读写和管理，一个 Redis-Cluster 集群包含 16384 个 slot。每个 Redis 分片负责其中一部分 slot。在集群启动时，按需将所有 slot 分配到不同节点，在集群系统运行后，按 slot 分配策略，将 key 进行 hash 计算，并路由到对应节点 访问。随着业务访问模型的变化，Redis 部分节点可能会出现压力过大、访问不均衡的现象，此时可以将 slot 在 Redis 分片节点内部进行迁移，以均衡访问。如果业务不断发展，数据量过大、TPS过高，还可以将 Redis 节点的部分 slot 迁移到新节点，增加 Redis-Cluster 的分片，对整个 Redis 资源进行扩容，已提升整个集群的容量及读写能力。​在启动 Redis 集群时，在接入数据读写前，可以通过 Redis 的 Cluster addslots 将 16384 个 slot 分配给不同的 Redis 分片节点，同时可以用 Cluster delslots 去掉某个节点的 slot，用 Cluster flushslots 清空某个节点的所有 slot 信息，来完成 slot 的调整。Redis Cluster 是一个去中心化架构，每个节点记录全部 slot 的拓扑分布。这样 Client 如果把 key 分发给了错误的 Redis 节点，Redis 会检查请求 key 所属的 slot，如果发现 key 属于其他节点的 slot，会通知 Client 重定向到正确的 Redis 节点访问。Redis Cluster 下的不同 Redis 分片节点通过 gossip 协议进行互联，使用 gossip 的优势在于，该方案无中心控制节点，这样，更新不会受到中心节点的影响，可以通过通知任意一个节点来进行管理通知。不足就是元数据的更新会有延时，集群操作会在一定的时延后才会通知到所有Redis。由于 Redis Cluster 采用 gossip 协议进行服务节点通信，所以在进行扩缩容时，可以向集群内任何一个节点，发送 Cluster meet 指令，将新节点加入集群，然后集群节点会立即扩散新节点，到整个集群。meet 新节点操作的扩散，只需要有一条节点链能到达集群各个节点即可，无需 meet 所有集群节点，操作起来比较便利。​在 Redis-Cluster 集群中，key 的访问需要 smart client 配合。Client 首先发送请求给 Redis 节点，Redis 在接受并解析命令后，会对 key 进行 hash 计算以确定 slot 槽位。计算公式是对 key 做 crc16 哈希，然后对 16383 进行按位与操作。如果 Redis 发现 key 对应的 slot 在本地，则直接执行后返回结果。如果 Redis 发现 key 对应的 slot 不在本地，会返回 moved 异常响应，并附带 key 的 slot，以及该 slot 对应的正确 Redis 节点的 host 和 port。Client 根据响应解析出正确的节点 IP 和端口，然后把请求重定向到正确的 Redis，即可完成请求。为了加速访问，Client 需要缓存 slot 与 Redis 节点的对应关系，这样可以直接访问正确的节点，以加速访问性能。​Redis-Cluster 提供了灵活的节点扩缩容方案，可以在不影响用户访问的情况下，动态为集群增加节点扩容，或下线节点为集群缩容。由于扩容在线上最为常见，我首先来分析一下 Redis-Cluster 如何进行扩容操作。在准备对 Redis 扩容时，首先准备待添加的新节点，部署 Redis，配置 cluster-enable 为 true，并启动。然后运维人员，通过client连接上一个集群内的 Redis 节点，通过 cluster meet 命令将新节点加入到集群，该节点随后会通知集群内的其他节点，有新节点加入。因为新加入的节点还没有设置任何 slot，所以不接受任何读写操作。然后，将通过 cluster setslot $slot importing 指令，在新节点中，将目标 slot 设为 importing 导入状态。再将 slot 对应的源节点，通过 cluster setslot $slot migrating 将源节点的 slot 设为 migrating 迁移导出状态。​接下来，就从源节点获取待迁移 slot 的 key，通过 cluster getkeysinslot $slot $count 命令，从 slot 中获取 N 个待迁移的 key。然后通过 migrate 指令，将这些 key 依次逐个迁移或批量一次迁移到目标新节点。对于迁移单个 key，使用指令 migrate $host $port $key $dbid timeout，如果一次迁移多个 key，在指令结尾加上 keys 选项，同时将多个 key 放在指令结尾即可。持续循环前面 2 个步骤，不断获取 slot 里的 key，然后进行迁移，最终将 slot 下的所有数据都迁移到目标新节点。最后通过 cluster setslot 指令将这个 slot 指派给新增节点。setslot 指令可以发给集群内的任意一个节点，这个节点会将这个指派信息扩散到整个集群。至此，slot 就迁移到了新节点。如果要迁移多个 slot，可以继续前面的迁移步骤，最终将所有需要迁移的 slot 数据搬到新节点。这个新迁移 slot 的节点属于主库，对于线上应用，还需要增加从库，以增加读写能力及可用性，否则一旦主库崩溃，整个分片的数据就无法访问。在节点上增加从库，需要注意的是，不能使用非集群模式下的 slaveof 指令，而要使用 cluster replication，才能完成集群分片节点下的 slave 添加。另外，对于集群模式，slave 只能挂在分片 master 上，slave 节点自身不能再挂载 slave。缩容流程与扩容流程类似，只是把部分节点的 slot 全部迁移走，然后把这些没有 slot 的节点进行下线处理。在下线老节点之前，需要注意，要用 cluster forget 通知集群，集群节点要，从节点信息列表中，将目标节点移除，同时会将该节点加入到禁止列表，1 分钟之内不允许再加入集群。以防止在扩散下线节点时，又被误加入集群。Redis 社区官方在源代码中也提供了 redis-trib.rb，作为 Redis Cluster 的管理工具。该工具用 Ruby 开发，所以在使用前，需要安装相关的依赖环境。redis-trib 工具通过封装前面所述的 Redis 指令，从而支持创建集群、检查集群、添加删除节点、在线迁移 slot 等各种功能。​Redis Cluster 在 slot 迁移过程中，获取key指令以及迁移指令逐一发送并执行，不影响 Client 的正常访问。但在迁移单条或多条 key 时，Redis 节点是在阻塞状态下进行的，也就是说，Redis 在迁移 key 时，一旦开始执行迁移指令，就会阻塞，直到迁移成功或确认失败后，才会停止该 key 的迁移，从而继续处理其他请求。slot 内的 key 迁移是通过 migrate 指令进行的。在源节点接收到 migrate $host $port $key $destination-db 的指令后，首先 slot 迁移的源节点会与迁移的目标节点建立 socket 连接，第一次迁移，或者迁移过程中，当前待迁移的 DB 与前一次迁移的 DB 不同，在迁移数据前，还需要发送 select $dbid 进行切换到正确的 DB。然后，源节点会轮询所有待迁移的 key/value。获取 key 的过期时间，并将 value 进行序列化，序列化过程就是将 value 进行 dump，转换为类 rdb 存储的二进制格式。这个二进制格式分 3 部分。第一部分是 value 对象的 type。第二部分是 value 实际的二进制数据；第三部分是当前 rdb 格式的版本，以及该 value 的 CRC64 校验码。至此，待迁移发送的数据准备完毕，源节点向目标节点，发送 restore-asking 指令，将过期时间、key、value 的二进制数据发送给目标节点。然后同步等待目标节点的响应结果。​目标节点对应的client，收到指令后，如果有 select 指令，就首先切换到正确的 DB。接下来读取并处理 resotre-asking 指令，处理 restore-asking 指令时，首先对收到的数据进行解析校验，获取 key 的 ttl，校验 rdb 版本及 value 数据 cc64 校验码，确认无误后，将数据存入 redisDb，设置过期时间，并返回响应。源节点收到目标节点处理成功的响应后。对于非 copy 类型的 migrate，会删除已迁移的 key。至此，key 的迁移就完成了。migrate 迁移指令，可以一次迁移一个或多个 key。注意，整个迁移过程中，源节点在发送 restore-asking 指令后，同步阻塞，等待目标节点完成数据处理，直到超时或者目标节点返回响应结果，收到结果后在本地处理完毕后序事件，才会停止阻塞，才能继续处理其他事件。所以，单次迁移的 key 不能太多，否则阻塞时间会较长，导致 Redis 卡顿。同时，即便单次只迁移一个 key，如果对应的 value 太大，也可能导致 Redis 短暂卡顿。​在 slot 迁移过程中，不仅其他非迁移 slot 的 key 可以正常访问，即便正在迁移的 slot，它里面的 key 也可以正常读写，不影响业务访问。但由于 key 的迁移是阻塞模式，即在迁移 key 的过程中，源节点并不会处理任何请求，所以在 slot 迁移过程中，待读写的 key 只有三种存在状态： 尚未被迁移，后续会被迁走； 已经被迁移； 这个 key 之前并不存在集群中，是一个新 key。slot 迁移过程中，对节点里的 key 处理方式如下： 对于尚未被迁移的 key，即从 DB 中能找到该 key，不管这个 key 所属的 slot 是否正在被迁移，都直接在本地进行读写处理； 对于无法从 DB 中找到 value 的 key，但key所属slot正在被迁移，包括已迁走或者本来不存在的 key 两种状态，Redis 返回 ask 错误响应，并附带 slot 迁移目标节点的 host 和 port。Client 收到 ask 响应后，将请求重定向到 slot 迁移的新节点，完成响应处理； 对于无法从 DB 中找到 value 的 key，且 key 所在的 slot 不属于本节点，说明 Client 发送节点有误，直接返回 moved 错误响应，也附带上 key 对应节点的 host 和 port，由 Client 重定向请求； 对于 Redis Cluster 集群方案，由社区官方实现，并有 Redis-trib 集群工具，上线和使用起来比较便捷。同时它支持在线扩缩，可以随时通过工具查看集群的状态。但这种方案也存在不少弊端。首先，数据存储和集群逻辑耦合，代码逻辑复杂，容易出错。其次，Redis 节点要存储 slot 和 key 的映射关系，需要额外占用较多内存，特别是对 value size 比较小、而key相对较大的业务，影响更是明显。再次，key 迁移过程是阻塞模式，迁移大 value 会导致服务卡顿。而且，迁移过程，先获取 key，再迁移，效率低。最后，Cluster 模式下，集群复制的 slave 只能挂载到 master，不支持 slave 嵌套，会导致 master 的压力过大，无法支持那些，需要特别多 slave、读 TPS 特别大的业务场景。" }, { "title": "Redis 主从复制", "url": "/posts/cache-redis-12/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2021-07-03 13:33:22 +0000", "snippet": "Redis 复制原理为了避免单点故障，数据存储需要进行多副本构建。同时由于 Redis 的核心操作是单线程模型的，单个 Redis 实例能处理的请求 TPS 有限。因此 Redis 自面世起，基本就提供了复制功能，而且对复制策略不断进行优化。通过数据复制，Redis 的一个 master 可以挂载多个 slave，而 slave 下还可以挂载多个 slave，形成多层嵌套结构。所有写操作都在 master 实例中进行，master 执行完毕后，将写指令分发给挂在自己下面的 slave 节点。slave 节点下如果有嵌套的 slave，会将收到的写指令进一步分发给挂在自己下面的 slave。通过多个 slave，Redis 的节点数据就可以实现多副本保存，任何一个节点异常都不会导致数据丢失，同时多 slave 可以 N 倍提升读性能。master 只写不读，这样整个 master-slave 组合，读写能力都可以得到大幅提升。master 在分发写请求时，同时会将写指令复制一份存入复制积压缓冲，这样当 slave 短时间断开重连时，只要 slave 的复制位置点仍然在复制积压缓冲，则可以从之前的复制位置点之后继续进行复制，提升复制效率。主库 master 和从库 slave 之间通过复制 id 进行匹配，避免 slave 挂到错误的 master。Redis 的复制分为全量同步和增量同步。Redis 在进行全量同步时，master 会将内存数据通过 bgsave 落地到 rdb，同时，将构建 内存快照期间 的写指令，存放到复制缓冲中，当 rdb 快照构建完毕后，master 将 rdb 和复制缓冲队列中的数据全部发送给 slave，slave 完全重新创建一份数据。这个过程，对 master 的性能损耗较大，slave 构建数据的时间也比较长，而且传递 rdb 时还会占用大量带宽，对整个系统的性能和资源的访问影响都比较大。而增量复制，master 只发送 slave 上次复制位置之后的写指令，不用构建 rdb，而且传输内容非常有限，对 master、slave 的负荷影响很小，对带宽的影响可以忽略，整个系统受影响非常小。在 Redis 2.8 之前，Redis 基本只支持全量复制。在 slave 与 master 断开连接，或 slave 重启后，都需要进行全量复制。在 2.8 版本之后，Redis 引入 psync，增加了一个复制积压缓冲，在将写指令同步给 slave 时，会同时在复制积压缓冲中也写一份。在 slave 短时断开重连后，上报master runid 及复制偏移量。如果 runid 与 master 一致，且偏移量仍然在 master 的复制缓冲积压中，则 master 进行增量同步。但如果 slave 重启后，master runid 会丢失，或者切换 master 后，runid 会变化，仍然需要全量同步。因此 Redis 自 4.0 强化了 psync，引入了 psync2。在 pysnc2 中，主从复制不再使用 runid，而使用 replid（即复制id） 来作为复制判断依据。同时 Redis 实例在构建 rdb 时，会将 replid 作为 aux 辅助信息存入 rbd。重启时，加载 rdb 时即可得到 master 的复制 id。从而在 slave 重启后仍然可以增量同步。在 psync2 中，Redis 每个实例除了会有一个复制 id 即 replid 外，还有一个 replid2。Redis 启动后，会创建一个长度为 40 的随机字符串，作为 replid 的初值，在建立主从连接后，会用 master的 replid 替换自己的 replid。同时会用 replid2 存储上次 master 主库的 replid。这样切主时，即便 slave 汇报的复制 id 与新 master 的 replid 不同，但和新 master 的 replid2 相同，同时复制偏移仍然在复制积压缓冲区内，仍然可以实现增量复制。Redis 复制分析在设置 master、slave 时，首先通过配置或者命令 slaveof no one 将节点设置为主库。然后其他各个从库节点，通过 slaveof $master_ip $master_port，将其他从库挂在到 master 上。同样方法，还可以将 slave 节点挂载到已有的 slave 节点上。在准备开始数据复制时，slave 首先会主动与 master 创建连接，并上报信息。具体流程如下。slave 创建与 master 的连接后，首先发送 ping 指令，如果 master 没有返回异常，而是返回 pong，则说明 master 可用。如果 Redis 设置了密码，slave 会发送 auth $masterauth 指令，进行鉴权。当鉴权完毕，从库就通过 replconf 发送自己的端口及 IP 给 master。接下来，slave 继续通过 replconf 发送 capa eof capa psync2 进行复制版本校验。如果 master 校验成功。从库接下来就通过 psync 将自己的复制 id、复制偏移发送给 master，正式开始准备数据同步。主库接收到从库发来的 psync 指令后，则开始判断可以进行数据同步的方式。前面讲到，Redis 当前保存了复制 id，replid 和 replid2。如果从库发来的复制 id，与 master 的复制 id（即 replid 和 replid2）相同，并且复制偏移在复制缓冲积压中，则可以进行增量同步。master 发送 continue 响应，并返回 master 的 replid。slave 将 master 的 replid 替换为自己的 replid，并将之前的复制 id 设置为 replid2。之后，master 则可继续发送，复制偏移位置 之后的指令，给 slave，完成数据同步。如果主库发现从库传来的复制 id 和自己的 replid、replid2 都不同，或者复制偏移不在复制积压缓冲中，则判定需要进行全量复制。master 发送 fullresync 响应，附带 replid 及复制偏移。然后， master 根据需要构建 rdb，并将 rdb 及复制缓冲发送给 slave。对于增量复制，slave 接下来就等待接受 master 传来的复制缓冲及新增的写指令，进行数据同步。而对于全量同步，slave 会首先进行，嵌套复制的清理工作，比如 slave 当前还有嵌套的 子slave，则该 slave 会关闭嵌套 子slave 的所有连接，并清理自己的复制积压缓冲。然后，slave 会构建临时 rdb 文件，并从 master 连接中读取 rdb 的实际数据，写入 rdb 中。在写 rdb 文件时，每写 8M，就会做一个 fsync操作， 刷新文件缓冲。当接受 rdb 完毕则将 rdb 临时文件改名为 rdb 的真正名字。接下来，slave 会首先清空老数据，即删除本地所有 DB 中的数据，并暂时停止从 master 继续接受数据。然后，slave 就开始全力加载 rdb 恢复数据，将数据从 rdb 加载到内存。在 rdb 加载完毕后，slave 重新利用与 master 的连接 socket，创建与 master 连接的 client，并在此注册读事件，可以开始接受 master 的写指令了。此时，slave 还会将 master 的 replid 和复制偏移设为自己的复制 id 和复制偏移 offset，并将自己的 replid2 清空，因为，slave 的所有嵌套 子slave 接下来也需要进行全量复制。最后，slave 就会打开 aof 文件，在接受 master 的写指令后，执行完毕并写入到自己的 aof 中。相比之前的 sync，psync2 优化很明显。在短时间断开连接、slave 重启、切主等多种场景，只要延迟不太久，复制偏移仍然在复制积压缓冲，均可进行增量同步。master 不用构建并发送巨大的 rdb，可以大大减轻 master 的负荷和网络带宽的开销。同时，slave 可以通过轻量的增量复制，实现数据同步，快速恢复服务，减少系统抖动。但是，psync 依然严重依赖于复制缓冲积压，太大会占用过多内存，太小会导致频繁的全量复制。而且，由于内存限制，即便设置相对较大的复制缓冲区，在 slave 断开连接较久时，仍然很容易被复制缓冲积压冲刷，从而导致全量复制。" }, { "title": "Redis崩溃后，如何进行数据恢复的", "url": "/posts/cache-redis-09/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2021-07-01 16:33:22 +0000", "snippet": "通过 RDB、AOF、混合存储等数据持久化方案解决如何进行数据恢复的问题。Redis 持久化是一个将内存数据转储到磁盘的过程。Redis 目前支持 RDB、AOF，以及混合存储三种模式。RDBRedis 的 RDB 持久化是以快照的方式将内存数据存储到磁盘。在需要进行 RDB 持久化时，Redis 会将内存中的所有数据以二进制的格式落地，每条数据存储的内容包括过期时间、数据类型、key，以及 value。当 Redis 重启时，如果 appendonly 关闭，则会读取 RDB 持久化生成的二进制文件进行数据恢复。触发构建 RDB 的场景主要有以下四种： 第一种场景是通过 save 或 bgsave 命令进行主动 RDB 快照构建。它是由调用方调用 save 或 bgsave 指令进行触发的； 第二种场景是利用配置 save m n 来进行自动快照生成。它是指在 m 秒中，如果插入或变更 n 个 key，则自动触发 bgsave。这个配置可以设置多个配置行，以便组合使用。由于峰值期间，Redis 的压力大，变更的 key 也比较多，如果再进行构建 RDB 的操作，会进一步增加机器负担，对调用方请求会有一定的影响，所以线上使用时需要谨慎； 第三种场景是主从复制，如果从库需要进行全量复制，此时主库也会进行 bgsave 生成一个 RDB 快照； 第四种场景是在运维执行 flushall 清空所有数据，或执行 shutdown 关闭服务时，也会触发 Redis 自动构建 RDB 快照。 save 是在主进程中进行 RDB 持久化的，持久化期间 Redis 处于阻塞状态，不处理任何客户请求，所以一般使用较少。而 bgsave 是 fork 一个子进程，然后在子进程中构建 RDB 快照，构建快照的过程不直接影响用户的访问，但仍然会增加机器负载。线上 Redis 快照备份，一般会选择凌晨低峰时段，通过 bgsave 主动触发进行备份。RDB 快照文件主要由 3 部分组成： 第一部分是 RDB 头部，主要包括 RDB 的版本，以及 Redis 版本、创建日期、占用内存等辅助信息； 第二部分是各个 RedisDB 的数据。存储每个 RedisDB 时，会首先记录当前 RedisDB 的DBID，然后记录主 dict 和 expire dict 的记录数量，最后再轮询存储每条数据记录。存储数据记录时，如果数据有过期时间，首先记录过期时间。如果 Redis 的 maxmemory_policy 过期策略采用 LRU 或者 LFU，还会将 key 对应的 LRU、LFU 值进行落地，最后记录数据的类型、key，以及 value。 第三部部分是 RDB 的尾部。RDB 尾部，首先存储 Redis 中的 Lua 脚本等辅助信息。然后存储 EOF 标记，即值为 255 的字符。最后存 RDB 的 cksum。RDB 采用二进制方式存储内存数据，文件小，且启动时恢复速度快。但构建 RDB 时，一个快照文件只能存储，构建时刻的内存数据，无法记录之后的数据变更。构建 RDB 的过程，即便在子进程中进行，但仍然属于 CPU 密集型的操作，而且每次落地全量数据，耗时也比较长，不能随时进行，特别是不能在高峰期进行。由于 RDB 采用二进制存储，可读性差，而且由于格式固定，不同版本之间可能存在兼容性问题。AOFRedis 的 AOF 持久化是以命令追加的方式进行数据落地的。通过打开 appendonly 配置，Redis 将每一个写指令追加到磁盘 AOF 文件，从而及时记录内存数据的最新状态。这样即便 Redis 被 crash 或异常关闭后，再次启动，也可以通过加载 AOF，来恢复最新的全量数据，基本不会丢失数据。AOF 文件中存储的协议是写指令的 multibulk 格式，这是 Redis 的标准协议格式，所以不同的 Redis 版本均可解析并处理，兼容性很好。但是，由于 Redis 会记录所有写指令操作到 AOF，大量的中间状态数据，甚至被删除的过期数据，都会存在 AOF 中，冗余度很大，而且每条指令还需通过加载和执行来进行数据恢复，耗时会比较大。AOF 数据的落地流程如下。Redis 在处理完写指令后，首先将写指令写入 AOF 缓冲，然后通过 server_cron 定期将 AOF 缓冲写入文件缓冲。最后按照配置策略进行 fsync，将文件缓冲的数据真正同步写入磁盘。Redis 通过 appendfsync 来设置三种不同的同步文件缓冲策略： 第一种配置策略是 no，即 Redis 不主动使用 fsync 进行文件数据同步落地，而是由操作系统的 write 函数去确认同步时间，在 Linux 系统中大概每 30 秒会进行一次同步，如果 Redis 发生 crash，就会造成大量的数据丢失； 第二种配置策略是 always，即每次将 AOF 缓冲写入文件，都会调用 fsync 强制将内核数据写入文件，安全性最高，但性能上会比较低效，而且由于频繁的 IO 读写，磁盘的寿命会大大降低； 第三种配置策略是 everysec。即每秒通过 BIO 线程进行一次 fsync。这种策略在安全性、性能，以及磁盘寿命之间做较好的权衡，可以较好的满足线上业务需要。 随着时间的推移，AOF 持续记录所有的写指令，AOF 会越来越大，而且会充斥大量的中间数据、过期数据，为了减少无效数据，提升恢复时间，可以定期对 AOF 进行 rewrite 操作。AOF 的 rewrite 操作可以通过运维执行 bgrewiretaof 命令来进行，也可以通过配置重写策略进行，由 Redis 自动触发进行。当对 AOF 进行 rewrite 时，首先会 fork 一个子进程。子进程轮询所有 RedisDB 快照，将所有内存数据转为 cmd，并写入临时文件。在子进程 rewriteaof 时，主进程可以继续执行用户请求，执行完毕后将写指令写入旧的 AOF 文件和 rewrite 缓冲。子进程将 RedisDB 中数据落地完毕后，通知主进程。主进程从而将 AOF rewite 缓冲数据写入 AOF 临时文件，然后用新的 AOF 文件替换旧的 AOF 文件，最后通过 BIO 线程异步关闭旧的 AOF 文件。至此，AOF 的 rewrite 过程就全部完成了。AOF 重写的过程，是一个轮询全部 RedisDB 快照，逐一落地的过程。每个 DB，首先通过 select $db 来记录待落的 DBID。然后通过命令记录每个 key/value。对于数据类型为 SDS 的value，可以直接落地。但如果 value 是聚合类型，则会将所有元素设为批量添加指令，进行落地。对于 list 列表类型，通过 RPUSH 指令落地所有列表元素。对于 set 集合，会用 SADD 落地所有集合元素。对于 Zset 有序集合，会用 Zadd 落地所有元素，而对于 Hash 会用 Hmset 落地所有哈希元素。如果数据带过期时间，还会通过 pexpireat 来记录数据的过期时间。AOF 持久化的优势是可以记录全部的最新内存数据，最多也就是 1-2 秒的数据丢失。同时 AOF 通过 Redis 协议来追加记录数据，兼容性高，而且可以持续轻量级的保存最新数据。最后因为是直接通过 Redis 协议存储，可读性也比较好。AOF 持久化的不足是随着时间的增加，冗余数据增多，文件会持续变大，而且数据恢复需要读取所有命令并执行，恢复速度相对较慢。混合持久化Redis 在 4.0 版本之后，引入了混合持久化方式，而且在 5.0 版本后默认开启。前面讲到 RDB 加载速度快，但构建慢，缺少最新数据。AOF 持续追加最新写记录，可以包含所有数据，但冗余大，加载速度慢。混合模式一体化使用 RDB 和 AOF，综合 RDB 和 AOF 的好处。即可包含全量数据，加载速度也比较快。可以使用 aof-use-rdb-preamble 配置来明确打开混合持久化模式。混合持久化也是通过 bgrewriteaof 来实现的。当启用混合存储后，进行 bgrewriteaof 时，主进程首先依然是 fork 一个子进程，子进程首先将内存数据以 RDB 的二进制格式写入 AOF 临时文件中。然后，再将落地期间缓冲的新增写指令，以命令的方式追加到临时文件。然后再通知主进程落地完毕。主进程将临时文件修改为 AOF 文件，并关闭旧的 AOF 文件。这样主体数据以 RDB 格式存储，新增指令以命令方式追加的混合存储方式进行持久化。后续执行的任务，以正常的命令方式追加到新的 AOF 文件即可。混合持久化综合了 RDB 和 AOF 的优缺点，优势是包含全量数据，加载速度快。不足是头部的 RDB 格式兼容性和可读性较差。" }, { "title": "Redis 读取请求数据后的协议解析和处理", "url": "/posts/cache-redis-06/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2021-06-20 11:33:22 +0000", "snippet": "Redis 协议解析及处理协议解析请求命令进入，触发 IO 读事件后。client 会从连接文件描述符读取请求，并存入 client 的 query buffer 中。client 的读缓冲默认是 16KB，读取命令时，如果发现请求超过 1GB，则直接报异常，关闭连接。client 读取完请求命令后，则根据 query buff 进行协议解析。协议解析时，首先查看协议的首字符。如果是 *，则解析为字符块数组类型，即 MULTIBULK。否则请求解析为 INLINE 类型。INLINE 类型是以 CRLF 结尾的单行字符串，协议命令及参数以空格分隔。解析过程参考之前课程里分析的对应协议格式。协议解析完毕后，将请求参数个数存入 client 的 argc 中，将请求的具体参数存入 client 的 argv 中。协议执行请求命令解析完毕，则进入到协议执行部分。协议执行中，对于 quit 指令，直接返回 OK，设置 flag 为回复后关闭连接。对于非 quit 指令，以 client 中 argv[0] 作为命令，从 server 中的命令表中找到对应的 redisCommand。如果没有找到 redisCommand，则返回未知 cmd 异常。如果找到 cmd，则开始执行 redisCommand 中的 proc 函数，进行具体命令的执行。在命令执行完毕后，将响应写入 client 的写缓冲。并按配置和部署，将写指令分发给 aof 和 slaves。同时更新相关的统计数值。" }, { "title": "Redis 对文件事件和时间事件的处理", "url": "/posts/cache-redis-05/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2021-06-15 14:33:22 +0000", "snippet": "Redis 事件驱动模型Redis 是一个事件驱动程序，与 Memcached 不同的是，Redis 并没有采用 libevent 或 libev 这些开源库，而是直接开发了一个新的事件循环组件。Redis 作者给出的理由是，尽量减少外部依赖，自己开发的事件模型也足够简洁、轻便、高效，也更易控制。Redis 的事件驱动模型机制封装在 aeEventLoop 等相关的结构体中，网络连接、命令读取执行回复，数据的持久化、淘汰回收 key 等，几乎所有的核心操作都通过 ae 事件模型进行处理。Redis 的事件驱动模型处理 2 类事件： 文件事件，如连接建立、接受请求命令、发送响应等； 时间事件，如 Redis 中定期要执行的统计、key 淘汰、缓冲数据写出、rehash等。文件事件Redis 的文件事件采用典型的 Reactor 模式进行处理。Redis 文件事件处理机制分为 4 部分： 连接 socket IO 多路复用程序 文件事件分派器 事件处理器连接 Socket文件事件是对连接 socket 操作的一个抽象。当端口监听 socket 准备 accept 新连接，或者连接 socket 准备好读取请求、写入响应、关闭时，就会产生一个文件事件。IO多路复用IO 多路复用程序负责同时监听多个 socket，当这些 socket 产生文件事件时，就会触发事件通知，文件分派器就会感知并获取到这些事件。虽然多个文件事件可能会并发出现，但 IO 多路复用程序总会将所有产生事件的 socket 放入一个队列中，通过这个队列，有序的把这些文件事件通知给文件分派器。Redis 封装了 4 种多路复用程序，每种封装实现都提供了相同的 API 实现。编译时，会按照性能和系统平台，选择最佳的 IO 多路复用函数作为底层实现，选择顺序是，首先尝试选择 Solaries 中的 evport，如果没有，就尝试选择 Linux 中的 epoll，否则就选择大多 UNIX 系统都支持的 kqueue，这 3 个多路复用函数都直接使用系统内核内部的结构，可以服务数十万的文件描述符。如果当前编译环境没有上述函数，就会选择 select 作为底层实现方案。select 方案的性能较差，事件发生时，会扫描全部监听的描述符，事件复杂度是 O(n)，并且只能同时服务有限个文件描述符，32 位机默认是 1024 个，64 位机默认是 2048 个，所以一般情况下，并不会选择 select 作为线上运行方案。Redis 的这 4 种实现，分别在 ae_evport、ae_epoll、ae_kqueue 和 ae_select 这 4 个代码文件中。文件事件收集及派发器Redis 中的文件事件分派器是 aeProcessEvents 函数。它会首先计算最大可以等待的时间，然后利用 aeApiPoll 等待文件事件的发生。如果在等待时间内，一旦 IO 多路复用程序产生了事件通知，则会立即轮询所有已产生的文件事件，并将文件事件放入 aeEventLoop 中的 aeFiredEvents 结构数组中。每个 fired event 会记录 socket 及 Redis 读写事件类型。这里会涉及将多路复用中的事件类型，转换为 Redis 的 ae 事件驱动模型中的事件类型。以采用 Linux 中的 epoll 为例，会将 epoll 中的 EPOLLIN 转为 AE_READABLE 类型，将 epoll 中的 EPOLLOUT、EPOLLERR 和 EPOLLHUP 转为 AE_WRITABLE 事件。aeProcessEvents 在获取到触发的事件后，会根据事件类型，将文件事件 dispatch 派发给对应事件处理函数。如果同一个 socket，同时有读事件和写事件，Redis 派发器会首先派发处理读事件，然后再派发处理写事件。文件事件处理函数分类Redis 中文件事件函数的注册和处理主要分为 3 种： 连接处理函数 acceptTcpHandler Redis 在启动时，在 initServer 中对监听的 socket 注册读事件，事件处理器为 acceptTcpHandler，该函数在有新连接进入时，会被派发器派发读任务。在处理该读任务时，会 accept 新连接，获取调用方的 IP 及端口，并对新连接创建一个 client 结构。如果同时有大量连接同时进入，Redis 一次最多处理 1000 个连接请求。 readQueryFromClient 请求处理函数 连接函数在创建 client 时，会对新连接 socket 注册一个读事件，该读事件的事件处理器就是 readQueryFromClient。在连接 socket 有请求命令到达时，IO 多路复用程序会获取并触发文件事件，然后这个读事件被派发器派发给本请求的处理函数。readQueryFromClient 会从连接 socket 读取数据，存入 client 的 query 缓冲，然后进行解析命令，按照 Redis 当前支持的 2 种请求格式，及 inline 内联格式和 multibulk 字符块数组格式进行尝试解析。解析完毕后，client 会根据请求命令从命令表中获取到对应的 redisCommand，如果对应 cmd 存在。则开始校验请求的参数，以及当前 server 的内存、磁盘及其他状态，完成校验后，然后真正开始执行 redisCommand 的处理函数，进行具体命令的执行，最后将执行结果作为响应写入 client 的写缓冲中。 命令回复处理器 sendReplyToClient 当 redis需要发送响应给client时，Redis 事件循环中会对client的连接socket注册写事件，这个写事件的处理函数就是sendReplyToClient。通过注册写事件，将 client 的socket与 AE_WRITABLE 进行间接关联。当 Client fd 可进行写操作时，就会触发写事件，该函数就会将写缓冲中的数据发送给调用方。 时间事件Redis 中的时间事件是指需要在特定时间执行的事件。多个 Redis 中的时间事件构成 aeEventLoop 中的一个链表，供 Redis 在 ae 事件循环中轮询执行。Redis 当前的主要时间事件处理函数有 2 个： serverCron moduleTimerHandlerRedis 中的时间事件分为 2 类： 单次时间，即执行完毕后，该时间事件就结束了； 周期性事件，在事件执行完毕后，会继续设置下一次执行的事件，从而在时间到达后继续执行，并不断重复。时间事件主要有 5 个属性组成： 事件 ID：Redis 为时间事件创建全局唯一 ID，该 ID 按从小到大的顺序进行递增； 执行时间 when_sec 和 when_ms：精确到毫秒，记录该事件的到达可执行时间； 时间事件处理器 timeProc：在时间事件到达时，Redis 会调用相应的 timeProc 处理事件； 关联数据 clientData：在调用 timeProc 时，需要使用该关联数据作为参数； 链表指针 prev 和 next：它用来将时间事件维护为双向链表，便于插入及查找所要执行的时间事件。时间事件的处理是在事件循环中的 aeProcessEvents 中进行。执行过程是： 首先遍历所有的时间事件； 比较事件的时间和当前时间，找出可执行的时间事件； 然后执行时间事件的 timeProc 函数； 执行完毕后，对于周期性时间，设置时间新的执行时间；对于单次性时间，设置事件的 ID为 -1，后续在事件循环中，下一次执行 aeProcessEvents 的时候从链表中删除。" }, { "title": "Redis 系统架构", "url": "/posts/cache-redis-04/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2021-06-10 13:33:22 +0000", "snippet": "Redis 组件的系统架构如下图所示：主要包括： 事件处理； 数据存储、管理； 用于系统扩展的主从复制/集群管理； 插件化功能扩展的 Module System 模块。事件处理机制采用的是作者自己开发的 ae 事件驱动模型，可以进行==高效的网络 IO 读写、命令执行，以及时间事件处理；==网络 IO 读写和命令执行网络 IO 读写处理采用的是 IO 多路复用技术： 通过对 evport、epoll、kqueue、select 等进行封装，同时监听多个 socket； 根据 socket 目前执行的任务，来为 socket 关联不同的事件处理器； 当监听端口对应的 socket 收到连接请求后，创建一个 client 结构，通过 client 结构来对连接状态进行管理。在请求进入时，将请求命令读取缓冲并进行解析，并存入到 client 的参数列表； 根据请求命令找到对应 redisCommand ，最后根据命令协议，对请求参数进一步的解析、校验并执行。时间事件时间事件比较简单，目前主要是执行 serverCron，来做一些统计更新、过期 key 清理、AOF 及 RDB 持久化等辅助操作。数据存储和管理Redis 的内存数据都存在 redisDB 中。Redis 支持多 DB，每个 DB 都对应一个 redisDB 结构。Redis 的 8 种数据类型，每种数据类型都采用一种或多种内部数据结构进行存储。同时这些内部数据结构及数据相关的辅助信息，都以 kye/value 的格式存在 redisDB 中各个 dict 字典中。数据在写入 redisDB 后，这些执行的写指令会及时追加到 AOF 中，追加的方式是： 先实时写入AOF 缓冲； 然后按策略刷缓冲数据到文件。由于 AOF 记录每个写操作，所以一个 key 的大量中间状态也会呈现在 AOF 中，导致 AOF 冗余信息过多，因此 Redis 还设计了一个 RDB 快照操作，可以通过定期将内存里所有的数据快照落地到 RDB 文件，来以最简洁的方式记录 Redis 的所有内存数据。Redis 进行数据读写的核心处理线程是单线程模型，为了保持整个系统的高性能，必须避免任何kennel 导致阻塞的操作。为此，Redis 增加了 BIO 线程，来处理容易导致阻塞的文件 close、fsync 等操作，确保系统处理的性能和稳定性。在 server 端，存储内存永远是昂贵且短缺的，Redis 中，过期的 key 需要及时清理，不活跃的 key 在内存不足时也可能需要进行淘汰。为此，Redis 设计了 8 种淘汰策略，借助新引入的 eviction pool，进行高效的 key 淘汰和内存回收。功能扩展Redis 在 4.0 版本之后引入了 Module System 模块，可以方便使用者，在不修改核心功能的同时，进行插件化功能开发。使用者可以将新的 feature 封装成动态链接库，Redis 可以在启动时加载，也可以在运行过程中随时按需加载和启用。在扩展模块中： 通过 RedisModule_init 初始化新模块； 用 RedisModule_CreateCommand 扩展各种新模块指令； 以可插拔的方式为 Redis 引入新的数据结构和访问命令。系统扩展Redis 作者在架构设计中对系统扩展倾注了大量关注。主从复制在主从复制功能中，psyn 在不断的优化，不仅在 slave 闪断重连后可以进行增量复制，而且在 slave 通过主从切换成为 master 后，其他 slave 仍然可以与新晋升的 master 进行增量复制。另外，其他一些场景，如 slave 重启后，也可以进行增量复制，大大提升了主从复制的可用性。使用者可以更方便的使用主从复制，进行业务数据的读写分离，大幅提升 Redis 系统的稳定读写能力。通过主从复制可以较好的解决 Redis 的单机读写问题，但所有写操作都集中在 master 服务器，很容易达到 Redis 的写上限，同时 Redis 的主从节点都保存了业务的所有数据，随着业务发展，很容易出现内存不够用的问题。Reids 分区基于上文，Redis 分区无法避免。虽然业界大多采用在 client 和 proxy 端分区，但 Redis 自己也早早推出了 cluster 功能，并不断进行优化。Redis cluster 预先设定了 16384 个 slot 槽，在 Redis 集群启动时，通过手动或自动将这些 slot 分配到不同服务节点上。在进行 key 读写定位时： 首先对 key 做 hash，并将 hash 值对 16383 ，做 按位与运算，确认 slot； 然后确认服务节点； 最后再对 对应的 Redis 节点，进行常规读写。 如果 client 发送到错误的 Redis 分片，Redis 会发送重定向回复。如果业务数据大量增加，Redis 集群可以通过数据迁移，来进行在线扩容。 " }, { "title": "使用支付功能演练 DDD", "url": "/posts/DDD-payment/", "categories": "Architecture Design, DDD", "tags": "Architecture Design, DDD", "date": "2021-06-01 15:33:00 +0000", "snippet": "上一篇，总结了软件退化的根源，以及如何利用 DDD 解决软件退化的问题。本文结合工作中的经验，以支付功能为例，演练一下基于 DDD 的软件设计以及变更的过程。当最开始收到关于用户付款功能的需求描述如图 1 所示：以往当拿到这个需求的时候，就会草草的设计之后就开始编码，设计质量也就不怎么高。正确的做法应该是在拿到新需求以后，采用领域驱动的方式，先进行需求分析，设计出领域模型。按照图 1 所示的业务场景，可以分析出： 该场景中有 “订单”，每个订单都对应一个用户； 一个用户可以有多个用户地址，但每个订单只能有一个用户地址； 此外，一个订单对应多个订单明细，每个订单明细对应一个商品，每个商品对应一个供应商。最后，对订单进行的“下单”、“付款”、“查看订单状态”等操作，最终形成了以下领域模型图，如图 2 所示：有了图 2 所示的领域模型就可以进行以下的程序设计，如图 3 所示： 通过领域模型的指导： 将 “订单”分为订单 Service 与 值对象； 将“用户”分为用户 Service 与值对象； 将“商品”分为商品 Service 与 值对象。在此基础上实现各自的方法。商品折扣的需求变更当付款功能按照领域模型完成了第一个版本的设计后，很快就迎来了第一次需求变更，即增加折扣功能，并且该折扣功能分为限时折扣、限量折扣、某类商品的折扣、某个商品的折扣与不折扣。当拿到这个需求时应当怎样设计呢？很显然，在 payoff() 方法中去插入 if 语句是不 OK 的。此时，按照领域驱动设计的思想，应当将需求变更还原到领域模型中进行分析，进而根据领域模型背后的真实世界进行变更。这是上一个版本的领域模型，现在要在这个模型的基础上增加折扣功能，并且还要分为限时折扣、限量折扣、某类商品的折扣等不同类型。此时，首先分析付款与则扣的关系。付款与折扣是什么关系呢？显然折扣是在付款的过程中进行的折扣，就认为应当将折扣写到付款中。这样对吗？显然不对，此时就因该基于一个重量级的设计原则 —— 单一职责原则。什么是高质量的代码。可能立即会想到“低耦合、高内聚”，以及各种设计原则，但这些评价标准都太“虚”。最直接、最落地的评价标准就是，当用户提出一个需求变更时，为了实现这个变更而修改软件的成本越低，那么软件的设计质量就越高。怎样才能在每次变更的时候都只修改一个模块就能实现新需求呢？这就需要在平时就不断地整理代码，将那些因同一个原因而变更的代码都放在一起，而将因不同原因而变更的代码分开放在不同的模块、不同的类中。这样，当因为这个原因而需要修改代码时，需要修改的代码都在这个模块、这个类中，修改范围就缩小了，维护成本降低了，自然设计质量就提高了。总之，单一职责原则要求在维护软件的过程中需要不断地进行整理，将软件变化同一个原因的代码放在一起，将软件变化不同原因的代码分开放照这样的设计原则，分析“付款”与“折扣”之间的关系呢只需要回答以下两个问题： 当“付款”发生变更时，“折扣”是不是一定要变？ 当“折扣”发生变更时，“付款”是不是一定要变？当这两个问题的答案是否定时，就说明“付款”与“折扣”是软件变化的两个不同的原因，那么把它们放在一起，放在同一个类、同一个方法中，合适吗？不合适，就应当将“折扣”从“付款”中提取出来，单独放在一个类中。同样的道理，举一反三。最后发现，不同类型的折扣也是软件变化不同的原因。将它们放在同一个类、同一个方法中，合适吗？通过以上分析，做出了如下设计：VIP 会员的需求变更在第一次变更的基础上，很快迎来了第二次变更，这次是要增加 VIP 会员，业务需求如下。 对不同类型的 VIP 会员（金卡会员、银卡会员）进行不同的折扣； 在支付时，为 VIP 会员发放福利（积分、返券等）； VIP 会员可以享受某些特权。拿到这样的需求又应当怎样设计呢？同样，先回到领域模型，分析三个领域对象之间的关系 分析“用户”与“VIP 会员”的关系，“ 付款”与“VIP 会员”的关系。在分析的时候，还是回答那两个问题： “用户”发生变更时，“VIP 会员”是否要变？ “VIP 会员”发生变更时，“用户”是否要变？通过分析发现，“用户”与“VIP 会员”是两个完全不同的事物。 “用户”要做的是用户的注册、变更、注销等操作； “VIP 会员”要做的是会员折扣、会员福利与会员特权；而“付款”与“VIP 会员”的关系是在付款的过程中去调用会员折扣、会员福利与会员特权。过以上的分析，做出了以下版本的领域模型：有了这些领域模型的变更，然后就可以以此作为基础，指导后面程序代码的变更了。支付方式的需求变更同样，第三次变更是增加更多的支付方式，我们在领域模型中分析“付款”与“支付方式”之间的关系，发现它们也是软件变化不同的原因。因此，果断做出了这样的设计：而在设计实现时，因为要与各个第三方的支付系统对接，也就是要与外部系统对接。为了使第三方的外部系统的变更对我们的影响最小化，在它们中间果断加入了“适配器模式”，设计如下：通过加入适配器模式，订单 Service 在进行支付时调用的不再是外部的支付接口，而是“支付方式”接口，与外部系统解耦。只要保证“支付方式”接口是稳定的，那么订单 Service 就是稳定的。比如： 当支付宝支付接口发生变更时，影响的只限于支付宝 Adapter； 当微信支付接口发生变更时，影响的只限于微信支付 Adapter； 当要增加一个新的支付方式时，只需要再写一个新的 Adapter。日后不论哪种变更，要修改的代码范围缩小了，维护成本自然降低了，代码质量就提高了。" }, { "title": "重构：烟囱式、平台化、中台化的架构同与异", "url": "/posts/reconstruce-2-23/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-05-07 05:33:00 +0000", "snippet": "系统稳定性的重构升级简单、通用的微服务架构如下图 1 所示，它包含一个应用服务和一个数据库作为存储。当上图中展示的架构出现如下一些问题时，可以采用上文中提到的“微服务中纯代码维度的升级重构”： 代码日志打印冗余，且因为直接使用大对象进行 JSON 序列化的方式打印日志，进而导致常常出现 CPU 飙升； 代码中未增加监控报警，导致用户先感知线上问题，研发再进行修复； 代码可维护性低，开发需求耗时长，且开发时代码“牵一发而动全身”，产生的 Bug 较多。具体的一些表现是： 一个类中有上千或者上万行代码； 一个类中的某一个方法有上百或者上千行代码； 代码中没有注释； 为防止影响历史业务，新需求开发均需将原有部分能复用的代码拷贝后，才能修改。 此时，可以将出现上述问题的微服务进行代码重构。具体来说，可以采用 23 种设计模式、SOLID 原则将包含上千上万行代码的类进行重构。此时，重构后的微服务的上线即可对应上一讲里提到的“第一种重构类型：纯代码重构”。它的架构如下图 2 所示： 完成纯代码的重构后，在日常维护中，当发现有以下问题时，可以考虑进行包含存储的重构。 随着业务的发展，微服务的流量由每秒几百的 QPS 上升至上千或上万的 QPS 时，可以将微服务的存储从数据库升级为缓存，以便有效应用业务增长带来的流量； 业务或者运营需要对数据库中的四五张表进行聚合（join）分页查询，而数据库面对这些繁杂查询性能会变得非常差。此时可以将微服务的存储从数据库升级为 ElasticSearch，进而满足多维度的富查询。上述这两类便为包含存储的重构升级，第一个是从数据库升级到缓存，第二个是从数据库升级到 ElasticSearch。它们的架构如下图 3 所示：烟囱式到平台化的重构升级完成上述两种重构之后，接下来就需要思考什么时候进行另一种存储类型均为数据库存储，但表结构不同的重构了。烟囱式架构以即时通信作为讨论案例。在 PC 时代，QQ 在即时通信市场的占有率是绝对领先者，相信你也使用过。从技术的抽象层面来看，QQ 主要提供消息发送和消息接收的功能，消息可以是图片、表情、文字、视频、语音等内容。支撑 QQ 消息发送和接收的简版架构如下图 4 所示： 图中编号 1 的模块为安装在电脑里的 QQ 客户端，它主要给用户提供可视化的聊天界面，以及发送和接收其他用户的消息。 编号为 2 的模块为网络接入层，它主要的作用是维护和客户端的网络连接，负责解析客户端发送到服务端的消息和推送其他用户发送的消息给到指定的客户端。在接收和发送消息时，接入层需基于 QQ 自有的数据协议进行消息内容的解码和编码。 编号为 3 的是数据接收模块，它对外提供保存消息的 RPC 接口，并由编号 2 的网络接入层在接收到客户端消息的时候调用。接收模块接收到消息后，会将消息保存至存储中并通知编号 4 的消息发送模块。 编号为 4 的是消息发送模块，它接收到通知后，会读取存储中的待发送消息并进行一定的逻辑处理，然后调用网络接入层进行消息的发送。关于即时通信后续的发展，就有亲身感受了。为了抓住移动互联网的发展浪潮，腾讯又推出了即时通信的王者级应用：微信。从产品上看，微信和 QQ 在定位、应用界面设计、附加功能设计等方面存在差异。但抽象地从技术和核心功能上分析，两者的功能均是给用户提供消息发送和消息接收。因此，在技术实现上，微信研发团队也需要建设和上述图 4 类似的提供消息发送和接收的技术架构，如下图 5 所示：从图 5 中不难发现，除了编号 1 的微信 App 和编号 2 的网络接入层模块与图 4 有差异，其余各模块的功能基本上与图 4 类似。网络接入层之所以有差异，是因为接入层需要进行通信协议的解析，而 QQ 和微信的网络通信协议是根据各自的客户端进行定制的，因此会有格式上的差异。除了微信外，现在大部分在线游戏也都提供了即时通信的能力。因此，游戏团队也需要按上述类似的架构实现自己的消息发送和接收业务系统。类似上述介绍的这种模式：即系统架构大体上类似，其中只有个别模块存在差异，但各个研发团队还是从零开始建设全部模块的方式，称为烟囱式架构。为了方便你理解，我把QQ、微信以及游戏语音的架构放在一张图中，如下图 6 所示：从上图可以看出，烟囱式架构是一种象形比喻，各个业务研发团队（如 QQ、微信、游戏语音团队）维护了一个类似烟囱式的、包含重复模块的系统架构。除了即时通信这个案例，还有很多其他会产生烟囱式架构的场景，比如电商，电商除了 PC、App、M 页面版本，现在还有很多购物场景，比如自动贩卖机、微信里的分享链接、小程序等。在实现上，如果他们的研发团队是封闭地进行自研，那么也会产生如下图 7 所示的电商版烟囱式架构。经过前面的分析，烟囱式架构存在的问题其实已经比较明显了，即存在大量的模块重复，进而导致人力重复、成本增加。此时，为了解决这个问题，便可以启动本小节开头提到的升级重构：均为数据库存储，但表结构不同的重构了。以上述消息接收模块为例，它包含了一个代码进程和对应的消息存储（假设为 MySQL）。为了解决此模块的重复，可以合并 QQ、微信和游戏语音里此模块的代码。同时在前期设计时，各个业务只考虑自己的消息格式，所以它们的数据库的表结构是偏定制的，无法直接被复用，因此在重构时，还需要设计一套全新的、兼容原有三个版本的数据库。此时消息接收模块的重构架构如下图 8 所示：完成上述模块的重构融合升级之后，消息发送模块也可以进行类似的融合重构。当所有的可复用模块均完成升级重构后，上述三个不同的即时通信软件的架构演化成如下图 9 的形态：融合后，三款软件有差异的系统模块依然各自维护，但消息接收和发送模块已经融合为一套。此时，从多个模块融合形成的、支持不同类型使用方的模块，称为平台化模块。而这个重构过程，有一个高大上的名称：从烟囱式架构朝着平台化演化。平台化到中台化的重构升级升级重构完成平台化之后，后续三款软件涉及消息接收和发送的新需求，都由平台化模块直接支持。这种需求支撑的模式，看起来十分像这几年兴起的中台化架构，但其实并不是。下面我们具体分析一下，你可以从以下两点来理解。 平台化架构的概念要早于中台化架构。平台化只是将重复模块进行融合，如在平台化之后，未做任何中台化的改造，便不能直接称为中台化模块。 平台化是从降低技术重复的角度出发，从而提升效率，而中台化是在平台化之后，从业务复用的角度出发，进一步提升业务需求的效率。下面，继续以上一小节的案例作为讨论对象。完成平台化，消灭重复技术之后，如果你遇到以下几种情况，则需要进一步重构，以便完成从平台化到中台化的演化。 融合后的模块代码量庞大、代码中业务逻辑分散。表现就是需求承接时，需要一周时间进行评审，而开发只要一到两天时间，沟通成本巨大。 当出现上述三个之外的新业务场景，融合的平台无法直接支持，而需要大量改造时。 假设 QQ 的某一个需求在平台上开发上线后，微信也提出同样类似的需求，但平台无法直接复用 QQ 的需求，而需要重新开发时。从平台化到中台化演化升级，可以从业务能力可视化、业务能力在线配置化的方法进行落地改造。业务能力可视化仍以上述的数据接收模块为例，可以在平台化之后，将数据接收模块对外的接口流程进行梳理并可视化地展现出来。格式为如下图 10 所示： 流程：数据合法性校验 → 图片压缩 → 图片尺寸裁剪 → 图片转存到 CDN → 语音自动识别成文字 → 保存。上述这个流程图，大多数情况是在需求提出时由产品经理进行绘制，而在代码上线后便不会再实时更新。而中台化之后，需要开发建设一套业务可视化平台，将业务平台中的代码流程可视化地登记到可视化系统中，同时要保证可视化平台能够在业务代码修改后，实时更新相对应的流程。在实现上，编写业务代码时，可以增加一些代码标记，供可视化平台进行自动化扫描，进而识别业务流程，最终更新到可视化平台的显示界面上。通过将业务能力可视化之后，前面提到的因为平台化融合了太多代码，导致代码量多、业务无法直接从代码中抽取的问题便解决了。因为可视化之后，业务逻辑可以直接在可视化平台上展现出来，业务方和产品经理不需要和研发来回沟通上周的时间来确认需求，可以极大地降低沟通时间，提升效率。业务能力配置化在上述图 10 的流程中，可以看到有些是实心的圆圈，有些是空心的圆圈。空心表示代码执行到此流程节点时会直接跳过，而实心表示会执行此流程节点。流程节点是为空心还是实心，是可配置的，此配置功能可以落地在上述介绍的可视化平台里。上图 10 的可视化、可配置化流程只有一份，但假设微信在保存消息数据时，不希望图片被压缩而用原图保存；而 QQ 在保存消息数据时，不希望图片尺寸被裁剪。此时，如何通过配置化解决这样的需求呢？答案便是：基于业务身份进行业务流程的配置化。业务身份是指给 QQ、微信及游戏语音等每一个复用中台能力的应用，都分配一个全局唯一的名称。在进行配置的时候，按业务身份进行隔离，每一个业务身份都拥有属于自己的上述流程的配置，如下图 11 所示：在执行时，各个即时通信应用在调用保存消息接口时，需传入自己的业务身份标识。对应的中台模块会根据业务身份获取相对应的配置，并按配置去编排属于此业务身份的流程。再回顾一下本小节开头提出的平台化架构会遇到的问题。 再来一个新的聊天应用时，无法快速、直接复用已有能力。采用配置化后，可以给新的聊天应用配置一个业务身份，同时基于此应用的需求，去配置它需要使用的业务节点。 QQ 先提出的某一个业务需求并开发上线后，后续微信也想要此功能，但无法快速直接复用。在完成上述业务功能可视化、配置化的基础上，当 QQ 先提出的需求上线后，可视化的工具会将此新功能直接更新到上述流程节点里。只是在 QQ 对应的业务身份的配置里，此新加入的节点为实心。而其他不使用此新功能的业务身份里，此节点为空心。当后续微信需要使用此新功能时，直接将此节点勾选为实心，便可直接复用。至此，从平台化到中台化的重构升级便具备基本雏形。当前中台化的建设理论还处于初期，有很多种探索的实现方式，但万变不离其宗，它的核心仍然是：在面对不断出现的新的业务场景和形态时（如电商里新出现的社区购等），中台需要快速地复用已有能力，去满足业务新建站点或不断扩宽业务边界的诉求。总结罗马不是一日建成的，系统建设也是一样，它是随着业务的发展不断演化而来的。当业务体量较小且没有类似像 QQ 和微信的多个前台应用时，没有必要在建设初期就采用平台化或中台化的建设方案。因为它们的建设人力成本和消耗的机器资源也更高。一个系统在建设时，假如预期未来的三到五年的用户量并不会增长太大，可以先采用烟囱式的架构，快速地满足业务需求。当发展到一定体量后，再发起从烟囱式到平台化及中台化演化即可。毕竟能够发展到百万、千万用户体量的系统是少数，所有的系统都提前建设会存在较大可能的成本浪费。" }, { "title": "重构：系统升级，实现不停服的数据迁移和用户切量", "url": "/posts/reconstruct-1-22/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-05-06 15:33:00 +0000", "snippet": "重构常见的形式升级重构有两种常见的形式： 纯代码式升级； 包含存储和代码的升级。纯代码式升级纯代码的重构升级是指只针对代码中存在的一些历史遗留问题进行修复，比如： 代码中的慢 SQL； 错误的日志打印方式； 代码中未显式开启事务等问题。 注：本文，将存在问题的历史版本称为 V1 版；修复问题后的升级重构版本称为 V2 版。纯代码的重构升级架构如下图1所示：含存储代码式升级包含存储和代码的重构升级是指除了纯代码之外，将原有架构里的存储也一起升级。存储升级有两种形式： 存储类型进行升级。 比如将数据库升级为缓存，将原有的读接口从数据库切换至缓存。做此类存储类型升级的目的是提升微服务的性能，同样的硬件配置下，缓存比数据库至少快一个量级。不同类型的存储升级架构如下图所示： 可以看到升级后，原有的代码从 V1 升级到 V2。同时，存储从读写都使用数据库，升级为写操作使用数据库，读操作使用缓存的架构。 表结构升级（将一个表结构的存储升级为同类型存储的另外一个更加合理的表结构）。 此类升级常见于系统构建时，为了快速满足业务需求，在时间紧张的情况下，简单快速地设计了不是特别合理的表结构。 比如，原有的表结构采用了一张宽表存储所有的数据，包含一对多的数据都进行冗余存储。升级重构时，需要采用更加合理的表结构存储数据，以便未来能够快速响应业务的发展。它的重构升级架构如下图所示，升级后，原有微服务的读写都将切换至新的表结构的存储里。 重构的切换纯代码重构的切换纯代码重构的切换比较简单。当上述 V2 版本通过测试环境和预发布环境的测试后，就可以直接在线上部署，替换原有的 V1 版本。当部署的 V2 版本出现问题后，直接进行回滚即可，这是最简单、粗暴的切换方式。但同时也存在隐患，采用此种方式部署的 V2 如果出现问题，会影响所有的用户，影响面较大。为了降低影响，可以采用灰度的方式。即用 V2 版本的代码替换一台或者一定比例 V1 版本的机器，比如： 线上有 100 台部署 V1 版本代码的机器，当 V2 版本测试完成准备上线时，可以先发布 10 台 V2 版本的代码。这样，假设 V2 版本的代码存在 Bug，也只会影响访问这 10 台部署了 V2 版本代码的用户，即 10%的线上流量，这样就缩小了影响面。假设发布了 10 台 V2 版本的代码后，没有发现任何 Bug，此时则可以继续发布，逐步进行替换。通过此种灰度的方式，既可以做到纯代码的升级重构切换，又可以缩小因此可能带来的线上问题的影响范围。含存储重构的切换与上述纯代码的切换相比，含存储的重构切换有一个重要步骤便是数据迁移。不管是不同类型的存储还是同类型不同表结构的存储重构，都需要将原有存储中的数据全部迁移至新的存储中，才能够称为完成切换。对于含存储重构的切换，最简单的方法便是停服，之后在无任何数据写入的情况下进行数据迁移，迁移之后再进行数据对比，对比无误之后，用重构的新版本连接新的存储对外提供服务即可。这种方式适合于以下 2 种场景： 业务有间断期或者有低峰期的场景。比如企业内网系统，下班或者周末期间几乎没有人使用； 金融资产类业务，这些业务对于正确性要求极高，因为用户对资产极度敏感，如果资产出现错误，用户是无法容忍的。为了资产安全无误，有时候需要用户容忍停服的重构升级。但对于用户量巨大，且大部分业务场景都需要提供 7*24 服务的互联网业务来说，停服切换方式，用户是不能接受。因此，就需要设计一套既不需要停服，又可以完成用户无感知的切换方案。切换架构为了实现不停服的重构升级，整体的新版本上线、数据迁移以及用户切量的架构如下 图 4 所示：上述的架构中，左边部分是老版本未重构的服务及对应的老数据存储（后续称为老存储），图中右边部分部署的是升级重构后的新版本的服务和对应的新版本存储（后续称为新存储）。这个存储可以是缓存或者是表结构不同的数据库。在图的下方，则是数据同步模块。它主要的作用是实时进行数据同步，将老存储里的历史数据、新增的写入以及更新的数据实时地同步至新存储里。实时数据同步是实现存储升级重构不停服切换的基础。在完成数据同步之后，便可以进行用户的灰度切量了，将用户逐步切换至升级重构的新版本上。见图 4 中最上面的部分。数据同步当升级重构后的新版本开发及测试完成后，便可以将新版本代码和存储进行线上部署了。新版本部署时，可以将新版本服务对外提供的接口的别名变更为一个新的名称，如为：new_version，具体见上图 4。因为修改了别名，即使新版本的服务上线部署并直接对外了，也不会引入老版本的流量。通过上述方式可以实现新老版本的隔离，进而完成新版本服务的线上部署。新版本线上部署及隔离后，便可以进行数据同步了。数据同步分为历史数据的全量同步和新增数据的实时同步。在上述“含存储重构的切换”里说过，存储重构涉及两个种类，第一种是数据库到缓存，第二种是数据库到另一种异构的表结构数据库里。这里以使用场景较多的数据库到缓存的重构举例讲解，另一种场景比较类似，可以按此方式推演。包含全量同步、增量数据的实时同步架构如下图 5 所示： 上述第一步的全量同步（见图 5 的编号 1 处）是将历史数据进行一次全量初始化同步，可以采用 Worker 的方式，对老版本的数据库的数据进行遍历，遍历的 SQL 大致如下： SELECT 数据 FROM t_table WHERE id &amp;gt; last_id LIMIT 一批次的数量; 从数据库遍历读取完之后，便会在同步服务模块里按缓存的格式进行数据格式的转换，转换后的数据写入缓存即可。 上述的数据同步 SQL 没有停止条件，且在未切量前，老库一直会有数据持续写入。使用上述 SQL 进行同步时，会导致全量同步一直执行，出现无法停止的情况。 针对这个问题，可以根据当前数据库已有数据量、数据增长的速度以及数据同步的速度，评估在数据同步期间能够产生的数据量，并评估出这期间最多可能产生的数据 ID（截止 ID），并将上述 SQL 修改如下： SELECT 数据 FROM t_table WHERE id &amp;gt; last_id AND id &amp;lt;截止 ID LIMIT 一批次的数量; 第二步的增量实时同步是在开始进行全量同步时启动的，增量同步使用的是 Binlog 进行。通过在增量同步模块订阅老版本数据库里的数据变更，可以实时获取老版本数据库中新增和变更的数据。 需要注意的是，增量同步需要在全量同步开始前便进行Binlog的订阅。如果在全量同步结束后，再订阅 Binlog 进行增量同步，可能会丢失在全量同步期间发生变更的数据。 比如一张待同步的数据表里有 100 条数据，如果在全量同步前未开启增量同步。当同步至第 90 条数据时，第80 条数据发生了 update 操作，因为此时还没有开启增量同步，那么这第 80 条数据对应的变更就丢失了。为了防止此问题，就需要前置开启增量同步。 最后，增量同步除了需要订阅 update 和 delete 操作外，还需要订阅 insert 操作。因为全量同步在上述截止 ID 之后的数据便不会再同步了，需要增量同步处理此类操作。 数据对比验证在完成数据迁移之后，并不是立马就能够开始用户切量。还需要做一步非常重要的事情，那便是进行测试。因为做了大规模的代码重构以及存储的切换，只靠人工测试是远远不够的，很容易出现场景遗漏。因此就需要借助自动化的方式进行测试，在完成全量数据同步后，可以录制老版本服务的流量，并进行自动化测试回归。通过一定时间区间的自动化回归，可以保证场景不被遗漏，极大地减少重构切换可能导致的问题。在自动化回归中，可能会出现的某一类问题需要特殊处理，因为增量同步延迟会导致数据对比不一致。原则上这类问题不应该存在，因为基于 Binlog 的主从同步延迟非常小。但如果遇到上述情况，因为增量同步的时延很小，所以可以等待几分钟后再次运行对比不一致的回放请求。用户切换完成数据对比之后，下一步需要落地的便是用户切换了。进行用户切换时，有几个原则需要遵循： 切量不能一刀切，即不能一次将所有的用户全部切换至新版本服务里，需要灰度逐步地将用户从老版本切换到新版本服务里； 在灰度切量时，需要尽早发现问题，而不是等到切量快完成的时候才发现问题。对于上述的几个要求，在切量的具体落地时，可以从以下几点着手落地。对于影响面小的要求。 首先，对于升级重构的系统涉及的所有用户进行分析并按等级划分。可以按用户的注册时间、是否为会员等进行划分。如果重构的模块是订单模块，可以将用户按历史以来的下单量、订单金额进行排序，订单量小、下单金额低的用户排在最前面。 在用户按上述的维度排序后，可以将用户分为几大批次，比如将所有用户按排序分为五等份。第一等份的用户因为单量小、下单金额少，可以最先进行切换，这样便满足前述提到的“出问题影响面少”的要求。对于在切量时，尽可能早发现问题的要求。对上述排序的第一等份的用户，再次进行分析和分类。我们知道，一个系统对外一般会提供多个功能点，比如： 用户模块会对外提供用户注册； 查询用户基本信息； 修改个人签名等功能。可以对第一等份里的用户进行数据分析，分析这些用户里哪些用户使用了较多的系统功能。在分析后，按使用功能的多少对第一等份里的用户进行排序，使用功能较多的用户排序在前面。在切量时，第一等份里使用系统功能最多的用户会优先进行切量，这也满足了前面所要求的尽可能多发现问题的要求。" }, { "title": "微服务高保真压测和服务扩容", "url": "/posts/pressure-test-expansion-21/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-05-05 12:33:00 +0000", "snippet": "在后台架构中，压测非常常见，也是必须的工作。它能够帮发现微服务架构中的性能瓶颈，以及知道构建的微服务能承载的流量极限值。但实际情况是，很多压测并不能发现瓶颈点和微服务所能承载的真实流量极限值： 一方面是因为压测时使用的是人为构造的压测参数； 另一方面有些时候压测场景是经过修改的，和实际的线上场景存在差异。因此构建一个更贴近真实场景的高保真压测，以及根据压测结果进行相对应的容量规划高一点经验总结实施高保真压测模拟参数进行压测是指人为构建符合被压测接口的一个或一组参数进行压测的方法，它存在以下几个问题： 首先，参数是模拟的，可能和线上真实环境有差异，进而导致压测数据失真。假设要压测使用了本地缓存和 Redis 来实现的查询用户基本信息接口，当使用一个或一组用户账号压测此接口时，压测出来的性能和 QPS 会非常好。因为，在压测的前几次调用之后，所有用户信息都已经缓存到用户模块的本地缓存里，后续的所有压测请求都可以直接使用本地缓存的数据，因此性能会非常好。而实际生产环境里，查询用户信息的接口请求里的用户账号并不是相同的，因此请求不会都命中本地缓存，所以它的性能要比压测时低。这就是产生失真的原因。 其次，即使抓取线上环境的一组参数进行压测，也不能完全代替真实环境，仍然存在失真的场景。高保真压测，从字面上就可以理解它的要求——使用和生产环境一模一样的用户请求进行压测。这样压测出的微服务的各项性能指标更加可信，因此可以作为限流和容量评估的参考标准。那如何模拟线上请求呢？只要完成生产环境的流量录制，并把它用来压测即可。基于生产环境的流量录制压测架构如下图所示：上面架构和自动化回归架构类似，其中包含内置于业务进程里的流量过滤器、日志保存模块、日志下发模块、压测模块以及压测管理端。 流量过滤器：采用 RPC 框架提供的拦截器实现，并将出入参基于 MQ 转发。 日志保存模块：根据压测设置的配置，进行压测日志的收集。压测的配置包含：当次需要压测的接口和方法、收集日志的时长和数量等。需要注意的是，相比自动化回归，此处日志收集的量应该要多很多，因为通常来说，压测需要的真实用户请求数据会更多一些。因此，日志的存储也可以升级到分布式文件系统里，如 Hadoop。 日志下发模块：主要功能是将压测的日志下发到压测机器里。为什么不是压测模块远程连接到分布式文件系统读取日志后，再进行压测呢？主要是因为考虑性能。压测希望在短时间内，给被压测应用一个洪峰流量。如果压测模块在发起压测请求前还需要调用其他远程接口获取数据，很大程度上，就实现不了这个洪峰流量了。因此，需要在压测前，将压测日志推送到压测机器上，压测模块可以读取本地磁盘的日志，性能将会有极大提升，可以短时间发起洪峰流量。 压测模块：主要的功能是读取本地日志并调用被压测机器，并将压测信息写入存储中。 压测管理端：用来设置各项压测配置以及查看压测结果值。上述便是一个高保真压测的架构和对应模块的功能。在了解压测模块的架构后，压测时，被测应用需要注意的问题。首先，最简单的被压测应用架构如下图所示：上述架构是一个非常简单的应用部署图，其中包含一个存储模块（Redis 或数据库）及一台应用机器。压测时，很大概率是应用程序所在的宿主机先达到资源的瓶颈，而不是数据存储先达到瓶颈。这可能是发生了宿主机的 CPU 利用率达到 100%，或者是内存使用率满了等情况。此时，获取到的 QPS 即为上述架构里单台机器能够承载的最大值。那么，是不是拿着上述单机的最大 QPS，通过机器数量乘以单机最大 QPS，即可计算出当前线上集群能够承载的最大 QPS 呢？答案显然是不行的，集群的QPS 并不是随着机器数量增加而线性增加的。主要原因是所有机器所处的网络是共享的、进程间的切换存在性能消耗，以及存储是共享的等因素。在实际压测中，压测完单台机器后，可以分多次部署 2台，4 台及 8 台应用进行压测，得到对应的压测 QPS。通过不断叠加机器进行压测获得损耗比，为后续线上大规模扩容做数据准备。假如得到 1 台机器的压测 QPS 为 100，2 台的QPS为180，4 台的QPS 为 360，那么可以计为损耗比 10%。假设线上有100 台机器，由此评估线上可以支撑的 QPS为9000。这里会有疑惑，为什么不直接对线上集群进行压测，而要采用这种按比例的方式？主要有以下两个原因。 线上机器并不是一成不变的。比如现在线上有100 台机器，在业务不断发展后，可能会不断扩容至 200 台或更多的机器。此时，之前直接按 100 台机器进行压测的数据就没法直接使用了。 直接对线上环境进行压测会影响线上正常的业务。因为压测的应用集群和存储都是和线上共享的。如果不希望它们互相影响，那么就需要重新部署一套和线上一模一样的环境，这个资源的消耗和部署的成本会非常高。那是不是通过上述方式压测之后，就可以根据压测值和损耗比进行压测了呢？比如，某一天运营计划做促销活动，预估带来的最大流量 QPS 为 18000/s。按上述数据，是不是扩容到 200 台机器即可了呢？其实，并不是。此种扩容评估法做了一个假设，即服务所能够承载的 QPS 是和机器绝对线性增长的，只要机器充足，那么能够承载的 QPS 是没有上限的。但服务依赖的存储是有上限的，微服务能够提供的极限 QPS，其实是由它本身和它依赖的存储的最小值共同决定的。以上述案例的压测值和损耗比为例，架构如下图 3 所示：当前图示中部署了 100 台机器，理论上可以支撑 9000/s的QPS，但如果所依赖的存储只能支撑 5000/s的QPS，那么即使部署 100 台或者更多的应用机器，它能够承载的QPS 也不能线性增长，最大只能支撑到 5000/s的QPS。因此，在实际压测中，除了寻找单机压测值和损耗比之外，还需要对微服务依赖的存储，以及除存储之外其依赖的其他微服务进行压测，寻找微服务压测中的最短板，进而确定微服务能够支撑的最大 QPS。如何做写压测读服务是无状态的，所以可以直接采用基于录制的压测方案进行压测。但写服务是有状态的，因为录制的流量是用户在线上产生的真实请求，比如下单请求，如果直接使用录制的流量进行回放，可能会给客户的账号误下一笔订单，在生产环境中是不允许的。如果出现这样的线上操作，就算是线上事故了。其实，保障相对高保真的写压测有以下两个常见的方式进行应对。第一种是采用模拟账号进行替换或数据修改进行压测。假设压测使用的数据是用户私有数据，比如压测发送微博的接口或提交订单的接口，就可以将录制数据里的用户账号替换为测试账号，这样压测时产生的微博和订单都隶属于测试账号，就不会对线上产生影响了。此外，如果压测使用的数据是公有数据，比如新闻投稿接口，只要新闻投稿了所有用户都可见。对于这种公有数据的接口，可以在业务上进行处理，比如修改录制的新闻投稿数据，将所有的投稿都修改为待审核。这样压测产生的数据都处于待审核状态，在线上是不可见的，所以此种压测对线上不会产生影响。第二种方式是采用压测数据打标 + 影子库的方式进行特殊处理，架构如下图 4 所示:上述架构里的数据打标和第一种里的数据修改是有区别的，它不会更改原始录制的任何数据，只是在压测的时候，对于压测模块发起的任何请求都增加一个标记，标记它为压测请求。在业务应用模块识别此标识，如果识别出压测请求，则将压测请求的数据全部写入上图 4 中的影子库中。影子库中的数据不会暴露给外部查询以及用于进一步的生产，因此不会产生线上影响。对于微服务依赖的其他微服务提供的写接口，可以在压测时继续传递标识，被依赖的微服务也识别此标识，将压测数据写入影子库即可。通过标识传递+影子库的方式，即构建了一个线上写压测环境。基于压测数据进行行动压测过程中有两方面重要的数据，一个是压测过程中的各项指标数据，另一个是压测的结果即服务所能够支撑的QPS。压测过程的各项指标数据有： 压测时机器的 CPU 利用率的变化； 内存的变化； 进程里各线程的 CPU 利用率； 微服务依赖的存储的 CPU 利用率、内存使用率等。压测过程中监控这些数据是为了发现系统瓶颈点，并快速优化，进而提升微服务能够支撑的QPS。简单列举一下可能存在的瓶颈点： 如果压测过程中，发现被压测应用的CPU 都被某一个或某一类线程消耗，同时通过堆栈信息，确定这个或这类线程的所有 CPU 消耗都集中在一个方法里。那么极大可能，这个方法里有十分消耗 CPU 的代码，可能是一个大对象的JSON 序列化或者是一段可以优化的多层嵌套 for 循环； 再比如，在只部署了一台应用机器和对应存储（MySQL）的情况下。理论上压测时，应该是单台应用机器的CPU 先达到 100%。但如果在实际压测中，是 MySQL 所在机器的CPU 先打满，那么很大概率上是被压测接口请求数据库的 SQL 是一个慢 SQL。引发这种情况的原因可能是未命中索引、一次请求的数量太多、存在 SQL 的深翻页等。此时，就需要对这些 SQL 进行调优，以便进一步提升微服务的性能。压测的极限 QPS 除可以了解微服务的最大支撑能力之外，另外一个作用就是参考此值来设置微服务的限流阈值。流量达到压测时的 QPS 时，微服务的各项指标如 CPU、内存等，均已达到极限，为了保证微服务的稳定，需要将进入微服务的流量限制在压测的 QPS 之下。根据压测值设置限流时，有以下几点需要注意。 上述案例中，单机压测的 QPS 为100/s。但限流时，不能直接设置单机限流阈值为 100/s，因为达到此 QPS 时，机器的CPU 已经达到 100% 了。正常情况下，线上机器的 CPU 利用率维持在 40%~50% 是安全的，再升高就需要扩容了。因此限流时，可以将压测的 QPS 适当打折，设置压测为 QPS*40%。 如果微服务提供不止一个接口，那么上述的限流阈值就还需要打折。比如微服务对外提供了两个接口，那么最简单的打折办法为：单机 QPS * 40% * 50%。 在前面提到过，微服务能够支撑的 QPS是有上限的，并不是随着机器数量无限增长的。因此，除了设置单机级别的限流之外，还需要设置微服务集群维度的限流阈值。限流阈值的设置方法可以上面两点。总结压测过程中的数据和压测结果不只是用来记录，还可以用于分析，寻找可以优化的瓶颈点。其次，需要根据压测极限值，设置微服务的限流阈值，防止流量超过压测极限值，进而将机器打挂，导致服务完全不可用。" }, { "title": "通过监控快速发现问题", "url": "/posts/acutor-20/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-05-04 03:33:00 +0000", "snippet": "基于“防备上游、做好自己、怀疑下游”的准则，通过系统设计、部署，以及代码编写的方式来构建一个更加高可用的后台系统。基于上述三个准则提出的方案可以预防部分问题，但百密一疏，即便做了很多防护措施，仍无法保证绝对安全，避免问题发生。此时作为系统的负责人，需要在第一时间，也就是用户感知前发现问题。发现问题的方法便是监控！什么是监控监控，是指对被监控体的运行状态数据进行持续地审查，并设置运行状态数据不符合要求的阈值，对不符合阈值的运行状态主动报警的一种方式。被监控体的运行状态数据通常以如下图 1 中 XY 轴的格式进行展示。上图： X 轴表示时间，由固定的间隔组成，此间隔可以是秒级或分钟级； Y 轴表示在该时间间隔里的运行状态数据的汇聚，它可以是间隔数据的累加、平均值、最大值等方式。次数监控次数监控中被监控体就是次数，具体指微服务里各项代码逻辑运行的次数，可以是微服务对外提供接口的被调用次数、某一个方法被执行的次数。它的示意图如下图 2 所示，其中 Y 轴表示在指定间隔内，被监控体的总体运行次数。性能监控性能监控里的被监控体是性能，可以是微服务对外提供接口的性能、微服务依赖下游其他接口或存储的性能。在性能监控里，有几个通用的运行状态数据：平均性能（AVG）、Max 以及TP999 。下面我们具体看一下它们的含义与区别： 平均性能（AVG）指上述时间间隔里的代码总运行次数的耗时平均值。计算公式：间隔内所有调用的耗时累加/总次数； Max 性能则是直接显示上述时间间隔里的耗时最高的一次。假设在指定间隔内代码运行了三次，对应的性能分别为 10ms、300ms、50ms，那么最终显示的 Max 性能就是 300ms； TP999 性能表示对上述时间间隔里所有的耗时进行升序排序，处在第 99.9% 位置的性能耗时。打个比方，假如在指定间隔里共计发生了 1000 次代码执行，那么TP999 表示这 1000 次经过升序排序的请求的第 999 次的耗时。此指标反映了指定间隔里，99.9% 的请求的性能都低于 TP999 所代表的值。以此类推，还有 TP50、TP9999，分别表示满足 50% 和 99.99% 的请求性能所处的水平。在实际工作中，上述三个性能指标常相互组合使用，而不是单看某一个指标。比如，某一个方法的 Max 性能很差，可能超过 1s，而 TP999 或 TP9999 却很好，控制在 50ms 以内。这说明它的大部分请求性能不错，只是每个时间间隔里，会有一次请求性能很差，此时就需要注意某次请求的数据是否和其他请求存在差异，或者是否发生了其他故障。此外，上述三个性能监控指标也通常展示在一幅图里，如下图 3 所示：可用率监控可用率里的被监控体是在指定时间里，代码执行成功的占比。假设在指定时间间隔里，代码运行了 100 次，其中 99 次经过判断都为成功，那么在可用率监控图里 Y 轴显示的值即为 99%，具体格式可以参考下图 4 所示：在不同的场景下，判断某一个方法执行是否成功的准则不同。同样的执行结果，在某些场景里被认为是成功，在某些场景里则认为是失败。接下来，在可用率的小节将详细描述这些场景及对应的规则。如何通过监控发现微服务的问题如下图 5 所示：从微服务的入口、微服务自身及微服务的依赖这三个方面，如何应用上述三种监控方式，以及对应的一些最佳实践准则，从而发现微服务里的各种潜在问题。微服务入口首先，微服务的入口必须设置次数监控、可用率监控和性能监控。因为微服务的入口，即对外供其他微服务调用的接口，是微服务自身能力对外提供的唯一通道，所以它的各项监控指标必须面面俱到。微服务入口在调用次数监控上，可以参考以下几点原则： 需要设置调用次数报警，调用次数的报警阈值，参考单机压测瓶颈值。 设置调用次数报警的目的是，通知你感知流量异常，并能够第一时间处理，比如进行扩容、排查异常流量等操作。当然，除了次数报警外，前置的设置限流不可少，因为有些时候是半夜收到报警，响应时间可能较长，通过限流可以规避因为处理报警不及时而导致的宕机问题。 按调用方设置调用次数监控。 正常情况下，一个微服务的调用方不止一个。当微服务的调用量在某些时刻突然暴涨时，你需要定位到是哪个调用方导致的。通过设置按调用方的次数监控，便可以排查到具体是哪个调用方导致的流量增加。 关于调用次数的阈值，需要设置调用次数的同环比监控。 在上述流量飙升的案例里，如果有调用次数的同环比监控，不通过排查，基于监控就可以自动知道具体是哪个调用方的流量异常，导致整体流量飙升。微服务入口在性能监控上，可以参考下面几点： 并不是每一个性能指标都需要设置报警： 比如 TP99、TP999 或 TP9999，只需要设置其中之一即可。但平均性能、TP9999 （或 TP999）以及 Max 这三个指标均需要设置报警阈值。 值得注意的是，报警阈值需要根据接口的 SLA 来设置，而不是“拍脑袋决策”。 和上述调用次数的第二点类似，需要配置按调用方的性能监控，用来观察是否存在不同的调用方，因为使用方式的差异，进而导致性能上的差异。 基于入参数据进行监控 假设一个接口支持调用方指定批量大小来查询用户信息。那么此时，可以按调用方传入的批量大小进行性能监控。比如将批量大小处在 0~10 之间设置一个监控点、处在 10~20 之间的设置另一个监控点，以此类推。 在添加上述的监控埋点后，会发现：批量大小处于 50 以内的性能都差不多，而批量大小处于 50 以后的性能会有较明显的增加。此时，通过监控，就可以把接口的最大批量大小设置为 50。如果调用方有一次查询需要超过 50 个数量的需求，他可以通过并发的方式查询，将单次查询需要超过 50 个的数量一分为二即可，此时的性能将比一次查询超过 50 个的性能更好一些。微服务入口在可用率监控上，参考下面的原则如下： 需要设置接口的可用率告警和按调用方设置报警（与“性能监控”第一条类似）； 判断是否需要降低可用率的方法由当次请求是业务异常还是非业务异常决定： 其中，业务异常不能判断为方法执行失败，还是成功，即业务异常不能降低可用率。而非业务异常（如网络故障、连接失败、机器宕机等导致的异常），需要降低可用率。 业务异常指的是用户没有按规定的要求输入数据，比如用户输入的参数如手机号码、邮箱地址不合法等场景。对于这类操作，不需要降低可用率，只需要提示客户按指定格式重新输入即可。 而非业务异常指的是网络故障、连接失败、机器宕机、代码执行出现空指针、调用下游超时等现象，是需要降低可用率的。出现上述异常情况，可能是因为你的代码未按预期执行、网络环境未按预计运行，此时，通过降低可用率进而报警，可以让研发人员排查导致问题的原因。 可用率的阈值需要按接口的等级差异化设置。 在线上环境里，网络并不是绝对稳定的，可能会产生偶发的抖动，进而导致接口调用出现几秒或几分钟的部分失败，产生可用率下降的现象。 为了规避网络抖动导致的可用率报警的情况，你可能会将可用率报警的阈值设置为低于 95% 才报警。虽然此种情况可以屏蔽误报警，但也有可能屏蔽掉真正的报警。因此你需要根据接口的等级设置报警，如果是提单接口，你就需要设置可用率低于 100% 时报警。虽然偶尔会收到一些误报警，但相对错过提单真正的报警而言，这样做还是值得的。微服务自身微服务自身执行的各个方法，可以根据需求选择使用上述三个监控指标：性能、可用率和次数监控并配置对应的预期阈值。微服务内的方法是否使用上述监控有几点准则。 并不是监控点越多越好。虽然记录监控数据对机器的性能损耗很小，但一个方法设置几十、上百个监控点仍会有一定影响； 太多的监控点会导致系统维护人员产生麻木。首先监控点太多，不知道从哪里看起。其次，当出现网络抖动、机器故障等异常时，所有的监控点都在告警，研发同学在排查时，无从下手； 建议对核心方法、怀疑性能较差的方法增加监控，这样可以快速发现和排查到核心方法和性能差方法存在的问题。其他方法的监控数据，通过微服务入口的监控即可查看到。除了微服务自身的各类方法需要监控，微服务所属的进程，以及它部署的机器也有很多被监控体可以监控。 微服务使用的 RPC 框架的剩余线程池数量： 当微服务框架的线程池变少或为零后，调用方新的请求都会被拒绝。因此，当监控到剩余的线程池快耗尽，就需要快速处理，如调整线程池的大小、扩容新的机器等。 如果是基于 JVM 的各类语言应用，对于 JVM 相关数据也需要监控。比如 Young GC、Full GC 的频率、每次 GC 的时间，以及堆内存的使用量。 微服务的进程存活监控： 每一个微服务的进程都有一个进程号，此外对外提供接口服务的进程还会有端口号。可以使用 ping 或者 ps 命令，每一个时间间隔检查一次，如果监听到进程的进程号或端口不存在，便进行报警。 监控微服务所在机器的内存的使用率： 类似于可用率，使用率也是一个比例值。它表示机器已使用内存占机器总内存的比值。如果机器的内存使用率很高，操作系统可能会主动将占用较高的进程关闭。因此需要监控此值，当使用率飙升，去排查具体对应的原因。 机器的 CPU 的使用率和内存使用率类似： 太高的 CPU 使用率会导致微服务卡顿或者微服务不可用。因此，需要主动监控并配置告警，以便提前去处理。 机器负载（CPU Load）： 这一监控容易被遗漏，它表示当前机器里有多少进程处在“正在执行”和“等待执行”这两个状态里。假设机器的 CPU 只有 4 核，而机器负载在多个监控间隔里都远超于 4，比如在 10 以上，那么说明当前机器负载过高，这些进程排队等待执行的时间较长，性能可能较差。此时，可以适当减少机器上部署的进程数。 微服务依赖在介绍微服务的依赖监控前，我们再回顾下微服务依赖的架构图，如下图 6 所示：对微服务每一个依赖的调用，类似上述“微服务自身的监控”小节里提到的对内部方法增加监控——并不是微服务内部的所有方法都要加监控，而是要挑选重点和可能存在问题的方法。而对于微服务的所有依赖都需要统一增加监控，从“怀疑下游”找到原因——因为依赖的下游随时可能出现问题，为了快速定位问题，所以所有的外部依赖都需要增加监控。如果是 Java 应用，监控的方式可以采用统一的 AOP 切面来实现。此外，也可以借助一些框架的功能统一拦截并进行监控，比如 MyBatis 里就提供用 Interceptor 拦截所有 SQL 的执行，在此处就可以添加统一的监控。除了依赖要增加监控，判断依赖的其他微服务的接口在执行上是否成功，也需要格外注意。有些依赖的微服务执行异常时，并不会抛出异常，而是返回一个经过包装的结果对象，比如 RPCResult，并将错误信息包装在其中。此时，如果你在对外部依赖的可用率监控中没有判断 RPCResult 中的值，有可能遗漏应该告警的结果，导致问题没有被发现，进而影响线上的业务。监控时间间隔在上述的讲解里，一直使用了时间间隔来表示监控图的横轴，但没有指出这个时间间隔是多少，是分钟级还是秒级的？理论上这个间隔是越小越好，最好是秒级。但很多监控系统都不提供秒级的间隔，原因是时间间隔越小，需要存储的数据量就越多。以可用率监控为例，间隔为 1min 时，表示 1min 里所有成功的次数和总的调用次数比例。而间隔为 1s 时，表示 1s 里所有的成功与总的调用次数的比例。秒级产生的可用率数据量是分钟级的 60 倍，所以因为存储容量的限制，很多监控系统只提供分钟级别的监控。但如果监控系统既提供分钟级又提供秒级监控，那么优先选择秒级。因为秒级监控发现问题的速度更快。以可用率监控为例，秒级监控在 1s 间隔达到后，即可算出可用率，而分钟监控要在 1min 间隔到达后，才可算出可用率，两者相差了 59s。总结几种常见的被监控指标，可用率、调用次数、性能监控，这三个监控指标是每一个微服务应用都必须配置的。其次，从微服务入口、微服务自身、微服务依赖这三个角度梳理了如何落地上述三个监控，以及一些附加的监控，如机器监控、内存监控等。" }, { "title": "怀疑下游 - 做好微服务间依赖的治理和分布式事务", "url": "/posts/suspected-downstream-19/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-05-03 08:33:00 +0000", "snippet": "为什么要怀疑下游从图一中可以看到，微服务会依赖很多其他微服务提供的接口、数据库、缓存，以及消息中间件等。这些接口及存储可能会因为代码 Bug、网络、磁盘故障、上线操作失误等因素引发线上问题。此时，由于依赖不可用，就会导致微服务对外提供的服务受到影响，出现接口可用率下降或者直接宕机的情况。为了防止上述情况的发生，在构建微服务时，就需要预先考虑微服务所依赖的各项“下游”出现故障时的应对方案。假设下游出现故障及预设计对应的方案的过程，便是在实践“怀疑下游”。如何落地下面将基于图 2 展示的三大类依赖：其他微服务、数据库、消息中间件，总结可能引发的故障的应对方案和最佳使用准则。对其他微服务的依赖在采用了微服务的架构后，各个模块间均通过 RPC 的方式进行依赖，有些模块在完成一项业务流程时可能会依赖多达几十、上百个外部微服务。比如在完成下单的流程里，就需要依赖用户、商品、促销、价格、优惠券等各个微服务提供的接口，这些被依赖的微服务的稳定性直接影响了用户是否能够成功下单。因此，需要对微服务依赖的其他微服务接口进行可用性的治理。从写服务的角度介绍了通过依赖后置、依赖并行化、设置超时和重试、服务降级等手段，来对它的依赖进行治理，进而保障写服务的高可用。其实这些手段依然可以用在读服务里。下面将重点讲解在采用微服务架构后，如何应对随之而来的分布式事务。这里以提单作为案例，介绍分布式事务的实际场景。在微服务架构下，订单和库存是两个单独的微服务，它们之间的架构如下图 3 所示：在提单时，订单模块需要调用库存模块进行商品的扣减，以便判断用户购买的商品是否有货。订单调用库存的扣减接口会有以下几种情况发生： 调用库存接口返回成功且库存数量充足，订单模块便将此用户订单保存至数据库，并返回用户下单成功消息； 调用库存接口返回成功且库存数量充足，但订单模块将此用户订单保存至数据库时出错并进行数据库回滚，同时订单模块返回用户下单失败； 调用库存接口超时，订单模块判断此次调用库存接口失败，返回用户下单失败； …在微服务化之后，上述订单模块和库存模块的交互会产生非常多的可能性场景。其中，上述的第 2 、3 点描述的场景里就存在分布式事务问题。在第 2 点里，因为订单模块本地的数据库事务回滚了，但调用库存接口产生的已扣减的商品数量并没有回滚，此时就会导致库存数据少于实际的数据。有一些基于 TCC 和 Saga 的成熟基础框架可以解决上述分布式事务问题，但理解和接入成本较高。此处介绍一种本质上和 TCC、Saga 理论相类似，但无须借助第三方框架的简单、易落地的解决方案。理解此方案也有助于你理解 TCC 和 Saga 的思想。此方案的架构图如下图 4 所示，图中订单模块的数据库里除了订单原有的表之外，会增加一张任务表。基于上述的架构，下单流程变更如下： 在接收到下单请求后，在调用任何外部 RPC 前，先将此订单的相关信息，如此次用户购买的商品、商品数量、用户账号、此次订单的编号等信息写入新增的任务表中； 调用库存的接口进行商品数量的扣减，并根据库存模块的返回值更新订单模块的数据库。这一步，又细分为以下几种场景情况： 如果调用库存接口成功，则在同一个事务中，将订单信息写入订单库中，同时更新第一步写入任务的状态为“已成功”； 如果调用库存接口明确返回失败，则直接更新订单库中的任务状态为“待回滚”，并返回用户下单失败； 如果调用库存接口超时，则直接更新订单库中的任务状态为“待回滚”，并返回用户下单失败； 无论调用库存接口是成功还是失败，只要在更新本地订单库时失败，就返回用户下单失败，同时任务库的状态保留为“初始化”。 下单完成后，异步 Worker 功能是扫描订单库新增的任务表，获取状态为“待回滚”，任务创建时间距扫描时间点超过一定时间区间（如 5 分钟）仍为“初始化”状态的任务。获取到这些任务之后，会基于任务表中的商品和对应的数量信息，异步地调用库存接口进行商品数据的返还。上述介绍的是用户下单的同步流程，完成这两个步骤后，用户下单便结束了。我们再来看看下单后的异步情况。通过上述方式，能够将各种失败场景里漏返回的商品数量进行返还，保证库存数量的最终一致性，完成分布式事务。上述保障数据最终一致性主要是依赖任务表和订单表在同一个数据库里，可以通过本地事务来保障订单表数据写入成功后，任务表里的任务状态绝对能够更新为“已成功”。而当提单失败后，任务表的状态为非成功状态，再通过类似 TCC 和 Saga 的异步补偿性 Worker 来进行业务回滚即可保证最终最一致性。在发起分布式事务的业务模块的数据库里创建补偿性任务，基本上可以复用在其他存在分布式事务的场景里。如果不希望引入更加复杂的 TCC 和 Saga 框架，可以尝试利用此方式来解决架构微服务化之后带来的分布式事务的问题。对数据库的依赖除了对其他微服务的依赖，微服务中最常见的便是对数据库的依赖。在使用时，需要遵守以下几点基本原则。原则一：数据库一定要配置从库，且从库部署的机房需要与主库不同，从而保障数据库具备跨机房灾备的能力。此外，对于测试环境的数据库依然要配置主从复制，防止某天测试环境的数据库磁盘损坏，需要耗费大量人力恢复测试环境。原则二：在能够完成功能的前提下，使用的 SQL 要尽可能简单。因为 SQL 和代码一样，除了完成功能之外，最重要的是清晰简单地表达其自身含义，以供后续研发人员进行维护。我曾经在线上遇到过为了不使用唯一索引，纯使用 SQL 来完成防重的语句，它包含了四层 insert、select、exists、select 的语法嵌套。这一语句因为无法调试（Debug），导致后续一个需求的上线时间延期了 2 天，最终还是痛定思痛地进行了重构。原则三：在业务需求不断更新迭代的场景里，最好不要使用外键。大学时期的数据库理论课曾提到，需要使用外键来校验数据完整性。比如，在 A、B 表之间有了外键约束之后，可以设置外键级联删除，当 A 表中的某条数据删除后，自动级联地删除 B 表中的数据。此方式表面上可以极大地简化代码操作，但实则隐藏着巨大风险。因为现今互联网需求的迭代速度非常快，上个月可能 A、B 表中还存在外键关系，到了下个月又因为需求不存在了，或者需要更多字段组合才能形成外键关系。此外，外键关系是隐藏在数据库的建表语句里的，在新需求开发时，很容易被遗忘、清除或者修改为新的外键关系。在新需求上线后，也可能因此疏漏导致线上数据被误删，进而引发线上问题。原则四：表结构中尽可能不要创建一个长度为上千或上万的 varchar 类型字段，且用其来存储类似 JSON 格式的数据，因为这会带来并发更新的问题。假设创建了一个长度一千的 varchar 字段，它存储了如下的信息：{ &quot;fieldA&quot;: &quot;valueA&quot;, &quot;fieldB&quot;: &quot;valueB&quot;}此时假设有两个请求同时对此字段进行修改，A 线程将此字段的值读取后修改了其中 filedA 的值，具体修改如下：{ &quot;fieldA&quot;: &quot;valueA&quot;, &quot;fieldB&quot;: &quot;valueBB&quot;}那么，最终数据库中此字段的值会变成什么呢？答案是不一定。这取决于 A、B 这两个线程的最终修改顺序。但不管顺序如何，最终的结果都是错误的。因为 A、B 两个线程各修改了JSON 内容的其中一个字段，最终期望的结果是 fieldA、fieldB 两个字段都得到更新，但实际只会有一个字段得到更新。因此，在创建表结构的时候，不建议设置此类型的字段。如果期望这两个字段都得到更新，你需要借助并发锁来实现，但这也增加了代码实现的难度。对消息中间件的依赖在微服务的架构里，微服务间的通信除了接口调用的方式外，当前最常见的方式便是基于消息中间件（如 RabbitMQ 和 Kafka）的消息通信。同样，在使用消息中间件时，仍有一些基础原则需要你尽可能地遵守。原则一：数据要先写入数据库或缓存后，再发送消息通知。因为很多消息接收方在接收到消息通知后，会调用发送消息的微服务的接口进行数据反查，以便获取更多信息来做下一步业务的流转。假设订单模块在判断用户的下单请求的库存能够满足后， 便向外发送下单成功的消息。此时，如果物流系统监听了此消息，就会在获取到下单成功的通知后，第一时间去反查订单的接口，以便获取更多订单相关信息（如用户期望的收货时间、用户是否为会员等）来辅助判断何时发货。在极端情况下，可能会因为订单模块的数据还未写入数据库，导致反查不到数据，进而影响业务的正常流转。原则二：发送的消息要有版本号。有些消息中间件为了提升消息消费的吞吐量，支持乱序消费。但如果发送的消息没有数据变更版本号，消息消费方会因此无法判断数据是否乱序，进而有可能导致数据错乱，产生线上问题。原则三：消息的数据要尽可能全，进而减少消息消费方的反查。微服务间使用消息通信的目的就是解耦，但如果消息中包含的信息量太少，消息消费方就无法基于其中的信息处理业务，此时消息消费方便需要反查发送方的接口，来获取更多信息，但这样处理就达不到解耦的目的了，你可以参考第一点物流系统的案例。因此，在可能的情况下，建议发送尽可能全的信息。原则四：消息中需要包含标记某个字段是否变更的标识。根据原则三，你可能会发送包含较多字段的消息，有些字段可能在当次消息中并未发生数据变更。如果没有标记字段是否变更，可能会产生无效通知的情况。比如一个消息包含两个字段（如为 A、B），而某一个消息的接收方（如用户模块）只关心 A 字段是否变更。如果没有标记变更字段，那么 B 字段变更后，消息发送方也会发送消息，这会导致“用户模块”误以为 A 字段发生了变更，进而触发“用户模块”执行一次本不应该执行的业务流程。总结采用微服务架构后，不可避免的分布式事务的解决方案，同时介绍了微服务常见的依赖：数据库、消息中间件的基本治理原则。 “我曾经在线上遇到过为了不使用唯一索引，纯使用 SQL 来完成防重的语句”这个之后的方案是啥? 加唯一索引了? 缓存前置判重。简单场景是可以使用唯一索引判断的。而如果标识一条数据唯一的条件会不断变化，比如经过业务的发展，会从两个字段组合标识一个字段，变成三个字段组合标识一个字段，这个时候，唯一索引就需要调整。对于需要频繁变更的唯一性索引，可以考虑在程序上兼容解决" }, { "title": "做好自己 - 设计防止宕机的微服务", "url": "/posts/microservice-prevent-downtime-18/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-05-02 15:33:00 +0000", "snippet": "做好微服务自身的设计和代码编写的常见手段。CPU 被打满的排查手段一台机器上会部署一至多个进程，它们可能是一个或多个业务应用进程和多个其他工具类进程（比如日志收集进程、监控数据收集进程等）。大概率导致机器 CPU 飙升的是业务应用进程，但仍需准确定位才可得出结论。 在 Linux 系统里，可以使用top 命令进行定位， top 命令可以按进程维度输出各个进程占用的 CPU 的资源并支持排序，同时还会显示对应的进程名称、进程 ID 等信息。 根据排序，便可以确定占用 CPU 资源最高的进程，但此时仍然不知道是哪段代码导致的 CPU 飙升。所以可以在此进程基础之上，做进一步的定位。top 命令支持查看指定进程里的各线程的 CPU 占用量，命令格式为：top -Hp 进程号。通过此方式便可以获得指定进程中最消耗 CPU 资源的线程编号、线程名称等信息。 假设导致 CPU 飙升的应用是基于 Java 开发的，此时，便可以通过 Java 里自带的 jstack 导出该进程的详细线程栈（包含每一个线程名、编号、当前代码运行位置等）信息，具体见下方示例代码： &quot;thread name&quot; prio=0 tid=0x0 nid=0x0 runnableat java.net.SocketInputStream.socketRead0(Native Method)at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)at java.net.SocketInputStream.read(SocketInputStream.java:171) 通过第三步定位的线程号和此步骤生成的线程栈，可以精准确定是哪行代码写的有 Bug，进而导致进程的 CPU 飙升。上述分析我排查 CPU 被打满常用的排查手段。 如何预防故障部署层面首先，微服务及存储需要双机房部署。双机房部署能够进一步提升服务的容灾能力。双机房部署的架构如下图 1 所示：上述部署里，同一个微服务分别在两个机房各部署了两台机器。在存储上，数据库的主从分别部署在两个机房里。当出现机房级别的故障，如网络不通时，可以直接将故障机房的机器从微服务的注册中心摘除。其次，如果故障发生在主库所在机房，就需要 DBA 进行协助，对主从数据库的数据对比、订正并进行数据库的主从切换。双机房部署使得微服务具备了机房级别的容灾能力，当机房出现故障时，可以快速地进行切换，而不用耗费几个小时甚至更久的时间，在一个新的机房进行微服务和数据库的重新部署。但上述的部署里，数据库其实是单机房部署的。因为在实际运行时，只有主库承载读写流量，从库只是跨机房进行数据复制，作为灾备使用。当真正出现机房故障时，整个微服务仍需停服一定时间，用来等待 DBA 进行主从切换，原则上只在秒级或者分钟级别。这在绝大部分场景里均可满足业务的需要，但有些用户使用高频的场景，如打车、即时通信等软件，需要业务尽可能 7*24 运行，减少或保障不出现业务停服的场景。对于此类需求，可以采用存储按机房多地部署、且每个机房的存储均支持部分用户的数据读写的方案进行升级，此方式有个特有名词，叫作单元化部署的架构，具体架构如下图 2 所示：在单元化架构里，两个机房里的数据库均为主库，它们都承载读写流量。对于用户的请求流量，在网关层进行了转发，一部分转发至机房 A，另外一部分转发至机房 B。假设当机房 A 出现故障时，机房 B 所承载的流量是完全不受影响的，即路由至机房 B 的用户对于故障无感知。而对于机房 A 里的用户，则可以在网关层进行前置再路由，将所有的请求全部转发至未故障的机房 B。在上图 2 中，有一条两个机房里的数据库主库互相同步的标识线，它是单元化里需要构建的数据同步模块。作用是发生故障时，减少机房 A 里的用户切换到机房 B 的时间。因为机房 A 里的用户可以切换到机房 B 的前置条件是，机房 A 里的数据已经全部同步至机房 B 里，实时的数据同步可以减少故障后 DBA 进行数据同步、对比和校准的时间。可以看到，单元化架构并不是机房故障后，对于业务完全无损，而是保障一部分用户完全无损来提高高可用能力。其次，机房内至少部署两台及以上机器。 上述第一条要求至少双机房部署，并不是两个机房各部署一台机器即可，而是要在同一个机房里至少部署两台机器，保障机房内机器互相灾备。此方式可以防止当某一台机器故障后，出现整个机房全部失联，进而将调用方的所有的流量都打至另外一个机房，引起请求的性能和稳定性下降，因为跨机房的请求的网络传输时间更长。单机房部署单容器故障时导致的跨机房调用如下图 3 所示，可以看到故障后，调用方的所有流量全部都路由至被调用方的单个机房里。再者，不同类型的接口需要单独部署。读服务的特点是调用次数特别大，对于性能要求高。写服务的特点是对于稳定性要求特别高，调用次数相比读服务会低很多。假设在微服务拆分时，在垂直拆分时没有按读写分离的方式将读和写服务拆分开，而是将代码编写在同一个工程里。那么部署的时候，可以将将二者的接口拆开部署，拆开后的结构如下图 4 所示：隔离开单独部署主要有以下几点考虑。 写服务对于稳定性要求较高。隔离后，读服务里因为代码 Bug 等因素导致的机器 CPU 飙升、内存占满等问题不会影响到写服务的性能和稳定性。 其次，读服务调用量较高，对于机器 CPU、内存、网络等占用也较高。隔离后，写服务将独享机器资源，性能和稳定性也较好。 最后，微服务的执行线程是根据机器的 CPU 提前设置好的，大小是固定的。读写混合部署时，读请求很容易将微服务框架的执行线程沾满，导致线程枯竭，进而导致写请求得不到执行。此时，通过隔离部署也可以解决此问题。最后，至少线程池隔离。 在某些时候，可能读服务的调用次数并不是特别大或机器资源有限，实现不了上述的纯机器隔离。此时，可以实现一个简版的隔离，即微服务框架的执行线程池隔离。现在主流的微服务框架都支持对于接口单独配置一个执行线程，这样在执行时，就可以做到线程池资源隔离，互不影响，具体架构见如下图 5 所示。在某些无法完成机器隔离的场景里，可以使用此方式实现一定程度的资源隔离。代码层面有很多编写优雅、易阅读、易维护代码的技巧，主要聚焦如何编写避免系统故障的编码准则。主要包含以下几点。第一，不要基于 JSON 格式打印太多、太复杂的日志。假设有一个特别复杂的类，其中包含了几十上百个字段，同时某些字段也是对象类型，该字段又嵌套了很多对象字段。如果在日志输出时，直接将该类通过 JSON 进行序列化，并进行日志输出，伪代码格式如下：复杂Object obj=new 复杂Object();logger.info(JSON.toJson(obj));如果每一次请求，微服务的代码都会按上述格式打印日志，那么当调用量稍微上升时，很容易将微服务的 CPU 占满，进而导致服务宕机，主要原因是：复杂的对象在序列化时非常消耗 CPU 资源。采用 toJson 方式序列化大对象，很多时候因为简单、粗暴，不需要太多开发量，被大量、广泛地使用，与此同时也带来了系统宕机的风险。两害相较取其轻，建议采用如下按需的方式输出日志，规避宕机风险。logger.info(&quot;内容1:{},内容2:{},内容3:{}&quot;,具体内容1,具体内容2,具体内容3);第二，需要具有日志级别的动态降级功能。假如上述按需输出日志的方式还没有被大家广泛接受，还是习惯使用 toJson 的方式输出日志。那么为了防止打印日志导致机器宕机，需要在日志输出前进行级别判断，使得当日志打印导致机器出现问题时，通过此方式可以将日志进行关闭。具体写法如下：if(logger.isInfoEnabled()){ 复杂Object obj=new 复杂Object(); logger.info(JSON.toJson(obj));}当 toJson 的日志打印把 CPU 占满之后，可以将日志级别调整为更高等级，比如 error 级别，禁止日志输出即可规避问题。此外，更进一步的是，此日志级别调整可以开发动态功能，结合配置中心，动态的修改日志级别，可以实现不重启应用即可生效日志级别修改的功能。第三，for 循环不要嵌套太深，原则上不要超过三层嵌套。实践中，for 循环迭代的数据是从数据库或远程 RPC 获取的，获取到的数据量是动态的，可多可少，极端情况下可能多达上千条。此时，三层嵌套下的时间复杂度则为：O(10003)=10 亿。上亿次的代码执行，分分钟就会把微服务打挂，建议在代码编写时，规避此种写法。第四，多层嵌套需要有动态跳出的降级手段。假设业务上无法规避上述的多层嵌套，在实现时，在嵌套内部开发主动跳出的降级开关。当上述数据量增多时，此方式可以通过开关主动地跳出嵌套，防止机器宕机。第五，如果使用应用内的本地缓存，需要配置容量上限。如果不显式地配置本地缓存的容量上限，有可能因为容量暴涨，导致进程 OOM。因此，需要根据机器的内存大小，显式地配置本地缓存的容量上线。" }, { "title": "防备上游 - 设计可靠的 SDK", "url": "/posts/Design-Reliable-Sdk-17/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-05-01 15:33:00 +0000", "snippet": " 维基百科对于微服务的定义：微服务是指通过技术语言无关的协议（如 HTTP、ProtoBuf等）向外提供业务服务（常以接口的形式）的独立进程。它具有规模小、支持异步消息通信、可独立部署，以及可实现构建和分发自动化的特点。结合上面的描述和前几模块里讲解过的架构图，便可以得到一个微服务所包含的内容，具体由以下 6 个部分组成： 对外暴露的接口，由它直接对外提供各类业务服务能力； 消费其他微服务发送过来的消息； 可独立部署的微服务的代码； 微服务持久化数据所依赖的数据库、缓存等存储； 微服务完成一项业务能力需要依赖的其他微服务，比如提供提单的微服务就需要依赖库存微服务的扣减接口; 微服务对外也会发送消息，来完成微服务间除接口以外的通信。基于上述介绍的微服务中涉及的几大组件，可以将它们进行归类，梳理出如下图 1 所示的架构：上述微服务里提到的 6 个组件分为三大类： 第一类为对外提供的接口和接收的外部消息，称为上游； 第二类为微服务本身； 第三类为微服务依赖的其他组件，称为下游（如存储、其他微服务接口等）。把这个分类和前几模块里提及的各种架构图对比，你会发现它们都包含上述三大类中的全部或部分内容，比如有的微服务依赖存储，有的依赖其他微服务。在构建高可用微服务时，可以从上述三大类微服务进行入手。日常工作中，我们团队就是按此思路，对微服务的上游、自身以及下游（外部依赖）进行设计，以便构建一个更加健壮的微服务。构建微服务三大类部分的高效法则（技术顺口溜）：防备上游、做好自己、怀疑下游。为什么说 SDK 是一锤子买卖微服务对外是以接口形式提供服务的，当接口开发完成上线，运行一段时间之后，形成的全局架构如下图 2 所示：从上述的架构图里可以看到，接口上线后外部使用方会不断增多。假设上述外部调用方使用的某一个接口里的某一个方法的格式如下：void func(long args1,int args2)此时，因为新的业务需求，需要对该接口的上述方法的名称和入参数量进行变更。修改后的格式如下：void func_new(long args1,int args2,long args3)如果要改成上述格式，不能直接升级，因为上述两个格式不兼容。如果先上线，所有的调用方都会报错，因为接口名和方法个数都变了。上述接口需要提供灰度过程，大致如下： 在微服务里同时提供上述两个接口； 推动所有的外部调用方切换到修改后的接口； 确认老接口没有调用量后，方可将老接口下线。从表面来看，只需要三个步骤就可以完成灰度发布过程，但上述第二步操作所需的时间远远超乎你的想象。如果调用方很多，推动所有调用方完成切换的时间短则几周，长则需要半年或者更久，成本非常高。所以，定义新的接口时需要考虑未来兼容性，如果接口上线后再想要修改，则需要花费较高的成本。因此，包含一个微服务所有对外接口的 SDK 是一锤子买卖，设计时需要考虑清楚。如何设计稳固的 SDK因为 SDK 一旦上线后，修改成本会非常高。因此在设计 SDK 时，有一些基本原则需要遵守，减少上线后的维护成本。第一个原则：增加接口调用鉴权当微服务对外提供的接口上线后，理论上所有需要此接口功能的使用方都可以随意调用此接口，微服务的提供方不应该设计鉴权等手段限制调用方的使用。考虑如下场景后，可能这种想法会稍微改变。 你的接口当前能够支持的最大 QPS 为 1W，而新的调用方会带来每秒 10W 的 QPS。如果这个新的调用方在你还没有完成扩容前，就直接上线，导致的结果可能是你的微服务被瞬间打挂。 接口的入参有一个 Map 字段，文档未有明确标注，但实际此 Map 字段最大支持 100 个 Key 的设置，如果超过 100 个 Key 就会报错。因为没有调用前的申请审批，新接入的调用方的场景里有可能会传入 150 个 Key，导致的结果是直接报错，进而可能产生线上问题。 SDK 提供了查询和写入的接口，但查询的接口是基于缓存或 ElasticSearch 实现的，是有毫秒级延迟的。而使用方期望写入后，通过查询接口可以立马查询到数据。如果新的接入方没有前置的审批沟通，直接接入后，会发现接口和预期并不一致，可能会使得此次接入变成无用功，导致成本浪费。通过增加鉴权，所有的调用方在使用前都需要申请接口调用的权限，在申请的过程中，你可以针对上述提到的问题和调用方一一进行确认，防止出现意外的情况。第二个原则：接口里的入参需要是对象类型，而不是原子类型原子类型是指非面向对象里的类，在里面不能再定义字段的类型。比如编程语言里的 int、long、float 等类型。对象类型是指面向对象里的类，比如如下格式：class ObjectA{ private long args1; private int args2;}对象类型的好处是当有新的需求时，可以在其中新增字段，而不是修改接口的签名。在上面总结了 SDK 是一锤子买卖的示例，如果原始接口定义的是如下格式：void func_new(ObjectA object1)当一个新的需求需要在入参增加 args3 字段时，便可以直接在 ObjectA 这个类里添加，而不是修改接口的签名。这样设计的好处是向后兼容，只有此次新需求需要使用 args3 字段的调用方才需要升级，而不关心此字段的历史调用方都不需要升级，可以节约推动外部所有客户升级的时间。第三个原则：接口的出入参不要设计为 Map&amp;lt;String,String&amp;gt; 等集合格式出入参使用了 Map 格式的设计如下：Map&amp;lt;String,String&amp;gt; func_new(Map&amp;lt;String,String&amp;gt; args);这样设计的好处是特别灵活，当接口在日常的升级中需要新增一个字段，如第二个原则里提到的，新增 args3 字段时，整个接口都不需要做任何更改。因为 Map 的 Key 是动态的，可以随意由外部客户传入的。虽然这样设计有灵活性的优势，但劣势也比较明显。 首先，代码非常难维护。因为 Map 里的 Key 是动态的且是文本的，要识别这些 Key，你需要在代码里使用魔术数或者硬编码进行识别。随着时间的流逝，这种方式会导致代码里随处可见的硬编码，代码阅读起来非常不直观。 其次，Map 的方式是动态，理论上调用方可以往 Map 中插入成百上千的数据。极端情况下，这些数据会把微服务的内存瞬间打挂，对系统的稳定性影响非常大。第四个原则：入参需要增加条件限制和参数校验可以分别对读和写接口进行分析。首先，对于对外暴露的写接口，如果不增加参数校验，可能会导致后续业务无法正常流转。 外部调用方可以传入超过数据库长度限制的参数，有些数据库会直接拦截，并生成数据超长的错误，而有些数据库可能会默认地将数据截断并存储。 对于如手机号、邮箱地址等自带业务格式的数据，如果不做格式拦截，将不符合格式的数据写入数据库之后，后续的业务可能无法流转。比如订单里的收货人的手机号码，如果写错，可能导致订单无法正常配送。其次，对于对外暴露的读接口，如果不增加参数校验，可能会把数据库打挂。 如果你提供的一个翻页查询功能，常见的查询是使用数据库的”limit startIndex,size order by xx 字段“来进行实现的。如果你不进行参数验证，理论上调用方可以传入值为 100000 的 startIndex。实际上，随着 startIndex 的增大，limit 的性能会非常差，极端情况下，如果量太大，数据库很容易挂。 如果你提供了如 like 等模糊匹配功能，如果外部传入一些正则表达式里非常耗费性能的语法，也是有可能把数据库打挂的。第五个原则：写接口需要保证幂等性考虑一种场景，如果外部客户调用你的接口超时，它能如何处理？答案是：只能进行重试或者反查，不然别无他法。因为超时后，调用方并不知道此次写入是否成功，有可能成功，也有可能不成功。通过反查调用方可以确定此次调用是否成功；通过重试，调用方期望你告诉它，上次写入已经成功，无须重试。上述的反查和重试，技术上称为幂等性。写接口的幂等可以在入参增加一个当次调用的全局唯一标识来实现，同时该唯一标识需要写入数据库中，并在数据库里将该字段设置为唯一索引即可。架构如下图 3 所示：第六个原则：接口返回的结果需要统一，可以直接抛出异常或者使用结果包装类（如 RPCResult）对外的 SDK 会包含一组接口，这些接口对外返回的格式需要保持统一，要么全是正常业务对象+异常的格式，要么全是通过 RPCResult 包装业务对象的格式。这两个格式没有绝对的优劣之分，但统一的格式有利于调用方统一处理，两种格式混合的方式会增加调用方的处理成本。显式抛出异常的格式如下：Object func_new(Object args1) throws RPCException其中 RPCException 中需要包含如下字段：class RPCException{ private boolean success;//是否成功 private int code；//如果错误，详细的错误码 private String msg;}使用 RPCResult 包装类的格式如下：RPCResult&amp;lt;Object&amp;gt; func_new(Object args1)其中 RPCResult 中需要包含的字段和上述 RPCException 需要包含的格式基本一样，此处不再赘述。可以看出，这两种方式中包含的错误信息基本一致。唯一的区别是：异常的方式除了会包含上述信息外，也会包含一些报错的堆栈信息，如下格式：&quot;thread name&quot; prio=0 tid=0x0 nid=0x0 runnableat java.net.SocketInputStream.socketRead0(Native Method)at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)at java.net.SocketInputStream.read(SocketInputStream.java:171)因为 RPCException 里已经包含当次请求是否错误，以及导致错误的详细原因，即其中的错误码（code 字段），此外异常的堆栈信息是为了方便微服务的提供方进行问题排查，调用方无须关心。因此，在实际开发中，显式地把 RPC 中抛出异常的堆栈信息屏蔽掉。现在主流的编程语言均已提供上述功能。最后，不管是 RPCException 还是 RPCResult 里都包含的错误码，即 code 字段，这样做是为了方便调用方能够快速知道导致出错的具体原因，进而根据不同的原因做相对应的处理。比如在有些情况下： 调用方传入的参数不合法，如电话号码传入了字符，导致检验不通过； 微服务提供方依赖的存储故障，如缓存、数据库等宕机等，进而导致当次调用产生错误。当出现上述两种错误时，对应的处理方式是不一样的。 如果是传入的参数格式错误了，你需要提示客户修改格式重新输入，而不需要联系此微服务的提供方进行处理。 如果是上述第二种错误，你需要立马通知微服务提供方，让对方尽快修复故障，因为下游出现错误，你能做的便是尽快通知。在实际实践中，使用 RPCException 还是 RPCResult 其实都可以，只要保持统一即可。不过不管格式如何，上述两个对象都需要包含上述字段。第七个原则：返回的数据量必须要分页如果存在以下格式的接口定义，它表示此接口的功能是返回一批数据：List&amp;lt;String&amp;gt; func_new(Object args1);如果接口的入参里没有显式地设置当次查询数据的具体数量，假设当次查询条件命中的数据量非常多，那么一次返回的数据量就会非常多，可能达到上千 KB 或者上百 MB 的数据。上述这个批量获取数据的接口，如果不分页，会存在以下两个问题： 获取这么大量数据的查询条件，在查询的时候，可能会把数据库或缓存打挂； 数据量越多，网络传输的时间也越长，直接的体现就是接口的性能非常差。因此，建议所有对外批量接口都增加分页，而不是一次吐出所有数据。这样既可以提升稳定性、又可以提升性能。第八个原则：所有的接口需要根据接口能力前置设置限流最后，即使经过上述的几个步骤后，仍有可能一个通过鉴权审批后的调用方，它的系统在某一个时间点出现故障，或者因为一些热门活动导致流量出现飙升，假如这个突发流量超过你的微服务的最大承载量，即使遵循了上述的第一个原则：调用前的鉴权，也无法限制通过鉴权后的调用方带来的突发流量。对于可能产生的异常流量，可以使用前置限流策略来预防。消息的消费消息消费指的微服务接受其他微服务发送的消息的场景，在实践中梳理时，此场景的高可用容易忽略。这里的消息是异步的形式，它和微服务间的同步调用架构如下图 4 所示：如果消息消费和接口调用相类似，那么上述的一些原则在消息里依然可以复用，可以参考以下内容。 消息消费需要有前置限流。当消息发送方发送量暴增时，限流可以保证消息消费服务的稳定。 对于消息消费需要保证幂等，不然当消息出现重试后，会出现业务上的脏数据。 消息数据在消费处理时需要进行前置参数检验。如果未做前置参数校验，同样也有可能写入一些不合法的脏数据。总结本文整理总结了对外接口的设计准则，即防备上游！具体手段无外乎对上游调用方进行鉴权、限流、入参前置校验与拦截等。" }, { "title": "保证热点扣减命中的存储分片不挂", "url": "/posts/hot-data-DeductionService-16/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-04-26 07:33:00 +0000", "snippet": "热点扣减的典型业务场景热点扣减有一个被熟知的名称，叫作秒杀。其实，秒杀并不等同于热点扣减，只是因为商品秒杀是热点扣减里最具有代表性、也最能体现热点扣减特点的场景，所以常常以秒杀代指热点扣减。秒杀的特点主要有以下两点。 首先，秒杀带来的热点量非常大，其他热点场景很难比拟。比如，在刚过去的 2020 年，大家在电商平台里准点抢购口罩，上百万人同时在线抢购同一商品，此时就带来了超大并发量。 其次，秒杀对于扣减的准确性要求极高。秒杀在绝大部分场景里是一种营销手段，如一元抢 iPhone。商家对有限的商品设置一个亏本价，吸引用户下载或注册 App，达到拉新、提升知名度等目的。因为是亏本营销，如果出现了大面积的超卖，业务上是绝不允许的。除了秒杀之外，其余的扣减场景，如账户金额的扣减、收费文章免费试读次数的扣减等场景，均很难同时满足上述两个要求，所以它们不是热点扣减的代表性场景。技术挑战因为需要保障高可靠的扣减，在应对秒杀时，可以在【缓存+数据库】的方案基础上进行升级改造。结合【如何应对热点数据的查询】总结的关于热点查询的分析内容，在面对热点扣减时，整个架构图和对应的存储命中如下图 1 所示：秒杀与热点扣减所带来技术问题是一样的——所有的热点请求均命中同一个存储分片。那为什么不能直接复用【06】介绍的“通过增加缓存副本以及使用本地缓存”的方式来应对呢？具体原因，如下： 首先，扣减是写请求，即每一次请求都会修改当前商品对应的总数量，且当商品数量扣减为零或当前剩余商品数量小于当次要扣减的数量时，均会返还失败。 而热点查询里的缓存副本或者本地缓存里的商品数量均是原始分片的数据镜像，不能被拿来进行扣减的，否则就会出现数据错乱，甚至超卖的现象。对应的架构示图如下图 2 所示： 其次，本地缓存里的数据是非持久化数据，易丢失。即使将本地缓存持久化至宿主机的磁盘，也会因磁盘故障、不满足 ACID 等原因而导致数据丢失。 应对秒杀流量既然不能采用热点查询里的方案，只能使用缓存单分片来应对秒杀的流量，但单分片能够支持的流量是有上限。当流量超过上限后如何处理呢？可以从秒杀的业务上进行分析，会发现虽然秒杀带来的热点扣减请求非常大，但每次参与秒杀的商品数量是有限的，可能就几百个或者上千个，而热点扣减的流量可能达到上百万。通过简单地计算可以得出，秒杀到商品的概率只有 0.1%，其中 99% 的扣减请求都是“陪跑”的。这些“陪跑”的请求对于使用者来说可能只是一次简单的点击，但很可能会把正在运行的扣减服务打挂。此时，可以对这些瞬间量非常大的“陪跑”请求进行一些前置处理，降低“陪跑”请求的瞬间请求量，或者降低它们对于系统的冲击，此方式就叫作流量削峰。体现在流量监控上如下图 3 所示：快速实现流量削峰第一步进行的削峰是，先做恶意用户拦截。 上述描述提到过，秒杀通常是基于低价商品的营销活动，抢到商品后转售会有很大的盈利空间。因此，秒杀会吸引来大批的黄牛和黑产用户，对于这些恶意用户可以基于以下几种方式进行拦截： 基于用户维护设置限制。比如同一个账号在 5 秒内最多可以请求扣减多少次。超过该次数，便进行拦截，直接返回失败信息给到商品页面，显示暂时无货。通过此类方式，可以拦截黑产跳过系统界面，直接调用对外暴露的 HTTP 形式的扣减接口所产生的瞬间爆点流量。 基于来源 IP 设置限制。有些黄牛会提前预申请很多账号，因此使用上述账户限制的方式并不能完全拦截住。在账户的基础上，可以对用户的来源 IP 设置限制。比如 5 秒内，同一个 IP 最多可以请求扣减多少次。 除了上述方式外，还有很多其他方式可以识别用户，比如现在每一个手机以及电脑都有唯一编码，如手机的 IMEI、电脑的网卡地址等。可以在限制账号、IP 之外，再增加对这些维度的限制。上述提到的拦截在实现上，可以采用比较成熟的漏桶算法、令牌桶算法。这两个算法在网络上有很多介绍，这里不再赘述。此外，现在有很多开源工具包提供了这两个算法的实现，比如 Java 里的 Guava 包就提供了开箱即用的实现。采用限流算法的架构如下图 4 所示：限流在实现上有两种方式，一种是集中式，另外一种是单机式。集中式是指设置一个总的限流阈值，并将此值存储在一个单独的限流应用中。所有的扣减应用在接收到请求后，均采用远程请求此限流应用的方式，来判断当前是否达到限流值。它的架构如下图 5 所示：集中式的限流方式最大的好处是设置简单，当对整个扣减应用的集群进行极限压测后，得到了极限值。便可以基于此值，设置集群的限流阈值。但这种限流方式也带来了一些问题： 首先，调用远程限流服务会增加一次网络消耗，这也降低了扣减服务的性能； 其次，远程限流服务提供的限流功能并不精确，因为调用远程的扣减服务会消耗一定的时间，在这个时间区间里，可能会有大批量的热点并发涌入扣减应用，瞬间就会击垮扣减服务； 最后，如果所有的请求都要经过限流服务，如何保障限流服务高可用以及能够高效应对热点也是一个难点。单机式限流是指将上述提到的限流阈值在管理端配置后，主动下发到每一台扣减应用中去，它的架构如下图 6 所示：单机式限流是将限流器内置到扣减应用内，可以规避上述集中式限流出现的问题，但它也会带来其他问题： 首先，每台机器的限流值需要根据机器的数量实时计算，并将计算后的限流值下发到每台应用机器里，同时更新扣减应用内的限流器； 其次，对于扩容的机器需要初始化对应的单机限流器。在实际的应用中，推荐采用单机维度的限流器，因为它会更加精准和实时。第二步进行的削峰是，业务层面需要设置权重等级。 秒杀是一种营销活动，营销是有目的的，比如激活许久未下单用户，或者优先让会员抢到商品，增加会员的续费意愿等。在秒杀接口实现时，可以根据业务规则配置相对应的优先级过滤一些低等级的用户。比如设置高与低的优先级比例为 10：5，它表示在一个时间区间内（如 5 秒），处理 10 个高优先级（如会员用户）的扣减请求时，最多才能处理 5 个低优先级的请求。在实现上，可以使用令牌桶算法，高低优先级各配置一个令牌桶，高优先级的令牌桶数量为 10，低优先级的设置为 5 即可。第三步进行的削峰是，增加一定的过滤比例。 如果上述两个方式过滤后，热点扣减的并发量仍然较大。可以设置一个固定比例，如 10% 的请求前置过滤并直接返回失败消息，告知用户“抢购火爆，请稍后再试”，也可以降低一部分无效请求。过滤比例可以根据预估流量和秒杀商品的库存进行设置，如预估流量 50W/S、实际商品库存只有 10 个，那么抢到商品的概率只有 0.002%，抢不到的概率为 99.998%，只要设置过滤率小于抢不到的概率即可。第四步进行的削峰是，兜底降级不可少。 即使做了上述的限流措施后，流量仍有可能超过【14】方案里的单分片的承载最大值，此时，可以从技术层面上增加限流阈值。首先对缓存的单分片进行压测，得到单分片能够承载的最大值，这个最大值乘以 50% 或者 60% 即可得到缓存单分片线上能够实际承载的最大流量值。之所以要乘以一定比例获得实际承载最大值，是因为在压测时，被压测的缓存单分片的各项指标（如 CPU、网络等）均已达到极限值，系统处在宕机的边缘了。为了保证系统稳定，线上环境的限流值不能设置为此极限值，只能进行一定的折扣。有了单分片的最大承载值，才可以做最后一步的兜底，兜底架构如下图 7 所示：在部署的所有扣减应用里，通过上图中编号为 0 的配置中心推送每台机器需要负责的每个缓存分片的限流值（单分片最大承载值/扣减应用机器数），在扣减应用中，按上述推送值，给每一个缓存分片设置一个限流器。此方案需要扣减应用和缓存中间件有一定的耦合性，即扣减应用需要判断当前请求隶属于哪一个缓存分片。实现上，具体隶属于哪个缓存分片，可以通过缓存中间件提供的路由算法工具来计算。获取到分片标识号后，就可以获取到此标识对应的限流器，然后再进行限流即可。通过上述方式，即使出现流量超预期，兜底策略既保障了秒杀业务可正常运行，同时又保障了系统不会被打挂。最后进行的削峰是，售完的商品需前置拦截。 秒杀商品会在瞬间售完，后续所有的请求都会返回无货。对于已经无货的商品，可以采用【06 】里的方案，将商品已经无货的标记记录在本地缓存里。在秒杀扣减前，先在本地缓存进行判断，如果无货直接返回即可。架构如下图 8 所示：水平扩展架构升级通过上述几种限流的组合，便可以应对秒杀的热点流量了。但上述的方式会牺牲一定的用户体验，比如按一定比例过滤用户请求、按缓存分片维度过滤用户请求等。我们可以在上述方案的基础上，做一定的升级来减少有损体验。升级后的架构如下图 9 所示：上述架构里，在设置秒杀库存时，将秒杀库存按缓存分片的数量进行平均等分，每一个缓存里均存储一等份即可。比如某一个商品（记为 SKU1）的秒杀库存为 10，当前部署的缓存分片共计 10 个，那么每一个分片里存储该 SKU 的库存数可以为 1，存储在各个缓存里的 key 可以为：SKU1_1、SKU1_2、…、SKU1_10。在处理秒杀请求时，不只是固定地命中某一个缓存分片，而是在每次请求时轮询命中缓存集群中的每一个缓存分片。将秒杀商品的库存前置散列到各个缓存分片，可以将原先热点扣减只能使用一个缓存分片升级至多个，提升吞吐量。但此方式有一个弊端，就是更加的定制化。其他手段除了上述介绍的手段之外，还有几个方式可以应用在秒杀场景里。首先，前端静态资源前置。 在秒杀开始之前及在秒杀中，焦急的用户会不断地刷新页面，判断秒杀是否开始，避免自己错过开始时间。刷新秒杀页面其实是热点查询的功能，可以借鉴【06】的方式采用应用内的前置缓存解决。对于前台页面上涉及的静态数据，如 JS、CSS、图片等，可以使用 CDN 来提升性能，具体架构如下图 10 所示：其次，业务上隔离。*秒杀与正常的购物是有区别的，它是短时间内抢购某一商品。在应对策略上，可以从根据其业务特点进行定制，降低系统的压力。正常的网上购物流程是用户先选购*多个商品，加入购物车后再提交订单并进行库存扣减。对于秒杀，可以定制它的前台页面，开发单独的秒杀页面。秒杀开始后，跳过添加购物车的过程，直接提交订单。这样设计，有几个好处。 跳过购物车再提单，增加了用户抢购到商品的概率，提升了用户体验。 业务流程跳过购物车，也降低了热点并发对于购物车后台系统的压力，提升了整体后台系统的稳定性。 秒杀商品直接提单时，就只会有秒杀这一个商品，这对于扣减应用的稳定性有极大的保障。一次扣减只有一个商品相比一次扣减有十几个商品，它在性能、网络带宽的消耗、对于扣减服务的资源占用（如 CPU、内存）等都有更大的节约。最后，部署隔离。 在完成上述业务隔离后，可以在机器部署上，更往前一步。对于秒杀所涉及的后端应用模块、存储均进行单独部署隔离。通过此种方式，可以更好地应对秒杀，此外也能够减少秒杀的热点并发流量对于原有扣减模块的影响。单独部署的架构如下图 11 所示：总结四种限流拦截策略，以及除了限流之外，可以实现水平扩展架构升级的方案。此外，还可以在部署架构、系统隔离、前端静态资源前置等方面进行升级改造来应对热点扣减。如果在秒杀抢购商品时，是否遇到过提示已经无货，后续稍等几秒又抢到的场景呢？" }, { "title": "数据库与缓存的扩展升级与扣减返还", "url": "/posts/Cache-Db-Extension-DeductionService-15/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-04-24 07:33:00 +0000", "snippet": "扣减中涉及的两个公共话题，分别是： 异步任务的设计； 扣减中的返还的设计。实现无主架构的任务对于无状态存储集群的数据同步任务，最简单的实现方式便是对于每一个分库启动一个自循环的 Worker，它的架构如下图 1 所示：自循环的 Worker 在启动时，会开启一个不跳出的循环或者借助一些开源工具（如 Java 中比较出名的 Quartz）来保证任务不间断执行。在上述的循环内，会使用类似如下的 SQL 来批量获取未执行的任务或未同步的数据并执行同步，在任务执行成功后修改任务状态为“完成”：SELECT task_id, task_body, ...FROM t_task WHERE id &amp;gt; lastId AND STATUS = &#39;未执行&#39; LIMIT 一批数量 ORDER BY task_id;上述的流程虽然在功能上能够满足需求，但在高可用及性能上还是有一些不足： 如果任务库中任务特别多，上述单 Worker 单库的方式不具备扩展性，随着任务不断变多，会出现任务积压的瓶颈且无法通过扩容解决； 单库单 Worker 的方式存在单点问题，在执行过程中，当 Worker 发生故障导致宕机，如果没有监控等机制发现故障，Worker 得不到执行，任务就会一直积压。对于上述两个问题，有一种可以提升任务执行速度，既具备扩展性、又能保障高可用的任务架构模式，如下图 2 所示：在上述的整体架构里，每个分库对应的 Worker 的执行流程都类似，因此，只对一个分库的 Worker 进行分析，其余的可以以此类推： 首先为了提升性能和高可用，单个分库的执行 Worker 配置的是多个并发进行执行； 单个分库配置的多个任务在执行时使用自助协调，协调流程如下： 每个 Worker 在启动时，会根据机器的 IP、随机数、当前时间戳等进行组合拼接计算一个唯一串，再在此基础上使用各种哈希工具计算一个无符号整形哈希值； 所有的 Worker 会将自己的无符号整形哈希值上报到强一致的 etcd 或 ZooKeeper 存储集群里； etcd 等集群具备通知功能（Watch）。借助通知功能，所有的 Worker 都去订阅某一个分库下的其他 Worker 的哈希值，比如一个新的 Worker 启动了或者扩容新增了一个新的 Worker； 每一个 Worker 都会获取到当前分库的所有其他 Worker 的哈希值。假设一个分库配置了四个 Worker，其中一个 Worker 会获取到自己及其他三个 Worker 的哈希值，假设为{200，300，500，800}。这四个 Worker 的 Hash 值便组成了一个环形区间，如下图 3 所示： 这个环形区间，类似一致性 Hash，每一个结点都代表一个 Worker，这个 Worker 负责任务编号在它区间范围内的任务的执行。 有了上述的哈希值列表后，就可以做任务分配了。如果当前 Worker 的哈希值为 300，那么当前 Worker 就处理任务 ID 在区间[200,300)里的值。比如哈希值为 200 的 Worker 则执行区间为[800，无穷大)和[0，200)的任务（即任务编号大于等于 800 和处在[0,200)区间内的任务），其他以此类推。区间处在[200，300)的 Worker 获取任务的 SQL 大致如下： SELECT * FROM task WHERE id &amp;gt;= 200 AND id &amp;lt; 300 AND STATUS = &#39;待执行&#39; ORDER BY id LIMIT 100; 通过上述方式，无论是某一台 Worker 发生故障还是新扩容一台 Worker，通过 etcd 和 ZK 的通知机制，所有的其他 Worker 都可以立马感知，并更新自己所负责的任务区间。 比如上述介绍的案例里，四个 Worker 代表 300 的那一个发生故障，整个哈希值列表就从{200，300，500，800}变成了{200，500，800}，此时负责 500 的 Worker 就会执行[200，500)这个区间里的所有任务了，扩容 Worker 的流程和上述类似。 最后，在 Worker 扩缩容的间隙里，可能存在临界的并发情况，即两个 Worker 可能获取到同一条任务。对于此问题，可以从两点着手解决： 首先，任务执行需要保持幂等，即任务可重复执行，这个可以从业务上着手实现； 其次，可以给任务增加状态，如上述 SQL 里的 status 字段。当某一个 Worker 处理到该任务时，可以去修改该任务为处理中。其他 Worker 在获取任务时，显式指定状态，只处理为待执行的任务即可。 如何设计和实现扣减中的返还另外一个公共话题的讨论，如何设计和实现扣减中的返还！什么是扣减的返还扣减的返还指的是在扣减完成之后，业务上发生了一些逆向行为，导致原先已扣减的数据需要恢复以便供后续的扣减请求使用的场景。以在购买商品时的扣减库存举例，其中常见的逆向行为有： 当客户下单之后，发现某个商品买错了（商品品类买错或是数量填错），客户便会取消订单，此时该订单对应的所有商品的库存数量都需要返还； 其次，假设客户在收到订单后，发现其中某一个商品质量有问题或者商品的功能和预期有差异，便会发起订单售后流程，比如退、换货。此时该订单下被退货的商品，也需要单独进行库存返还。返还实现原则从上述的业务场景里可以看出，相比扣减而言， 返还的并发量比较低，因为下单完成后发生整单取消或者个别商品售后的情况概率较低。比如，对于热点商品或者爆品的抢购带来的扣减并发量是非常大的，但抢到爆品后再取消订单的概率是非常低的。此种场景里，扣减和返还的并发量的差距可能会达到上万倍。因此，返还在实现上，可以参考商家对已有商品补货的实现，直接基于数据库进行落地。但返还自身也具备一些需要注意的实现原则，可以总结为以下几点。原则一：扣减完成才能返还返还接口在设计时，必须要有扣减号这个字段。因为所有的返还都是依托于扣减的，如果某一个商品的返还没有带上当时的扣减号，后续很难对当时的情况做出准确判断： 当前商品是否能够返还。 因为没有扣减号，无法找到当时的扣减明细，无法判断此商品当时是否做了扣减，没有做扣减的商品是无法进行返还的； 当前返还的商品数量是否超过扣减值。假设外部系统因为异常，传入了一个超过当时扣减值的数量，如果不通过扣减号获取当时的扣减明细，你无法判断此类异常。原则二：一次扣减可以多次返还假设购买的一个订单里包含了 A、B 两件商品，且这两个商品各买了 5 件，在产生购买订单时即对应一次扣减。后续使用过程中可能会对某件不满的商品发起售后退货申请。极端情况下，可能会发生四次退货的行为，如：第一次，先退 2 个 A；第二次，再退 3 个 B；最后一次退货，一起将剩余的 3 个 A 和 2 个 B 退回。由上述案例可以看出，一次扣减（即一个订单）在业务上可以对应多次返还。因此，在实现时需要考虑多次返还的场景。返还主要基于数据库实现，下面是支持多次返还的数据库表的设计：CREATE TABLE t_return { id BIGINT NOT NULL COMMENT &#39;自增主健&#39;,occupy_uuid BIGINT NOT NULL COMMENT &#39;扣减的ID&#39;,return_uuid BIGINT NOT NULL COMMENT &#39;返还的唯一ID&#39;,UNIQUE idx_return_uuid ( occupy_uuid, return_uuid ) COMMENT &#39;返还标识唯一索引&#39; } COMMENT &#39;返还记录表&#39;;CREATE TABLE t_return_detail { id BIGINT NOT NULL COMMENT &#39;自增主健&#39;,return_uuid BIGINT NOT NULL COMMENT &#39;返还标识&#39;,sku_id BIGINT NOT NULL COMMENT &#39;返还的商品ID&#39;,num BIGINT NOT NULL COMMENT &#39;返还的商品数量&#39;,UNIQUE idx_return_sku ( return_uuid, sku_id ) COMMENT &#39;返还商品唯一标识&#39; } COMMENT &#39;返还商品记录表&#39;; 返还记录表，实现了一次扣减多次返还的数据记录； 返还商品记录表实现了一次返还里有多个商品的场景，也就是上述案例里的最后一次一起退了 A 和 B 两个商品的场景。原则三：返还的总数量要小于等于原始扣减的数量看到原则三，可能觉得这内容不需要管，因为从业务上来看，这是一个必要条件。但在真正实现时，很容易出现处理遗漏。依然以“原则二”里的案例来讲解，在并发返还时，即同一时刻有两个返还请求，一个请求返还 2 个 A，另一个请求返还 4 个 A。如果技术上没有并发的时序控制，在处理两个请求时，有可能都判断为可返还并实际进行返还，最终就会出现返还 6 个 A（实际当时只扣减了 5 个）的超返还的场景。具体如下图 4 所示：对于上述潜在的风险，可以在返还前，对返还所属的扣减 ID 进行加锁来保证串行化操作，规避超卖的风险，架构如下图 5 所示：在扣减 ID 上加锁，会导致该扣减 ID 下的所有返还都串行执行，有一定的性能损耗。但从业务上看，同一个扣减 ID 并发产生返还的场景极低且返还的调用次数也相对较少，从“架构是技术与业务场景的取舍”这个角度来看，暂不需要花费太大的人力去构建一个更加复杂的加锁架构。原则四：返还要保证幂等最后，设计的返还接口需要支持幂等性。比如外部系统调用返还接口超时后，因为外部系统不知道是否调用成功，就会再一次重试。如果返还接口不满足幂等性要求，且上次超时实际是执行成功的，则会导致同一个返还号产生两次数据的返还。处理这个问题最简单的做法是：在返还接口增加返还编号（上述表结构中的 return_uuid）字段并由外部系统传入，通过数据库唯一索引来防重，进而实现幂等性，大致的架构如下图 6 所示： 提供的分布式 Worker 扩容两台机器后，etcd 或 ZK 里的哈希列表值，以及后续任务执行的区间是如何变化的： 针对 worker的扩容，就是 hash环，通过 watch 机制， 实现动态的范围 改变。有个 疑问的是”&amp;gt;Quartz， 那是 专门部署一个 应用， 连接 指定的分库，进行 数据同步吗？多个 worker 并发执行的话，就是 部署 多个 应用？ 那这样，有多个分库的话，那需要 针对 每个 分库，部署 多个 同步应用？ 这样的理解 多吗？取消订单后，除了要返还商品的库存数量，还需要做哪些内容的返还呢？" }, { "title": "利用缓存 + 数据库构建高可靠的扣减方案", "url": "/posts/Cache-Db-DeductionService-14/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-04-23 07:33:00 +0000", "snippet": "使用数据库和纯缓存实现的扣减方案。在需求层面上，两者都能实现业务需求。但均存在一些缺陷： 数据库方案的性能较差； 纯缓存方案虽不会导致超卖，但因缓存不具备事务特性，极端情况下会存在缓存里的数据无法回滚，导致出现少卖的情况。且因是异步写库，也可能发生异步写库失败，导致多扣的数据再也无法找回的情况。顺序写的性能更好借助了“顺序写要比随机更新性能好”这个特性进行设计的。在向磁盘进行数据操作时，向文件末尾不断追加写入的性能要远大于随机修改的性能。因为对于传统的机械硬盘来说，每一次的随机更新都需要机械键盘的磁头在硬盘的盘面上进行寻址，再去更新目标数据，这种方式十分消耗性能。而向文件末尾追加写入，每一次的写入只需要磁头一次寻址，将磁头定位到文件末尾即可，后续的顺序写入不断追加即可。对于固态硬盘来说，虽然避免了磁头移动，但依然存在一定的寻址过程。此外，对文件内容的随机更新和数据库的表更新比较类似，都存在加锁带来的性能消耗。数据库同样是插入要比更新的性能好。对于数据库的更新，为了保证对同一条数据并发更新的一致性，会在更新时增加锁，但加锁是十分消耗性能的。此外，对于没有索引的更新条件，要想找到需要更新的那条数据，需要遍历整张表，时间复杂度为 O(N)。而插入只在末尾进行追加，性能非常好。借力顺序写的架构有了上述的理论基础后，只要对上一讲的架构稍做变更，就可以得到兼具性能和高可靠的扣减架构了，整体架构如下图 1 所示：上述的架构和纯缓存的架构区别在于，写入数据库不是异步写入，而是在扣减的时候同步写入。这里你可能会有些疑问：同步的写入数据库是不是和“第 12 讲”讲述的内容类似？且数据库扣减的性能对于海量并发是扛不住的，这个方案是不是在倒退？如果你仔细看架构图，会发现并非如此。同步写入数据库使用是 insert 操作，也就是顺序写，而不是 update 做数据库数量的扣减。因此，它的性能较好。insert 的数据库称为任务库，它只存储每次扣减的原始数据，而不做真实扣减（即不进行 update）。它的表结构大致如下：CREATE TABLE task { id BIGINT NOT NULL COMMENT &quot;任务顺序编号&quot;, task_id BIGINT NOT NULL }任务表里存储的内容格式可以为 JSON、XML 等结构化的数据。以 JSON 为例，数据内容大致可以如下：{ &quot;扣减号&quot;: uuid, &quot;skuid1&quot;: &quot;数量&quot;, &quot;skuid2&quot;: &quot;数量&quot;, &quot;xxxx&quot;: &quot;xxxx&quot;}在上述架构里，还有一个**正式业务库，这里面存储的才是真正的扣减明细和 SKU 的汇总数据。对于正式库里的数据，通过任务表的任务进行同步即可，此种方式保证了数据的最终一致性。扣减流程在引入了任务表之后，整体的扣减流程如下图 2 所示：上述的流程和纯缓存的区别在于增加了事务开启与回滚的步骤，以及同步的数据库写入流程，详细分析如下： 首先是前置业务参数检验（包含基础参数、数量检验等），此步骤在本讲和前两讲的方案里都有。可以说，任何对外接口此功能都是不可或缺的，是完成业务验证性的必要一环； 然后在图中编号 2 处，开始数据事务； 当开始事务后，首先将此次序列化后的扣减明细写入到扣减数据库中的任务表里； 假设数据库插入扣减明细失败，则事务回滚，任务表中无新增数据，数据一致，无任何影响； 当数据库插入扣减明细成功后，便针对缓存进行扣减。和上一讲保持一致，使用 lua 等功能进行扣减即可； 如果缓存扣减成功，也就是流程正常结束，提交数据库事务，给客户返回扣减成功； 如果缓存扣减失败，有可能有两大类原因： 一类原因是此次扣减的数量不够，比如缓存里有 5 个数量，而实际此次扣减需要 10 个。当判断数量不够后，便调用缓存的归还并将数据库进行回滚即可； 第二类原因是缓存出现故障，导致扣减失败。缓存失败的可能性有很多，比如网络不通、调用缓存扣减超时、在扣减到一半时缓存突然宕机（故障 failover）了，以及在上述返回的过程中产生异常等。针对上述请求，都有相应的异常抛出，根据异常进行数据库回滚即可，最终任务库里的数据都是准的。 完成上述步骤之后，便可以进行任务库里的数据处理了。任务库里存储的是纯文本的 JSON 数据，无法被直接使用。需要将其中的数据转储至实际的业务库里。业务库里会存储两类数据： 一类是每次扣减的流水数据，它与任务表里的数据区别在于它是结构化，而不是 JSON 文本的大字段内容； 另外一类是汇总数据，即每一个 SKU 当前总共有多少量，当前还剩余多少量（即从任务库同步时需要进行扣减的），表结构大致如下： CREATE TABLE 流水表 { id BIGINT NOT NULL, uuid BIGINT NOT NULL COMMENT &#39;扣减编号&#39;, sku_id BIGINT NOT NULL COMMENT &#39;商品编号&#39;, num INT NOT NULL COMMENT &#39;当次扣减的数量&#39; } COMMENT &#39;扣减流水表&#39; 商品的实时数据汇总表，结构如下： CREATE TABLE 汇总表 { id bitint NOT NULL,sku_id UNSIGNED BIGINT NOT NULL COMMENT &#39;商品编号&#39;,total_num UNSIGNED INT NOT NULL COMMENT &#39;总数量&#39;,leaved_num UNSIGNED INT NOT NULL COMMENT &#39;当前剩余的商品数量&#39; } COMMENT &#39;记录表&#39; 原理分析数据库+缓存的架构主要利用了数据库顺序写入要比更新性能快的这一特性。此外，在写入的基础之上，又利用了数据库的事务特性来保证数据的最终一致性。当异常出现后，通过事务进行回滚，来保证数据库里的数据不会丢失。在整体的流程上，还是复用了纯缓存的架构流程。当新加入一个商品，或者对已有商品进行补货时，对应的新增商品数量都会通过 Binlog 同步至缓存里。在扣减时，依然以缓存中的数量为准。补货或新增商品的数据同步架构如下图 3 所示：通过任务库同步至正式业务库里那份数据岂不是没用了？当然不是。正式业务库异构的那份扣减明细和 SKU 当前实时剩余数量的数据，是最为准确的一份数据，以它作为数据对比的基准。如果发现缓存中的数据不一致，就可以及时进行修复。比如，当缓存扣减完成后，应用客户端重启了，此时外部调用方的连接会断开，外部调用方判断此次调用失败。但因突然重启，当次完成的扣减在缓存里是没有完成返还的。但数据库采用的是事务，客户端重启时，事务就自动回滚了。此时，数据库的数据是正确的，但缓存的数据是少的。在纯缓存的方案里，如果当时的异步刷库也失败了，则缓存数据一直都是少卖的。而数据库+缓存的方案，只会在一定时间出现少卖的情况，最终的数据一定是一致的。此方案会保证任务数据库和正式业务数据库里的数据准确性，出现故障后基于正式数据库进行异步对比修复即可。这便是两种方案的差异所在。性能提升进行方案升级后，完成了一个更加可靠的扣减架构，且使用任务数据库的顺序插入也保证了一定的性能。总的来说，即使是基于数据库的顺序插入，缓存操作的性能和数据库的顺序插入也不是一个量级，那么如何提升顺序插入任务数据库的性能和吞吐量呢？回顾一下在（无状态存储）里介绍的内容和理念——通过无状态的存储提升可用性。同样的逻辑，任务库主要提供两个作用： 一个是事务支持 其次是随机的扣减流水任务的存取。这两个功能均不依赖具体的路由规则，也是随机的、无状态的。因此，升级后的架构如下图 4 所示：采用无状态存储后，任务库便可以进行水平扩展了，在性能和高可用上得到进一步的加强。数据同步任务库和业务正式库之间的数据同步和无状态的存储基本类似，但整体实现上会更加简单。因为在业务上，扣减前置依赖的均是缓存里的数据，业务正式库里的数据只用来做兜底使用。因此最终只要使用 Worker 将数据从任务库同步至业务正式库即可，架构如下图 5 所示：总结缓存和数据库结合的方式，实现了一个更加可靠的扣减方案。相比纯缓存方案，即使使用了无状态的分库存储，它的性能也会有一定的损耗。但此方案的好处在于数据更精准、更可靠。对于类似额度扣减、实物库存扣减等场景，此方案均适用。对于一些虚拟的次数限制，同时业务上能够容忍在一定概率下数据不准确的场景，也可以选择纯缓存的扣减方案。此外，“顺序追加写要比随机修改的性能好”这个技巧，其实在很多场景里都有应用，是一个值得深入学习和理解的技能。比如： 数据库的 Redo log、Undo log； Elasticsearch 里的 Translog 都是先将数据按非结构化的方式顺序写入日志文件里，再进行正常的变更。当出现宕机后，采用日志进行数据恢复。 汇总表 - 在任务库里的任务同步到正式库时，会根据任务库里的数据（用户购买的商品和数量），实时的去更新（扣减）正式库里的汇总数据 如果不追加写任务表，而是把异步线程更新数据库换成写 mq 是不是也可以？用 rocketmq 的事务消息机制来保证消息一定发送成功，然后消费方消费 mq消息去异步更新数据库的库存不就好了？用事务消息机制，和用数据库的事务是异曲同工的。都是可以的。" }, { "title": "利用缓存实现万级并发扣减", "url": "/posts/Cache-DeductionService-13/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-04-22 07:33:00 +0000", "snippet": "采用纯数据库的扣减实现方案，如果以常规的机器或者 Docker 来进行评估，此方案较难实现单机过万的 TPS。架构是面向业务功能、成本、实现难度、时间等因素的取舍，而不是绝对地追求高性能、高并发及高可用等非功能性指标。另外，扣减业务的技术实现需求点、数据库表结构信息等内容，其实是和技术无关的，它们属于通用的基本信息和标准定义。纯缓存方案浅析纯数据库的方案虽然避免了超卖与少卖的情况，但因采用了事务的方式保证一致性和原子性，所以在 SKU 数量较多时性能下降较明显。 注：事务本质上有四个特点 ACID： 原子性（Atomicity）； 一致性（Consistency ）； 隔离性（Isolation）； 持久性（Durability）。 因为扣减有一个要求，即当一个 SKU 购买的数量不够时，整个批量扣减就要回滚，所以需要使用类似 for 循环的方式对每一个扣减 SQL 的返回值进行检查。另外一个原因是，当多个用户买一个 SKU 时，它的性能也并不乐观。因为当出现高并发扣减或者并发扣减同一个 SKU 时，事务的隔离性会导致加锁等待以及死锁情况出现。现在看来，实现单机万级的并发扣减好像遥遥无期了。别急，架构是在对问题清晰定义之后演化来的理念！再次梳理一遍，进而寻找可升级演化的方案： 首先，扣减只需要保证原子性即可，并不需要数据库提供的 ACID。在扣减库存时，重点是保证商品不超卖不少卖。而持久化这个功能，只有在数据库故障切换及恢复时才有需要，因为被中断的事务需要持久化的日志进行重演，也就是说持久化是主功能之外的后置功能、附加功能； 其次，在提升性能方面最简单、最快速的方案便是升级硬件。不管使用的是哪一个厂商的数据库实现，提升或者替换部署数据库机器的硬件配置，都可以显著提升性能。虽然提升硬件可以解决问题，但与此同时也有另外一个问题——硬件的资金成本非常昂贵，动辄上百万、千万。为什么当年阿里浩浩荡荡的发起了去 IOE 运动，转而采用性能相对较弱的 MySQL 及相对应的硬件呢？究其原因也是资金成本的考虑。此时，转换一个思路，既然提升或者替换机器配置可以提升性能，按此套路，是不是提升或者替换数据库存储也是一种方案？在不改变机器配置的情况下，把传统的 SQL 类数据库替换为性能更好的 NoSQL 类数据存储试试？是不是有一个性能又好同时又能够满足扣减多个 SKU 具有原子性的 NoSQL 数据库呢？答案显然是可以的。Redis 作为最近几年非常流行的 NoSQL 数据库，它的原始版本或者改造版本基本上已经被国内所有互联网公司或者云厂商所采用。不管是微博爆点事件的流量应对，还是电商的大促流量处理，它的踪影无处不在，可见它在高性能上的能力是首屈一指。另外，因为 Redis 是开源软件且架构简单，部署在普通的 Docker 即可，成本非常低。此外，Redis 采用了单线程的事件模型，保障了对于原子性的要求。对于单线程的事件模型，简单的比喻就是说当多个客户端给 Redis 同时发送命令后，Redis 会按接收到的顺序进行串行的执行，对于已经接收而未能执行的命令，只能排队等待。基于此特性，当扣减请求在 Redis 执行时，也即是原子性的。此特性刚好符合对于扣减原子性的要求。方案实现剖析在确定了使用缓存来完成扣减和高性能后，结合扣减服务的整体架构图来进一步分析：上图中的扣减服务和上一讲里的扣减服务一样，都提供了三个在线接口。但此时扣减服务依赖的是 Redis 缓存而不是数据库了。我们顺着上一讲的思路，继续以库存为场景讲解扣减服务的实现。缓存中存储的信息和上一讲中的数据库表结构基本类似，包含当前商品和剩余的库存数量和当次的扣减流水，这里要注意两点： 首先，因为扣减全部依赖于缓存不依赖数据库，所有存储于 Redis 的数据均不设置过期并全量存储； 其次，Redis 是以 k-v 结构为主，伴随 hash、set 等结构，与 MySQL 以表 + 行为主的结构有一定的差异。Redis 中的库存数量结构大致如下：在实际应用中，上述 key 的 sku_stock_ 前缀一般会简写成 ss_ 或者可以起到和其他 key 区分的较短形式。当我们存储的 SKU 有百万、千万级别时，此方式可极大地降低存储空间，从而降低成本，毕竟内存是比较昂贵的。对于 Redis 中存储的流水表采用 hash 结构，即 key + hashField + hashValue 的形式。结构大致如下：在一次扣减时，会按 SKU 在 Redis 中先扣减完库存数量再记录流水信息。扣减接口支持一次扣减多个 SKU + 数量。查询 Redis 的命令文档时会发现： Redis 对于 hash 结构不支持多个 key 的批量操作； Redis 对于不同数据结构间不支持批量操作，比如 KV 与 Hash 间。如果对于多个 SKU，Stock Keeping Unit（库存量单位）不支持批量操作，就需要按单个 SKU 发起 Redis 调用。在上文中提到过，Redis 不对命令间保证单线程执行。如果采用上述 Redis 的数据结构，一次扣减必须要发起多次对 Redis 的命令才可完成。这样，上文提到的利用 Redis 单线程来保证扣减的原子性此时则满足不了了。针对上述问题，采用 Redis 的 lua 脚本来实现批量扣减的单线程诉求。 lua 是一个类似 JavaScript、Shell 等的解释性语言，它可以完成 Redis 已有命令不支持的功能。用户在编写完 lua 脚本之后，将此脚本上传至 Redis 服务端，服务端会返回一个标识码代表此脚本。在实际执行具体请求时，将数据和此标识码发送至 Redis 即可。Redis 会和执行普通命令一样，采用单线程执行此 lua 脚本和对应数据。当用户调用扣减接口时，将扣减的 SKU 及对应数量 + 脚本标示传递至 Redis 即可，所有的扣减判断逻辑均在 Redis 中的 lua 脚本中执行，lua 脚本执行完成之后返还是否成功给客户端。lua 脚本执行流程当请求发送到 Redis 后，lua 脚本执行流程如下图 2 所示：Redis 中的 lua 脚本执行时，首先会使用 get 命令查询 uuid 是否已存在，如已存在则直接返回，并提示用户请求重复。当防重通过后，会按 SKU 批量获取对应的剩余库存状态并进行判断，如果其中一个 SKU 此次扣减的数量大于剩余数量，则直接给扣减服务返回错误并提示数量不足。通过 Redis 的单线程模型，确保当所有 SKU 的扣减数量在判断均满足后，在实际扣减时，数量不够的情况是不会出现的。同时，单线程保证判断数量的步骤和后续扣减步骤之间，没有其他任何线程出现并发的执行。判断数量满足之后，lua 脚本后续就可以按 SKU 进行循环的扣减数量并记录流水。当 Redis 扣减成功后，扣减接口会异步的将此次扣减内容保存至数据库。异步保存数据库的目的是防止出现极端情况—— Redis 宕机后数据未持久化到磁盘，此时我们可以使用数据库恢复或者校准数据。最后，在纯缓存的架构图（图 2）中还有一个运营后台，它直接连接了数据库，是运营和商家修改库存的入口。当商品补齐了新的货物时，商家在运营后台将此 SKU 库存数量加回。同时，运营后台的实现需要将此数量同步的增加至 Redis，因为当前方案的所有实际扣减都在 Redis 中。至此，采用纯缓存扣减的基本方案已经介绍结束了。因为实际的压测和很多因素相关，比如机器配置、压测的参数等，此处就不给出具体数字。但目前这个方案已经可以满足支撑单机万级的扣减了。下面我们再来看一看如何应对异常情况。异常情况分析因为 Redis 不支持 ACID 特性，导致在使用 Redis 进行扣减时相比纯数据库方案有较多异常场景需要处理： 第一个场景是 Redis 突然宕机的场景： 如果 Redis 宕机时，请求在 Redis 中只进行了前置的防重和数量验证，此时则没有任何影响，直接返回给客户扣减失败即可。 但如果此时 Redis 中的 lua 脚本执行到了扣减逻辑并做了实际的扣减，则会出现数据丢失的情况。因为 Redis 没有事务的保证，宕机时已经扣减的数量不会回滚。宕机导致扣减服务给客户返回扣减失败，但实际上 Redis 已经扣减了部分数据并刷新了磁盘，当此 Redis 故障处理完成再次启动后或者 failover 之后，部分库存数量已经丢失了。 为了解决此问题，可以使用数据库中的数据进行校准。常见方式是开发对账程序，通过对比 Redis 与数据库中的数据是否一致，并结合扣减服务的日志。当发现数据不一致同时日志记录扣减失败时，可以将数据库比 Redis 多的库存数据在 Redis 中进行加回。 扣减 Redis 完成并成功返回给客户后，异步刷新数据库失败了的情况： 此时，Redis 中的数据库是准的，但数据库中的库存数据是多的。 在结合扣减服务的日志确定是 Redis 扣减成功但异步记录数据失败后，可以将数据库比 Redis 多的库存数据在数据库中进行扣减。 升级纯缓存实现方案上述的纯缓存方案在使用了 Redis 进行扣减实现后，基本上完成了扣减的高性能和高并发，满足了最初的需求。那整体方案上还有哪些可以优化的空间呢？扣减服务不仅包含扣减接口还包含数量查询接口。查询接口的量级相比写接口至少是十倍以上，即使是使用了缓存进行抗量，但读写都请求了同一个 Redis，将会导致扣减请求被读影响。其次，运营在后台进行操作增加或者修改库存时，是在修改完数据库之后在代码中异步修改刷新 Redis。因为数据库和 Redis 不支持分布式事务，为了保证在修改时它们数据的一致性，在实际开发中，需要增加很多手段保证数据一致性，成本较高。对于上述两个问题，两方面的改造： 和 MySQL 的优化方案思路一样，增加一个 Redis 从结点，在扣减服务里根据请求类型路由到不同的 Redis 节点。使用主从分离的好处是，不用太多的数据同步开发，直接使用 Redis 主从同步方案，成本低开发量小； 运营后台修改数据库数量后同步至 Redis 的逻辑使用 binlog 进行处理。 当商家修改了数据库中的数量之后，MySQL 等数据库的 binlog 会自动发出，在数据转换模块接受 binlog 并转换格式插入 Redis 即可。因为 binlog 消费是采用 ack 机制，如果在转换和插入 Redis 时出错，ack 不确定即可。下一次数据转换代码运行时，会继续上一次未消费的 binlog 继续执行。最终，binlog 的机制不需要太多逻辑处理即可达到最终一致性。相比采用不借助 binlog 的方式，此方案成本和复杂度均较低。 优化后的整体方案如下图 3 所示：纯缓存方案适用性分析相比于纯数据库扣减方案，纯缓存方案也存在一定的优缺点和适用性。纯缓存方案的主要优点是性能提升明显。使用缓存的扣减方案在保证了扣减的原子性和一致性等功能性要求之外，相比纯数据库的扣减方案至少提升十倍以上。除了优点之外，纯缓存的方案同样存在一些缺点。Redis 及其他一些缓存实现，为了高性能，并没有实现数据库的 ACID 特性。导致在极端情况下可能会出现丢数据，进而产生少卖。另外，为了保证不出现少卖，纯缓存的方案需要做很多的对账、异常处理等的设计，系统复杂度会大幅增加。纯缓存在抗并发流量时，效果非常显著。因此，它较适合应用于高并发、大流量的互联网场景。但在极端情况下，可能会出现一些数据的丢失。因此，它优先适合对于数据精度不是特别苛刻的场景，比如用户购买限制等。但如果上述的异常场景都有降级方案应对，保证最终一致性。它也是可以应用在库存扣减、积分扣减等等场景。总结作为一名优秀的开发人员，架构图是一个最终态，是静止的，并不能 100% 直接应用到所面对的场景，而分析思路却是可以复制和模仿的。在实战中，正常流程是简单的，而异常流程的思考与处理十分的复杂与烦琐，同时也最能体现技术性，务必注意与加强。 如果此处的 Redis 是一个集群，而不是一个单独实例，方案应该如何演化？ 集群之后，数据会分散在各个分片里，所以，lua 脚本也需要部署在每个数据分片里并保证都需要执行" }, { "title": "使用纯数据库实现的扣减方案", "url": "/posts/Db-DeductionService-12/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-04-22 07:33:00 +0000", "snippet": "在后台开发领域，高并发的扣减一直是比较热门的话题。在各类技术博客、大会分享以及面试问题中，出现频率都非常高，可见它的重要性和技术知识点的密集性。从本文总结基于纯数据库实现的扣减方案。什么是扣减类业务秒杀只是扣减类业务中的一个有代表性、具备一定技术复杂度的场景，它并不能代表扣减类业务的全部场景。常见的扣减类业务有： 购买一个或多商品时扣减的库存； 商家针对用户设置的某个或几个商品最多购买次数； 支付订单时扣减的金额； …上述业务场景有几个共性点： 购买的或设置需要扣减的数量一次可以是一个或多个； 数量是共享的，每个用户都可以扣减某一个数据的数量。基于此，扣减类业务下一个定义： 需要通过对一个或多个已有的、用户间或用户内共享的数量，精准扣减成功才能继续的业务。通过定义，扣减类业务圈定了一个边界和清晰的概念。涉及的读业务与 UGC 写业务的区别。 读业务的特点是写少读多，同时写入为非在线类运营操作，写入的 SLA（Service Level Agreement，服务等级协议）要求级别较低，对于读的 SLA 最高。读数据因为不会改或者频率很低，所以可以采用数据不断前置应对性能等的要求。 UGC 写业务和扣减业务类似。写入均是 C 端（客户）操作，对写入的 SLA 要求级别最高。但 UGC 写业务的特点是写入的数据是用户私有的而不是共享的，同时写入不需要依赖已有的数据。对于 UGC 写业务，只要尽最大可能将数据存储下来即可。相比上两类业务的各自特点与技术实现关注点，扣减类业务着重关注对历史已有数据的增减上。扣减类业务的技术关注点发生扣减必然就会存在归还。比如用户购买了商品之后因为一些原因想要退货，这个时候就需要将商品的库存、商品设置的购买次数及订单金额等进行归还。因此，在实现的时候需要考虑归还。但是因归还的实现较通用，且归还是后置流程对并发性要求并不高，基于扣减类业务的定义，关于扣减的实现，需要关注的技术点总结如下： 当前剩余的数量需要大于等于当次需要扣减的数量，即不允许超卖； 对同一个数据的数量存在用户并发扣减，需要保证并发一致性； 需要保证可用性和性能，性能至少是秒级； 一次的扣减会包含多个目标数量； 当次扣减有多个数量时，其中一个扣减不成功即不成功，需要回滚。返还的实现需要关注的技术点如下： 必须有扣减才能返还； 返还的数量必须要加回，不能丢失； 返还的数据总量不能大于扣减的总量； 一次扣减可以有多次返还； 返还需要保证幂等。了解扣减类业务的场景、定义，确定了在实现时需要包含的功能点，以及各个功能点的实现要求后，有三种不同方式的实现方案。这三个方案都能够满足上述要求的功能和对应的技术点要求，但三个方案的实现复杂度以及能够支撑的性能和并发量级均有一定的区别。下面介绍的实现方案将直接以库存扣减为蓝本。其他扣减场景，比如：限次购买、支付扣减等技术方案基本类似，举一反三。下面先总结第一种方案——纯数据库的扣减。纯数据库式扣减实现顾名思义，纯数据库的方案就是扣减业务的实现完全依赖数据库提供的各项功能，而不依赖其他额外的一些存储和中间件了。纯数据库实现的好处是逻辑简单、开发及部署成本低。纯数据库的实现之所以能够满足扣减业务的各项功能要求，主要是依赖各类主流数据库提供的两个特性： 基于数据库乐观锁的方式，保证数据并发扣减的强一致性； 基于数据库的事务，实现批量扣减部分失败时的数据回滚。基于上述特性实现的架构方案如下图 1 所示：包含： 一个扣减服务； 一个数量数据库。数量数据库存储扣减中的所有数据，主要包含两张表： 扣减剩余数量表； 流水表。扣减剩余数量表是最主要的表，包含实时的剩余数量。主要结构如下表 1 所示： 字段名 英文表示 含义 商品 ID SKU 商品标识 当前剩余可购买数量 LeavedAmount 剩余可购买的商品数量，随着扣减实时变化 如上表所示： 对于当前剩余可购买的数量，当用户进行取消订单、售后等场景时，都需要把数量加回到此字段。同时，当商家补齐库存时，也需要把数量加回； 从完成业务功能的角度看，只要扣减剩余数量表即可；但在实际场景中： 首先，需要查看明细进行对账、盘货、排查问题等需求； 其次，在扣减后需要进行返还时是非常依赖流水的。因为只能返还有扣减记录的库存数量； 最后，在技术上的幂等性，也非常依赖流水表。流水表的主要结构，如下表 2 所示： 字段名 英文表示 含义 扣减编号 uuid 表示一次成功的扣减记录 商品 ID SKU 同上 此次扣减数量 num 此次调用扣减服务扣减对应商品的数量 扣减接口实现完成了存储的数据结构设计后，咱们再来学习一下扣减服务提供的扣减接口的实现。扣减接口接受用户提交的扣减请求，包含用户账号、一批商品及对应的购买数量，大致实现逻辑如下图 2 所示：在图 2 的流程开始时： 首先进行的是数据校验，在其中可以做一些常规的参数格式校验； 其次，进行库存扣减的前置校验; 比如当数据库中库存只有 8 个时，而用户要购买 10 个，此时在数据校验中即可前置拦截，减少对于数据库的写操作。纯读不会加锁，性能较高，可以采用此种方式提升并发量； 当用户只购买某商品 2 个时，如果在校验时剩余库存有 8 个，此时校验会通过。 但在后续的实际扣减时，因为其他用户也在并发的扣减，可能会出现了幻读，即此用户实际去扣减时不足 2 个，导致失败。 这种场景就会导致多一次数据库查询，降低了整体的扣减性能。 其次，即使将校验放置在事务内，先查询数据库数量校验通过后再扣减，也会增加性能。 在实践中，前置校验是需要的。相比读，扣减的事务性能更差，两弊相衡取其轻，能避免则避免。 此外，扣减服务提供的数量查询接口和校验中的反查底层实现是相同的，如果反查走库， 则都走库。 正常情况下，读比写的量级至少大十倍以上。因此，查询的性能问题仍须解决。 在事务之后，是数据库更新操作。因为用户扣减的商品数量可以是一个或多个，只要其中一个扣减不成功，则判定用户不能购买。 注意，因为在事务之后，对商品使用 for 循环进行处理，每一次循环都需要判断结果。如果一个扣减失败，则进行事务回滚。基于上述提供的两张表结构，单条商品的扣减 SQL 大致如下： UPDATE stock SET leavedAmount = leavedAmount - currentAmount WHERE skuid = &#39;jiagou-123456&#39; AND leavedAmount &amp;gt;= currentAmount 此 SQL 采用了类似乐观锁的方式实现了原子性，在 where 条件里判断此次需要的数量小于等于剩余的数量。在扣减服务的代码里，判断此 SQL 的返回值，如果值为 1 表示扣减成功，即用户此次购买的数量，当前的库存可以满足否则，返回 0 进行回滚即可。 扣减完成之后，需要记录流水数据。每一次扣减时，都需要外部用户传入一个 uuid 作为流水编号，此编号是全局唯一的。用户在扣减时传入唯一的编号有两个作用。 当用户归还数量时，需要带回此编号，用来标识此次返还属于历史上的具体哪次扣减。 进行幂等性控制。当用户调用扣减接口出现超时时，因为用户不知道是否成功，用户可以采用此编号进行重试或反查。在重试时，使用此编号进行标识防重。 当每一个 SKU 按上述流程都扣减成功了，则提交事务，说明整个扣减成功。 扣减接口实现升级前置校验的好处及存在的问题：多一次查询，就会增加数据库的压力，同时对整体服务性能也有一定影响。此外，对外提供的查询库存数量的接口也会对数据库产生压力，同时读的请求量要远大于写，由此带来的压力会更大。根据业务场景分析，读库存的请求一般是顾客浏览商品时产生，而调用扣减库存的请求基本上是用户购买时才会触发。用户购买请求的业务价值相比读请求会更大，因此对于写需要重点保障。转换到技术上，价值相对低的读来说是可以降级的、有损的。对于写要尽可能性能好、尽量减少不必要的读与写请求（写本身非常消耗性能）等。针对上述的问题，可以对整体架构进行升级，升级后的架构如下图 3 所示：整体的升级策略采用了读写分离的方式，另外主从复制直接使用了 MySQL 等数据库已有功能，改动上非常小，只要在扣减服务里配置两个数据源。当客户查询剩余库存数量、扣减服务中的前置校验时，读取从数据库即可。而真正的数据扣减还是使用主数据库。读写分离之后，根据二八原则，80% 的均为读流量，主库的压力降低了 80%。但采用了读写分离也会导致读取的数据不准确的问题，不过库存数量本身就在实时变化，短暂的差异业务上是可以容忍的，最终的实际扣减会保证数据的准确性。不过，在上面提到的因为在扣减前，为了降低数据库的压力，增加的前置校验导致的性能下降问题，并没有得到太多实质性的升级解决。扣减接口实现再升级在基于数据库的主从复制降低了主库流量压力之后，还需要升级的就是读取的性能了。使用 Binlog 实现简单、可靠的异构数据同步的技能，应用此方案后整体的架构如下图 4 所示：和上面第 2 点实现的区别是增加了缓存，用来提升读取从库的性能。在技术实现上，采用 Binlog 技术。经过此次方案升级后，基本上解决了在前置扣减里校验环节及获取库存数量接口的性能问题，提高了用户体验性。纯数据库扣减方案适用性要知道，任何方案都是根据业务需求、实现成本进行综合分析和取舍，很难有一个实现方案将所有诉求 100% 满足，它都是有一定的优点也有对应的缺点。对于采用数据库实现扣减服务的方案也不例外，整体实现方案上也是有它适用的场景以及它不适用的场景。纯数据库方案主要有以下几个优点： 实现简单，即使读使用了前置缓存，整体代码工程就两个，即扣减服务与数据映射服务，在需求交付周期非常短、人力紧张的场景是非常适用的； 使用了数据库的 ACID 特性进行扣减。业务上，库存数量既不会出现超卖，也不会出现少卖。但不足之处是，当扣减 SKU 数量增多时，性能非常差。因为对每一个 SKU 都需要单独扣减，导致事务非常大，极端情况下，可能出现几十秒的情况。在上述的优点和不足背景下，请你思考以下两个问题： 此方案在落地上有适用场景吗？ 或者有哪些适用场景呢？在一些企业内部 ERP 系统里的次数限制、中小电商站点的库存管理、政府系统等场景里，其实此方案是比较适合的。因为此类系统的用户并发数、对于请求的耗时要求、购买商品的数量都比较小，如果一开始就采用后面的方案，是一种浪费。当业务不断发展时，对上述指标有要求时，再去升级也不迟，毕竟系统是演化迭代来的，不是一天建成的。 除了使用主从同步来提升读取性能，是否可以使用数据库索引来提升性能？另外，数量数据库应如何设置索引来满足如防重等诉求？ 能否将数据库进行分库或者分表，利用分库或分表来提升扣减写入的并发性？" }, { "title": "利用依赖管控，提升写服务的性能和可用性", "url": "/posts/dependency-control-WritingServices-10/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-04-15 12:08:00 +0000", "snippet": "在写业务的系统架构里，除了需要关注存储上的高可用，写链路上的各项外部依赖的管控同样十分重要。因为即使存储的高可用做好了，也可能会因为外部依赖的不可用进而导致系统故障。比如写链路上依赖的某一个接口性能抖动或者接口故障，都会导致系统不可用。对于此问题。链路依赖的全貌完成一个写请求时，不仅需要依赖存储，大部分场景还需要依赖各种外部第三方提供的接口，比如： 发布一条微博，在数据存储至数据库前，不仅需要依赖用户模块校验用户的有效性、还需要依赖安全过滤非法内容等； 在创建订单时，同样是先要校验用户有效性、再校验用户的收货地址合法性，以及获取最新价格、扣减库存、扣减支付金额等。完成上述的校验和数据获取，最后一步才是写存储。其他的写场景，比如发布短视频、发布博客等亦是如此。上述几种场景的架构如下图 1 所示：对于整个链路依赖的各项外部接口，可能是出现了以下几个问题，导致系统不可用： 外部接口性能抖动严重，比如从 50ms 飙升至 500 ms，进而导致你的接口超时，此时会影响你的系统可用率； 完成上述某一个写业务时，如果需要依赖外部的接口过多，也会导致你的接口性能太差； 外部接口可用率下降，也会影响系统的可用率。依赖并行化当依赖外部接口过多时，可以从几个方面进行优化，来提升整体的性能和写接口的稳定性。将依赖的串行改并行假设一次写请求要依赖二十个外部接口，可以将这些依赖全部并行化，优化的架构如下图 2 所示：如果一个依赖接口的性能为 10ms，以串行执行的方式，请求完所有外部依赖就需要 200ms（10ms*20）。但改为并行执行后，只需要 10ms 即可完成。上述情况中，我们假设每个接口的性能都是 10ms，但在实际场景中并没有这么精确的数字，有的外部依赖可能快一点、有的可能慢一点。实际并行执行的耗时，等于最慢的那个接口的性能。全部外部依赖的接口都可以并行是一种理想情况。接口能否并行执行有一个前置条件，即两个接口间没有任何依赖关系，如果 A 接口执行的前置条件是需要 B 接口返回的数据才能执行，那么这两个接口则不能并行执行。按相互依赖梳理后的并行执行方案如下图 3 所示。对于并行中存在相互依赖的场景，并行化后的性能等于最长子串（下图 3 中红色框）的性能总和。依赖后置化虽然整个链路上会有较多外部接口，但大部分场景里，很多接口都是可以后置化的。后置化是指当接口里的业务流程处理完成并返回给用户之后，后置去处理一些非重要且对实时性无要求的场景。比如在提交订单后，用户只需要查看订单是否下单成功，以及对应的价格、商品和数量是否正确。而对于商品的详细描述信息、所归属的商家名称等信息并不会特别关心，如果在提单的同时还需要获取这些用户不太关心的信息，会给整个提单的性能和可用率带来非常大的影响。鉴于这种情况，可以在提单后异步补齐这些仅供展示的信息。采用依赖后置化后，需要增加一个异步 Worker 进行数据补齐。架构如下图 4 所示：对于一些可以后置补齐的数据，可以在写请求完成时将原始数据写入一张任务表。然后启动一个异步 Worker，异步 Worker 再调用后置化的接口去补齐数据，以及执行相应的后置流程（比如发送 MQ 等）。通过依赖后置化移除一些不必要的接口调用，会提升你的写接口的整体性能和可用性。显式设置超时和重试即使是使用了后置化的方案，仍然会有一些外部接口需要同步调用。如果这些同步调用的接口出现性能抖动或者可用率下降，就需要通过显式设置超时和重试来规避上述问题。超时设置设置超时是为了防止依赖的外部接口性能突然变得太差。比如从几十毫秒飙升至十几秒及以上，进而导致你的请求被阻塞，此请求线程得不到释放，还会导致你的微服务的 RPC 线程池被占满。此时又会带来新的问题，进程的 RPC 线程池被占满之后，就无法再接受任何新的请求，系统基本上也就宕机了。导致上述问题的架构如下图 5 所示：在设置依赖的接口的超时阈值时，很多时候为了简便快速，大家都习惯设置一个不会太大，但下游接口实际执行时间远小于它的值，比如设置 3s 或者 5s。我建议在设置此值时，通过系统上线后的性能监控图进行设置，设置超时时间等于 Max 的性能值，依据数据说话而不是“拍脑袋”做决定。如果你依赖的下游接口毛刺特别严重，表现就是它的接口性能的 Max 和 TP999，或与 TP99 相差特别大，比如 TP999 在 200ms 左右，但 Max 在 3~5s 左右，如下图 6 所展示的情况：产生此现象的原因可能是网络环境不好，偶尔会抖动，导致 Max飙高。遇到此种情况，为了防止接口因下游太高的 Max 导致线程阻塞，你可以将此接口的超时时间设置为 TP999 和 Max 之间的值。但此时也会带来一个问题，就是超时时间控制在此区间值范围之后，TP999 之外的 0.1% 请求都会因为超时而失败，应对方案见下述“重试设置”小节。重试设置除了超时之外，还可以对依赖的读接口设置调用失败自动重试，重试次数设置为一次。自动重试只能设置读接口，读接口是无副作用的，重试对被依赖方无数据上的影响。而写接口是有状态的，如果你的依赖方没有做好幂等，设置自动重试可能会导致脏数据产生。设置自动重试是为了提高接口的可用性。因为依赖的外部接口的某一台机器可能会因为网络波动、机器重启等导致当次调用超时进而失败。如果设置了自动重试，就可能重试到另外一台正常的机器，保障服务的可用性。了保证接口性能，将超时时间设置为 TP999 和 Max 之间的值，但因此可能会带来 0.1% 的失败。如果搭配重试，可以将失败的比例降低到 0.0001%（即两次都失败，0.1%*0.1%）。即使使用了重试一次，你的接口性能也会较好。比如设置超时时间为大于上述 TP999 的值，比如 500ms，重试一次最大的耗时才为 1s，远比上述的 Max 低。通过超时并灵活搭配重试，可以极大地提升接口的性能，但仍然存在非常低概率的失败（0.01%）。对于此问题，很多人的处理方式是简单粗暴地设置一个非常大的超时时间，这种做法并不能解决根本问题。要寻找导致毛刺的根源，比如： 是否为某一台机器的网卡年老失修，丢包率高！ 缓存里是否存在数据量比较大的 Key，导致一请求就是几秒的耗时？ 是否调用不合法，每次请求获取上百条数据，网络消耗太大？降级方式现在业界有很多开源工具，比如 Hystrix 等，均可实现服务熔断和触发降级的功能。但此类技术框架并不提供业务如何降级，以及降级到哪里。比如依赖的接口可用率下降了，Hystrix 可以设置可用率持续多久都低于具体某个阈值时，可以自动进行降级。但降级方案如何实现，是直接报错？还是调用替代接口？这个需要自己去考虑。依赖系统故障时，有以下一些降级方式可供你选择： 当依赖的是读接口，同时该接口返回的数据只用来补齐本次请求的数据时，可以对其返回的数据采用前置缓存。 当出现故障时，使用前置缓存顶一段时间，给依赖提供方提供一定的时间去修复缓存； 对产生故障的依赖进行后置处理。 比如发布微博前需要判断是否为非法内容，可能要依赖风控的接口进行合规性判断。 当风控接口故障后，可以直接降级，先将新微博数据写入存储并标记未校验。但此数据可能是不合规的，可以在业务上进行适当降级，未校验的数据只允许用户自己看，待风控故障恢复后再进行数据校验，校验通过后再允许所有人可见。通过有损+异步最终校验，也是一种常见的降级方案； 对于需要写下游的场景，比如提单时扣减库存，当库存不够便不能下单的场景，处理方式和上述第二点类似。当库存故障时，可降级直接跳过库存扣减，但需要提示用户后续可能无货。修复故障后进行异步校验库存，如果校验不通过，系统取消订单或发送消息通知客户进行人工判断是否要等待商家补货。此方式是一种预承接，但最终有可能失败的有损降级方案。 总结写请求时，除了保障存储高可用之外，对于外部依赖，如何保障高可用，以及在出现故障时的可选降级措施。当在实现一个高可用写服务时，可以参考依赖并行、显式的设置超时和重试来保障性能和可用性。不仅适用于写接口，对于读接口和扣减接口依然适用。只是大部分场景里，写接口的外部依赖较多且写接口担负一个公司的营收重任（外卖下单、购买电影票等），故将此讲内容放到此模块内。" }, { "title": "打造无状态存储，随时切库的写服务", "url": "/posts/NoStatus-WriteService-09/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-04-10 02:41:00 +0000", "snippet": "分库分表只解决了容量的问题，并没有解决写服务的高可用问题，或者说分库分表在一定程度上增加了系统故障的概率。从概率上看，原有的单库架构有 50% 的可能性会发生数据库故障。但如果是 5 个分库，则会有 96%的可能性出现故障。 共计有 5 个分库，每一个分库不故障的概率都是 50%。如果整个集群不发生故障，就需要每一个分片都不故障，那么整个集群不发生故障的概率为：\\(1-0.5^2=0.96\\)因此，采用分库分表的架构之后，系统的稳定性变得更低了。在读服务里，可以采用数据冗余来保障架构的高可用；但在写服务里则无法使用此方案，因为写入服务的数据是用户提交产生的，无法在写入时使用冗余来提高高可用性。写冗余需要有满足 CAP 原则的存储支持，CAP 原则最多只能同时满足两个特性，要么 CP 要么 AP，因此写冗余无法直接满足。本文总结一种能够实现随时切库的高可用写服务方案，不管是单库还是分库分表的原有架构，它都可以原生支持。写入业务的目标是成功写入写业务是指需要将用户传入的数据进行全部存储的一种场景。常见的案例有： 在各大网站提交的申请表单，比如落户申请、身份证办理申请、护照办理申请等； 在电商、外派平台里的购物订单，其中会包含商品、价格、收货人等信息； 在重要期刊和一些论坛里，提交的论文、博客等。假设明天是论文提交截止日，一定不希望论文提交系统出现问题。即使系统出现故障，在论文提交后晚几分钟才能查看内容，但只要你提交成功了，这类故障并不会产生太大的影响。其他场景也是类似。对于写入业务，当出现各种故障时，最重要的是保证系统可写入。只要有可用存储即可写入系统可随时写入，就是当出现任何故障，如网络中断、CPU 飙升、磁盘满等问题时，系统依然可以随时写入数据。无状态存储架构设计分库分表架构里，假设当前只有两个分库，并且这两个分库分别部署在不同机房里，架构如下图 1 所示：当其中一个分库所处的机房出现网络故障，导致该分库不可达时，理论上系统就出现故障了。分库分表后，数据在写入时按固定规则（比如用户账号）路由到具体分库的，当某个分库不可达时，对应规则的数据就无法写入了。写服务最重要的是保障数据写入，为了保障可写入，在某一个分库故障（如网络不可达）后，将原有的数据全部写入当前可用的数据库！从保障数据可随时写入的角度看，此方式是可行的。升级后的架构如下图 2 所示：这种当分库分表里一个分库出现故障后，就随机寻找一个可用的数据库进行写入的方式即是一种保障系统高可用的架构方案。此方案可以将图 2 和按固定规则路由的分库分表方案进行结合，方案如下图 3 所示：结合后的架构里，存储依然使用分库分表，但写入规则则发生了一些变化。它不再按固定路由进行写入，而是根据当前实时可用的数据库列表进行随机（如顺序轮流）写入。如果某一台数据库出现故障不可用后，则把它从当前可用数据库列表移除。如果数据库大面积不可用，可用列表中的数据库变少时，适当地扩容一些数据库资源，并将它添加至当前可用的数据列中。因为此架构可以实现随时切换问题数据库、随时低成本扩容数据库，故又称它为无状态存储架构设计。维护可用列表在写服务运行过程中，通过自动感知或人工确认的方式维护可用的数据库列表。在写服务调用数据库写入时，设置一个阈值。如果写入某一台数据库，在连续几分钟内，失败多少次，则可以判定此数据库故障，并将此判定进行上报。当整个写服务集群里，超过半数都认为此数据库故障了，则可以将此数据库从可用列表中剔除。此判定方法类似于 Paxos 算法，它在分布式协调和故障迁移中十分流行，此处也适当进行了一些借鉴。判定某一台数据库故障并将其下线是一个挺耗费成本的事情，为了防止误剔除某一台只是发生网络抖动的数据库，在真正下线某一个机器前，增加一个报警，给人工确认一个机会。可以设置当多少时间内，人工未响应，即可自动下线。上述是将可用列表机器进行下线的方案。对于新扩容的数据库资源，通过系统功能自动加入即可。虽然，可以按顺序进行随机写入的，但还是建议在实现时将顺序随机写入升级为按权重写入，比如对新加入的机器设置更高的写入权重。因为新扩容的机器容量为空，更高的写入权重，可以让数据更快地在全部数据库里变得均衡。增加权重的架构如下图 4 所示：写入的处理问题通过数据库写入的随机化，实现了写服务的高可用方案。但不得不说，虽然解决了写入的高可用，但想要达成一个完整的架构方案，此设计还有几个重要的技术点需要解决： 如果某一个分库故障后便将其从可用列表中移除，应该如何处理其中已写入的数据呢？ 因为数据是随机写入，应该如何查询写入的数据呢？对于上述的问题，一个整体的架构解决方案。如下图 5 所示：简单来说，整体的架构方案就是在分库分表的方案基础上，做了进一步升级。架构图的右边部分和分库分表几乎一模一样，左边部分则是本文的方案，它主要包含以下两个部分的内容： 数据随机写入模块，保证在故障时数据依然可以写入。 数据同步模块，将数据从随机写入的数据库集群实时地同步至分库分表集群里。后续的所有流程，都和原有的保持一致了。 通过写入模块和同步模块的配合，即实现了基于无状态存储的高可用架构的整套方案。数据同步问题（解决数据延迟）在采用同步模块后，从逻辑上是可以实现写入后数据可查询的。但只是逻辑上的，因为增加了同步模块后数据延迟是不可避免。更有甚者，可能因数据同步存在 Bug 导致数据一直未同步的场景。针对这个问题，解决的架构方案如下图 6 所示：在数据写入后，用户需要立即查看写入内容的场景并不太多。比如上传完论文后，只要立刻确定论文上传成功，且查看系统里的论文内容和上传的一致即可。对于这些有时延要求的场景，图 5 的架构里已经进行了单独预处理。当数据写入随机存储成功后，在请求返回前，主动地将数据写入缓存中，同时将此次写入的数据全部返回给前台。但此处并不强制缓存一定要写成功，缓存写入失败也可以返回成功。对时延敏感的场景，可以直接查询此缓存。对于无状态存储中的数据，在写入请求中主动触发同步模块进行迁移，如上图 5 中标号 X 的流程。同步模块在接收到请求后，立刻将数据同步至分库分表及缓存中，后续流程就和上一讲保持一致了。主动触发同步模块的请求及同步模块本身运行都有可能出现异常，对于可能出现的异常情况，可以设计兜底策略进行处理。兜底策略和同步模块比较类似，主要架构如下图 7 所示：兜底的同步对于无状态存储中的数据按创建时间进行不断轮询，轮询会对超过设置的时间阈值（如 5S）仍未得到同步的数据进行主动同步。此兜底方案保证了当上述缓存预写入和主动同步故障时，数据仍然可以写入分库分表。此外，如果兜底策略的时间阈值设置得过小，有可能和主动同步产生重复同步。对于重复同步，在分库分表处设置数据库唯一索引、插入前查询进行简单防重即可。缓存可降级因为主动写入缓存可能存在异常，导致数据未写入缓存，且主动数据同步和兜底同步是先写分库分表再通过 Binlog 刷新缓存，存在一定的延迟。因此在查询时需要具备降级功能，当缓存中未查询到时，可以主动降级到数据库进行一次兜底查询，并将查询到的值存储至缓存中。后续再有数据变更，和原有保持一致即可。可降级的架构方案如下图 8 所示：其他功能流程保持复用采用无状态存储后，原有分库分表及无状态存储集群，除了上述之外的一些架构细节，是否还会有什么变化？比如，假设某一台无状态存储的数据库故障后，如果该故障数据库中仍有数据未同步如何处理？其实此数据库故障后的处理流程和任何线上数据库故障一样，都是经由 DBA 确认该数据库的从库和主库数据是否一致。如一致，升级该库的从库为主库，并将其加回无状态集群即可。对于其余的一些架构细节亦是如此。总结虽然当某一台无状态存储出现故障时，其中遗留的数据会出现短时的同步延迟，但此方案可以将出问题的无状态存储立刻移除，保障了写入的 7*24 高可用。对于一些对写入有极致要求的场景，比如在电商的大促零点时刻，每一秒钟都会产生上百万、千万的下单金额。可以进行适当地体验降级，比如让用户晚几秒看到历史订单列表，但一定要保障在此时刻用户可以下单并看到此订单的结果，以及对应的收件信息（如收货地址、电话、收货人）。对于上述场景，此方案也可以很好地应对。此方案虽然应对了对写入有极致要求的场景，但它会将系统变得更加复杂且落地的开发成本和部署成本都更高，这也是它的弊端。架构是为了达到解决问题的目标而做的平衡，而不是技术比拼。" }, { "title": "使用分库分表，支持海量数据的写入", "url": "/posts/SplitDataBase-SplitTable-WriteService-08/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-04-01 16:41:00 +0000", "snippet": "写服务不说并发百万，只要并发上万，一天产生的数据量也在亿级左右如何存储这些海量数据，同时保证相对应的写入和查询的性能，以及业务流程不发生太大变化。不管是打车的订单、电商里的支付订单，还是外卖或团购的支付订单，都是后台服务中最重要的一环，关乎公司的营收。因此，以订单业务作为案例进行分析。是否真的要分库？分库能够解决存储的问题！假设原先单库只能最多存储 2 千万的数据量。采用分库之后，存储架构变成下图 1 所示的分库架构，每个分库都可以存储 2 千万数据量，容量的上限一下提升了。容量提升了，但也带来了很多其他问题。比如： 分库数据间的数据无法再通过数据库直接查询了。比如：跨多个分库的数据需要多次查询或借助其他存储进行聚合再查询； 分库越多。出现问题的可能性越大，维护成本也变得更高（可能出现的数据倾斜）； 无法保障跨库间事务。只能借助其他中间件实现最终一致性。所以在解决容量问题上，可以根据业务场景选择，不要一上来就要考虑分库，分表也是一种选择。分表分表是指所有的数据均存在同一个数据库实例中，只是将原先的一张大表按一定规则，划分成多张行数较少的表。它与分库的区别是，分表后的子表仍在原有库中，而分库则是子表移动到新的数据库实例里并在物理上单独部署。分表的拆分架构如下图 2 所示：以订单案例来说，假设订单只是单量多而每一单的数据量较小，这就适合采用分表。单条数据量小但行数多，会导致写入（因为要构建索引）和查询非常慢，但整体对于容量的占用是可控的。采用分表后，大表变成小表，写入时构建索引的性能消耗会变小，其次小表的查询性能也更好。如果采用了分库，虽然解决了写入和查询的问题，但每张表所占有的磁盘空间很少，也会产生资源浪费。两种方案的对比如下图 3 所示：在实际场景里，因为要详细记录用户的提单信息，单个订单记录的数据量均较多，所以不存在行数多但单条数据量小的情况。但在其他写入服务里，经常会出现上述场景，可以优先采用分表的方案。因为分表除了能解决容量问题，还能在一定程度上解决分库所带来的三个问题： 分表后可以通过 join 等完成一些富查询，相比分库简单得多； 分表的数据仍存储在一个数据库里，不会出现很多分库。无须引入一些分库中间件，因此维护成本和开发成本均较低； 因为在同一个数据库里，也可以很好地解决事务问题。 分表的维度主要考虑是否能够在单表里直接进行数据查询和筛选，但如果分表的维度无法满足直接查询，也可以通过多表的join来满足。此外，分表因为在同一个数据库里，不用担心分库维度带来的的事务问题 对于分表，表结构是不会变化的。假设原先的订单的表明为：order。分表之后，在同一个数据库里，会出现多张分表，分别命名为：order_0，order_1，order_2等。在数据写入时，根据订单号进行 hash，计算命中那张分表，假设命中的为 order_1。那么，这个订单后续的写入和更新都直接访问order_1这张分表即可。如何应对行数多且单行数据量较大的场景。通过前面的分析 —— 采用分库的方案。分库的实现在决定对数据库进行分库后，首先要解决的问题便是如何选择分库维度。不同的分库维度决定了部分查询是否能直接使用数据库，以及是否存在数据倾斜的问题。分库维度的选择下面以订单为案例，两种常见不同维度的分库方式： 直接满足最重要的业务场景划分 最细粒度随机分。直接满足最重要的业务场景划分在业务上，所有的订单数据都是隶属于某一个用户的。在选择分库维度时，按订单归属的用户这个字段进行分库。按此维度分库后，同一个用户的订单都在某一个分库里。分库后的场景如下图 4 所示：订单模块除了提供提交订单接口外，还会提供给售卖商家对自己店铺的订单进行查询及修改等功能。这些维度的查询和修改需求，在采用了按购买用户进行分库之后，均无法直接满足了。订单模块最重要的功能是保证客户（即买家）的各项订单功能能够正常使用，比如下单、下单后立刻（无延迟）查看已购的订单信息、待支付、待发货、待配送的订单列表等。相对来说，订单里的商品售卖方（即卖家）所使用的功能并不是优先级最高的。因为当对卖家和买家的功能做取舍时，卖家是愿意降低优先级的，毕竟卖家是买卖的受益方。按购买用户划分后，用户的使用场景都可以直接通过分库支持，而不需要通过异构数据（存在数据延迟）等手段解决，对用户来说体验较好。其次，在同一个分库中，便于修改同一用户的多条数据，因此也不存在分布式事务问题。通过上述订单案例抽象出一个分库准则，即在确定分库字段时应该以直接满足最重要的业务场景为准。很多其他的业务都参考了这一准则，比如： 对于微博和知乎等用户生产内容（UGC）的业务，均会按用户进行分库。因为用户新发布文章后就会去查看列表。 支付系统里，也会按用户的支付记录进行分库。 在技术上，比如一个微服务下的监控数据，同样会按微服务进行划分。同一个微服务的监控数据均存储在一个分库里，可以直接在一个分库里查看微服务下的所有监控数据。上述划分方法虽然直接满足了最重要的场景，但可能会出现数据倾斜的问题，比如出现一个超级客户（如企业客户），购买的订单量非常大，导致某一个分库数据量巨多，就会重现分库前的场景。这属于最极端的情况之一。最细粒度的拆分对于倾斜的问题，可以采用最细粒度的拆分，即按数据的唯一标示进行拆分，对于订单来说唯一标示即为订单号。采用订单号进行分库之后，用户的订单会按 Hash 随机均匀地分散到某一个分库里。这样就解决了某一个分库数据不均匀的问题。对于上面案例，也可以用此手段进行处理。比如： 按用户的每一条微博随机分库； 按用户的每一笔支付记录随机分库； 同一个微服务里的每一个监控点的数据随机分库。采用最细粒度分库后，虽然解决了数据均衡的问题，但又带来了其他问题。 除了细粒度查询外，其他任何维度的查询均不支持。这就需要通过异构等方式解决，但异构有延迟、对业务是有损的； 采用最细粒度后，对于防重逻辑在数据库层面已经无法支持。比如用户对同一个订单在业务上只能支付一次这一诉求，在支付系统按支付号进行分库后便不能直接满足了。因为上述分库方式会导致不同支付单分散在不同的分库里，此时，期望在数据库中通过订单号的唯一索引进行支付防重就不可实施了。上述两种分库的方式，在解决问题的同时又带来一些新的问题。在架构中，没有一种方案可以解决所有问题的，更多的是根据场景去选择更适合自己的方案。全局唯一标示不管采用何种维度的分库方式，使用原有单库的数据库自增主键生产数据标示的方案已经不可以使用了。对于全局的数据唯一标示，有两种常见的生成方式：使用算法随机生成比如使用机器 IP、时间戳、随机数等进行组合，生成一个唯一编号。业界成熟的有 Twitter 推出的雪花算法。需要注意的是，为了保证唯一性，雪花算法增加了很多随机因子，导致计算出来的唯一标示特别长，达到 19 位。在 JavaScript 里，数据精度和 Java 等语言不完全一致，太长的雪花 ID 在前端存在溢出的问题。因为雪花算法生成的 ID 为 Long 类型，可以采用类似 Base64 等算法，对原始 ID 进行压缩转换为 String 类型，降低长度并避免和 JavaScript 精度不统一导致的问题。 转换为string，就不是有序的了。索引本来就是排好序的，mysql里索引实现是基于 B+ 树，这个不会影响。基于数据库主键构建一个 ID 生成服务虽然不能在插入的时候使用数据库唯一主键，但可以在插入前通过一个服务获取全局唯一的 ID。ID 生产服务可以基于一张单表实现，每一次外部请求时，均生产一个新的 ID。通过此方式，可以获得长度较短且为数值类型的全局唯一编号。但如果每次获取 ID 时，ID 生成服务都需要从数据库实时获取，性能会比较差。为了解决性能问题，可以在生成 ID 的数据库前置一个具备持久化功能的内存缓存，预生成一批 ID。具体架构如下图 5 所示：分库中间件选择现在开源提供分库支持的中间件较多，如 Mycat、阿里的 tddl、携程的 dal、shardingsphere 等等 。整体上各类分库中间件可以分为两大类： 一种是代理式； 另外一种是内嵌式。代理式分库中间件代理式分库中间件对于业务应用无任何侵入，业务应用和未分库时一样使用数据库，分库的选择及分库的维度对业务层完全隐藏，接入和使用成本极低。代理式的架构如下图 6 所示：代理式虽有使用成本低的好处，但也存在其他一些问题： 代理式在业务应用和数据库间增加了一层，导致了性能下降； 代理式需要解析业务应用的 SQL，并根据 SQL 中的分库字段进行路由。需要解析和适配所有 SQL 语法，增加了代理模块复杂度和出错的可能性； 代理层是单独进程，需要部署占用资源，带来一定的成本。内嵌式分库中间件内嵌式分库中间件是将分库中间件内置在业务应用中，它只负责分库的选择，并不会解析用户的 SQL。在使用时，业务应用需将分库字段传递给内嵌中间件去计算具体对应的分库。它相比代理式性能更好。内嵌式的架构如下图 7 所示：除了性能优势外，内嵌式同样存在问题。 有一定侵入性，业务应用与原始单库模式相比，需要进行一定的改造去适配内嵌式的 API。 分库在故障转移、数据迁移等运维工作时，需要业务应用感知。不过现在的一些内嵌式代理，已经具备非常良好的配置功能，在分库运维时，业务应用需要配合的内容较少。其他问题几个常见问题的应对策略： 是否一定需要进行分表或者分库 不一定。虽然很多互联网公司的体量很大，用户非常多，但不要被这些现象迷惑了。 实际上，90% 以上的系统能够发展到上百万、上千万数据量已经很不错了。对于千万的数据量，开源的 MySQL 都可以很好地应对，更别说一些商业数据库了。 另外，当数据增长到一定量级后，可以在业务层面做一些处理。比如根据业务特点，对无效数据、软删除数据，以及业务上不会再查询的数据进行统一归档，这也是一个成本低、效果明显的方式了。 使用业务字段分库后，如何处理数据倾斜 如果数据量不是特别大，可以在分库基础上，再进行分表。针对数据量较大的场景，可以使用二次分库的方式。对于订单量较多的用户，可以在用户账号基础上再增加一个字段，做进一步的分库，但此用户的查询就会有损了。 如何满足富查询 富查询是一个无法回避的问题，即采用分库分表之后，如何满足跨越分库的查询？ 解决跨多库的修改导致的分布式事务 跨多库的修改及多个微服务间的写操作导致的分布式事务问题！！！ 总结不断进行分库分表一定能解决容量问题，但“杀敌一千，自损八百”的事情少做为宜。使用分库分表会将代码和架构的复杂度变高，带来资源成本上升等问题。另外，在使用系统时，用户（不管是客户还是管理员）的查询体验也存在一定的降级。在使用分库分表前，需要确定这是否是最优选择，是否能通过其他更简单的手段处理无效数据清理！！！架构是通过最小代价解决问题，而不是技术工具的比拼。 架构是成本的权衡，互联网支持是因为它的体量大，营收能够支持。 首先，上规模的互联网公司，都有自己自研的存储，做技术上的优化。 除此之外，一次经历里，把一份业务数据全部存储在缓存里，发现要几十TB的容量才能支持，显然成本上是cover不住的。 我们是对用户的查询进行了分析，发现90%的用户查询都集中在某一个时间区间里。我们就把这个区间里的数据放在缓存里了，其余仍存在数据库或其它文件系统里。可以看到，这就是一个架构权衡，最大程度满足业务诉求。" }, { "title": "分库分表化后，如何满足多维度查询", "url": "/posts/SpiltDataBase-SplitTable-Query-WriteServices-11/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-03-20 13:41:00 +0000", "snippet": "分库分表以及无状态存储也带来了另外一个问题，即数据按路由规则分散后，满足无路由字段的多维度富查询。异构定制化实现关于订单模块的分库分表案例，处理方式： 在提交订单时，采用用户账号作为分库字段； 在查询时，只有携带用户账号的 SQL 才能直接执行； 在下单后，售卖商品的商家可能希望查询自己店铺里的所有订单，此时按用户维度的分库分表则不能满足上述查询需求。为了满足和原有分库维度不一样的查询，最简单的方式是按新的维度异构一套数据，它的架构如下图 1 所示，数据异构可以采用在本专栏模块二中介绍的 Binlog 进行处理。采用数据异构满足了上述按商家维度查看数据的诉求，但如果又来一个新的需求，需要按订单所属的来源（小程序、App、M 页或者 PC 站点）进行订单数据查询呢？此时，是否需要按来源维度进行数据异构呢？答案显然是不行的，主要有两个原因： 一是数据同步程序需要开发，如果来一个新需求就开发一套同步，浪费人力成本； 二是异构数据浪费资源。正是因为数据量太大才进行分库分表，如果异构一套会导致数据量翻倍，资源消耗也会加倍。本讲后面会介绍两种方案，来解决上述两个问题。借助分库网关实现代理式的分库分表的架构方案：分库代理中间件解析用户指定的 SQL 并提取路由字段，根据路由字段去访问具体的分库进行数据的查询。如下图 2 所示：当用户没有指定路由字段时，可以在分库代理中间件进行转换处理。以订单为例，假设路由字段为用户账号，当查询时只指定了订单号，代理层无法计算到具体命中了哪个分库。但是代理层可以多线程并发地去请求所有的分库，查询此条订单信息。此方式，也可以查询到指定的订单信息。但如果用户指定的查询带有排序和数量诉求，比如查询所有用户最近提交的 100 单，SQL 可能如下：select 订单信息 from t_order order by createdTime limit 100在没有路由字段时，分库分表的前 100 个订单如何获取呢？因为在极端情况下全局的前 100 条数据可能都分布在某一个分库里，为了保障一定能够获取到全局的前 100 条数据，代理层需要向每一个分库（上述有三个分库）都获取 100 条数据，并在代理层进行汇总排序，如下图 3 所示：从上图中可以看到，不管是不带路由字段的条件查询还是排序聚合的查询，代理层都可以通过扫描分库来实现，比如上述的获取前 100 条订单数据。但在实现时，其实总共需要获取 300 条数据才能实现上述目标，这对于代理层的内存和 CPU 占用是非常巨大的，因为一次代理层的查询需要分裂出分库数量的查询，才能满足上述目标，这增加了调用量。对于内嵌式的分库中间件的实现就更不行了，因为内嵌式的分库架构是和业务应用部署在同一台机器上的，它会消耗业务应用所在机器的网络、内存和 CPU 等资源，进而影响业务服务。总的来说，数据库最重要的特征是为了满足写时的 ACID。对于读业务而言，数据库需要借助索引来提升性能。但过多的索引也会反过来导致写的性能变差，因为索引是在写入的时候实时构建的。因此，目前来看其实 MySQL+ 代理层并不十分适合。基于 ElasticSearch 实现借助分库网关+分库虽然能够实现多维度查询的能力，但整体上性能不佳且对正常的写入请求有一定的影响。除了上述的方案外，业界应对多维度实时查询的最常见方式便是借助 ElasticSearch。为了方便，后面都简称 ElasticSearch 为 ES。什么是 ESES 是基于 Lucene 之上进行封装的可开箱即用的搜索引擎。其中，Lucene 提供了基于倒排序的全文索引的构建功能和查询的能力，但在更加贴近应用层的数据结构设计、存储架构层面涉及较少，它更多地被称为一个底层工具。ES 在 Lucenne 的全文检索功能之外，还具备以下 3 个特点： 自带了分布式的系统架构，能够很好地应对海量的数据，且分布式架构更加高可用，能够有效地满足生产环境的要求。 支持带结构的数据（如数据库的 schema），提供了非常丰富的数据结构，可以直接映射数据库等存储的数据结构，更方便易用； 另外是 ES 提供了近实时的数据索引功能，数据写入后就可以搜索查询，而不用像传统的搜索引擎要分钟级或更高的异构索引构建。更加详细的介绍可以参考 ES 的官网。倒排序索引倒排序索引的内容较多，此处只做简单介绍。对于倒排序索引，它是借助分词维护的二维表。“分库分表后如何满足多维度查询”（假设编号为 1）为例，写入 ES 后，会建立如下表格： 单词 文档 ID 说明 分库 1(1) 1(1),前一个1表示文档编号后一个“分库”这个词表示出现频率 分表 1(1) 同上 后 1(1) 同上 ··· ··· 同上 上述的“单词”列得到的各个词，是和各个语言特定的，中文有中文分词器、英文有英文的分词器。所有写入 ES 的内容，都会按上述模式进行分词。相比数据库，ES 里所有的内容都可以分词建立索引且 ES 不需要保障数据的 ACID 等特性，因此 ES 整体上更适合查询类和模糊匹配等场景。对于模糊匹配，数据库只能使用 like 等手段，性能是非常差的。ES 里的所有内容都可以建立索引，虽然能带来提升性能的好处，但也会带来副作用，就是会非常消耗存储空间，这个在使用时需要预先考虑。如何使用在使用 ES 满足多维度查询时，第一步需要做的便是数据异构，将数据库的数据同步至 ES 中。在进行数据异构时，仍建议采用在模块二中介绍了Binlog进行： 一是因为 Binlog 可以保障数据最终一致性； 二是基于 Binlog 的方式，同步代码编写更加简单且不易出错。只需要订阅 Binlog 发出来的数据即可，不用在业务代码的每一个修改的地方进行特殊处理。基于 Binlog 的 ES 数据异构如下图 4 所示：上述异构的数据同步至 ES 时，ES 中的数据结构应该如何设计来满足存储呢？在正式设计前，需要搞明白 ES 中的几个重要概念。接下来将以数据库中的几个概念进行类比，如下表所示： 数据库 ES 描述 数据库(Database) 索引(Index)   表(Table) 类型(Type)   行(Row) 文档(Document)   列(Column) 字段(Field)   以数据库作为类比，ES 中的类型（Type），它并不完全对应数据库的表。数据库中的表与表之间是隔离的，没有关联的。而 ES 中同一个索引（Index）下的不同类型（Type）里，如果存在相同的字段，ES 会认为它们是同一个字段。这个隐含逻辑对熟悉数据库概念的用户来说，有很大的迷惑性。因此，ES 从版本 5 中已经逐渐将类型（Type）移除了。或者你可以直接这样简单理解，ES 中一个索引就只能包含一个类型（Type）即可，更加详细的介绍见这里。了解了上述概念后，现在以一个实际的案例进行演练。以购物时的用户作为参考，用户数据库需要存储用户信息和用户的多个收货地址才能完成业务需求。数据库至少会有两张表，一张为用户表（user），另一张为收货地址表（delivery_address ），为一对多的关系。数据库表结构大致如下：CREATE TABLE USER { id BIGINT NOT NULL,user_id VARCHAR ( 30 ) NOT NULL COMMENT &#39;用户账号编号&#39;,nick_name VARCHAR ( 50 ) NOT NULL,telephone_num VARCHAR ( 50 ) NOT NULL,email VARCHAR ( 80 ) } CREATE TABLE delivery_address { id BIGINT NOT NULL,user_id VARCHAR ( 30 ) NOT NULL COMMENT &#39;用户账号编号&#39;,prov_id BIGINT NOT NULL,city_id BIGINT NOT NULL,county_id BIGINT NOT NULL,detail_address BIGINT NOT NULL }基于上述的数据库表结构，完成的 ES 的结构设计如下：{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;user_id&quot;: { &quot;type&quot;: &quot;long&quot; }, &quot;nick_name&quot;: { &quot;type&quot;: &quot;keyword&quot; }, &quot;telephone_num&quot;: { &quot;type&quot;: &quot;keyword&quot; }, &quot;email&quot;: { &quot;type&quot;: &quot;keyword&quot; }, &quot;delivery_address&quot;: { &quot;type&quot;: &quot;nested&quot;, &quot;properties&quot;: { &quot;prov_id&quot;: { &quot;type&quot;: &quot;long&quot; }, &quot;prov_name&quot;: { &quot;type&quot;: &quot;text&quot; }, &quot;city_id&quot;: { &quot;type&quot;: &quot;long&quot; }, &quot;city_name&quot;: { &quot;type&quot;: &quot;text&quot; }, &quot;county_id&quot;: { &quot;type&quot;: &quot;long&quot; }, &quot;county_name&quot;: { &quot;type&quot;: &quot;text&quot; }, &quot;detail_address&quot;: { &quot;type&quot;: &quot;text&quot; } } } } }}可以看到上述的 ES 结构和数据库中的表结构还是有一定的差异，具体的差异和产生的原因主要有以下几点。 数据库中是一对多的两张表，而在 ES 中只用了一个冗余宽表（用户和用户的多个收货地址都放在一个 Document 结构里）。使用冗余宽表是因为ES 即使在冗余的情况下，被冗余的收货地址仍然支持搜索（上述的 Nested 关键字支持此特性），而数据库如果在用户表里设置了冗余字段存储用户的多个收货地址后，该冗余的地址字段就不支持查询了。此外，ES 对于多张单独的 Document 的级联查询性能不好，ES 首推冗余存储，更加详细的解释见这里。 并不是所有的字段都设置了分词，比如电话号码就没有分词（使用了 keyword 关键字）。因为电话号码在业务上是不需要支持模糊匹配的。在你设计索引时也最好遵守此原则，对于不需要模糊匹配的字段不设置分词，因为分词需要构建倒排序索引，浪费存储。 在 ES 的收货地址结构里，增加了省份名称、市名称等，而数据库里没有。因为在实际业务场景里，有根据中文名称查询地址的需求。即使不分库分表，上述的数据库表结构里的字段也不能支持按名称查询，因为它没有存储省市县的名称。ES 的目的就是面向查询，因此在设计 ES 结构时，需要根据查询需求冗余一些字段进来。ES 的架构与深翻页在介绍 ES 架构前，首先要明确 ES 架构中的三个重要概念：节点（Node）、分片（Shard）、集群（Cluster）。 节点简单理解就是部署的机器，可以是物理机或者是 Docker。 分片类似数据库分库分表架构里的一个分库，存储一部分数据。此外，分片还分为主分片和副本分片。主分片类似数据库分库里的主分库，副本分片就是从分库。分片部署在节点里，一个节点可以包含一至多个分片。 多个节点在一起便组成了集群。基于上述概念的架构如下图 5 所示：可以看到 ES 的架构里没有代理式网关，ES 里所有的节点都可以接受用户的请求。对于类似第二小节里提到的排序+数量查询，ES 和代理式分库分表的架构比较类似，接受请求的节点并行地去获取所有其他节点，并在该节点里进行集群排序和过滤，具体流程如下图 6 所示：虽然 ES 使用了倒排序增加了检索的性能，但如果你要搜索第 1000 条数据之后的 100 条数据，在接受请求的节点就需要获取 1100*节点数量条数据，即使如 ES 这种面向查询的存储也是搞不定的。因此，ES 默认有一个设置，最多只能查询 10000 条数据，超过了直接报错。上述描述的案例，有一个通俗的叫法：深翻页。对于深翻页，不管是 ES 和代理式网关都是无法直接支持的。解决上述问题，有一个牺牲用户体验的做法，就是按游标查询，或者叫每次查询都带上上一次查询经过排序后的最大 ID。以 SQL 举例，大致语法如下：SELECT 内容 FROMTABLE WHERE id &amp;gt; lastMaxId ORDER BY id LIMIT pageSize上述的有损用户体验主要体现在，用户无法指定页码进行翻页，只能在文章列表里一页一页地翻。注意：ES 是近实时的但不是实时的，默认有 1s 的延迟。所以需要你根据具体业务情况进行取舍考虑。总结从最简单、但资源消耗严重的异构定制化的方案，到使用 ES 来最终应对多维度查询的方案。各个方案都有各自的好处，但也有带来负面的影响。比如要使用 ES，你就需要学习 ES 的知识并要有专业的人去维护它。此外，在 ES 的方案里，引用了很多英文的官方文档。学习一项新的技能，需要从源头获取信息，而不是借助搜索引擎，把东一点西一点的信息拼凑起来零零散散地学习。" }, { "title": "基于流量回放实现读服务自动化测试回归", "url": "/posts/test-readService-07/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-03-20 13:41:00 +0000", "snippet": "前面四篇文章，可以直接落地的、能够支撑百万并发的读服务的系统架构，包含懒加载缓存、全量缓存，以及数据同步等方案的技术细节。基于上述方案及细节，可以直接对所负责的读服务进行架构升级，将性能进一步提升。在升级系统架构时，有一个很重要的点容易忽略——只评估了升级的工作量，就直接开干。最后提测的时候，发现改造范围太大，测试根本回归不完，升级重构上线遥遥无期。随着业务的发展，系统相应地进行升级重构是不可避免的，但与此同时也带来了巨大的测试回归量。本文总结一种自动化的测试回归架构方案，它能够极大地降低读业务因升级重构而带来的回归问题。此外，还能适配日常需求上线的回归工作量，真正做到了回归自动化、研发自助化，避免用户场景全覆盖遗留导致漏测的问题。自动化的必要性针对架构升级的场景不管是因为技术还是业务导致的系统重构架构升级，它的改造量和范围都是非常大的。对于此类系统级的重构，测试回归的工作量至少都是以月为单位，对于人力的消耗巨大。 一种应对方案是，先不改造，到系统实在扛不住了再想办法。 另一种应对方案是，先暂停需求，全力进行改造。 但在实际工作场景中，上述应对策略往往很难实现。针对日常需求的场景对于后台系统，基本上都是微服务架构，对外会提供一至多个接口。一般一个需求只会涉及一个或者几个接口的某些场景的修改，测试同学也只会对改造的接口及对应的场景进行测试。但在实际案例里，需求涉及的接口或场景上线后均不会出现 Bug，但同一个服务里的其他接口，即当次未涉及修改的被测接口，就比较容易出现 Bug。因为虽然对外的接口是不同的，但底层的逻辑很可能是复用了相同的代码，导致相互影响。为了避免此情况，对于任何一个需求，测试同学都需要回归所有接口，这个工作量是巨大的。针对上述问题，通过自动化的方式提升效率便是不二选择。如何实现自动化回归？自动化回归方案的基本原理。读服务能够实现测试回归自动化有两个前置条件： 读服务均是查询，它是无状态的 无状态，是指每次请求都是无副作用的、可以重复的； 同样的请求入参，在后台数据无变化的情况下，多次重试的结果均一样； 相比而言，写或者扣减业务就不行。比如用户的余额变化就会产生副作用。具体来说，上一次购物订单支付成功，第二次和上一次使用同样的金额进行支付就不一定能成功。因为上一次的支付产生了副作用，用户的余额减少了，导致此次支付不成功。 不管是架构升级，还是日常需求，读服务对外接口的出入参格式是没有变化的； 不管服务内部的逻辑如何变化，只要接口出入参格式不变（下图 1 中标记的 1、2 表示架构升级，但对外接口未变），就可以利用读服务的无状态特性进行流量回放。 整体架构下图 2 是读服务的自动化测试回归的整体架构：在这个架构里包含三大模块，分别为日志收集、数据回放，以及差异对比。它们主要有以下 3 个功能： 日志收集，主要作用是收集被测系统的真实用户请求，基于一定规则处理后作为系统用例； 数据回放是基于收集的用例，对被测系统进行数据回放，发起自动化测试回归； 差异对比，类似人工测试发现 Bug 的过程，通过差异对比自动发现与预期不一致的用例，进而确定 Bug。日志收集不管是提供 HTTP 形式还是 RPC 形式接口的后台读服务，都可以通过现在比较成熟的过滤器进行日志收集，比如 Spring 里的 Interceptor 过滤器、Servlet 里的 Filter 过滤器等。采用过滤器的架构如下：在过滤器中，会对所有请求的入参和出参进行记录，并通过 MQ 发送出去。记录的数据格式类似如下：{ &quot;应用名&quot;: &quot;XXX&quot;, &quot;接口方法名&quot;: &quot;RPC记录的接口方法名/HTTP记录域名及路径&quot;, &quot;入参&quot;: &quot;XXX&quot;, &quot;出参&quot;: &quot;XXX&quot;}对于发送出去的 MQ，自动化回归的消费服务会按应用和接口进行处理。对于应用和接口方法的标识数据可以存储在数据库里，对于入参和出参可以存储在 NoSQL 里，如 HBase。之所以将出入参数据存储在 HBase 里，是因为入参和出参数据量较大，存储在数据库里查询性能会比较差。即使将参数存储在原生支持分布式的 NoSQL 里，也要对采集的日志进行一些过滤、去重以及无效数据定期清理等操作。此外，对于压测等非业务场景，需要关闭日志采集。毕竟，日常业务请求带来的数据量可能一天都上亿或十几亿次，如果不做管控，对于存储的消耗将是巨大的。当前日志收集采用的过滤器是和业务应用同属于一个进程，它会占用业务应用的内存资源，同时对于业务也存在一定的侵入性。当遇到此种情况，可以将日志收集独立出来，采用单独进程进行运行。升级后的架构如下图 4 所示：采用单独进程后，业务应用需要按上述格式将出入参日志打印到指定文件。单独的数据收集进程只需要对此文件进行监控并将变化数据发送至 MQ 即可。在操作系统里，可以单独对进程设置资源占用的限制。为了保证业务应用不被日志采集所影响，可以对采集进程设置内存、CPU 等限制，并配置资源占用报警等。 单独进程的日志收集，业务接口也要进行拦截请求 ! 业务接口需要拦截请求，并把拦截的内容输出到本地文件即可。 单独进程进行日志收集是指：异步的收集这个日志文件里的内容，不去影响业务应用的进程。数据回放有了上面的出入参之后，便可以在测试时进行回放了。数据回放主要的作用是基于日志里记录的接口信息（HTTP 便为域名）和入参，去实时调用被测系统，并存储实时回放返回的出参信息。整体架构如下图 5 所示：实时回放时： 如果是 HTTP 形式的接口，采用主流的 HTTP 客户端即可； 如果是 RPC 形式接口，需要使用该 RPC 框架提供能够调用任意被测接口的客户端。三种模式 离线回放模式，指在回放时只调用进行改造的新服务，将新服务返回的数据与收集日志里的出参进行比较。架构如下图 7 所示： 优点：减少了对于线上老版本的调用量，避免对线上产生影响也节约了资源； 缺点：如果后台存储中的数据已经发生了变化，就不能使用收集的日志里的出参。因为从新版本实时查出的数据与历史收集的日志数据已经不准了。 实时回放模式，为解决离线回放模式里，因为数据变化导致收集的日志里的出参无效问题，可以采用实时回放的模式，如下图 8 所示的架构图： 上述架构在收集的日志里，只记录入参而不记录出参，收集流程是图 8 中的标记 3。 实时回放的模式会在上图 8 中的标记 4，研发或测试手动触发回放功能后，使用入参实时的调用新老版本的被测应用，并对比双方返回的出参，通过此方式可以规避数据变更的问题。 无录制的实时回放模式 不管是离线回放还是实时回放都存在一个问题，由于是对接口的入参进行录制（存储至 NoSQL 里，如上图 8 里的 HBase）再回放的。因存储容量有限，只能存储一定数量的数据，很多日志用例可能会被丢弃。这就可能导致有些重要场景会被漏测。 针对上述的问题，可以采用无录制的实时回放模式，架构如下图 9 所示： 无录制实时回放不再记录入参数据了（见图 9 和图 8 里的标记 3 的差异)。 当日志消费模块接收到收集的日志用例后，实时调用新老版本被测服务并进行数据对比。 使用此方式，进行几周或者更久的回放，基本能够覆盖全部场景了。 差异对比在完成了数据回放后，就可以对回放产生的结果数据与预期数据（比如收集日志里的出参）进行比较。数据对比有很多形式，比如： 基于二进制校验和；此方式只是告诉数据是否整体一致，而不会展示具体哪里不一致。即使展示了，但因为是二进制，也无法查看（抛弃）。 基于文本等；可以看到数据整体不一致，以及具体是何处导致的不一致！因此此处数据对比采用文本形式，先将数据转换为 JSON 格式，再进行对比。如下图 6 所示：可以看到，同一个 SKU 的名称，在两边出现了不一致。如果只是进行了系统重构，相同的 SKU 对应的数据应该是一样，此处出现了不一致，代表出现了 Bug。采用文本对比，可以直观地看到哪个字段数据有差异，从而更快定位到问题。正常情况下，只要存在差异的数据，均可认为是 Bug，是需要进行修复的。但有些时候，比如接口会对每次请求产生一个唯一标识，用此标识来打印日志并返回给调用方。在差异对比时，此类接口每次都会因为唯一标识不同而导致对比出现差异，但此差异又不会对业务产生影响。对于此类场景，需要差异对比工具具备忽略能力，在对比时忽略唯一标识字段。通过数据收集、回放和差异对比，能够实现自动化、可视化，将读业务的测试回归完全自动化。注意事项在线上部署及使用自动化回归工具时，有一些需要注意与规避的点： 在进行自动化回归时，写接口一定要屏蔽。以注册用户举例，如果在回放时没有屏蔽，使用线上入参进行回放，将会产生很多垃圾用户，给后续的业务流程带来巨大影响； 上述几种回放模式里，除了离线模式外，实时回放模式和无录制的实时回放模式都会对线上系统产生一定的流量压力。假设被测系统的性能比较差或者机器数较少，自动化回放的流量会把线上系统打挂，进而影响业务不可用。特别无录制的实时回放模式，带来的流量更大且持续时间更长； 即使采用了无录制的实时回放模式，也只是通过更长时间的回放尽可能地覆盖更多的业务场景。但也并没有足够的证据表明，一定不会出现漏测。对于此种问题，可以借助一些代码覆盖率的工具，如 Java 里的 JaCoCo，来统计一次回放后被测系统的代码覆盖率，通过数据来判断是否存在可能的漏测。总结读业务在系统重构及日常需求开发时，均存在的测试回归耗时长和可能存在漏测的问题。根据读业务无状态及可重复执行的特点，针对上述问题构建了一套基于业务日志的自动化回归平台，主要包含三大子模块，分别是： 日志采集 数据回放 差异对比 录制线上的流量，按接口参数，相同的只录制一份，然后在测试环境进行回放，这种方式没有太大的问题，不过需要注意公司是否需要将线上数据搬到测试环境！！！" }, { "title": "使用简洁架构构建高性能的读服务", "url": "/posts/concise-readService-03/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-03-16 11:33:00 +0000", "snippet": "利用“目的性”这一维度，将后台系统的架构归类为读业务、写业务以及扣减业务。在实际业务中，绝大多数情况下都是读场景高于写场景。因此，本文总结下构建读业务的经验总结，具体内容包括： 读业务在实现上需要满足的功能特点 遵守的基本原则 常见的实现方案存在的优缺点 读业务这个名词偏业务和产品，研发在做系统设计或日常沟通中，会采用偏技术实现的名词，如读接口或者读服务。这里的读服务代指读业务 读业务的功能实现要求读业务在实现流程上，基本上是纯粹地从存储一次或者多次获取原始数据，进行简单的逻辑加工，或者直接返回给用户/前端业务系统。它是无状态或者是无副作用的，也就是说每一次执行都不会在存储中记录数据或者修改数据，每一次读请求都和上一次无关。比如，打开资讯类 App , 会看到两类场景：一类是业务后台系统直接从存储中获取今日的新闻列表；另一类是推荐系统生成一个新闻推荐列表，给到业务前台系统并展示给用户。又如在电商 App 里，首页展示的商品和促销等信息，是运营根据营销策略配置的，业务后台接收到读请求，然后直接去存储数据并进行加工后返回给业务前台系统。其他系统亦然。结合经验和上诉介绍，可以分析出读服务在实现上需要满足的功能要求，主要有以下 3 点： 保证高可用。其实不管是不是读服务，都应该满足高可用 保证高性能。先定一个较大的指标。TP 999 要在 100 ms 以内。就像浏览新闻或者电商 App，如果首页打开得非常慢，体验一定非常差。 支持的 QPS 非常高（比如上万-百万峰值的 QPS ）。因为大部分业务场景都是“读多写少”针对这些技术功能性指标，下面讲解如何实现。架构尽量不分层在上一篇总结了如何利用拆分降低系统架构复杂度，通过水平拆分将一些共性的，不容易变的代码逻辑单独封装成一个模块对外服务。这样能够减少重复，提升效率。此时的读服务架构如下图 1 所示：但是实际中，通过监控系统可以发现此种架构下，读服务性能的平均值离 TP99 或 TP999 有较大的差距，通常在一倍以上。另外，性能的毛刺也比较多。产生这种情况有以下两个原因： 采用分层架构后，网络传输相比不分层的架构多了一倍 另一方面，读服务的业务逻辑都比较简单，性能主要消耗在网络传输上。因此，请求查询的数据越少，性能越好，假设为 10 ms; 数据多的时候，假设为 50ms . 当叠加上分层架构，性能就会翻倍。比如数据少时，从 10 ms 变成 20 ms ; 数据多时，从 50 ms 变成 100 ms，分层后，数据的多与少带来的性能差距达到了 80 ms, 这也是产生毛刺差的原因。于是，为了提高查询的性能，减少毛刺，同时降低部署机器的数量，可以将水平拆分的数据访问层代码工程保留独立，但是在实际编译时，直接编译到读服务里。以 Java 举例，直接将数据访问层编译为 JAR 包并由读服务进行依赖。这样在部署时，它们在同一个进程里， 去掉网络传输升级后架构如下图 2 所示：在实际的场景中，当进行此项升级后，性能有了较明显的变化（特别是数据量大的时候），TP 999 基本上下降了一半，平均性能提升了 20% ~ 30%，下图 3 展示了一个大致的效果图：上诉优化隐含的原则是：读服务要尽可能和数据靠近，减少网络传输。此项原则其实已经应用在很多类似的场景里了。比如： 目前很多浏览器自带的本地缓存功能，当浏览一个网页后，在一段时间内再次访问时，此网页的数据就是从浏览器缓存里获取的。直观的感觉就是网页打开速度非常快。 此外，CDN 也是一样的道理，把需要的数据推送到距离用户最近的机房里，缩短网络传输的距离。代码尽可能简单读服务的实现流程非常简单。读服务执行的大致流程如图 4 所示：结合图 4 所展示的内容，读服务的执行步骤，接口在接受请求时： 首先将外部的入参解析成内部模型并进行校验； 之后会根据具体的存储类型使用内部模型构建查询条件并请求对应的存储； 获取到数据后，进行反序列化转换为内部模型 根据业务情况进行适当地处理 将处理后的数据转换为对外的 SDK 的模型并返回在上述的流程里，有大量的模型映射，比如将外部的模型映射为代码中的内部模型、将内部模型映射为查询条件等。这些不同层次的模型，字段都差不多。为了提升映射效率，有时候会借助一些框架对相同的字段进行自动化的转换。对于其他能够提升效率的地方，会引入一些框架。在读服务对于性能要求非常严重的情况下，要尽可能地减少引入框架。如果非要引入不可，必须经过严格的压测。在实际的应用中，很多能够提升效率的框架性能都非常的差。比如 Java 中的 Bean.copyProperties，它采用了反射的机制进行字段的 copy ，在数据量较大时，性能较低。另外，在读服务的处理链路上，为了方便排查问题，经常直接将请求的入参以及从存储中获取的数据，使用 JSON 进行序列化为字符串，并进行日志打印，这种粗暴的方式，对性能也会有非常大的消耗，建议不要直接全量序列化，而是精细化地按需打印。最后，对于读取的内容要在存储处按需取，而不是全量取回后再在服务内部进行进行过滤。如果存储为 MySQL , 则不要使用 select * , 需要手动指定要查询的字段。如果存储为 Redis，则使用 Redis 的 hash 结构存储数据，因为 hash 结构可以在查询时指定需要返回哪些字段，其他存储结构，如 ElasticSearch 等亦然。存储的选型和架构读服务最主要依赖的中间件是存储，因此存储的性能很大程度上决定了读服务的性能。对于 MySQL、HBase 等数据库，即使使用分库分表、读写分离、索引优化等手段，在并发量大时，性能也很难达到 200ms 以内。为了提升性能，实际业务中的架构通常选用基于内存的、性能更好的 Redis 作为主存储，MySQL 作为兜底构建，如下图 5 所示的架构：上图所示的架构称为懒加载模式。在初始的时候，所有数据都存储在数据库中。当读服务接受请求时，会先去缓存中查询数据，如果没有查询到数据，就会降级到数据库中查询，并将查询结果保存在 Redis 中，以供下一次请求进行查询。保存在 Redis 中的数据会设置一个过期时间，防止数据库的数据变更了，请求还一直读取缓存中的脏数据。上述的架构设计简单清晰且实现成本较低。但是依然还存在一些潜在的问题。不能满足高可用以及完全高性能的要求。主要有以下几大类问题： 存在缓存穿透的风险如果恶意请求不断使用缓存中不存在的数据发送请求，就会导致该请求每次都会被降级到数据库中。因为数据库能够支持的并发有限，如果请求量很大，可能会把数据库宕机，进而引起读服务不可用。这样就不满足高可用这个要求。针对数据库中没有数据，可以在缓存中设置一个占位符。在第二次请求处理时，读取缓存中的占位符即可识别数据库中没有此数据，然后直接返回给业务前台系统即可。使用占位符虽然解决了穿透的问题，但是也带来了另外一个问题。如果恶意请求不断变换请求的条件，同时这些条件对应的数据在数据库中均不存在，那么缓存中存储的表示无数据的占位符也会把整个缓存撑爆，进而导致有效数据被缓存清理策略清除或者整个读服务宕机。对于此种恶意请求，就需要在业务上着手处理。对于请求的参数可以内置一些 token 或者一些验证数据，在读服务中前置进行校验并拦截，而不是透传到缓存或数据库中。 缓存集中过期导致雪崩对存储在缓存中的数据设置过期时间是为了定期获取数据库中的变更，但如果设置不合理，可能会导致缓存集中过期，进而所有的读请求都会因缓存未命中，而直接请求到数据库中。因缓存支持的量级至少是数据库的十倍以上，此类瞬时高并发的流量会直接将数据库打挂，进而宕机。对于数据库的过期时间，可以在设置时进行加盐操作。假设原先统一是 2 小时过期，设置时根据随机算法在一个区间内获取一个随机值，在 2 个小时的过期时间上再加上此随机值，这就做到了各个缓存的过期时间不一致，同时过期的缓存数量最可控。 懒加载无法感知实时变更在缓存中设置过期时间，虽然可以让用户感知到数据的变更。但感知并不是实时的，会有一定延迟。在某些对于数据变更不敏感的场景是可以的，比如编辑新发布一个新闻，但你没有看到，因为你都不知道编辑新发布了一个新闻。如果想要做到实时看到数据的变更，可以将架构升级，升级后的架构如下图 6 所示：在每次修改完数据之后，主动将数据更新至缓存里。此种方案下，缓存里的数据均和数据库保持一直。但是在细节上，还是存在一些问题。如果修改完了数据库再更新缓存，在异常情况下，可能出现数据库更新成功了，但缓存更新失败了的情况。因为数据库和缓存是两个存储，如果没有分布式事务的机制，缓存更新失败了，数据库的数据是不会回滚的。此时，缓存和数据库中的数据依然不一致，因此这个方案并没有完美解决问题。如果先更新缓存，再更新数据库，同时会因为没有分布式事务的保障，出现缓存中存在脏数据的问题。另外，在更新数据库后主动更新缓存的模式，在实际的实施层面很容易出现遗漏。因为需要在所有更新数据库的地方都加上主动更新缓存的代码，当开发人员不断变更时，很容易出现遗漏的情况，比如在某一个需求里，开发人员只更新了数据库而没有更新缓存。除了容易遗漏之外，在所有更新数据库的地方，都利用缓存和数据库的分布式事务来保证数据完全一致的成本较高。在实际工作中，成本也是一个必须要考虑的问题（之后详细总结）。 懒加载无法摆脱毛刺的困扰使用懒加载的缓存过期方案，还有一个无法避免的问题，就是性能毛刺。当缓存过期时，读服务的请求都会穿透到数据库中，对于穿透请求的性能和使用缓存的性能差距非常大，时常是毫秒和秒级别的差异。大部分普通业务场景可以容忍此问题，但在一些对性能要求极高的场景里，比如 App 首页，毛刺问题仍需要重视和解决。关于此问题的解决方法另外详细讨论。至此，懒加载架构的四个问题以及对应的潜在解决方案已总结完了。虽然懒加载架构存在一些问题，但在实际应用中，此方案及其变种方案因为实现简单、成本低、仍是使用较多的解决方案。总结本文总结了读服务在实现时应该满足的技术功能性要求，由此确定了读服务实现时应该满足的目标——应该遵循两个基本原则： 架构尽量不要分层 代码尽可能简单" }, { "title": "热点数据的查询的问题", "url": "/posts/hot-data-readService-06/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-03-15 14:33:00 +0000", "snippet": "基于 Binlog 实现的全量缓存的读服务，以及实现一个低延迟、可扩展的同步架构。可以构建出一个无毛刺且平均性能在 100ms 以内的读接口。对缓存进行分布式部署后，抗住秒级百万的 QPS 毫无压力。但上述的“百万 QPS”有一个非常重要的限制条件，即这百万的 QPS 都是分属于不同用户的。试想一下如果这百万 QPS 都属于同一个用户，系统还扛得住吗？如果采用上面的方案，必然抗扛不住的！为什么扛不住相同用户百万的流量当百万的 QPS 属于不同用户时，因缓存是集群化的，所有到达业务后台的请求会根据一定路由规则（如 Hash），分散到请求缓存集群中的某一个节点，具体架构如下图 1 所示：假设一个节点最大能够支撑 10W QPS，只需在集群中部署 10 台节点即可支持百万流量。但当百万 QPS 都属于同一用户时，即使缓存是集群化的，同一个用户的请求都会被路由至集群中的某一个节点，整体架构如图 2 所示：即使此节点的机器配置非常好，当前能够支持住百万 QPS。但随着流量上涨，它也无法满足未来的流量诉求。原因有 2 点： 单台机器无法无限升级； 缓存程序本身也是有性能上限的。此类并发次数非常大、数据完全相同的请求称为热点查询。类似的场景还有热点写，区别在于它在请求后会对数据进行写入或变更。热点查询场景除了一个用户并发百万次请求产生热查询的案例外，在实际日常生活中，会导致热查询的案例也非常多： 微博热点吃瓜事件，百万用户同一时间查询某条微博内容。对此条微博的查询就是热查询； 电商里的秒杀或者低价薅羊毛活动，为了第一时间抢到心仪的商品，成百上千万的用户会不断地刷新商品页面，等待秒杀倒计时。对此商品的查看就是热查询。主从复制进行垂直扩容虽然单机的机器配置和程序的性能是有上限的，但可以利用节点间的主从复制功能来进行节点间的扩容。主从复制开启后，一个主节点可以挂一至多个从。升级后的架构如下图 3 所示（方案解读在架构图中）：在查询时，将应用内的缓存客户端开启主从随机读。此时，包含一个从的分片的并发能力，可以提升至原来的一倍。随着从节点的增加，单分片的并发性能会不断翻倍。这对于所有请求只会命中某一个固定单分片的热点查询能够很好地应对。但此方案存在一个较大的问题，就是浪费资源。主从复制除了有应对热点的功能，另外一个主要作用是为了高可用。当集群中的某一个主节点发生故障后，集群高可用模块会自动对该节点进行故障迁移，从该节点所属分片里选举一个从节点为主节点。为了高可用模块在故障转移时的逻辑能够简单清晰并做到统一，会将集群的从节点数量设置为相同数量。相同从节点数量也带来了较大的资源浪费。为了应对热点查询，需要不断扩容从分片。但热点查询只会命中其中一个分片，这就导致所有其他分片的从节点全部浪费了。为了节约资源，可以对高可用模块进行改造，不强制所有分片的从节点必须相同，但这个代价也是非常高昂的。另外，热点查询很多时候是随时出现的，并不能提前预测，所以提前扩容某一个分片意义并不大。总的来说，主从复制能够解决一定流量的热点查询且实施起来较简单。但不具备扩展性，在应对更大流量的热点时会有些吃力。利用应用内的前置缓存热点查询是对相同的数据进行不断重复查询的一种场景。特点是次数多，但需要存储的数据少，因为数据都是相同的。针对此类业务特性，可以将热点数据前置缓存在应用程序内来应对热点查询，并解决前一小节里主从复制方案的扩展性问题。使用了前置缓存的架构如下图 4 所示：应用内的缓存存储的均是热点数据。当应用扩容后，热点缓存的数量也随之增加。在采用了前置缓存后，在面对热查询时只需扩容应用即可。因为所有应用内均存储了所有的热点数据，且前端负载均衡器（如 Nginx 等）会将所有请求平均地分发到各应用中去。使用应用内的前置缓存应对热点查询时，需要重点关注下面问题： 应用内缓存需要设置上限 应用所属宿主机的内存是有限的，且其内存还要支持业务应用使用。 因此在使用应用内的前置缓存时，必须设置容量的上限且设置容量满时的逐出策略。 逐出策略可以是 LRU，将最少使用的缓存在容量满时清理掉，因为热点缓存需要存储的是访问次数多的数据。 此外，前置缓存也需要设置过期时间，毕竟太久无访问的缓存也肯定是非热点数据，所以可以及时清理掉，提前释放内存空间。 根据业务对待延迟的问题 前置缓存的延迟问题的解决方案。要么采用定期刷新，要么采用主动刷新。 如果业务上可以容忍一定时间的延迟，可以在缓存数据上设置一个刷新时间即可。实现起来非常简单。 如果想要实时感知变化，可以采用 Binlog 的方式，在变更时主动刷新。 binlog 数据读取到 redis 的 hash 中，取数据是时候排序： redis中支持排序的类型是zset； 使用redis存储数据，将数据查询到应用代码中，进行排序； 使用 es 等其它类型的存储进行排序。 但前置缓存的主动感知不能在前置缓存的应用里实现，因为应用代码也运行在此机器上，通过 MQ 感知变更会消耗非常多的 CPU 和内存资源。 另外，前置缓存里数据很少，很多变更消息都会因不在前置缓存中而被忽略掉。为了实现前置缓存的更新，可以将前置缓存的数据异构一份出来用作判断，升级的方案如下图 5 所示： 通过异构前置缓存用作判断，可以过滤出需要处理的数据，并实时调用对应机器更新即可。此方案实现起来较复杂且异构本来也导致了延迟，实际上大部分场景设置刷新时间即可满足。 异构的前置缓存跟前置缓存不在同一台应用里，独做个工具出来。异构缓存的目的是过滤出在应用内的缓存条目，进而减轻全量数据变更的压力。 把控好瞬间的逃逸流量 应用初始化时，前置缓存是空的。 假设在初始化时，瞬间出现热点查询，所有的热点请求都会逃逸到后端缓存里。可能这个瞬间热点就会把后端缓存打挂。 其次，如果前置缓存采用定期过期，在过期时若将数据清理掉，那么所有的请求都会逃逸至后端加载最新的缓存，也有可能把后端缓存打挂。这两种情况对应的流程图如下图 6 所示： 对于这两种情况，可以对逃逸流量进行前置等待或使用历史数据的方案。不管是初始化还是数据过期，在从后端加载数据时，只允许一个请求逃逸。这样最大的逃逸流量为部署的应用总数，量级可控。架构如下图 7 所示： 对于数据初始化为空时，其他非逃逸的请求可以等待前置缓存的数据并设置一个超时时间。 对于数据过期需要更新时，并不主动清理掉数据。其他非逃逸请求使用历史脏数据，而逃逸的那一个请求负责把数据取回来并刷新前置缓存。 如何做到只有一个逃逸流量从应用发出： caffeine这个java实现的缓存，可以做到只有一个逃逸流量从应用发出。 如何判定是逃逸流量，它和正式流量怎么区分呢？ 当本地缓存过期了，才会有逃逸流量。它的作用是更新本地缓存的数据。 如何发现热点缓存并前置 除了需要应对热点缓存，另外一个重点就是如何发现热点缓存。 对于发现热点有两个方式： 一种是被动发现 借助前置缓存有容量上限实现的。 在被动发现的方案里，读服务接受到的所有请求都会默认从前置缓存中获取数据： 如不存在，则从缓存服务器进行加载。因为前置缓存的容量淘汰策略是 LRU； 如果数据是热点，它的访问次数一定非常高，因此它一定会在前置缓存中。借助前置缓存的容量上限和淘汰策略，即实现了热点发现。 缺点：所有的请求都优先从前置缓存获取数据，并在未查询到时加载服务端数据到本地的前置缓存里，此方式也会把非热点数据存储至前置缓存里，导致非热点数据产生非必要的延迟性。 另外一种是主动发现： 需要借助一些外部计数工具来实现热点的发现。 外部计数工具的思路大体比较类似，都是在一个集中的位置对于请求进行计数，并根据配置的阈值判断某请求是否会命中数据。 对于判定为热点的数据，主动的推送至应用内的前置缓存即可。 下图 8 为在缓存服务器进行计数的架构方案： 采用主动发现的架构后，读服务接受到请求后仍然会默认的从前置缓存获取数据，如获取到即直接返回。 如果未获取到，会穿透去查询后端缓存的数据并直接返回。但穿透获取到的数据并不会写入本地前置缓存。数据是否为热点且是否要写入前置缓存，均由计数工具来决定。此方案很好地解决了因误判断带来的延迟问题。 降级兜底不可少在采用了前置缓存并解决了上述四大类问题之后，当再次遇到百万级并发时，基本没什么疑难问题了。但这里还存在一个前置条件，即当热点查询发生时，所部署的容器数量所能支撑的 QPS 要大于热点查询的 QPS。但实际情况并非如此，所部署的机器能够支持的 QPS 未必都能够大于当次的热点查询。对于可能出现的超预期流量，可以使用前置限流的策略进行应对。在系统上线前，对于开启了前置缓存的应用进行压测，得到单机最大的 QPS。根据压测值设置单机的限流阈值，阈值可以设置为压测值的一半或者更低。设置为压测阈值的一半或更低，是因为压测时应用 CPU 基本已达到 100%，为了保证线上应用能够正常运转，是不能让 CPU 达到 100% 的。架构如下图 9 所示：根据此方案你可以看到，在做架构设计时，即使已经做了非常多的应对方案，最后的兜底降级还是必不可少，因为超出预期的事情说来就来。其他前置策略应对热点查询时，除了采用后端应用内的前置缓存进行应对外，在前端的架构里也有一些应对手段。比如： 在接入层（如 Nginx）进行前置缓存、数据前置至离用户更近的 CDN 及开启浏览器缓存。总结使用主从复制，以及前置缓存应对热点查询。同时，对于前置缓存中存在的四大类问题，缓存上限、延迟问题、逃逸流量应对，以及热点数据发现应对方案。在流量超出预期之后，如何进行降低兜底，以及在使用了后端的各种应对策略之后，在前端和接入层有哪些架构方案可供选择。 异构的前置缓存，数据是存在于哪里？是redis ，还是跟前置缓存一样都存在于应用服务器？ 不在同一台应用里，可以单独做个工具出来。异构缓存的目的是过滤出在应用内的缓存条目，进而减轻全量数据变更的压力。 异构的前置缓存跟前置缓存都存在于一台服务器吗？ redis中支持排序的类型是zset，你可以查看是否能够满足你的需求。其它的排序方式有，1）使用redis存储数据，将数据查询到应用代码中，进行排序。2）正如你所说的，使用es等其它类型的存储进行排序" }, { "title": "异构数据的同步一致性", "url": "/posts/consistency-readService-05/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-03-08 14:33:00 +0000", "snippet": "基于 Binlog 完成数据同步的全量缓存的读服务架构方案，虽然可以实现平均性能在一百毫秒以内的高可用方案。满足缓存同步的实时性要求，够降低同步的复杂度，解决分布式事务问题。但是还不能直接落地复用，还有好多的坑需要注意！！！基于 Binlog 的全量缓存架构问题问题一：Binlog 延迟低是指纯 MySQL 的主从同步从上图 1 中可以看出，基于 Binlog 的缓存数据同步和纯 MySQL de 主从同步在架构上是存在 4 个区别： MySQL 的主从同步是纯数据同步，格式和协议完全适配，因此性能损耗极低。而 Binlog 同步经过协议转换的，有一定的性能损耗； 基于 Binlog 的同步比MySQL的主从同步多了两个模块，因此整体链路比也较长； 在实际场景里，为保持稳定性，同步的是从库的 Binlog，这也会导致延迟进一步加大； 因为 Binlog 是串行的，这会导致同步的吞吐量太低，进一步加大同步的延迟。以上这 4 个问题，都会导致 Binlog 的实际延迟时间要比预期的要高。问题二：Binlog 格式解析抽象来看，程序是数据和逻辑的组合。所有的程序都要按照一定的业务规则对某种数据处理才能产生价值。Binlog 的同步程序也是一样，Binlog 同步转换程序处理的是 Binlog 的数据。那 Binlog 的格式是什么样的？是每次变更的 SQL，还是其他维度的数据？这关系到同步程序的设计方案，以及对应的实现的复杂度。问题三：如何保证数据不丢失或错误MySQL 的主从同步逻辑是和业务数据无关的，正式版本发布之后，修改的频率比较低。而基于 Binlog 实现的业务数据同步程序是易变的，因为互联网业务需求迭代周期非常快，在业务高速迭代的过程中，如何保证写出没有 Bug 的代码？如何保证同步的数据不丢失、不出错呢？问题四：如何设计缓存数据格式？现在主流的数据库（如 Memcache、Redis 等）不只提供 Key-Value 的数据结构，还提供了其他丰富的数据结构类型。如何利用和设计这些数据结构，来提升数据查询和写入时的性能，同时降低代码的复杂度呢？发送 Binlog此处的方案，以 MySQL 作为示例进行讲解，其他类型的数据库以此类推，举一反三。MySQL 的 Binlog 分为三种数据格式： statement 格式； row 格式； mixed 格式。基于下面示例表来，分别介绍上述三种格式：CREATE TABLE `demo_table` ( `id` bigint(0) NOT NULL AUTO_INCREMENT COMMENT &#39;主键&#39;, `message` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL COMMENT &#39;消息&#39;, `status` tinyint(0) NOT NULL COMMENT &#39;状态&#39;, `created` datetime(0) NOT NULL COMMENT &#39;创建时间&#39;, `modified` datetime(0) NOT NULL COMMENT &#39;修改时间&#39;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Dynamic;statement 格式statement 格式把每次执行的 SQL 语句记录到 Binlog 文件里，在主从复制时，基于 Binlog 里的 SQL 语句进行回放来完成主从复制。比如执行了如下 SQL 成功后：update demo_table set status=&#39;无效&#39; where id =1Binlog 中记录的便是上述这条具体的 SQL。采用 SQL 格式的 Binlog 的好处： 是内容太少，传输速度快。缺点是： 在基于 Binlog 进行数据同步时，需要解析上述的 SQL， 获取变更的字段，存在一定的开发成本。row 格式row 格式的 Binlog 会把当次执行的 SQL 命中的那条数据库行的变更前和变更后的内容，都记录到 Binlog 文件里。以上述 statement 格式里的 SQL 作为示例，该 SQL 在 row 格式下执行后会产生如下的数据：{ &quot;before&quot;: { &quot;id&quot;: 1, &quot;message&quot;: &quot;文本&quot;， &quot;status&quot;: &quot;有效&quot;, &quot;created&quot;: &quot;xxxx-xx-xx&quot;, &quot;modified&quot;: &quot;xxxx-xx-xx&quot; }, &quot;after&quot;: { &quot;id&quot;: 1, &quot;message&quot;: &quot;文本&quot;， &quot;status&quot;: &quot;无效&quot;, &quot;created&quot;: &quot;xxxx-xx-xx&quot;, &quot;modified&quot;: &quot;xxxx-xx-xx&quot; }, &quot;change_fields&quot;: [&quot;status&quot;]}上述案例记录的 Binlog 数据非常全面，包含了 demo_table 中所有字段对应的变更和未变更的数据，同时标记了具体哪些字段发生了变更。在数据同步时，可以完全以它为准。优点：实现代码会非常简单；缺点：产生的数据量较大。mixed 格式mixed 模式是上述两种模式的动态结合。采用 mixed 模式的 Binlog 会根据每一条执行的 SQL 动态判断是记录为 row 格式还是 statement 格式。比如一些 DDL 语句，如新增加字段的 SQL，就没有必要记录为 row 模式，记录为 statement 即可，因为它本身并没有涉及数据变更。在实际应用中，推荐使用 row 模式或者 mixed 模式，主要有以下两个原因。 这两种格式的数据量全，可以做更多的逻辑。因为随着业务需求的发展，同步逻辑会出现非常多的个性化需求，越多信息的数据，在编写代码时会越简单。 row 模式无须解析SQL，实现复杂度非常低。在执行的 SQL 非常复杂时，对 statement 模式里记录的 SQL 的解析需要耗费大量开发精力，越复杂的解析越容易产生 Bug，所以推荐更加简单的 row 模式的数据格式。 Binlog 如何高效消费在技术上，数据消费有两种常见模式：串行和并行。全串行的方式进行消费以 MySQL为例，不管是表还是 SQL 维度的数据，都需要将整个实例的所有数据变更写入一个 Binlog 文件。在消费时，对 Binlog 文件使用 ACK 机制进行串行消费，每消费一条确认一条，然后再消费一条，以此重复。具体消费形式如下所示：此类模式的消费存在两个问题： 串行消费效率低，延迟大。假设一次同步 20ms 左右，同步 10W 条数据就需要 30min 左右； 单线程无法利用水平扩展，架构有缺陷。当前数据量小，可以满足。但当数据量增大后，此模式是无法通过水平扩展来提升性能。采用并行的方式提升吞吐量及扩展性Binlog 的单文件及 ACK 机制，导致必须去串行消费。实际上，通过一些技术手段，能够对 Binlog 文件里的不同库、不同表的数据进行并行消费的，因为不同库之间的数据是不相关的。为了在 Binlog 原有的串行机制下完成按库的并行消费，整体架构需要进行一定升级，具体如下图所示：上述架构里，借用了 MQ 进行拆分。在 Binlog 处仍然进行串行消费，但只是 ACK 数据。ACK 后数据直接发送到 MQ 的某一个 Topic 里即可。因为只做 ACK 并转发至 MQ，不涉及业务逻辑，所以性能消耗非常小，大概只有几毫秒或纳秒。现在大部分的 MQ 中间件都支持数据并行消费，在开发时，上图中的数据转换模块在消费数据时，开启并行乱序消费即可。此时虽然完成了从串行消费到并行消费的升级，提升了吞吐量和扩展性，但也因并行性带来了数据乱序的问题。 比如对某一条微博连续修改了两次，第一次为 A1，第二次为 A2。如果使用了并行消费，可能因为乱序的原因，先接收到 A2 并写入缓存再接受到 A1。此时，微博中就展示了 A1 的内容，但缓存中的数据 A1 是脏数据，实际数据应该是 A2。因此需要继续对升级后的方案进行改造。对于并行带来的数据错乱问题，有两个解决方案： 方案一：加分布式锁实现细粒度的串行 此方案和 Binlog 的串行区别是粒度。以上述修改微博为例： 在数据同步时，只需要保证对同一条微博的多次修改串行消费即可，而多条微博动态之间在业务上没有关系，仍然可以并行消费。 在实施时，加锁的维度可以根据数据是否需要串行处理而定，它可以是表中的一个字段，也可以是多个字段的组合。 确定加锁的维度后，数据库中的多张表可根据需要使用此维度进行串行消费，具体可参考后面的“缓存数据结构设计及写入”。 此方案虽然可以解决乱序问题，但引入了分布式锁，且需要业务系统自己实现，出错率及复杂度均较高。 方案二：依赖 MQ 中间件的串行通道特性进行支持 采用此方案后，整个同步的实现会更加简单。还是以上述修改微博为例： 在“Binlog 订阅及转发模块”转发 Binlog 数据前，会按业务规则判断转发的 Binlog 数据是否在并发后仍需要串行消费，比如上面提到的同一条微博的多次修改就需要串行消费，而多条微博间的修改则可以并行消费，它不存在并发问题。 判断需要串行消费的数据，比如同一条微博数据，都会发送到 MQ 中间件的串行通道内。在同步模块进行同步时，MQ 中间件里的串行通道的数据均会串行执行，而多个串行通道间则可以并发执行。借助 MQ 中间件的此特性，既解决了乱序问题又保证了吞吐量。 很多开源的 MQ 实现都具备此小节介绍的功能，如 Kafka 提供的 Partition 功能。改造后的架构如图 4 所示： 最后，在采用了 MQ 进行纯串行转并行时，将 Binlog 发送到 MQ 可以根据情况进行调整，当数据量很大或者未来很大时，可以将 Binlog 的数据按表维度发送到不同的 Topic。 一是能够实现扩展性； 二是可以提升性能； 三是通过不同表使用不同的 Topic，可以起到隔离的作用，减少表之间的相互影响。 缓存数据结构设计及写入现在常用的缓存大多数为 Redis 或者它的变种，所以以 Redis 支持的为准，总结缓存结构设计。非 Redis 缓存提供的数据结构可能有所差异，但思路类似（举一反三）。数据库表是按技术的范式来设计的，会将数据按一对一或一对多拆分成多张表；缓存中则是面向业务设计的，会尽可能地将业务上一次查询的数据存储为缓存中的一个 Value 值。比如，订单至少要包含订单基本信息和用户的购买商品列表： 在数据库中，会设计订单基本信息表和商品表； 在缓存中，会直接将订单基本信息和商品信息存储为一个 Value 值，方便直接满足用户查询订单详情的需求，减少和 Redis 的交互次数。这种在数据库中多张表存储，而在缓存中只用 K-V 结构进行冗余存储的数据结构，需要在数据同步的时候进行并发控制，防止因为多张表的变更导致并发写入，从而产生数据错乱。多张表间共享分布式锁进行协调以上述订单为例，数据库中的订单信息表和商品表均存储了订单号，在数据同步时，可以使用订单号进行加锁。当订单基本信息或订单中的商品同时发生变更后，因为使用了订单号进行加锁控制，在数据同步时，两张表归属同一订单号的数据实际为串行执行。因缓存中同一个订单的基本信息和商品是存储在一起的，更新时需要把缓存中的数据读取至同步程序并替换掉此次变更的内容（如某一个发生变更的商品信息），再回写至缓存中即可。在 Redis 中，可以考虑使用 Lua 脚本完成上述过程。此方式虽然可以解决因 Redis 和数据库表设计不匹配带来的问题，当多张表之间加锁又降低了吞吐量。采用反查的方式进行全量覆盖在同步时，采用反查数据库的方式来补齐 Redis 需要的数据。以上述订单为例，当订单基本信息变更时，可以在同步模块通过数据库反查此订单下的所有商品信息，按 Redis 的格式组装后，直接更新缓存即可。采用反查的方式虽然简单，但反查库会带来一定性能消耗和机器资源（如 CPU、网络等）的浪费。而且在变更量大的情况下，反查的量可能会把数据库打挂。因此，在采用反查方案时，建议反查发送 Binlog 的从库，从而保障主库的稳定性。采用 Redis 的 Hash 结构进行局部更新参考数据库的多张表设计，缓存中也可以进行多部分存储。在 Redis 中，可以采用 Hash 结构。对于一个订单下的不同表的数据，在 Redis 中存储至各个 field 下即可，同时 Redis 支持对单个 field 的局部更新。结构如下图所示：在上述订单案例的多张表变更时，同步程序无须对多张表间进行分布式加锁协调，哪张表变更就去更新缓存中对应的局部信息即可。不管是同步性能还是实现难度均较好。在查询时，直接使用订单号即可查询到所有信息。为什么使用 Hash 结构，而不使用所有缓存都支持的 Key + Value 的结构呢？其中，Key 设计为订单号+子表标识，如 Key 为 OrderId_BaseInfo，表示某一个订单的基本信息，或者 Key 为 OrderId_SkuId，表示某一个订单下的某个商品基本信息。主要有三个原因： 首先，使用了 KV 结构后，查询时需要使用多个命令。如果提供了批量命令，也可以使用批量命令解决此问题； 其次，一个订单下的商品是动态的，无法提前固定。如果全部改为 KV 结构，就无法查询到订单详情了。除非再异构一份订单下所有商品的 ID 列表； 最后，现在主流缓存都是分布式部署的。如果采用 KV 的分割设计，很有可能一个订单的基本信息和商品信息被存储在两个分片上，此处查询的性能和复杂度也会上升。因 Redis 是使用 Key 进行分布式路由的，采用 Hash 结构的数据都存储在同一个分片上，不会出现跨分片查询的问题。数据对比发现错误数据同步模块是基于业务进行数据转换的，在开发过程中，需要基于业务规则不断地迭代。此外，为了保证吞吐量和性能，整个基于 Binlog 的同步方案做了很多升级和改造。在这个不断迭代的过程中，难免会出现一些 Bug，导致缓存和数据库不一致的情况。为了保障数据的一致性，可以采用数据对比进行应对，架构如下图所示：数据对比以数据库中的数据为基准，定期轮询对比缓存和数据库的数据。如果发现不一致后，可以增加延迟重试，再次对比。如果多次对比不一致后，可以增加报警并保留当时的数据，之后以数据库中的数据为准刷新缓存。延迟重试是为了防止因同步的时差，出现短暂的数据不一致但最终数据一致的情况。其次，保留出错现场的数据是为了排查定位问题。 ps: 数据对比需要寻找一个基准对比源。比如缓存和数据库的数据对比，一般会选择数据库作为基准源，因为数据库具备 ACID，数据原则上是准的。 在对比时，需要规避因为对比带来的技术问题。因为对比实现时，很多时候是基于数据库进行扫描，此扫描的速度有可能很大，有可能会把数据库打挂。 建议在对比时，使用数据库的从库对比缓存的从分片。 最后，对比时，对于错误数据需要多次重复对比。因为某一次对比有可能是因为数据延迟导致的，再对比一次就 ok 了。如果多次对比均错误，才判断为错误数据。 对于物理删除的情况，可以基于 binlog 进行 监听发现最后的兜底，直接写入虽然上述在提升同步吞吐量上做了非常多地设计，但不可否认延迟总是存在的，即使是纯数据库主从同步间也会因为网络抖动和写入量大的情况出现毫秒或者秒级延迟，本讲基于 Binlog 的改良方案自然不例外。绝大部分的业务和场景，对于毫秒或秒级延迟无感知。但为了方案的完整性和极端场景的应对，可以在异步同步的基础上，增加主动同步。方案如下图所示： 当多个用户同时修改一个值时，直接写缓存就会出现脏数据，兜底直接写入应该怎么处理这样的情况 ! 缓存中可以记录数据的版本号。同时，在数据库中，每一个表里都需要有一个版本号字段，为version。在每一次修改数据库中的数据式，可以使用乐观锁的方式，对版本号进行加1。在使用binlog刷新缓存时，基于版本号进行并发控制即可。上述的架构是对一些关键场景在写完数据库后，主动将数据写入缓存中去。但对于写入缓存可能出现的失败可以不处理，因为主动写入是为了解决缓存延迟的问题，主动写入导致的丢失数据由 Binlog 保障最终一致性。此架构是一个技术互补的策略，Binlog 保证最终一致性但可能存在延迟，主动写入保障无延迟但存在丢数据。在架构中，可以采用此思路。一个单项技术无法完美解决问题时，可以对短板寻找增量方案，而不是整个方案完全替换。 解决多条件的查询和分页查询？？？难道把用户订单全部拿到，再在代码里按条件筛选和分页？？？如果多条件的查询也需要很高的性能，可以采用空间换时间的方式，将数据按查询的条件异构到缓存里。 而对于翻页查询，一般来说都隶属于管理后台，对于性能要求中等，可以使用 ElasticSearch 来实现支持 如果不使用 Binlog，最简单的方式便是job。但在执行时，job需要去扫从库，看到很多场景，因为跑全量job把主库打挂，进而影响业务的。" }, { "title": "使用全量缓存构建毫秒级的读服务", "url": "/posts/full-cache-readService-04/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-03-07 11:33:00 +0000", "snippet": "利用全量缓存打造一个无毛刺、平均性能在 100ms 以内的读服务。全量缓存的基本架构全量缓存是指将数据库中的所有数据都存储在缓存中（需要高性能查询的数据，而不是全部业务数据。如果高性能的数据依然很多，可以对数据按查询频率进行区分。缓存里只存储高频率访问数据，其他低频率数据可以放到 hbase 或者数据库里。 ），同时在缓存中不设置过期时间的一种实现方式，此实现的架构如下图所示： 优点：所有数据都存储在缓存里，读服务在查询时不会再降级到数据库里，所有的请求都完全依赖缓存。此时，因降级到数据库导致的毛刺问题就解决了。 缺点：全量缓存并没有解决更新时的分布式事务问题，反而把问题放大了。因为全量缓存对数据更新要求更加严格，要求所有数据库已有数据和实时更新的数据必须完全同步至缓存，不能有遗漏。 对于此问题，一种有效的方案是采用订阅数据库的 Binlog 实现数据同步。基于 Binlog 的全量缓存架构Binlog 是 MySQL 及大部分主流数据库的主从数据同步方案： 主数据库会将所有的变更按一定格式写入它本机的 Binlog 文件中。 在主从同步时，从数据库会和主数据库建立连接，通过特定的协议串行地读取主数据库的 Binlog 文件，并在从库进行 Binlog 的回放，进而完成主从复制。现在很多开源工具（如阿里的 Canal、MySQL_Streamer、Maxwell、Linkedin 的 Databus 等）可以模拟主从复制的协议。通过模拟协议读取主数据库的 Binlog 文件，从而获取主库的所有变更。对于这些变更，它们开放了各种接口供业务服务获取数据。基于 Binlog 的全量缓存架构正是依赖此类中间件完来成数据同步的，架构如下图所示：将 Binlog 的中间件挂载至目标数据库上，就可以实时获取该数据库的所有变更数据。对这些变更数据解析后，便可直接写入缓存里。采用了 Binlog 的同步方案后，全量缓存的架构变得更加完整，主要表现在以下 3 个方面： 降低了延迟；缓存基本上是准实时的，数据库的主从同步保持在毫秒级别，数据库的数据变更可以实时地反映到缓存里； 解决分布式事务的问题；Binlog 的主从复制是基于 ACK 机制，如果同步缓存失败了，被消费的 Binlog 不会被确认，下一次会重复消费，数据最终会写入缓存中。这就解决了因无法满足分布式事务而导致的丢数据问题，保障了数据的最终一致性； 提升代码的简洁性和可维护性；因为所有对数据库的修改最终都会反映到 Binlog 里，只要数据库的表结构不变更，对 Binlog 数据的处理程序就能保持固定。想象一下，如果采用在代码里添加主动更新缓存的方式，那么每增加一个对数据库修改的接口，都需要加上更新缓存的代码。相比 Binlog 的方式，它的维护成本和出错概率就高得多。任何方案在带来某一方面的提升时，必然是在其他方面做出了一些取舍，架构其实是一门平衡的艺术。使用 Binlog 的全量缓存存在的问题在使用的了 Binlog 的全量缓存时，会带来两个问题： 提升了系统的整体复杂度。当架构中只存在一个数据库中间件时，系统相对比较简单。当使用了 Binlog 后，整个数据同步的流程变长，且关注点和出错点由一个中间件变为了两个； 缓存的容量会成倍上升，相应的资源成本也大幅上升。在一些对性能要求极致且实时性高的场景下，只能进行取舍，为了获取这些增强的能力，需要付出一定的代价。除了取舍之外，在技术上还有几点可以进行提升： 首先是存储在缓存中的数据需要经过筛选，有业务含义且会被查询的才进行存储。比如数据库常见的修改时间、创建时间、修改人、数据有效位等一些记录性字段可以不存储在缓存中； 其次是存储在缓存中的数据可以进行压缩。可以采用 Gzip、Snappy 等一些较常见的压缩算法进行处理，但压缩算法通常较消耗 CPU。在实际选型时，可以先压测再评估是否选择。如果无法承受压缩带来的 CPU 消耗，希望在缓存中直接存储 JSON 格式的数据或 Redis 的 Hash 结构的数据。三个节约缓存的小技巧： 将数据按 JSON 格式序列化时，在字段上添加替代标识，表示在序列化后此字段的名称用替代标识进行表示。假设有一个 DemoClass 类，采用了替代标识后的格式，如下所示： package cn.happymaya.jvmpractise; public class DemoClass { @Field(&quot;1&quot;) private field1; @Field(&quot;2&quot;) private field2; } 采用了此方式序列化后的数据如下所示： { &quot;1&quot;: &quot;field1Value&quot;, &quot;2&quot;: &quot;field2Value&quot;} 而没有采用此标识的数据如下所示： { &quot;field1&quot;: &quot;field1Value&quot;, &quot;field2&quot;: &quot;field2Value&quot;} 从上面的示例来看，虽然只节约了 field1 和 field2 两个 key 的长度，数据量并不大。 但是要在缓存中存储上千万、上亿条类似的数据，整体数据量还是非常可观的。 另外，当前主流的 JSON 序列化工具均已支持此技巧，比如 Java 里的 Gson、FastJSON 等。 如果使用的缓存是 Redis 且使用了其 Hash 结构存储数据。其 Hash 结构的 Field 字段，也可以使用和上述 JSON 标识一样的模式，使用一个较短的标识进行代替。在使用全量缓存时，节约的数据也是非常可观的。 使用全量缓存承接读服务所有的请求时，会出现无法感知缓存丢失的问题。比如 Redis 等缓存实现虽然提供了持久化、主从备份等功能，但它为了性能，并没有提供类似数据库的 ACID 等功能，在极端情况下仍然会丢失数据。 为了保留全量缓存的优点同时解决此极端问题，可以采用异步校准加报警及自动化补齐的方式来应对。此方案的架构如下图所示：当读服务查询缓存中无数据后，会直接返回空数据给到调用方（上图中标记 1 处）。与此同时，它会通过 MQ 中间件发送一条消息（上图标记 2）。此消息的消费程序会异步查询数据库（见上图的标记 3），如果数据库确实存在数据，则会进行一次告警或者一次记录，并自动把数据刷新至缓存中去（见上图的标记 4）。此方案是一个有损方案，如果数据在数据库中真实存在而在缓存中不存在，调用方第一次调用请求获取到的是空数据，为什么还要使用此方案呢？其实这种情况在现实场景中出现的概率极低。在我的经历里，在线上已经关闭了此异步校准方案，主要从以下 4 个方面来考虑： 根据数据统计， 数据在数据库中存在而在缓存中不存在的概率几乎为零； 对数据库进行大量无效的异步校准查询会导致数据库性能变差； 即使缓存里数据丢失，只要此条数据存在变更，Binlog 都会把它再次刷新至缓存里。如果此条数据一直不存在变更，说明它是死数据，价值也不会太大； 如果将此方案应用到生产环境里，同时开启了异步校准，依然存在大量数据丢失的情况，那说明对于缓存中间件的使用和调优还有很大的提升空间。毕竟，此类数据丢失大多都是中间件自身导致的。不应该本末倒置，为了弥补缓存中间件的问题，而让业务团队做太多的补偿工作。虽然最后没有采用此有损补偿方案，但这个思考和论证过程收获满满（ps：在工作中遇到类似的问题，需要决定是否采用某个技术方案时，可以类比上述方法，通过推理和数据验证来做最终决定）其他优化点在使用了 Binlog 的同步方案后，整个数据同步变得非常简单。数据同步模块接到 Binlog 的数据后，进行一定规则的数据转换后，便可直接写入缓存。多机房实时热备为了提升性能和可用性，将数据同步模块写入的缓存由一个集群变成两个集群，此时的架构演化为如下图的所示：在部署上，如果资源允许，两套缓存集群可以分别部署到不同城市的机房或者同城市的不同分区。另外，读服务也相应地部署到不同城市或不同分区。在承接请求时，不同机房或分区的读服务只依赖同样属性的缓存集群。此方案有两个好处： 提升了性能。此方式和上一讲里提到的原则——读服务不要分层，服务要尽可能地和数据靠近其实是一个逻辑； 增加了可用性。当单机房出现故障时，可以无缝地将所有流量都切换至存活的机房或分区。此方案的切换时间可以达到分钟级或秒级，高可用毋庸置疑。此方案虽然带来了性能和可用性的提升，但代价是资源成本的上升。异步并行化最简单的读服务场景是一次请求只和存储交互一次，但实际上很多时候交互都不止一次。对于需要多次和存储交互的场景，可以采用异步并行化的方式——接收到一次读请求后，在读服务内部，将串行与存储交互的模式改为异步并行与存储进行交互，形式如下图所示：如果一次读请求和存储需要交互三次，假设每次交互时间为 10ms，采用串行的方式总耗时为 30ms，而采用了异步并行的方式后，三次交互为并行执行，总耗时仍为 10ms。整体的性能提升了很多。但异步并行缺点如下： 首先，异步并行增加了线程的消耗，每一个异步并行都对应一个线程，进而带来 CPU 的消耗； 其次，异步并行的多线程开发也带来的编程复杂度和维护难度； 最后，异步并行化只能应用在每一次和存储交互都是独立的、无先后关系的场景里。除了上述场景可以采用异步并行化外，对于一次请求查询一批数据的场景也可以进行异步并行化。当查询的一批次数据较多时，大部分性能都消耗在串行的等待网络传输上。可以将这个批次拆分成多个子批次，对每个子批次使用异步并行化的方式和存储交互，性能也会有很大的提升。具体子批次设置为多少，可以在实践中根据压测来决定。总结本文总结了一种能够解决毛刺和更新实时性问题的全量缓存的完整架构方案。此方案提供的性能和高可用的能力，基本上可以满足各大互联网的业务场景里对于读服务的性能和容灾指标的要求。即使是近些年越来越热闹、流量瞬间并发非常大的电商大促活动，只要对方案稍加改造，也能够轻松应对。但上述方案带来了另外的问题，即成本的消耗。任何新的架构和应对方案都不是完美的，架构是解决问题的方案，是取舍的艺术。 全量缓存应对多个参数查询： 将多个参数组成缓存key来进行筛选，这是一种空间换时间的方案。比如用户有待审核、已审核、已驳回几种状态，假设要查询已审核的用户，那么在redis中存储数据时，就需要以用户+状态作为缓存 key。 订阅 Binlog 读取位置： 订阅 Binlog 那一刻起的，所有插入和更新的数据，都不会遗漏。 在此之前的历史数据，有一种简单的方式是： 如果表里有如：修改时间这些非业务字段，可以在订阅binlog 之后，手动的更新所有历史数据的修改时间，便可以触发 Binlog了，就可以实现同步历史数据了。 redis 缓存集群方式用哨兵模式还是cluster模式： 哨兵是用来做主从故障自动发现和切换，如果要用哨兵来实现集群，需要开发集群数据分布方案。如果没有自己的中间件开发和运维团队，可以直接使用redis的cluster模式。 缓存数据和底层数据出现数据不一致的情况： 数据同步随着需求需要不断变更和开发，难免会存在同步的BUG，因此数据会出现不一致，使用 Binlog也避免不了。在上线后，可以开发数据校准模块，定时的校准缓存和数据库中的数据。 肯定会有延迟： 正常在几秒左右，极端会有几分钟。如果业务上可以容忍，则可以不降级查库； 如果业务上不允许，对于对延迟要求非常高的场景，也可以在写入数据库后，主动的将数据写入缓存。不过这个主动写入不用花太多精力保证强成功，最终数据强成功可以通过 Binlog 进行兜底。这个主动写入的目的是保证实时性。 多维度查询，比如类型、时间。目前的想法是存多份不同维度指向ID，不同条件查询取并集。数据量大概在 50W 以内： 如果太复杂的查询场景，不建议使用缓存。因为条件不断增加，异构的成本会特别高，也不具备扩展性。 除了redis之外，可以考虑 HBase，并结合一些开源的方案，实现HBase的二级索引。 针对新的需求加进来（新的查询需求场景），修改数据同步模块以实现" }, { "title": "利用“拆分”降低架构复杂度", "url": "/posts/use-splitting-to-reduce-complexity-architecture-02/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-03-05 12:45:00 +0000", "snippet": "上一文，从技术的目的性这个维度，将业务后台系统的类型归为三大类： 读业务； 写业务； 扣减业务。除了基于技术维度的拆分，在后台架构里还有很多其他形式的拆分，比如上文里面的外卖系统架构，整个后台可以拆分为： 用户 订单 商品 价格等模块通过拆分可以将一个涉及面广、复杂度高的系统拆解为多个小模块，让各个团队单独负责并专项击破。拆分是架构设计大型复杂系统的第一步，对降低系统复杂性有着决定性的意义，它也是架构师的必备技能之一。拆分的必要我们解决复杂问题的能力是有限的，当问题涉及面广、情况复杂时，自然会去寻找方法提升效率。我们的业务后台系统就是一个“复杂问题”，而解决“这个问题”的方法便是拆分——将复杂问题拆解为多个相对简单的小问题，分而治之、各个击破，这样做极大地提高了解决复杂问题的可能性和效率。以电商系统为例，建设一个支持千万用户同时使用、日均流量上亿的电商系统也是一个十分庞大、复杂、涉及面广的工程。至少需要上百名研发、产品、测试等不同职能的人员共建。想象一下，如果没有系统拆分，所有人聚集在一起，开会讨论需求和技术方案的场景。先不说别的，把所有人召集齐都是一项十分费时的事情，应该是一个顶级的项目经理才能做到。其次是沟通的效率，上百人一起开会，七嘴八舌，一天都得不出什么重要结论，更别说梳理出细节流程了。最后，在开发的时候，上百名研发使用同一个代码仓库，每天你至少需要花费几个小时在解决代码冲突的问题上。使用此方式推进项目，上线可能遥遥无期了。即使能够上线，线上质量也是堪忧。面对这样的情况： 首先，应该是几个业务专家、技术架构一起对整个系统进行全面的梳理和分析。然后便是对系统进行拆分，将大系统按一定的规则（按照不同维度）拆分成多个小模块； 拆分后，各个产研测团队就可以认领工作了，而后安心处理自己模块内的工作（产品设计、代码开发等）。对于模块间需要交互的地方，每个团队出一个资深的产品和架构师进行沟通即可。沟通成本降低了，效率自然也就提升了。通过上述案例，可以看出拆分在大型系统开发中的重要作用： 一个好的拆分能够降低各个模块间的耦合性，极大地降低效率消耗，提升系统成功的可能性。什么时候拆分此外，一些系统在最开始的时候并没有预计到最终会发展到什么量级，为了满足快速上线的要求，没有经过拆分就直接采用了单体架构的模式进行落地。对于此类系统，是不是需要立马做拆分呢？其实并不是。当单体架构存在以下 3 类问题时，建议考虑进行拆分： 系统常年处于多需求并行开发，导致代码维护成本太大 如果一个系统变得非常庞大，它的需求及并行的需求就会非常多。 每个并行需求都会有一个开发分支，当开发需求完成陆续上线，那么在代码合并时就会产生冲突，比如自己写的一个功能块被别人改了、新增了一个类似的公共类等，此时就可以考虑进行系统拆分。 在开发一个需求时，花费几天时间阅读代码，而开发只要一半甚至更少的时间 在一个过于庞大的系统里，如果长期处于熟悉代码的时间和实际编写的时间配比不对等的情况，此时就应考虑进行系统拆分。 因为信息过载了，无法完全掌握此系统。 如果不拆分，每次重新熟悉代码并进行修改，可能会导致 Bug 频发，线上质量低下。 经过拆分后，你只需负责你熟悉的小模块，其他部分依赖第三方接口即可，让更专业的人保证它的质量。 在一个需求上线前，需要召集上百人对齐上线风险和上线步骤 对于上面的第二个问题，可能会说，找不同的人负责系统里的不同部分不就好了。 这虽然可以解决熟悉的问题，但耦合度是解决不了的。一个需求由十个人开发一个系统和三波人开发三个系统，这期间的沟通成本、耦合度等是无法比拟的。 想象一下，一屋子开会的人你一言我一语和确定好边界后，三波人单独开会，一定是后者更高效。 拆分的前提条件当系统满足上述三个场景，心中已经有了准备好大干一场的想法。但在开始拆分之前，有几个点要注意： 第一是团队和人员的准备。 拆分的主要原因是信息过载，因此在拆分后，对应的系统也要划分给新增的团队，而不是原有人员继续维护，这样只是换汤不换药，信息依然过载。 亚马逊的 CEO 曾提出“两个比萨”理论——一个团队的人员能够吃下两个比萨刚好。因此拆分后，建议以三个人左右为单位进行组织和划分； 第二是在进行拆分后，各个模块不在一个进程内，需要采用接口依赖的形式。 此时，对于 RPC 框架及对应的监控、降级等中间件基础设施要求较高了。如果公司的中间件和基础设施跟不上，建议先将此加强再进行拆分，以免拆分后还要填此处留下的坑。 如何进行拆分拆分就和切蛋糕类似，将一整块蛋糕切分成多个小块的方法有很多。可以横着切或者竖着切，不管怎么切，蛋糕最终都将被分成很多小块。只是选择不同的切法，最终得到的蛋糕形状不一样。同样架构的拆分也有很多方式，对一个业务系统采用不同的拆分方法将会得到不同的子模块。上文中，采用了“目的性”这个维度，将后台业务系统拆分或者说归类为三大类，读服务、写服务和扣减服务。拆分后可以得到三种类型的模块，当然难道所有的后台系统都只有这三个模块？以上图 1 的电商后台拆分架构为例，电商后台一般包含： 用户模块 商品模块 促销模块 购物车模块 订单模块等这正好验证了架构拆分有很多方式，只是不同拆法得到的结果不一样。“目的性”是一种偏技术视角的拆分方式，而图 1 展示的则是一种偏业务视角的拆分方式。可以发现拆分后的模块和平时购物的流程极为相似，比如，在购物时，要先通过认证登录 App，再浏览商品、查看促销活动及价格，确认购买后添加购物车并提交订单。这种按用户使用流程进行拆分的方法叫作业务流拆分法。并没有哪一种架构拆分绝对正确，因为架构是论述题而不是选择题。在实际工作中中，需要针对不同复杂度的业务类型灵活选择。不管经过多少次重复拆分和组合，目标只有一个——降低系统复杂度、减少耦合提升效率。对于复杂的系统架构，先竖着切一刀，称为垂直拆分。经过垂直拆分后，如果系统还是太复杂，就可以横着再来一刀，称为水平拆分或者分层拆分。在实际场景中，可以根据需求重复做垂直拆分或者水平拆分。每一次拆分的依据可以不一样，可以依据业务流程，也可以依据纯技术。虽没有绝对的架构拆分标准，但从经验来看，大型系统通常先采用两次垂直拆分再加一次水平拆分。这两次垂直拆分有一定的先后顺序，先按业务维度再按技术维度。接下来，以最近经常使用的微信读书作为分析案例，从零开始对一个陌生的后台架构进行拆分。首先，微信读书的业务流程，然后再开始第一步的垂直拆分。使用微信读书阅读大致经过以下几个步骤： 注册并登录 App； 在书城里查找你想要阅读的图书，并浏览书籍的版本目录和介绍； 确定是你想要的书籍并且价格适合，就会产生购买； 支付完成之后，开始阅读。微信读书业务流程图，如下图所示：结合前面文字描述与图 2 的流程图展示之后，第一次垂直拆分的结果，如下： 注册和登录应该是用户模块； 在微信读书里书籍属于售卖商品，因此浏览书籍称为商品模块； 书籍价格展示区域称为价格模块或者叫促销模块，有些时候价格是根据运营节奏或者用户是否为会员等条件计算出来的，这种经过计算输出价格的区域叫促销模块； 购买是订单模块； 最后完成支付则是支付模块； 支付完成后支持实际阅读的区域是阅读模块，它记录了我每次的阅读进度、书签、笔记等。至此，按照业务流程完成了对微信读书后台的第一次拆分，得到了微信读书的六大后台模块。因为第二次按技术维度的垂直拆分较通用，上述每个模块均可以按此进行拆分，所以以只选取用户模块和支付模块进行分析。用户模块用户模块对外会提供两个基本功能：是用户注册，另一个则是登录验证，可以称为查询，因为除了验证之外，App 里还显示了用户的名称及其他信息。按技术维度进行拆分时，可以将注册和修改归为写业务。除此之外均为读业务，包含验证、查询基本信息、查询用户是否为会员等。经过第二次拆分后，用户模块一分为二——用户读模块和用户写模块。按此套路，可以将用户读模块进一步拆分（这里只是说解拆分的思想，并不是鼓励你一直拆分下去）拆分太细会出现多个模块，如果团队人手不够，即使利用“拆分”也达不到降低复杂度的目的。支付模块支付模块通常会对外提供余额查询与支付扣减的能力，此处可以将支付模块分为额度查询模块和支付扣减模块，同样完成了支付模块的第二次技术维度的垂直划分。拆分结果如下图 3 所示：至此，已经完成了两次垂直维度的拆分。拆分的结果与后台系统的三大类：读模块、写模块及扣减模块基本一致，只是本次模块都带上了具体的业务前缀。任何技术都是服务于业务的，脱离业务的技术无法发挥它的价值。本文目的是深入理解这些业务背后的通用点，从而更好地服务于业务。完成两次垂直拆分后，就可以做最后一步拆分了，即水平拆分或者叫作分层拆分。 水平拆分的依据是按易变度或共性度 。经过水平拆分，上层的称为易变模块，下层的称为非易变模块。越靠下面的模块越稳定、越共性、越不易变化； 拆分后，对于非易变的模块，只需要编写、修改一次或者零星几次即可，对于易变的模块则需要投入更多的人力去维护。因为易变与非易变模块已经拆开，易变模块进行需求改造对非易变模块基本上没有任何影响。下面我们以第二步垂直拆分形成的模块作为分层拆分的实战。经过第二步按技术维度的垂直拆分，形成了用户的读模块、写模块、任务模块等。在设计或者开发时，会发现这些模块都会连接数据库或者其他存储。对于这些连接数据库的代码，基本都是对象映射，将入参的对象转换为数据库 ER 格式的对象。如果是 Java 应用，还会包含 MyBatis 或者 Hibernate 相关的数据库 ORM 映射的脚手架代码，其他语言以此类推。这些通用的代码只编写一次即可形成一个模块，比如数据访问模块，可以供用户的读模块和写模块共同使用，这就是水平拆分的结果，它的上层是读写模块、下层为共性的数据访问模块。具体形式见下图 4：如果单独部署水平拆分得到的数据访问模块会因为网络、数据序列化等因素降低读写模块的性能。为了规避此问题，在实际应用中，可以将数据访问模块的代码单独一个工程，但在编译时以动态包的形式链接进用户读模块、用户写模块里，这样就两全其美了。 以 Java 举例，就是把数据访问层编译为 Jar 包，在读模块里，通过 maven依赖上述的 Jar 包。在部署时，数据访问层和读模块就在同一个进程里了，避免了网络开销等这些问题。至此完成了两次垂直拆分、一次水平拆分的架构拆分实战。在实际工作中，可以根据需求选择比此次实践多或少的拆分。另外，架构和历史也是一样，分久必合合久必分，但在分分合合的过程中应遵循上述原则和手段。架构拆分不是完美无缺的，它也会存在一些问题，比如拆分后带来的分布式事务、调用链路变长、模块变多消耗机器变多等问题。针对这些问题，我将在模块五详细讲解。总结系统拆分的几个主要原因： 当需求不断叠加导致并行开发和上线时，通过拆分可以减少相互影响； 当维护一个覆盖范围比较广的业务系统，从而导致研发人员业务专业度不够高时，通过拆分可以适当聚焦，提升专业度； 当一个系统范围较广同时线上 Bug 不断时，就需要适当拆分，逐个击破。在落地进行拆分时，有几个重要准则牢牢记住： 拆分是按维度逐层进行，从顶层逐步向下。在顶层按业务及业务流程进行垂直拆分，而不是按技术或其他； 在此之后，对于拆分得到的具体模块，可以按读写分离、在线离线分离、快慢分离、场景分离等方式做进一步的水平拆分； 在模块内部的垂直拆分完成之后，可以按易变与稳定、共性与非共性进行水平拆分。需要注意的是，第二步的垂直拆分和最后的水平拆分是交替进行的，并无非常清晰的边界和先手顺序。通过拆分得到了可以独自进行详细架构设计的读服务模块、写服务模块及扣减服务模块，可以对各个类型的模块进行逐个击破了。" }, { "title": "Java 集合中的快速失效机制", "url": "/posts/fail-fast/", "categories": "Java, Collection", "tags": "RefulAPI", "date": "2021-03-02 15:33:00 +0000", "snippet": "深入了解快速失效机制以后，不仅能避免一些比较隐蔽的 Java 集合访问方面的问题，还能通过快速失效机制的底层源码，提升你对 Java 多线程并发的理解和掌握程度。除此之外，集合方面的快速失效机制，还能用于在面试中展示你的能力。快速失效是一个很有意思的知识点，就算你没有丰富的项目经验，只要提前准备好就可以把它梳理清楚。在面试中，求职者如果能做到边写快速失效底层源码，边进行有条理地分析，就一定能给面试官留下“熟悉 Java 核心技能”的印象。本文将围绕快速失效的内涵和外延、集合中快速失效机制的表现形式和设计动机、快速失效机制的底层源码，以及快速失效在面试场景中的应用等方面，总结快速失效的种种门道。从遍历集合的角度观察快速失效快速失效的英文名词 fail-fast，在 Java 集合遍历的场景中，它是一种检测异常的机制。具体地说，快速失效的表现形式是，在遍历 ArrayList 等 Java 集合对象时，如果该集合对象中的数据发生改变，此时就有可能出现快速失效的情况，即会抛出 ConcurrentModificationException 异常。比如在如下的 FailFastDemo.java 范例中，在 main 函数的第 13 行到第 19 行里，是通过 Iterator 迭代器遍历了 ArrayList 类型的 list 对象。在遍历的同时，还通过了第 14 行到第 16 行的代码，通过 list 对象删除了对象中的数据。import java.util.ArrayList;import java.util.Iterator;import java.util.List;public class FailFastDemo {    public static void main(String[] args) {        List&amp;lt;String&amp;gt; list = new ArrayList&amp;lt;&amp;gt;();        for (int cnt = 0 ; cnt &amp;lt; 5 ; cnt++ ) {            list.add(Integer.valueOf(cnt).toString());        }        Iterator&amp;lt;String&amp;gt; it = list.iterator();        int index = 0 ;        while(it.hasNext()) {            if (index == 2) {                list.remove(2);            }            System.out.println(it.next());            index ++;        }    }}从中能看到，在通过迭代器遍历集合对象时，一旦迭代器感知到被遍历的集合对象发生变化，那么就会抛出 ConcurrentModificationException 异常。但是请注意，只有用迭代器边遍历边修改集合对象时，才会出现快速失效的情况。如果像下面代码一样，通过集合的索引来边遍历边修改集合对象，虽然这种做法不推荐，但不会抛出异常。在如下第 10 行到第 15 行的遍历集合的代码中，是通过索引变量 cnt 来遍历，虽然在遍历的过程中也通过第 12 行的代码删除了集合中的对象，但是这段代码能正常运行，并输出“1 2 3 4”的结果。import java.util.ArrayList;import java.util.List;public class NotFailFastDemo { public static void main(String[] args) { List&amp;lt;String&amp;gt; list = new ArrayList&amp;lt;&amp;gt;(); for (int cnt = 0 ; cnt &amp;lt; 5 ; cnt++ ) { list.add(Integer.valueOf(cnt).toString()); } int index = 0; for (int cnt = 0 ; cnt &amp;lt; 5 ; cnt++ ) { if (index == 2) { list.remove(2); } System.out.println(list.get(cnt)); } }}从以上两段代码中，能看到快速失效的现象：如果用迭代器遍历集合的同时再修改集合对象，那么就会抛出 ConcurrentModificationException 异常，这其实是一种限制机制，即限制“边遍历边修改”的情况。从底层源码解释快速失效的原因以 ArrayList 和 Iterator 等集合对象的底层源码，以此来分析快速失效的原因。第一，在通过 it.next() 方法遍历 ArrayList 集合中下一个对象时，会调用 checkForComodification 方法来检查集合的情况，相关底层源码如下所示。 由于这里是 JDK 底层源码，不能直接运行，所以就以截图形式给出。第二，checkForComodification 方法的源码如下所示，其中会判断 modCount 和 expectedModCount 两个值，如果不一致，则会抛出 ConcurrentModification 异常。其中 expectedModCount 变量表示在遍历前集合的长度，而在遍历集合中的每个元素时，都会再次计算 modCount 值。第三，在用类似 list.remove(2) 的代码删除集合中元素时，会通过如下的代码变更 modCount 值，不过在变更 modCount 值的同时，不会修改 expectedModCount 值。请注意这里仅是以删除集合对象为例，如果再用迭代器遍历集合的同时添加集合元素，也会类似地变更 modCount 值。第四，在用 remove 等方法变更集合中元素的下一次遍历的同时，同样会触发 next 方法，而在 next 方法里同样会调用 checkForComodification 方法来检查 expectedModCount 和 modCount 这两个值是否一致，此时由于在 remove 时已经修改了 modCount 值，所以就会抛出 ConcurrentModification 异常。快速失效为何如此为难快速失效其实是一种限制机制，限制了在用迭代器边遍历边修改集合的行为。那么为什么要在 Java 语法中引入机制呢？原因是为了保证在多线程场景下遍历时的数据一致性。比如线程 1 在用迭代器遍历某个 ArrayList 对象的同时，线程 2 添加或删除了该 ArrayList 对象中的数据。如果不引入快速失效机制，那么线程 1 在读取 ArrayList 对象的数据时，就会读到修改后的结果，而不是原始数据，从而就有可能引发数据问题。如果不引入快速失效机制，由于这种读错数据的问题不会抛出异常，所以是很难排查的。为了避免这种潜在的而且比较隐蔽的问题，Java 语法的设计者们就干脆引入快速失效的机制。引入快速失效机制后，在多线程环境下边读边修改集合时，Java 虚拟机就会抛出异常，这样就能让程序员介入排查问题，从而就能有效地杜绝可能出现的数据问题。开发中如何避免快速失效从上文的描述中你可以看到，快速失效其实是一种保护机制，保护了集合在多线程环境的数据一致性。但是，如果在一些单线程的环境里，确实存在“边遍历边修改集合”的需求，那么该怎么办呢？如下的代码给出了实现的方法。请注意，在如下第 12 行到第 19 行的遍历代码中，依然存在删除数据的动作，但在第 15 行删除 ArrayList 对象中的数据时，是用到了迭代器的 remove 方法，而不是集合本身的 remove 方法，这样就能正确地删除数据，同时也不会抛出异常。import java.util.ArrayList;import java.util.Iterator;import java.util.List;public class FailFastDemo { public static void main(String[] args) { List&amp;lt;String&amp;gt; list = new ArrayList&amp;lt;&amp;gt;(); for (int cnt = 0 ; cnt &amp;lt; 5 ; cnt++ ) { list.add(Integer.valueOf(cnt).toString()); } Iterator&amp;lt;String&amp;gt; it = list.iterator(); int index = 0 ; while(it.hasNext()) { if (index == 2) { //list.remove(2); 会出异常 it.remove(); } System.out.println(it.next()); index ++; } }}但是请注意，在遍历集合对象时要尽量避免修改操作。否则，即使引入了不抛异常的机制，但相当于撤销了检查数据一致性的保护机制，所以非常有可能引入潜在的不易排查的数据问题。 先说下快速失效的表现，即用迭代器遍历集合时，如果同时删除或添加数据，会抛出 ConcurrentModification 异常。 再根据底层源码，说明为什么会抛出异常，这方面的要点是，通过源码说明边遍历边修改的过程，同时说明抛异常的直接原因是 modCount 值被修改了。 再说一下在 Java 语法中引入快速失效机制的动机，这里可以说明，引入快速失效的动机是确保多线程遍历集合时的数据一致性。 最后为了让面试官感觉你确实了解快速失效背后的设计动机，你还可以补充说，在我们项目中，确实有边遍历集合边修改的需求，但我们通过改写代码，把修改集合的动作放到了遍历循环之外，这样就彻底杜绝了因集合数据不一致性而导致的问题。表现、动机和源码" }, { "title": "Spring 依赖注入", "url": "/posts/Spring-dependency-injection/", "categories": "Java, Spring", "tags": "Spring, Ioc", "date": "2021-03-02 15:33:00 +0000", "snippet": "Spring 依赖注入在 IDEA 中使用 Spring 框架时，经常遇到这样一个提示：Field injection is not recommended !这个提示的意思是：不建议使用字段注入。众所周知，Spring 的依赖注入有三大类，分别是： 字段注入 构造器注入 Setter 方法注入其中的字段注入就是把一个 Bean 以字段的形式注入到另一个 Bean 中，日常开发中这种注入方式用的非常多。既然字段注入这么常用，那为什么 IDEA 会不推荐使用呢？和其他注入类型相比，字段注入存在哪些问题呢？这就需要从 Spring 底层实现机制的角度分析这些问题，从而得知产生的原因。如果想要创建一个对象，就需要知道如何创建、使用以及销毁这个对象。这个过程显然是繁杂而重复的。这时候，可以把这部分工作交给一个容器，容器负责控制对象的生命周期和对象之间的关联关系。而 Spring 框架扮演的角色就是这样一个容器，如下图所示。Spring 会在适当的时候创建一个 Bean，然后像注射器一样把它注入到目标对象中，这样就完成了对各个对象之间关系的控制。那么，面试时关于依赖注入的一个基本问题就是：如何在 Spring 中注入对象呢？Spring 为开发人员提供了三种不同的依赖注入类型，分别是字段注入、构造器注入和 Setter 方法注入，如下图所示。下面用代码示例来解释这三种依赖注入类型。假设，有如下所示的一个 HealthRecordService 接口以及它的实现类：public interface HealthRecordService {    public void recordUserHealthData();}public class HealthRecordServiceImpl implements HealthRecordService {    @Override    public void recordUserHealthData () {        System.out.println(&quot;HealthRecordService has been called.&quot;);    }}现在，如果让你把这个 HealthRecordServiceImpl 实现类注入到其他类中，我一般会在这样做：字段注入首先，使用字段注入。想要在一个类中通过字段的形式注入某个对象，可以采用这样的方式：public class ClientService {    @Autowired    private HealthRecordService healthRecordService;    public void recordUserHealthData() {        healthRecordService.recordUserHealthData();    }}可以看到，通过 @Autowired 注解，字段注入的实现方式非常简单而直接，代码的可读性也很强。事实上，字段注入是三种注入方式中最常用、也是最容易使用的一种。但它也是三种注入方式中最应该避免使用的。就像开篇说的，在 IDEA 中使用字段注入时会遇到Field injection is not recommended的提示。这样的原因有三点： 字段注入的最大问题是对象的外部可见性ClientService 类中，通过定义一个私有变量 HealthRecordService 来注入该接口的实例。显然，这个实例只能在 ClientService 类中被访问，脱离了容器环境就无法访问这个实例，来看下面这段代码： ClientService clientService = new ClientService(); clientService.recordUserHealthData (); 执行这段代码的结果就是抛出一个 NullPointerException 空指针异常，原因就在于无法在 ClientService 的外部实例化 HealthRecordService 对象。 采用字段注入，类与容器的耦合度过高，无法脱离容器来使用目标对象。 如果编写测试用例来验证 ClientService 类的正确性，那么想要使用 HealthRecordService 对象，就只能使用反射的方式，这种做法实际上是不符合 JavaBean 开发规范的，而且可能一直无法发现空指针异常的存在。 字段注入的第二个问题是可能导致潜在的循环依赖即两个类之间互相进行注入，例如下面这段示例代码： public class ClassA { @Autowired private ClassB classB; } public class ClassB { @Autowried private ClassA classA; } 这里的 ClassA 和 ClassB 发生了循环依赖。上述代码在 Spring 中是合法的，容器启动时并不会报任何错误，而只有在使用到具体某个 ClassA 或 ClassB 时才会报错。 字段注入的第三个问题是无法设置需要注入的对象为 final，也无法注入那些不可变对象这是因为字段必须在类实例化时进行实例化。基于以上三点，无论是 IDEA，还是 Spring 官方，都不推荐开发人员使用字段注入这种注入模式，而是推荐构造器注入。构造器注入关于构造器注入，面试中往往会以这样的形式考察你：构造器是 Spring 官方推荐的依赖注入类型，你知道它有哪些特性吗？或者换种问法，构造器注入相比字段注入的优势在哪里？接下来，我们就一起来分析这些问题。构造器注入的形式也很简单，就是通过类的构造函数来完成对象的注入，示例代码如下所示：public class ClientService {    private HealthRecordService healthRecordService;    @Autowired    public ClientService(HealthRecordService healthRecordService) {        this.healthRecordService = healthRecordService;    }    public void recordUserHealthData() {        healthRecordService.recordUserHealthData();    }}可以看到构造器注入能解决对象外部可见性的问题，因为 HealthRecordService 是通过 ClientService 构造函数进行注入的，所以势必可以脱离 ClientService 而独立存在。关于构造器注入， Spring 官方文档是这样解释它的功能特性： The Spring team generally advocates constructor injection as it enables one to implement application components as immutable objects and to ensure that required dependencies are not null. Furthermore constructor-injected components are always returned to client (calling) code in a fully initialized state.这段话的核心意思在于：构造器注入能够保证注入的组件不可变，并且确保需要的依赖不为空。这里的组件不可变也就意味着你可以使用 final 关键词来修饰所依赖的对象，而依赖不为空是指所传入的依赖对象肯定是一个实例对象，避免出现空指针异常。同时，基于构造器注入，如果存在诸如前面介绍的 ClassA 和 ClassB 之间的循环依赖关系，如下所示：public class ClassA { private ClassB classB; @Autowired    public ClassA(ClassB classB) {        this.classB = classB;    }}public class ClassB { private ClassA classA; @Autowired    public ClassB(ClassA classA) {        this.classA = classA;    }}那么在 Spring 容器启动的时候，就会抛出一个循环依赖异常，从而提醒避免循环依赖。如此看来，字段注入的三大问题都可以通过使用构造器注入的方式来解决。但是，当构造函数中存在较多依赖对象的时候，大量的构造器参数会让代码显得比较冗长。假设一个类的构造器需要 10 个参数，那么想要使用这个类时，就需要事先准备好这 10 个参数，并严格按照构造器指定的顺序一一进行传入。那么，无论从代码的可读性还是可维护角度而言，这都不是很符合最佳实践。这时候就可以引入 Setter 方法注入。Setter 方法注入它的实现代码如下所示：public class ClientService {    private HealthRecordService healthRecordService;    @Autowired    public void setHealthRecordService(HealthRecordService healthRecordService) {        this.healthRecordService = healthRecordService;    }    public void recordUserHealthData() {        healthRecordService.recordUserHealthData();    }}Setter 方法注入和构造器注入看上去有点类似，但它比构造函数更具可读性，因为可以把多个依赖对象分别通过 Setter 方法逐一进行注入。而且，Setter 方法注入对于非强制依赖项注入很有用，我们可以有选择地注入一部分想要注入的依赖对象。换句话说，可以实现按需注入，帮助我们只在需要时注入依赖关系。另一方面，Setter 方法可以很好解决应用程序中的循环依赖问题，如下所示的代码是可以正确执行的：public class ClassA { private ClassB classB; @Autowired    public void setClassB(ClassB classB) {        this.classB = classB;    }}public class ClassB { private ClassA classA; @Autowired    public void setClassA(ClassA classA) {        this.classA = classA;    }} 请注意，上述代码能够正确执行的前提是 ClassA 和 ClassB 的作用域都是“Singleton”。最后，通过 Setter 注入，可以对依赖对象进行多次重复注入，这在构造器注入中是无法实现的。那么，概括起来就是： 构造器注入适用于强制对象注入； Setter 注入适合于可选对象注入； 字段注入应该避免，因为对象无法脱离容器而独立运行。好了，现在你能回答出三种依赖注入类型的相关内容了，那么放到一些场景中，该怎么灵活使用呢？这也是面试中会重点考察的问题。依赖注入用得好，Spring 框架轻松搞 把一个 Bean 注入到 Spring 容器中，如何合理设置它的作用域？ 在实现依赖注入时，想要针对一个接口注入多个实现类，你有什么办法？ Bean 的注入是需要消耗性能的，那么如何高效完成这一过程呢？把握好 Bean 的作用域作用域描述了 Bean 在 Spring IoC 容器上下文中的生命周期和可见性。通过注解来设置 Bean 的作用域，可以使用这样的示例代码：@Configurationpublic class AppConfig {              @Bean @Scope(&quot;singleton&quot;)    public HealthRecordService createHealthRecordService() {        return new HealthRecordServiceImpl();    }}可以看到这里使用了一个 @Scope 注解来指定 Bean 的作用域为单例的“singleton”。在 Spring 中，除了单例作用域之外，还有一个“prototype”，即原型作用域，也可以称为多例作用域，以示与单例进行区别。使用方式上，同样可以使用枚举值来对它们进行设置：@Scope(value = ConfigurableBeanFactory.SCOPE_SINGLETON) @Scope(value = ConfigurableBeanFactory.SCOPE_PROTOTYPE)在 Spring IoC 容器中，Bean 的默认作用域是单例，也就是说不管对 Bean 的引用有多少个，容器只会创建一个实例。而原型作用域则不同，每次请求 Bean 时，Spring IoC 容器都会创建一个新的对象实例。从两种作用域的效果来看，总结一条开发上的结论：对于无状态的 Bean，应该使用单例作用域，反之应该使用原型作用域。那么，什么样的 Bean 是属于有状态的呢？结合 Web 应用程序，我们可以明确对于每次 HTTP 请求而言，都应该创建一个 Bean 来代表这一次的请求对象。同样，对于会话而言，也需要针对每个会话创建一个会话状态对象。这些都是常见的有状态的 Bean。怎样灵活使用注解配置？在使用 Spring 依赖注入类型时，通常可以使用 XML 配置、Java 代码配置以及注解配置这三种实现方式。随着 Spring Boot 框架的流行，使用注解配置已经成为目前最主流的开发方式。除了前面已经给出的最常见的 @Autowired 注解，Spring 框架还提供了一组非常有用的注解帮助更好地管理所注入的对象，包括 @Primary 注解和 @Qualifier 注解。在 Spring IoC 容器中，针对 HealthRecordService 这样一种接口类型，原则上只允许注入一个实现类。如果存在该类型的多个对象实例，那么容器就会报 NoUniqueBeanDefinitionException，意味着容器无法决定选择哪一个实例来进行注入。这时候就可以使用 @Primary 注解来帮助容器做出选择，该注解使用方式如下所示：@Componentpublic class HealthRecordServiceImplA implements HealthRecordService {    …}@Component@Primarypublic class HealthRecordServiceImplB implements HealthRecordService {    …}现在，Spring IoC 容器只会注入 HealthRecordServiceImplB 这个实例类，这在管理针对某种类型的多个实例时非常有用。和 @Primary 注解的应用场景类似，@Qualifier 注解为我们如何选择实例类进行注入提供了更加灵活的实现方式，如下所示：@Component@Qualifier(&quot;healthRecordServiceA&quot;)public class HealthRecordServiceImplA implements HealthRecordService {    …}@Component@Qualifier(&quot;healthRecordServiceB&quot;)public class HealthRecordServiceImplB implements HealthRecordService {    …}可以看到这里针对不同的实现类，我们通过 @Qualifier 注解设置了不同的名称，这样在使用时就可以通过该名称获取不同的实例：@Autowired@Qualifier(&quot;healthRecordServiceB&quot;)private HealthRecordService healthRecordService;不同配置的性能有哪些异同？在 Spring 中，可以通过设置组件扫描范围来简化 JavaBean 的注入配置。因为任何的类都是位于某一个包结构之下，所以 Spring 提供了一个 @ComponentScan 注解，该注解在需要大规模对象注入的场景下非常有用，其基本用法如下所示：@Configuration @ComponentScan(basePackages=&quot;com.lagou.spring&quot;) public class AppConfig { }在这个示例中，Spring 会扫描由 basePackages 指定的包路径 “cn.happymay.spring” 及其子路径下的所有 Bean，并把它们注入到容器中。当然，首先需要在这些类上添加 @Component 注解以及由该注解衍生的 @Service、@Repository、@Controller 等注解。然而，因为该注解会扫描 basePackages 指定包中的所有组件，所以如果所指定包中的组件并不需要在应用程序启动时就全部加载到容器中，那么对包路径进行精细化设计就是一项最佳实践。例如，我们可以通过设置一个列表来细化具体的包结构路径，如下所示：@Configuration @ComponentScan(basePackages=&quot;com.lagou.spring.service&quot;, &quot;com.lagou.spring.controller&quot;) public class AppConfig { }单例模式和原型模式对性能的影响在 Spring 中，默认情况下定义的所有 Bean 都是单例的。但当把 Bean 范围设置为“prototype”，每次请求 Bean 时，Spring IoC 容器都会创建一个新的对象实例。所以，使用原型模式在创建过程中会对性能产生影响，对那些初始化过程需要消耗巨大资源的对象而言尤其如此，这类对象常见的包括网络连接对象、数据库连接对象等。因此，针对这些对象，应该完全避免使用原型模式。或者，你应该仔细设计并对性能进行充分测试。Spring IoC 容器的延迟加载（Lazy Loading）和预加载（Preloading）机制通过 @Autowired 注入的 Bean 都是在 Spring IoC 容器启动时被创建和初始化的，这个过程被称为预加载。但有时候，希望能够延迟 Bean 的加载时机，这时候就可以使用 @Lazy 注解，使用方法如下所示：@Component@Lazypublic class HealthRecordServiceImpl implements HealthRecordService {    …}添加了 @Lazy 注解的效果在于只有在使用到这个 Bean 的时候才会去初始化，而不是在 Spring IoC 容器启动时，这样就可以节省容器资源。延迟加载确保在请求时动态加载 Bean，预加载确保在使用 Bean 之前加载 Bean。Spring IoC 容器默认使用预加载。然而，在容器启动时就加载所有类（即使它们没有被使用）并不是一个明智的决定，因为有些 Bean 实例会非常消耗资源。所以应该根据实际情况选择具体的加载方法。如果需要尽可能快地加载应用程序，那么就采用延迟加载；如果需要应用程序尽可能快地运行并更快地为请求提供服务，那么就进行预加载。" }, { "title": "后端业务系统共性总结", "url": "/posts/Business-System-Commonalities-01/", "categories": "Architecture Design, Backend System", "tags": "Architecture Design, Backend System", "date": "2021-03-02 06:33:00 +0000", "snippet": "国内的各大互联网公司业务模式非常丰富，所提供的业务服务也是形态各异，比如： 腾讯：主要提供即时通信、游戏等服务 京东、阿里等电商平台提供商品购买、快递寄收件、金融投资等服务 滴滴提供打车服务 今日头条、新浪微博提供短视频、新闻资讯类阅读服务 美团提供商品购买、外卖订购服务 百度提供搜索查询服务虽然上述列举的公司业务类型不同，但对后台开发岗位的招聘要求却很相似。除了要求掌握某种开发语言和相关框架外，还要掌握分布式、多线程、缓存、数据库等。那么，产生这种现象的根本原因——是因为这些业务在技术实现上存在共性，比如技术点或者架构模式。所谓大道归宗，就是面对层出不穷的新业务、新模式时，洞穿背后的架构本质，套用通用架构模式，轻松应对各种应用指标。因此，本文以一个外卖订购服务为例子，探寻问题背后的原因，总结出一个分析业务架构共性的标准。1. 什么是业务后台系统以一个外卖订购服务为例子，帮助理解和明确“业务后台系统（后台开发）”的定义和边界。外卖系统全局架构上面展示的是一个外卖系统的全局架构图，如果某个公司要进军外卖业务，就需要开发一套外卖系统。其中包含用户可直接使用的各个终端，如 IOS、Android、M页以及PC端。提供内容展示和系统交互，称为业务前台系统。业务前台系统中展示的内容，比如外卖商品种类、商品图片等信息，其实是由业务后台系统提供的，业务前台并没有这些数据，它只负责展示。当用户在前台选择商品店外卖后，实际接受并存储订单以及调度配送的系统称为业务后台系统。总体来看，业务后台系统是指直接接受前台的请求，同时给前台返回数据或者保存前台数据的系统。“业务”这个词只是代称，代指各类业务系统。很多公司还有一些大数据、BI、数据挖掘等相关系统，它们并不属于业务后台系统，则称为业务大数据系统，现在时髦的叫法是：数据中台。数据中台有一个特点，不直接接受业务前台系统的请求也不直接生产数据，而是直接对业务后台系统产生的数据进行分析、再加工等。还有一些研究算法的小伙伴，他们主要对算法进行研究和调优，并将这些成熟的、可用于生产环境的算法提供给业务后台开发工程师。再有他们（业务后台开发工程师）集成业务后台系统里，提升业务的体验，比如推荐算法、语音识别算法等。至此，可以清晰的看出业务后台系统的边界。通过上述，还可以举一反三： 短视频和资讯类（微博、新闻）等业务里，提供查询视频基本信息和资讯的系统、能够保存短视频的系统，都属于后台系统。 在电商业务里，提供查询商品信息的系统、保存订单的系统，都属于后台系统其他互联网公司提供的系统，也可以按照上面的思路来判断它们的业务后台系统类别。2. 后台系统的共性探究与归类如果不明确“目的性”概念，会发现不同业务后台系统的所有技术实现都是CRUD（增、查、改、删）。按照 CRUD 归类，很难看出各类业务后台系统之间的区别，更别说从中提炼共性技术和通用架构模式了。因此，首先要确定归类的维度。具体的说，就是需要寻找一个新的维度来对业务后台系统进行归类，并基于此归类提炼技术共性。而这个维度就是目的性。确定归类的维度后，就可以对各类业务后台进行分类了。最后将业务后台系统的目的性找到后，如何基于目的性对业务后台系统进行分类呢？于是，我自己比较熟悉的资讯、发布以及购买、库存以及支付这三类业务为案例来进行总结。2.1 资讯类业务思考一个问题：对于微博、知乎、小红书这类资讯业务系统，它们的目的性是什么？针对上面的问题，先从日常使用的角度来思考就显而易见，这类APP（微博、知乎、小红书等）主要为用户提供阅读和浏览信息的业务，这就是资讯业务系统的目的性。比如：平时上微博和知乎的目的是娱乐或学习。因此总结为阅读“新鲜事”。当然，也会偶尔发布几条动态。想象一下，如果系统出现了 Bug ，导致曾经发布的某条信息丢失，当下可能感知不到，直到某一天查看历史动态。但是如果系统异常导致不能阅读“新鲜事”，用户当下即可感知到系统出现故障。互联网时代舆论传播非常快，这会给公司带来很大的负面影响。因此这类事故产生的影响更大。总之，资讯类业务系统的主要目的性就是尽量大的可能性保证读的可用性和优化用户体验。根据最终目标，称它为读类型的业务后台系统，或者读业务。到这里，我又有了疑问，读业务和 CRUD 中的 R 不是一个意思吗？答案当然是：完全不同。因为它们主要又以下两点区别 这里的“读”是从业务后台系统的目的性推导而来，是有限定要求的，它要求“读”是能够满足用户体验的高性能和高可用； 定义的方面不同，读业务最重要的是保证系统可读，但此系统仍然会提供写删修的功能，但对这些功能的关注度和要求指标均较低。和上述读业务类似的场景有 短视频业务，在系统出现故障的极端情况下，可以不能发送新的短视频，但需要能够浏览历史视频； 电商或者外卖业务里的的商品系统，在系统出现故障的极端情况下，商家可以不能创建新的商品，但历史商品需要能够被客户浏览并下单。上面的两个场景中，第一个场景短视频是一个大业务，第二个场景是某个业务中的一个系统。因此从这两个场景案例的差异中可以看出：目的性这个衡量指标可以是某一个大业务，也可以是某一个业务中的一个具体细小场景。除了读业务外，还有另外两个场景，分别是数据写场景和扣减场景。2.2 发布及购买类业务思考一个问题：对于电商、外卖和打车等交易类业务场景，它们最重要的目的性是什么？不管这些业务里有多少个形形色色的系统。当出现一些难以恢复的故障时，比如钻石会员不能使用优先打车通道、不能显示此次打车的是出租车还是快车等，但只要用户能够提交订单打车即可，因为如果不能提交订单，将直接减少企业真金白银的收入，在商业上是不允许的。因此提单的写入是此类业务场景中的重点，也是提供电商、外卖和打车服务等企业的最终目的，所以提供一个高可用的写入服务十分重要。2.3 库存及支付类业务提单扣减流程上图展示的是扣减场景中的提单扣减流程，属于一个大型业务之下的某个系统的技术诉求，比如库存的扣减、次数的限制、支付金额的扣减等。虽然这几个系统都会对外提供诸如查询库存、次数等能力，但它最重要是保障扣减的高可用，因为一般扣减都是和提单共同发生。如果扣减失败，那么提单也无法成功，所以，扣减业务也是一个需要重点保障的场景。以上，我使用目的性这个维度对不同类型的公司业务（短视频、微博、新闻资讯、电商、打车等）梳理分类，得出了以下的结论： 业务后台系统在系统实现上均可分为读业务、写业务、扣减业务 由于业务类型是可归类的、通用的、所以得出结论：这三大类业务后台系统在技术实现上也是类似的，甚至可以说是统一的。3. 各类型的技术实现关注点3.1 读业务是越快越好任何业务最基本的要求是高可用，随时保障服务可用。读业务除了此要求外还有其他什么要求吗？从上面的案例中，发现资讯类业务（微博、知乎、短视频），它们的“写”（即发微博、发短视频）和用户的“读”（即浏览新鲜事）的次数相差非常大。一个正常的用户，可能阅读了 100 条微博，才会发 1 条微博。这里的读写比例在十倍、百倍的量级。因此读的并发量级非常大。此外，阅读作为一切业务的发起点，对于速度的要求至关重要。不管是电商还是现在的短视频、微博里的直播带货，首先保障用户能够快速浏览和切换商品，然后才是进入下一步的购买页面。可以想象一下，如果一个商品图片加载很慢，或一个资讯类应用新闻半天不展示，你还会耐心等待吗 ？因此，作为大多数业务的起点，除了完成高可用之外，读业务的实现还要去能够在海量读请求下保障高性能。3.2 写业务需要 101% 高可用上面说到读业务的技术实现分析里提到，保障高可用的基本要求。那么，写入业务如提交订单等场景，还需要再提及高可用吗？显然，答案是必须的。写入基本上是提交订单，它和实实在在的企业收入相关。因此，我们需要尽“101%”的努力来保障可用性。如果读服务出现存储或应用故障，可以在前端或终端进行前置缓存抗一段时间。缓存给研发或者运维提供了分钟级别的故障处理、数据修复的可能。但写入服务是无法使用缓存的。此外，对于各大电商、打车、外卖平台来说，故障恢复时间需要在一分钟内或者秒级别。故我们在架构设计时需要做到 “101%”的高可用。这样在实际生产环境才能高校应对故障的发生。3.3 扣减业务要抗住并发和保障数据一致性对于扣减业务、从目的性上讲，最重要的就是抗住并发的扣减量。除了高可用外，扣减业务和写入业务有很多类似的地方，甚至可以归为一类。对于写业务，以提交订单为例，在写入的时候，所有的数据都是用户从表单提交过来的，比如购买商品的名称和数量、收货地址等，这些数据是这个用户私有的，在技术实现上，我们只要能够尽 “101%”的可能性把它保存下来即可。而对于扣减则不是。以库存为例，扣减的请求只会包含购买的商品和对应的数量，而具体能不能买，则依赖后台系统存储的当前剩余库存数量。另外，不同用户在同一时刻可能购买同一个商品，此处就存在并发更新，这种在高并发情况下的扣减一致性就需要格外的注意。虽然，扣减类业务也会对外提供一些诸如查询剩余数量和金额的查询接口，但扣减类系统提供的最重要的能力是被各类订单所依赖的扣减接口。它的稳定性决定了提交订单的稳定性。因此，扣减类业务的重点就是在高并发下保障扣减的准确性和抗击高并发的能力。总结本文，通过各大公司的具体业务分析，总结了一个对形态各异的业务模式归类的方法。然后，不管各大公司提供的具体业务是什么，从技术的目的性上看，它们都是提供了三大类技术角度的业务。此外，还有一个需要注意的地方，经过上述定性后，是否可以认为某个公司系统都是某一特性的业务了（不是读业务就是写业务）？这样认为就太草率了。如订单系统，从大的目的性来看它属于写业务。但进一步会发现它对外提供的订单详情和订单列表是读业务。本文的案例只是在比较高的维度对各大公司的业务进行了划分，还可以继续利用目的性对它们做进一步的划分。 技术是服务于业务的，各类中间件的出现就是来解决不同的业务场景问题的： 比如缓存的出现就是为了解决性能的问题，因此，中间件的选型要紧贴业务场景的诉求。 消息是否要可靠达到，看业务场景，如果场景是做日志收集，则不完全需要。但如果是金融资产的对账等场景，则必须保证数据一定不能丢。 至于是业务架构保障还是中间件保障，则要看中间件的能力了，如果可以就交给中间件，没必要重复造轮子。 " }, { "title": "共识问题", "url": "/posts/consensus/", "categories": "Architecture Design, Distributed", "tags": "distributed, Consensus", "date": "2020-11-05 14:44:22 +0000", "snippet": "共识问题：区块链如何确认记账权区块链起源于中本聪的比特币，并作为比特币的底层技术，本质上是一个去中心化的数据库，其特点是去中心化、公开透明，作为分布式账本技术，每个节点都可以参与数据库的记录。区块链是一个注重安全和可信度胜过效率的一项技术，如果说互联网技术解决的是通讯问题，区块链技术解决的则是信任问题。其中有一项是区块链中的核心问题：作为分布式账本，每个参与者都维护了一份数据，那么如何确认记账权，最终的账本以谁为准呢？区块链的共识区块链的共识问题实际来源于分布式系统的一致性问题。共识（Consensus），共同的认识，研究的是多个成员如何达成一致，典型的比如投票选举。共识机制在区块链中扮演着核心的地位，因为它决定了谁有记账的权利，以及记账权利的选择过程和理由。不同的虚拟货币采用共识机制也不同，常见的共识机制有 POW、POS、DPOS等。Consistency（一致性）与 Consensus 的区别Consistency，指的是 CAP 中的 C。Consistency 侧重的是内容在时间顺序上的一致和统一；Consensus 则是指由许多参与者对某项内容达成共识。所以一般把 Consistency 翻译为“一致性”，把 Consensus 翻译为“共识”。拜占庭将军问题如果把共识机制延伸到分布式系统中，就是系统需要有一个主进程来协调，系统的所有决定都由主进程来达成一致性。到了区块链中，由于区块链是一种去中心化的分布式系统，所以区块链中是没有类似于团队里的领导，以及分布式系统中的 master 角色，这样就需要有某种共识机制，以便保证系统一致性。一般在网络通信中，把节点故障，也就是信道不可靠的情况称为“非拜占庭错误”，恶意响应，也就是系统被攻击，传递错误消息称为“拜占庭错误”。拜占庭错误来自于一个故事模型： 拜占庭帝国就是中世纪的土耳其帝国，拥有巨大的财富，周围 10 个邻邦垂诞已久，但拜占庭高墙耸立，固若金汤，没有一个单独的邻邦能够成功入侵。任何单个邻邦入侵都会失败，同时也有可能自身被其他 9 个邻邦入侵。 拜占庭帝国防御能力如此之强，至少要有十个邻邦中的一半以上同时进攻，才有可能攻破。然而，如果其中的一个或者几个邻邦本身答应好一起进攻，但实际过程出现背叛，那么入侵者可能都会被歼灭。 于是每一方都小心行事，不敢轻易相信邻国，这就是拜占庭将军问题。在拜占庭问题里，各邻国最重要的事情是：所有将军如何能过达成共识去攻打拜占庭帝国。拜占庭将军问题核心描述是军中可能有叛徒，却要保证进攻一致，由此引申到计算机领域，发展成了一种容错理论：一群将军想要实现某一个目标，比如一致进攻或者一致撤退，单独行动是行不通的，必须合作，达成共识；由于叛徒的存在，将军们不知道应该如何达到一致。拜占庭将军问题（Byzantine Generals Problem）和 Paxos 算法、逻辑时钟等，都是由 Leslie Lamport 提出的。Lamport 可以说是分布式系统的奠基人之一，由于在分布式领域的一系列贡献，Lamport 获得了 2013 年的图灵奖。拜占庭将军问题和记账权的联系在记账权的归属中，关键的是如何避免恶意共识的出现，也就是错误的记账，类似如何处理拜占庭将军中的叛徒。比特币是区块链技术最广泛的应用，在比特币中如何决定记账权呢？答案就是 POW 机制。POW 工作量证明PoW（Proof of Work，工作量证明）被认为是经过验证最安全的拜占庭解决机制，最早是用来防垃圾邮件的，典型的就是 Google 邮箱的反垃圾邮件系统。Google 邮箱强制要求每一个给 Google 服务器发送邮件的发送者，必须先完成一定量的计算工作，造成一小段时间的延迟，比如延迟 1 秒，如果是正常的邮件发送，这个时间是可以接受；如果是广告邮件发送者，因为要进行大量的发送工作，这种无价值的计算是无法忍受的。挖矿的由来挖矿是比特币系统中一个形象化的表达。比特币挖矿是将一段时间内比特币系统中发生的交易进行确认，并记录在区块链上形成新区块的过程，由于需要竞争记账权，利用计算机去计算 Hash 数值，随机碰撞解题，这个过程就是挖矿。换句话说，就是比特币系统出一道数学题，大家抢答最优解，挖矿就是记账的过程，矿工是记账员，区块链就是账本。比特币的 POW 实现比特币中的 POW 实现，是通过计算来猜测一个数值（Nonce），得以解决规定的 Hash 问题，下面是比特币的区块结构，可以看到区块头有个随机数字段，这个就是 Nonce 值：中本聪在比特币系统中设置了一道题目，通过不断调节 Nonce 的值，来对区块头算 Hash，要求找到一个 Nonce 值，使得算出来的 Hash 值满足某个固定值。具体的 Hash 方法一般是使用 SHA256 算法 ，可以查看这个小工具来测试 https://tool.oschina.net/encrypt?type=2。简化一下计算过程，假设第 100 个区块给出的区块值是下列字符串，最早计算出该字符串的节点可以获得比特币：f7684590e9c732fb3cf4bf0b8e0f5ea9511e8bbaacb589892634ae7938e5700c由于 Hash 算法是一个不可以逆的算法，没法通过具体的 Hash 值倒推出原文，这样每个节点只能采用穷举的方法，也就是选择各种字符串，比如开始的 a、b、c、1、2、3、…，不断的尝试。比特币系统自身会调节难度，控制解题的时间，一般来讲，约每 10 分钟挖出一个区块，在这 10 分钟内，计算机只能不停地去计算，去试各种字符串。这个过程实际上是考验各个节点的服务器性能，也就是算力。如果算力非常强大，有几万台服务器，可以很快得到 Nonce 值，也就是正确答案：happynaya，对应 Hash 值和题目要求一致。接下来就可以把这个 Nonce 值放在结构体里，通过 P2P 网络广播出去，其他的系统节点收到后，发现这个 Nonce 值是合法的，能满足要求，会认为你挖矿成功。由于解出了题目，会获得系统对应的比特币奖励，以及本区块内所有交易产生的手续费。其他节点发现有人已经算出来了，就会放弃本次计算，然后开启下一个区块的题目，去寻找下一个区块头的 Nonce 值。作为落地的最大区块链系统，比特币的区块信息一直在动态生成。区块信息来自 https://www.blockchain.com/.区块链分叉和 51% 攻击Hash 问题具有不可逆的特点，主要依靠暴力计算，谁的算力多，谁最先解决问题的概率就越大。当掌握超过全网一半算力时，就能控制网络中链的走向，这也是所谓 51% 攻击的由来。因为区块链中每个节点都可以参与记账，系统中可能出现链的分叉（Fork），最终会有一条链成为最长的链。但是在现实社会中，牵扯到参与各方巨大的利益关系，算力之间的博弈往往并没有那么简单，以比特币为例，已经分裂出了 BCH（比特币现金）、BTG（比特币黄金）等分叉币。POW 机制优缺点 POW 是第一个完全实现去中心化共识算法的，并且节点自由进出，容易实现，由于对算力的高要求，破坏系统花费的成本也巨大； POW 机制的缺点也是显而易见的，最大的就是浪费能源，巨大的算力被浪费在了无尽的挖矿行为中，并且本身并不产生任何价值； 这也是区块链被很多人指责的一点，浪费了大量的能源，收获的仅仅是一堆无价值的数据存储，换个角度来思考，这也说明了在去中心化的场景下，实现信任是多么的困难； 另一方面也可以看到，大量的数字货币矿场都是建设在西南地区的深山中，利用当地价格低廉的电力资源，或者就直接和发电站建设在一起。 其他共识方法除了 POW 机制，还有其他的共识方法，典型的就是 POS 和 DPOS 等。POS 权益证明POS（Proof of Stake，权益证明）类似现实生活中的股东大会机制，拥有股份越多的人拥有越多的投票权，也就越容易获取记账权。POS 是通过保证金来对赌一个合法的块成为新的区块，收益为抵押资本的利息和交易服务费。提供证明的保证金越多，则获得记账权的概率就越大，合法记账者可以获得收益。著名的数字货币 ETH（以太坊），就在共识算法中采用了 POS 机制。DPOS 委托权益证明采用 DPOS（Delegated Proof of Stake，委托权益证明）机制的典型代表是 EOS，如果说 POS 类似股东大会，比较的是谁持有的股份多，那么 DPOS 类似于公司董事会制度，在 DPOS 共识制度下，会选出一定数量的代表，来负责生产区块。总结区块链可以说是分布式系统最广泛的应用之一，主要总结区块链共识问题的由来、拜占庭将军问题，以及典型的 POW 解决机制。区块链是一个非常广的主题，以拜占庭将军问题为例，在区块链之前，还有两种解决方案： 口头协议 书面协议书籍： 《区块链开发指南》 《区块链核心算法解析》 比特币的区块结构解析 区块链技术发展及应用场景 The Byzantine Generals Problem 8btc" }, { "title": "ZooKeeper 的数据一致性", "url": "/posts/zookeeper-consistency/", "categories": "Architecture Design, Distributed", "tags": "distributed, ZooKeeper, Zab", "date": "2020-11-04 14:44:22 +0000", "snippet": "ZooKeeper 如何保证数据一致性在分布式场景中，ZooKeeper 应用非常广泛，比如： 数据发布和订阅 命名服务 配置中心 注册中心 分布式锁ZooKeeper 提供了一个类似 Linux 文件系统的数据模型，和基于 Watcher 机制的分布式事件通知，这些特性都依赖于 ZooKeeper 的高容错数据一致性协议。Zab 一致性协议ZooKeeper 通过 Zab 协议保证分布式事务的最终一致性。Zab（ZooKeeper Atomic Broadcast，ZooKeeper 原子广播协议）支持崩溃恢复。基于该协议，ZooKeeper 实现了一种主备模式的系统架构，来保持集群中各个副本之间数据一致性、Zab 系统架构如下图：在 ZooKeeper 集群中，所有客户端的请求都是写入到 Leader 进程中的，然后，由 Leader 同步到其他节点，称为 Follower。在集群数据同步的过程中，如果出现 Follower 节点崩溃或者 Leader 进程崩溃时，都会通过 Zab 协议来保证数据一致性。Zab 协议具体实现可以分为以下两个部分： 消息广播阶段 Leader 节点接受事务提交，并将新的 Proposal 请求广播给 Follower 节点，收集各个节点的反馈，决定是否进行 Commit，在这个过程中，也会使用 Quorum 选举机制。 崩溃恢复阶段 如果在同步过程中出现 Leader 节点宕机，会进入崩溃恢复阶段，重新进行 Leader 选举，崩溃恢复阶段还包含数据同步操作，同步集群中最新的数据，保持集群的数据一致性。 整个 ZooKeeper 集群的一致性保证就是在上面两个状态之前切换： 当 Leader 服务正常时，就是正常的消息广播模式； 当 Leader 不可用时，则进入崩溃恢复模式，崩溃恢复阶段会进行数据同步，完成以后，重新进入消息广播阶段。 Zab 协议中的 ZxidZxid 在 ZooKeeper 的一致性流程中非常重要。Zxid 是 Zab 协议的一个事务编号。Zxid 是一个 64 位的数字： 低 32 位，是一个简单的单调递增计数器。针对客户端的每一个事务请求，计数器加 1； 高 32 位，代表 Leader 周期年代的变化。 Leader 周期的英文 epoch，可理解为当前集群所处的年代或者周期，对比 Raft 中的 Term 概念，在 Raft 中，每一个任期的开始都是一次选举，Raft 算法保证在给定的一个任期，最多只有一个领导人。 Zab 协议的实现也类似（Raft 中的 Term），每当有一个新的 Leader 选举出现时，就会从这个 Leader 服务器上取出其本地日志中最大事务的 Zxid，并从中读取 epoch 值，然后加 1，以此作为新的周期 ID。总结一下，高 32 位代表了每代 Leader 的唯一性，低 32 位则代表了每代 Leader 中事务的唯一性。Zab 流程Zab 的具体流程可拆分为：消息广播、崩溃恢复（不对外服务，服务关闭）和数据同步三个过程。消息广播在 ZooKeeper 中所有的事务请求都由 Leader 节点来处理，其他服务器为 Follower，Leader 将客户端的事务请求转换为事务 Proposal，并且将 Proposal 分发给集群中其他所有的 Follower。完成广播之后，Leader 等待 Follwer 反馈，当有过半数的 Follower 反馈信息后，Leader 将再次向集群内 Follower 广播 Commit 信息，Commit 信息就是确认将之前的 Proposal 提交。这里的 Commit 可以对比 SQL 中的 COMMIT 操作来理解，MySQL 默认操作模式是 autocommit 自动提交模式，如果你显式地开始一个事务，在每次变更之后都要通过 COMMIT 语句来确认，将更改提交到数据库中。Leader 节点的写入也是一个两步操作，第一步是广播事务操作，第二步是广播提交操作，其中过半数指的是反馈的节点数 &amp;gt;=N/2+1，N 是全部的 Follower 节点数量。消息广播的过程，如下图： 客户端的写请求进来之后，Leader 会将写请求包装成 Proposak 事务，并添加一个递增事务 ID，也就是 Zxid，Zxid 是单调递增的，以保证每个消息的先后顺序； Proposal 事务，Leader 节点和 Follower 节点是解耦的，通信都会经过一个先进先出的消息队列，Leader 会为每一个 Follower 服务器分配一个单独的 FIFO 队列，而后把 Proposal 放到队列中； Foller 节点收到对应的 Proposal 之后会把它持久到磁盘上，当完全写入之后，发一个 ACK 给 Leader； Leader 收到超半数 Follower 机器的 ack 之后，会提交本地机器上的事务，同时开始广播 commit，Follwer 收到 commit 之后，完成各自的事务提交。崩溃恢复（选举的流程）消息广播通过 Quorum 机制，解决了 Follower 节点宕机的情况。如果在广播过程中 Leader 节点崩溃，就需要 Zab 协议支持的崩溃恢复。崩溃恢复可以保证在 Leader 进程崩溃的时候可以重新选出 Leader，并且保证数据的完整性。崩溃恢复和集群启动时的选举过程是一致的，也就是说，下面的几种情况都会进入崩溃恢复阶段： 初始化集群，刚刚启动的时候； Leader 崩溃，因为故障宕机； Leader 失去了半数的机器支持，与集群中超过一半的节点断连崩溃恢复模式将会开启新的一轮选举，选举产生的 Leader 会与过半的 Follower 进行同步，使数据一致，当与过半的机器同步完成后，就退出恢复模式， 然后进入消息广播模式。Zab 中的节点有三种状态，伴随着的 Zab 不同阶段的转换，节点状态也在变化： 状态 说明 following 当前节点是跟随者，服从Leader节点的命令 leading 当前节点是Leader,负责协调事务 election/looking 节点处于选举状态 栗子：假设正在运行的集群有五台 Follower 服务器，编号分别是 Server1、Server2、Server3、Server4、Server5，当前 Leader 是 Server2，若某一时刻 Leader 挂了，此时便开始 Leader 选举。选举过程如下： 各个节点变更状态，变更为 Looking ZooKeeper 中除了 Leader 和 Follower，还有 Observer 节点。 Observer 节点不参与选举，Leader 挂了之后，剩下的 Follower 节点都会将自己的状态变更为 Looking，而后开始进入 Leader 选举过程。 各个 Server 节点都发出一个投票，参与选举 在第一次投票中，所有的 Server 都会投自己（在初始阶段，每个节点不知道其他节点的zxid信息，也就是默认自己的最大，先投票给自己，经过一轮广播后，就有其他节点的信息了），而后各自将投票发送给集群中所有机器。 在运行期间，每个服务器上的 Zxid 大概率不同。 集群接收来自各个服务器的投票，并开始处理投票和选举 处理投票的过程对比 Zxid 的过程。 假设 Server3 的 Zxid 最大，Server 1 判断 Server3 可以成为 Leader，那么 Server1 就投票给 Server3，判断的依据如下： 首先选举 epoch 最大的 如果 epoch 相等，则选 zxid 最大的 若 epoch 和 zxid 都相等，则选择 server id 最大的，就是配置 zoo.cfg 中的 myid 在选举过程中，假如有节点获得超过半数的投票数，则会成为 Leader 节点，反之重新投票选举。 选举成功，修改服务器的状态 数据同步崩溃恢复完成选举以后，接下来就是数据同步。在选举过程中，通过投票已经确认 Leader 服务器是最大Zxid 的节点，同步阶段就是利用 Leader 前一阶段获得的最新 Proposal 历史，同步集群中所有的副本。Zab 与 Paxos 算法的联系与区别Paxos 的思想在很多分布式组件中都可以看到，Zab 协议可以认为是基于 Paxos 算法实现的。联系： 都存在一个 Leader 进程的角色，负责协调多个 Follower 进程的运行； 都是 Quorum 机制的应用，Leader 进程都会等待超过半数的 Follower 做出正确的反馈后，才会将一个提案进行提交； 在 Zab 协议中，Zxid 中通过 epoch 来代表当前 Leader 周期，在 Paxos 算法中，同样存在这样一个标识，叫做 Ballot Number区别： Paxos 是理论，Zab 是实践； Paxos 是论文性质的，目的是设计一种通用的分布式一致性算法； Zab 协议应用在 ZooKeeper 中，是一个特别设计的、崩溃可恢复的、原子消息广播算法。Zab 协议增加了崩溃恢复的功能，当 Leader 服务器不可用，或者已经半数以上节点失去联系时，ZooKeeper 会进入恢复模式选举新的 Leader 服务器，使集群达到一个一致的状态。参考资料：《从Paxos到Zookeeper》《ZooKeeper:分布式过程协同技术详解》" }, { "title": "Base 理论", "url": "/posts/data-consistency-model/", "categories": "Architecture Design, Distributed", "tags": "distributed, CAP, Paxos", "date": "2020-11-03 14:44:22 +0000", "snippet": "对于 CAP 来说，放弃强一致性，追求分区容错性和可用性，是很多分布式设计时的选择。在工程实践中，基于 CAP 定理的逐步演化，提出了 Base 理论。Base 理论Base 是三个短语的简写，即： 基本可用 Basically Available 软状态 Soft State 最终一致性 Eventually ConsistentBase 理论的核心思想是最终一致性，即使无法做到强一致性（Strong Consistency），但每个应用都可以根据自身的业务特点，采用适当的方式使系统达到最终一致性（Eventual Consistency）。Base 三要素基本可用不追求 CAP 中【任何时候，读写都是成功的】，而是系统能够【基本运行，一直提供服务】。强调了分布式系统在出现不可预知故障的时候，允许损失部分可用性，相比正常的系统，可能使响应时间延迟，或者使服务被降级。比如，在双十一秒杀活动中，如果抢购人数太多超过了系统的 QPS 峰值，可能会排队或者提示限流，这就是通过合理的手段保护系统的稳定性，保证主要的服务正常，保证基本可用。软状态可以对应 ACID 事务中的原子性，在 ACID 的事务中，实现的使强制一致性，要么全做、要么不做，所有用户看到的数据是一致的。其中原子性（Atomicity）要求多个节点的数据副本都是一致的。强调数据的一致性。原子性可以理解为一种“硬状态”。软状态则是允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。最终一致性数据不可能一直是软状态，必须在一个时间期限之后达到各个节点的一致性，在期限过后，应当保证所有副本保持数据一致性，也就是达到数据的最终一致性。在系统设计中，最终一致性实现的时间取决于网络延时、系统负载、不同的存储选型、不同数据复制方案设计等因素。全局时钟和逻辑时钟双刃剑。虽然分布式解决了传统单体架构的单点问题和性能容量问题，另一方面也带来了很多心得问题，其中一个问题就是多节点的时间同步问题。不同机器上的物理时钟难以同步，导致无法区分在分布式系统中多个节点的事件时序。没有全局时钟，绝对的内部一致性是没有意义的，一般来说，讨论的一致性都是外部一致性，而外部一致性主要指的是多并发访问时更新过的数据如何获取的问题。与全局时钟相对的，是逻辑时钟，逻辑时钟描绘了分布式系统中事件发生的时序，是为了区分现实中的物理时钟提出来的概念。一般情况下，提到的时间都是指的是物理时间，但实际上很多应用中，只要所有机器有相同的时间就够了，这个时间不一定要跟实际的时间相同。更进一步，如果两个节点之间不进行交互，那么它们的时间甚至都不需要同步。因此问题的关键点在于节点间的交互要在事件的发生顺序上达成一致，而不是对于时间达成一致。逻辑时钟的概念也被用来解决分布式一致性问题。数据一致性模型一般而言，数据一致性模型有如下分类： 强一致性（又叫做线性一致性） 弱一致性（以下是弱一致性的特性情况。根据不同业务场景，又可以分解为更细分的模型，对应不同的） 最终一致性 因果一致性 会话一致性 单调写一致性 单调读一致性 对于一致性，又可以从服务端和客户端两个不同的视角，由于全局时钟的概念，这里主要关注的是外部一致性。强一致性更新操作完成之后，任何多个后续进程的访问都会返回最新更新过的值，这种是对用户最友好的，就是用户上一次写什么，下一次就能保证读到什么。根据 CAP 理论，这种实现需要牺牲可用性。弱一致性系统在数据写入成功之后，不承诺立即读到最新写入的值，也不会具体的承诺多久之后可以读到。用户读到某一操作，对系统数据的更新需要一段时间，这段时间称为：“不一致性窗口”。最终一致性最终一致性，是弱一致性的特例。强调的是所有的数据副本，在经过一段时间同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是：需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。到达最终一致性的时间，就是不一致窗口时间，在没有故障发生的前提下，不一致窗口的时间主要受通信延迟、系统负载和复制副本个数影响。最终一致性模型根据提供的不同保证，又可以划分为更多模型，其中包括因果一致性和会话一致性等。因果一致性要求有因果关系的操作顺序得到保证，非因果关系的操作顺序是无所谓的。进程 A 在更新某个数据项之后通知了进程 B，那么进程 B 之后对该数据项的访问都应该能够获取到进程 A 更新后的最新值，并且如果进程 B 要对该数据项进行更新操作的话，务必基于进程 A 更新后的最新值。栗子：在微博或者微信进行评论时，就像在朋友圈发了一张照片，朋友对此进行了回复，这条朋友圈的显示中，自己的回复必须在朋友之后，这是一个因果关系，而其他没有因果关系的数据，可以允许不一致。会话一致性会话一致性将对系统数据的访问过程框定了一个会话当中，约定了系统能保证在一个有效的会话中实现“读己之所写”的一致性。在一次访问中，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。还有一个分布式的 Session 一致性问题，可认为是会话一致性的一个应用。CAP 和 Base 的关系Base 理论是在 CAP 上发展的。CAP 理论描述了分布式系统中数据一致性、可用性、分区容错性之间的制约关系。当选择了其中的两个，就不得不对剩下的一个做一定程度的牺牲。Base 理论是对 CAP 理论的实际应用，也就是在分区和副本存在的前提下，通过一定的系统设计方案，放弃强一致性，实现基本可用，这是大多数分布式系统的选择，比如 NoSQL 系统、微服务架构。在这个前提下，如何将基本可用做到最好，是分布式工程师（我们）的追求。除了 CAP 和 Base ，ACID 原理，它是一种强一致性模型。它强调原子性、一致性、隔离性和持久性。主要用于数据库实现中。Base 理论面向的是高可用、可扩展的分布式系统，ACID 更适合传统金融等业务。在实际的场景中，不同业务对数据的一致性要求不一样，ACID 和 Base 理论要结合使用。" }, { "title": "Paxos 算法", "url": "/posts/Paxos/", "categories": "Architecture Design, Distributed", "tags": "distributed, CAP", "date": "2020-11-02 14:33:00 +0000", "snippet": "Paxos 算法在分布式领域具有非常重要的地位。开源分布式锁组件 Google Chuby 的作者 Mike Burrows 说过：这个世界上只有一种一致性算法——Paxos 算法，其他的算法都是残次品。虽然 Paxos 算法很重要，但也因复杂而著名。Quorum 机制分布式系统中的 Quorum 选举算法。在各种一致性算法中都可以看到 Quorum 机制的身影。Quorum 的主要数学思想来源于抽屉思想。就是在 N 个副本中，一次更新成功的如果有 W 个，那么在读取数据时从大于 N-W 个副本中读取，这样就能至少读到一个更新的数据了。WARO与 Quorum 机制对应的时 WARO，就是 Write All Read one，是一种简单的副本控制协议。当 Client 请求某副本写数据（更新数据），只有当所有的副本都更新成功之后，这次的写操作才算成功，否则认为失败。WARO 优先保证读服务，因为所有的副本更新成功，才能视为更新成功，从而保证了所有的副本一致。这样的话，只需要读任何一个副本上的数据即可。写服务的可用性降低，因为只要有一个副本更新失败，此次写操作就视为失败了。假设有 N 个副本，N-1 个都宕机了，剩下的那个副本仍能提供读服务；但是只要一个副本宕机了，写服务就不会成功。WARO 牺牲了更新服务的可用性，最大程度地增强了读服务的可用性，而 Quorum 就是在更新服务和读服务之间进行的一个折衷。Quorum 定义Quorum 的定义如下：假设有 N 个副本，更新操作 wi 在 W 个副本中更新成功之后，才认为此次更新操作 wi 成功，把这次成功提交的更新操作对应的数据叫做：“成功提交的数据”。对于读操作而言，至少需要读 R 个副本才能读到此次更新的数据，其中，W+R&amp;gt;N ，即 W 和 R 有重叠，一般，W+R=N+1。 N = 存储数据副本的数量 W = 更新成功所需要的副本 R = 一次数据对象读取要访问的副本数量 Quorum 限定了一次需要读取只少 N+1-w 的副本数据。比如：假设维护了 10 个副本，一次成功更新三个，那么只少需要读取八个副本的数据，可以保证读到最新的数据。Quorum 应用Quorum 机制无法保证强一致性，也就是无法实现任何时刻或任何用户或任何节点都可以读到最近一次成功提交的副本数据。Quorum 机制的使用需要配合一个能够获取最新成功提交的版本号的 metadata 服务，这样可以确定最新已经成功提交的版本号，然后从已读到的数据中确认最新写入的数据。Quorum 是分布式中常用的一种机制，用来保证数据冗余和最终一致性的投票算法，在 Paxos、Raft 和 Zookeeper 的 Zab 等算法中，都有 Quorum 机制的应用。Paxos 节点的角色和交互Paxos 的节点角色在 Paxos 协议中，有三类节点角色，分别是 Proposer 提案者、Acceptor 批准者 和 Learner 学习者，此外还有一个 Client 产生议题者。这三类角色只是逻辑上的划分，在工作中，一个节点可以同时充当这三类角色。Proposer 提案者Proposer 可以有多个。在流程开始时，Proposer 提出议案，也就是 value（在工程中，可以是任何操作，比如“将某个变量的值更新为某个新值”），Paxos 协议 中统一将这些操作抽象为 value。不同的 Proposer 可以提出不同的甚至矛盾的 value，比如某个 Proposer 提议 “将变量 X 设置为 1”，另一个 Proposer 提议 “将变量 X 设置为 2”，但对同一轮 Paxos 过程，最多只有一个 value 被批准。Acceptor 批准者在集群中，Acceptor 有 N 个，Acceptor 之间完全对等独立，Proposer 提出的 value 必须获得超过半数（N/2 + 1）的 Acceptor 批准后才能通过。Learner 学习者Learner 不参与选举，而是学习被批准的 value，在Paxos中，Learner主要参与相关的状态机同步流程。这里Leaner的流程就参考了Quorum 议会机制，某个 value 需要获得 W=N/2 + 1 的 Acceptor 批准，Learner 需要至少读取 N/2+1 个 Accpetor，最多读取 N 个 Acceptor 的结果后，才能学习到一个通过的 value。Client 产生议题者Client 角色，作为产生议题者，实际不参与选举过程，比如发起修改请求的来源等。Proposer 与 Acceptor 之间的交互Paxos 中，Proposer 和 Acceptor 是算法核心角色。Paxos 描述的是在一个由多个 Proposer 和多个 Acceptor 构成的系统中，如何让多个 Acceptor 针对 Proposer 提出的多种提案达成一致的过程，Learner 只是“学习”最终被批准提案。Proposer 与 Acceptor 之间的交互主要有 4 类消息通信，如下图：Paxos 选举过程Phase 1 准备阶段Proposer 生成全局唯一并且递增的 ProposalID，向 Paxos 集群的所有机器发送 Prepare 请求，这里不会携带 value，只携带 N，即 ProposallD。Acceptor 收到 Prepare 请求后，判断收到的 ProposalID 是否比之前已经响应的所有提案的 N 大，如果是，则： 在本地持久化 N，可记为 Max_N； 回复请求，并带上已经 Accept 的提案中 N 最大的 value，如果此时还没有已经 Accept 的提案，则返回 value 为空； 做出承诺，不会 Accept 任何小于 Max_N 的提案，如果否，则不回复或者回复 Error。Phase 2 选举阶段为了方便描述，将 Phase 2 选举阶段拆分为 P2a、P2b 和 P2c。P2a：Proposer 发送 Accept经过一段时间后，Proposer 收集到一些 Prepare 回复，有下列几种情况： 若回复数量 &amp;gt; 一半的 Acceptor 数量，且所有回复的 value 都为空时，则 Porposer 发出 accept 请求，并带上自己指定的 value。 若回复数量 &amp;gt; 一半的 Acceptor 数量，且有的回复 value 不为空时，则 Porposer 发出 accept 请求，并带上回复中 ProposalID 最大的 value，作为自己的提案内容（在投票后发现有其他的节点唯一ID超过自己，放弃本地操作，选择其他节点的提案）。 若回复数量 &amp;lt;= 一半的 Acceptor 数量时，则尝试更新生成更大的 ProposalID，再转到准备阶段执行。 P2b：Accept 应答 AcceptAccpetor 收到 Accpet 请求 后，判断： 若收到的 N &amp;gt;= Max_N（一般情况下是等于），则回复提交成功，并持久化 N 和 value； 若收到的 N &amp;lt; Max_N，则不回复或者回复提交失败。 P2c：Proposer 统计投票经过一段时间后，Proposer 会收集到一些 Accept 回复提交成功的情况，比如： 当回复数量 &amp;gt; 一半的 Acceptor 数量时，则表示提交 value 成功，此时可以发一个广播给所有的 Proposer、Learner，通知它们已 commit 的 value； 当回复数量 &amp;lt;= 一半的 Acceptor 数量时，则尝试更新生成更大的 ProposalID，转到准备阶段执行。 当收到一条提交失败的回复时，则尝试更新生成更大的 ProposalID，也会转到准备阶段执行。Paxos 常见的问题1. 如果半数以内的 Acceptor 失效，如何正常运行？在Paxos流程中，如果出现半数以内的 Acceptor 失效，可以分为两种情况：第一种，如果半数以内的 Acceptor 失效时（算法就不工作了，具体要看各家的工程实现）还没确定最终的 value，此时所有的 Proposer 会重新竞争提案，最终有一个提案会成功提交。第二种，如果半数以内的 Acceptor 失效时已确定最终的 value，此时所有的 Proposer 提交前必须以最终的 value 提交，也就是Value实际已经生效，此值可以被获取，并不再修改。2. Acceptor 需要接受更大的 N，也就是ProposalID有什么意义？这种机制可以防止其中一个Proposer崩溃宕机产生阻塞问题，允许其他Proposer用更大ProposalID来抢占临时的访问权。3. 如何产生唯一的编号，也就是 ProposalID？在《Paxos made simple》论文中提到，唯一编号是让所有的 Proposer 都从不相交的数据集合中进行选择，需要保证在不同 Proposer之间不重复，比如系统有 5 个 Proposer，则可为每一个 Proposer 分配一个标识 j(0~4)，那么每一个 Proposer 每次提出决议的编号可以为 5*i + j，i 可以用来表示提出议案的次数。Paxos 的论文地址： The PartTime Parliament Paxos Made Simple fast-paxos" }, { "title": "CAP", "url": "/posts/CAP/", "categories": "Architecture Design, Distributed", "tags": "distributed, CAP", "date": "2020-11-02 14:33:00 +0000", "snippet": "分布式系统技术是用来解决集中式架构的性能瓶颈问题，来适应快速发展的业务规模。一般来说，分布式系统是建立在网络之上的硬件或者软件系统，彼此之间通过消息等方式进行通信和协调。分布式系统的特点： 可扩展性，通过对服务、存储扩展，来提高系统的处理能力 不出现单点故障，单点故障（Single Poin Failure）指的是系统中某个组件一旦失效，会让这个系统无法工作。而不出现单点故障，单点不影响整体，是分布式系统的设计目标之一 服务或存储无状态。无状态的服务才能满足部分机器宕机不影响全部，可随时进行扩展的需求。由于分布式系统的特点，在分布式环境中更容易出现问题，比如节点之间通信失败、网络分区故障、多个副本的数据不一致等，为更好地在分布式系统下进行开发，学者们提出了一系列的理论，其中具有代表性的就是 CAP 理论。CAPCAP 理论可以表述为，一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容忍性（Partition Tolerance）这三项中的两项。 一致性，指的是“所有节点同时看到相同的数据”，即更新操作成功，并返回客户端完成后，所有节点在同一时间的数据完全一致，等同于所有节点拥有的数据是最新版本 可用性，指的是“任何时候哦，读写都是成功的”，即服务一直可用，并且是正常响应时间。平时会看到一些 IT 公司的对外宣传，比如系统稳定性已经做到 3 个 9、4 个 9，即 99.9%、99.99%，这里的 N 个 9 就是对可用性的一个描述，叫做 SLA，即服务水平协议。比如我们说月度 99.95% 的 SLA，则意味着每个月服务出现故障的时间只能占总时间的 0.05%，如果这个月是 30 天，那么就是 21.6 分钟。 分区容忍性：指“当部分节点出现消息丢失或者分区故障的时候，分布式系统仍能够继续运行”，即系统容忍网络出现分区， 并且在遇到某节点或网络分区之间网络不可达的情况下，仍然能够对外提供满足一致性和可用性服务。在分布式系统中，由于系统的各层拆分，P 是确定的，CAP 的应用某型就是 CP 架构 和 AP 架构。分布式系统所关注的，就是在 Partition Tolerance 的前提下，如何实现更好的 A 和更稳定的 C。CAP 理论的应用CAP 理论提醒我们，在架构设计中，不要把精力浪费在如何设计能满足三者的完美分布式系统上，而是要合理进行取舍，CAP 理论类似数学上的不可能三角，只能三选二，不能全部获得。不同业务对于一致性的要求是不同的。比如说： 在微博上发表评论和点赞，用户对不一致是不敏感的，可以容忍相对较长时间的不一致，只要做好本地的交互，并不会影响用户体验； 在电商购物场景，产品价格数据则是要求强一致性的，如果商家更改价格不能实时生效，则会对交易成功率有非常大的影响。需要注意的是，在 CAP 理论中是忽略网络延迟的。也就是当事务提交时，节点间的数据复制一定是需要花费时间的。即便是同一个机房，从节点 A 复制到节点 B ，由于现实中的网络不是实时的，所有总会有一定的时间不一致。CP 和 AP 架构的取舍在平常的分布式系统中，为保证数据高可用，通常会将数据保留多个副本（Replica），网络分区是即成的现实，于是只能在可用性和一致性两者之间做出选择。CAP 理论关注的是在绝对情况下，在工程上，可用性和一致性并不是完全对立的，我们关注的往往是如何在保持相对一致性的前提下，提高系统的可用性。业务上对一致性的要求会直接反应系统设计之中，典型的就是 CP 和 AP 结构。 CP 结构：对于 CP 来说，放弃可用性，追求一致性和分区容错性。 例如 Zookeeper，就是采用了 CP 结构，ZooKeeper 是一个分布式的服务器框架，主要用来解决分布式集群中应用系统的协调和一致性问题。 ZooKeeper 的核心算法是 Zab，所有的设计都是为了一致性。在 CAP 模型中，Zookeeper 是 CP，这意味着面对网络分区时，为了保持一致性，它是不可用的。 AP 结构：对于 AP 来说，放弃强一致性，追求可用性和分区容错性，这是很多分布式系统设计时的选择，Base 也是根据 AP 来扩展的。 与 Zookeeper 相对的是 Eureka，Eureka 是 Spring Cloud 微服务技术栈中的服务发现组件，Eureka 的各个节点都是平等的，几个节点挂掉不影响正常节点的工作，剩余的节点依然可以提供注册和查询服务，只要有一台 Eureka 还在，就能保证注册服务可用，只不过查到的信息可能不是最新的版本，不保证一致性。 " }, { "title": "二级缓存的思考：Redis 与 JPA 如何结合", "url": "/posts/redis-jpa/", "categories": "Java, Spring", "tags": "Spring Security, React", "date": "2020-09-12 13:33:00 +0000", "snippet": "使用 Mybatis 的时候，基本不用关心什么是二级缓存。而如果你是 Hibernate 的使用者，一定经常听说和使用过 Hibernate 的二级缓存，那么我们应该怎么看待它呢？这一讲一起来揭晓 Cache 的相关概念以及在生产环境中的最佳实践。二级缓存的概念一级缓存的实体的生命周期和 PersistenceContext 是相同的，即载体为同一个 Session 才有效；而 Hibernate 提出了二级缓存的概念，也就是可以在不同的 Session 之间共享实体实例，说白了就是在单个应用内的整个 application 生命周期之内共享实体，减少数据库查询。由于 JPA 协议本身并没有规定二级缓存的概念，所以这是 Hiberante 独有的特性。所以在 Hibernate 中，从数据库里面查询实体的过程就变成了：第一步先看看一级缓存里面有没有实体，如果没有再看看二级缓存里面有没有，如果还是没有再从数据库里面查询。Hibernate 中二级缓存的配置方法Hibernate 中，默认情况下二级缓存是关闭的，如果想开启二级缓存需要通过如下三个步骤。第一步：引入第三方二级缓存的实现的 jar。因为 Hibernate 本身并没有实现缓存的功能，而是主要依赖第三方，如 Ehcache、jcache、redis 等第三方库。下面我们以 EhCache 为例，利用 gradle 引入 hibernate-ehcace 的依赖。代码如下所示。implementation &#39;org.hibernate:hibernate-ehcache:5.2.2.Final&#39;如果我们想用 jcache，可以通过如下方式。compile &#39;org.hibernate:hibernate-jcache:5.2.2.Final&#39;第二步：在配置文件里面开启二级缓存。二级缓存默认是关闭的，所以需要我们用如下方式开启二级缓存，并且配置 cache.region.factory_class 为不同的缓存实现类。hibernate.cache.use_second_level_cache=truehibernate.cache.region.factory_class=org.hibernate.cache.ehcache.EhCacheRegionFactory第三步：在用到二级缓存的地方配置 @Cacheable 和 @Cache 的策略。import javax.persistence.Cacheable;import javax.persistence.Entity;@Entity@Cacheable@org.hibernate.annotations.Cache(usage = CacheConcurrencyStrategy.READ_WRITE)public class UserInfo extends BaseEntity {......}二级缓存的思考二级缓存主要解决的是单应用场景下跨 Session 生命周期的实体共享问题，可是我们一定要通过 Hibernate 来做吗？答案并不是，其实我们可以通过各种 Cache 的手段来做，因为 Hibernate 里面一级缓存的复杂度相对较高，并且使用的话实体的生命周期会有变化，查询问题的过程较为麻烦。同时，随着现在逐渐微服务化、分布式化，如今的应用都不是单机应用，那么缓存之间如何共享呢？分布式缓存又该如何解决？比如一个机器变了，另一个机器没变，应该如何处理？似乎 Hiberante 并没有考虑到这些问题。此外，还有什么时间数据会变更、变化了之后如何清除缓存，等等，这些都是我们要思考的，所以 Hibernate 的二级缓存听起来“高大上”，但是使用起来绝对没有那么简单。那么经过这一连串的疑问，如果我们不用 Hibernate 的二级缓存，还有没有更好的解决方案呢？利用 Redis 进行缓存Spring Cache 和 Redis 结合第一步：在 gradle 中引入 cache 和 redis 的依赖，代码如下所示：第二步：在 application.properties 里面增加 redis 的相关配置，代码如下：第三步：通过 @EnableCaching 开启缓存，增加 configuration 配置类，代码如下所示：第四步：在我们需要缓存的地方添加 @Cacheable 注解即可。为了方便演示，我把 @Cacheable 注解配置在了 controller 方法上，代码如下:第五步：启动项目，请求一下这个 API 会发现，第一次请求过后，redis 里面就有一条记录了.可以看到，第二次请求之后，取数据就不会再请求数据库了。Spring Cache 介绍Spring 3.1 之后引入了基于注释（annotation）的缓存（cache）技术，它本质上不是一个具体的缓存实现方案（例如 EHCache 或者 Redis），而是一个对缓存使用的抽象概念，通过在既有代码中添加少量它定义的各种 annotation，就能够达到缓存方法的返回对象的效果。Spring 的缓存技术还具备相当的灵活性，不仅能够使用 SpEL（Spring Expression Language）来定义缓存的 key 和各种 condition，还提供开箱即用的缓存临时存储方案，也支持主流的专业缓存，例如 Redis，EHCache 集成。而 Spring Cache 属于 Spring framework 的一部分，在下面图片所示的这个包里面。Spring cache 里面的主要的注解@Cacheable应用到读取数据的方法上，就是可以缓存的方法，如查找方法：先从缓存中读取，如果没有再调用方法获取数据，然后把数据添加到缓存中。@CachePut调用方法时会自动把相应的数据放入缓存，它与 @Cacheable 不同的是所有注解的方法每次都会执行，一般配置在 Update 和 insert 方法上。其源码里面的字段和用法基本与 @Cacheable 相同，只是使用场景不一样，我就不详细介绍了。@CacheEvict删除缓存，一般配置在删除方法上面。代码如下所示。此外，还有 @CacheConfig 表示全局 Cache 配置；@EnableCaching，表示是否开启 SpringCache 的配置。Spring Cache Redis 里面主要的类org.springframework.boot.autoconfigure.cache.CacheAutoConfigurationorg.springframework.cache.annotation.CachingConfigurerSupportorg.springframework.boot.autoconfigure.cache.RedisCacheConfigurationSpring Cache 结合 Redis 使用不同 cache 的 name 在 redis 里面配置不同的过期时间默认情况下所有 redis 的 cache 过期时间是一样的，实际工作中一般需要自定义不同 cache 的 name 的过期时间，我们这里 cache 的 name 就是指 @Cacheable 里面 value 属性对应的值。主要步骤如下。第一步：自定义一个配置文件，用来指定不同的 cacheName 对应的过期时间不一样。代码如下所示。第二步：通过自定义类 MyRedisCacheManagerBuilderCustomizer 实现 RedisCacheManagerBuilderCustomizer 里面的 customize 方法，用来指定不同的 name 采用不同的 RedisCacheConfiguration，从而达到设置不同的过期时间的效果。第三步：在 CacheConfiguation 里面把我们自定义的 CacheManagerCustomize 加载进去即可，代码如下。第四步：使用的时候非常简单，只需要在 application.properties 里面做如下配置即可。自定义 KeyGenerator 实现，redis 的 key 自定义拼接规则假如我们不喜欢默认的 cache 生成的 key 的 string 规则，那么可以自定义。我们创建 MyRedisCachingConfigurerSupport 集成 CachingConfigurerSupport 即可，代码如下。@Component@Log4j2public class MyRedisCachingConfigurerSupport extends CachingConfigurerSupport { @Override public KeyGenerator keyGenerator() { return getKeyGenerator(); } /** * 覆盖默认的redis key的生成规则，变成&quot;方法名:参数:参数&quot; * @return */ public static KeyGenerator getKeyGenerator() { return (target, method, params) -&amp;gt; { StringBuilder key = new StringBuilder(); key.append(ClassUtils.getQualifiedMethodName(method)); for (Object obc : params) { key.append(&quot;:&quot;).append(obc); } return key.toString(); }; }}当发生 cache 和 redis 的操作异常时，我们不希望阻碍主流程，打印一个关键日志即可.只需要在 MyRedisCachingConfigurerSupport 里面再实现父类的 errorHandler 即可，代码变成了如下模样。@Log4j2public class MyRedisCachingConfigurerSupport extends CachingConfigurerSupport { @Override public KeyGenerator keyGenerator() { return getKeyGenerator(); } /** * 覆盖默认的redis key的生成规则，变成&quot;方法名:参数:参数&quot; * @return */ public static KeyGenerator getKeyGenerator() { return (target, method, params) -&amp;gt; { StringBuilder key = new StringBuilder(); key.append(ClassUtils.getQualifiedMethodName(method)); for (Object obc : params) { key.append(&quot;:&quot;).append(obc); } return key.toString(); }; } /** * 覆盖默认异常处理方法，不抛异常，改打印error日志 * * @return */ @Override public CacheErrorHandler errorHandler() { return new CacheErrorHandler() { @Override public void handleCacheGetError(RuntimeException exception, Cache cache, Object key) { log.error(String.format(&quot;Spring cache GET error:cache=%s,key=%s&quot;, cache, key), exception); } @Override public void handleCachePutError(RuntimeException exception, Cache cache, Object key, Object value) { log.error(String.format(&quot;Spring cache PUT error:cache=%s,key=%s&quot;, cache, key), exception); } @Override public void handleCacheEvictError(RuntimeException exception, Cache cache, Object key) { log.error(String.format(&quot;Spring cache EVICT error:cache=%s,key=%s&quot;, cache, key), exception); } @Override public void handleCacheClearError(RuntimeException exception, Cache cache) { log.error(String.format(&quot;Spring cache CLEAR error:cache=%s&quot;, cache), exception); } }; }}改变默认的 cache 里面 redis 的 value 序列化方式默认有可能是 JDK 序列化方式，所以一般我们看不懂 redis 里面的值，那么就可以把序列化方式改成 JSON 格式，只需要在 CacheConfiguration 里面增加默认的 RedisCacheConfiguration 配置即可，完整的 CacheConfiguration 变成如下代码所示的样子。@EnableCaching@Configuration@EnableConfigurationProperties(value = {MyCacheProperties.class,CacheProperties.class})@AutoConfigureAfter({CacheAutoConfiguration.class})public class CacheConfiguration { /** * 支持不同的cache name有不同的缓存时间的配置 * * @param myCacheProperties * @param redisCacheConfiguration * @return */ @Bean @ConditionalOnMissingBean(name=&quot;myRedisCacheManagerBuilderCustomizer&quot;) @ConditionalOnClass(RedisCacheManagerBuilderCustomizer.class) public MyRedisCacheManagerBuilderCustomizer myRedisCacheManagerBuilderCustomizer(MyCacheProperties myCacheProperties, RedisCacheConfiguration redisCacheConfiguration) { return new MyRedisCacheManagerBuilderCustomizer(myCacheProperties,redisCacheConfiguration); } /** * cache异常不抛异常，只打印error日志 * * @return */ @Bean @ConditionalOnMissingBean(name = &quot;myRedisCachingConfigurerSupport&quot;) public MyRedisCachingConfigurerSupport myRedisCachingConfigurerSupport() { return new MyRedisCachingConfigurerSupport(); } /** * 依赖默认的ObjectMapper，实现普通的json序列化 * @param defaultObjectMapper * @return */ @Bean(name = &quot;genericJackson2JsonRedisSerializer&quot;) @ConditionalOnMissingBean(name = &quot;genericJackson2JsonRedisSerializer&quot;) public GenericJackson2JsonRedisSerializer genericJackson2JsonRedisSerializer(ObjectMapper defaultObjectMapper) { ObjectMapper objectMapper = defaultObjectMapper.copy(); objectMapper.registerModule(new Hibernate5Module().enable(REPLACE_PERSISTENT_COLLECTIONS)); //支持JPA的实体的json的序列化 objectMapper.configure(MapperFeature.SORT_PROPERTIES_ALPHABETICALLY, true);//培训 objectMapper.deactivateDefaultTyping(); //关闭 defaultType，不需要关心reids里面是否为对象的类型 return new GenericJackson2JsonRedisSerializer(objectMapper); } /** * 覆盖 RedisCacheConfiguration，只是修改serializeValues with jackson * * @param cacheProperties * @return */ @Bean @ConditionalOnMissingBean(name = &quot;jacksonRedisCacheConfiguration&quot;) public RedisCacheConfiguration jacksonRedisCacheConfiguration(CacheProperties cacheProperties,GenericJackson2JsonRedisSerializer genericJackson2JsonRedisSerializer) { CacheProperties.Redis redisProperties = cacheProperties.getRedis(); RedisCacheConfiguration config = RedisCacheConfiguration.defaultCacheConfig(); config = config.serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(genericJackson2JsonRedisSerializer));//修改的关键所在，指定Jackson2JsonRedisSerializer的方式 if (redisProperties.getTimeToLive() != null) { config = config.entryTtl(redisProperties.getTimeToLive()); } if (redisProperties.getKeyPrefix() != null) { config = config.prefixCacheNameWith(redisProperties.getKeyPrefix()); } if (!redisProperties.isCacheNullValues()) { config = config.disableCachingNullValues(); } if (!redisProperties.isUseKeyPrefix()) { config = config.disableKeyPrefix(); } return config; }}" }, { "title": "Spring Data ElasticSearch 在 Spring Data 中的用法", "url": "/posts/spring-data-elastic-search/", "categories": "Java, Spring", "tags": "Spring Security, React", "date": "2020-09-10 13:33:00 +0000", "snippet": "Spring Data ElasticSearchSpring Data 和 Elasticsearch 结合的时候，唯一需要注意的是版本之间的兼容性问题。Elasticsearch 和 Spring Boot 是同时向前发展的，而 Elasticsearch 的大版本之间还存在一定的 API 兼容性问题，所以必须要知道这些版本之间的关系，如下:| Spring Data Release Train | Spring Data Elasticsearch | Elasticsearch | Spring Boot || ————————- | ————————- | ————- | ———– || 2020.0.0[1] | 4.1.x[1] | 7.9.3 | 2.4.x[1] || Neumann | 4.0.x | 7.6.2 | 2.3.x || Moore | 3.2.x | 6.8.12 | 2.2.x || Lovelace | 3.1.x | 6.2.2 | 2.1.x || Kay[2] | 3.0.x[2] | 5.5.0 | 2.0.x[2] || Ingalls[2] | 2.1.x[2] | 2.4.0 | 1.5.x[2] | 利用 Helm Chart 安装一个 Elasticsearch 集群 7.9.3 版本; 在 gradle.build 里面配置 Spring Data ElasticSearch 依赖的 Jar 包; 建一个目录，结构如下图所示，方便我们测试; 在 application.properties 里面新增 es 的连接地址，连接本地的 Elasticsearch; spring.data.elasticsearch.client.reactive.endpoints=127.0.0.1:9200 新增一个 ElasticSearchConfiguration 的配置文件，主要是为了开启扫描的包； 新增一个 Topic 的 Document，它类似 JPA 里面的实体，用来保存和读取 Topic 的数据，代码如下所示； 新建一个 Elasticsearch 的 Repository，用来对 Elasticsearch 索引的增删改查，代码如下所示； 新建一个 Controller，对 Topic 索引进行查询和添加； 发送一个添加和查询的请求测试一下； Elasticsearch Repository 的测试用例写法，如下面的代码和注释所示。ESRepository 和 JPARepository 同时存在怎么区分不同的 Repository 用什么呢： 将对 Elasticsearch 的实体、Repository 和对 JPA 操作的实体、Repository 放到不同的文件里面； 新增 JpaConfiguration，用来指定 Jpa 相关的 Repository 目录； 新增 User 实体，用来操作用户基本信息; 新增 UserRepository，用来进行 DB 操作； 写测试用例进行测试;Spring Data 对 JPA 等 SQL 型的传统数据库的支持是非常好的，同时对 NoSQL 型的非关系类数据库的支持也比较友好，大大降低了操作不同数据源的难度，可以有效提升我们的开发效率。" }, { "title": "利用 Repository 中的方法返回值解决实际问题", "url": "/posts/repository-method-return-value/", "categories": "Java, Spring", "tags": "Spring Security, React", "date": "2020-09-06 14:33:00 +0000", "snippet": "Repository 的返回结果有哪些Repository 接口支持的返回结果有哪些，如下图所示：打开 SimpleJpaRepository 直接看它的 Structure 就可以知道，它实现的方法，以及父类接口的方法和返回类型包括：Optional、Iterable、List、Page、Long、Boolean、Entity 对象等，而实际上支持的返回类型还要多一些。由于 Repository 里面支持 Iterable，所以其实 java 标准的 List、Set 都可以作为返回结果，并且也会支持其子类。Spring Data 里面定义了一个特殊的子类 Steamable，Streamable 可以替代 Iterable 或任何集合类型。它还提供了方便的方法来访问 Stream，可以直接在元素上进行 ….filter(…) 和 ….map(…) 操作，并将 Streamable 连接到其他元素。再看个关于 UserRepository 直接继承 JpaRepository 的例子。public interface UserRepository extends JpaRepository&amp;lt;User,Long&amp;gt; {}还用之前的 UserRepository 类，在测试类里面做如下调用：User user = userRepository.save(User.builder().name(&quot;jackxx&quot;).email(&quot;123456@126.com&quot;).sex(&quot;man&quot;).address(&quot;shanghai&quot;).build());Assert.assertNotNull(user);Streamable&amp;lt;User&amp;gt; userStreamable = userRepository.findAll(PageRequest.of(0,10)).and(User.builder().name(&quot;jack222&quot;).build());userStreamable.forEach(System.out::println);然后就会得到如下输出：User(id=1, name=jackxx, email=123456@126.com, sex=man, address=shanghai)User(id=null, name=jack222, email=null, sex=null, address=null)这个例子 Streamable&amp;lt;User&amp;gt; userStreamable，实现了 Streamable 的返回结果，如果想自定义方法，可以进行如下操作。自定义 Streamable官方提供了自定义 Streamable 的方法，不过在实际工作中很少出现要自定义保证结果类的情况，看如下例子：class Product { // (1) MonetaryAmount getPrice() { … }}@RequiredArgConstructor(staticName = &quot;of&quot;)class Products implements Streamable&amp;lt;Product&amp;gt; { // (2) private Streamable&amp;lt;Product&amp;gt; streamable; // (3) public MonetaryAmount getTotal() { return streamable.stream() .map(Priced::getPrice) .reduce(Money.of(0), MonetaryAmount::add); }}interface ProductRepository implements Repository&amp;lt;Product, Long&amp;gt; { // (4) Products findAllByDescriptionContaining(String text);}以上四个步骤介绍了自定义 Streamable 的方法，分别为： Product 实体，公开 API 以访问产品价格； Streamable 的包装类型可以通过 Products.of(…) 构造（通过 Lombok 注解创建的工厂方法）； 包装器类型在 Streamable 上公开了计算新值的其他 API； 可以将包装器类型直接用作查询方法返回类型。无须返回 Stremable 并将其手动包装在存储库 Client 端中。通过以上例子就可以做到自定义 Streamable。其原理很简单，就是实现 Streamable 接口，自己定义自己的实现类即可。也可以看下源码 QueryExecutionResultHandler 里面是否有 Streamable 子类的判断，来支持自定义 Streamable，关键源码如下：返回结果类型 List/Stream/Page/Slice在实际开发中，如何返回 List/Stream/Page/Slice 呢？首先，新建 UserRepository：package cn.happymaya.jpaguide;import org.springframework.data.domain.Pageable;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.Query;import java.util.stream.Stream;public interface UserRepository extends JpaRepository&amp;lt;User,Long&amp;gt; { // 自定义一个查询方法，返回Stream对象，并且有分页属性 @Query(&quot;select u from User u&quot;) Stream&amp;lt;User&amp;gt; findAllByCustomQueryAndStream(Pageable pageable); //测试Slice的返回结果 @Query(&quot;select u from User u&quot;) Slice&amp;lt;User&amp;gt; findAllByCustomQueryAndSlice(Pageable pageable);}然后，修改一下测试用例类，如下，验证一下结果：package cn.happymaya.jpaguide;import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.databind.ObjectMapper;import org.assertj.core.util.Lists;import org.junit.Assert;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;import org.springframework.data.domain.Page;import org.springframework.data.domain.PageRequest;import org.springframework.data.domain.Slice;import org.springframework.data.util.Streamable;import java.util.List;import java.util.stream.Stream;@DataJpaTestpublic class UserRepositoryTest { @Autowired private UserRepository userRepository; @Test public void testSaveUser() throws JsonProcessingException { // 新增 7 条数据方便测试分页结果 userRepository.save( User.builder() .name(&quot;jack1&quot;) .email(&quot;123456@126.com&quot;) .sex(&quot;man&quot;) .address(&quot;shanghai&quot;) .build() ); userRepository.save( User.builder() .name(&quot;jack2&quot;) .email(&quot;123456@126.com&quot;) .sex(&quot;man&quot;) .address(&quot;shanghai&quot;) .build() ); userRepository.save( User.builder() .name(&quot;jack3&quot;) .email(&quot;123456@126.com&quot;) .sex(&quot;man&quot;) .address(&quot;shanghai&quot;) .build() ); userRepository.save( User.builder() .name(&quot;jack4&quot;) .email(&quot;123456@126.com&quot;) .sex(&quot;man&quot;) .address(&quot;shanghai&quot;) .build() ); userRepository.save( User.builder() .name(&quot;jack5&quot;) .email(&quot;123456@126.com&quot;) .sex(&quot;man&quot;) .address(&quot;shanghai&quot;) .build() ); userRepository.save( User.builder() .name(&quot;jack6&quot;) .email(&quot;123456@126.com&quot;) .sex(&quot;man&quot;) .address(&quot;shanghai&quot;) .build() ); userRepository.save( User.builder() .name(&quot;jack7&quot;) .email(&quot;123456@126.com&quot;) .sex(&quot;man&quot;) .address(&quot;shanghai&quot;) .build() ); // 利用 ObjectMapper 将返回结果 Json to String ObjectMapper objectMapper = new ObjectMapper(); // 返回 Stream 类型结果（1） Stream&amp;lt;User&amp;gt; userStream = userRepository.findAllByCustomQueryAndStream(PageRequest.of(1,3)); userStream.forEach(System.out::println); // 返回分页数据（2） Page&amp;lt;User&amp;gt; userPage = userRepository.findAll(PageRequest.of(0,3)); System.out.println(objectMapper.writeValueAsString(userPage)); // 返回 Slice 结果（3） Slice&amp;lt;User&amp;gt; userSlice = userRepository.findAllByCustomQueryAndSlice(PageRequest.of(0,3)); System.out.println(objectMapper.writeValueAsString(userSlice)); // 返回 List 结果（4） List&amp;lt;User&amp;gt; userList = userRepository.findAllById(Lists.newArrayList(1L,2L)); System.out.println(objectMapper.writeValueAsString(userList)); }}这个时候分别看下四种测试结果：第一种：通过 Stream&amp;lt;User&amp;gt; 取第二页的数据，得到结果如下： User(id=4, name=jack4, email=123456@126.com, sex=man, address=shanghai) User(id=5, name=jack5, email=123456@126.com, sex=man, address=shanghai) User(id=6, name=jack6, email=123456@126.com, sex=man, address=shanghai)Spring Data 的支持可以通过使用 Java 8 Stream 作为返回类型来逐步处理查询方法的结果。需要注意的是：流的关闭问题，try catch 是一种常用的关闭方法，如下所示：Stream&amp;lt;User&amp;gt; stream;try { stream = repository.findAllByCustomQueryAndStream() stream.forEach(…);} catch (Exception e) { e.printStackTrace();} finally { if (stream!=null){ stream.close(); }}第二种：返回 Page&amp;lt;User&amp;gt; 的分页数据结果，如下所示：{ &quot;content&quot;: [ { &quot;id&quot;:1, &quot;name&quot;:&quot;jack1&quot;, &quot;email&quot;:&quot;123456@126.com&quot;, &quot;sex&quot;:&quot;man&quot;, &quot;address&quot;:&quot;shanghai&quot; }, { &quot;id&quot;:1, &quot;name&quot;:&quot;jack1&quot;, &quot;email&quot;:&quot;123456@126.com&quot;, &quot;sex&quot;:&quot;man&quot;, &quot;address&quot;:&quot;shanghai&quot; }, { &quot;id&quot;:1, &quot;name&quot;:&quot;jack1&quot;, &quot;email&quot;:&quot;123456@126.com&quot;, &quot;sex&quot;:&quot;man&quot;, &quot;address&quot;:&quot;shanghai&quot; } ],  &quot;pageable&quot;:{ &quot;sort&quot;:{ &quot;sorted&quot;:false, &quot;unsorted&quot;:true, &quot;empty&quot;:true }, // 当前页码 &quot;pageNumber&quot;:0, // 页码大小 &quot;pageSize&quot;:3, // 偏移量 &quot;offset&quot;:0, // 是否分页了 &quot;paged&quot;:true, &quot;unpaged&quot;:false }, // 一共有多少页 &quot;totalPages&quot;:3, // 是否是到最后 &quot;last&quot;:false, // 一共多少调数 &quot;totalElements&quot;:7, // 当前数据下标 &quot;numberOfElements&quot;:3, &quot;sort&quot;:{ &quot;sorted&quot;:false, &quot;unsorted&quot;:true, &quot;empty&quot;:true }, // 当前content大小 &quot;size&quot;:3, // 当前页面码的索引 &quot;number&quot;:0, // 是否是第一页 &quot;first&quot;:true, // false是否有数据 &quot;empty&quot;:}这里可以看到 Page 返回了第一个页的数据，并且告诉我们一共有三个部分的数据： content：数据的内容，现在指 User 的 List 3 条； pageable：分页数据，包括排序字段是什么及其方向、当前是第几页、一共多少页、是否是最后一条等； 当前数据的描述：“size”：3，当前 content 大小；“number”：0，当前页面码的索引； “first”：true，是否是第一页；“empty”：false，是否没有数据。通过这三部分数据可以知道要查数的分页信息。接着看第三种测试结果。**第三种：返回 Slice 结果，如下所示：**{   &quot;content&quot;:[      {         &quot;id&quot;:4,         &quot;name&quot;:&quot;jack4&quot;,         &quot;email&quot;:&quot;123456@126.com&quot;,         &quot;sex&quot;:&quot;man&quot;,         &quot;address&quot;:&quot;shanghai&quot;      },      {         &quot;id&quot;:5,         &quot;name&quot;:&quot;jack5&quot;,         &quot;email&quot;:&quot;123456@126.com&quot;,         &quot;sex&quot;:&quot;man&quot;,         &quot;address&quot;:&quot;shanghai&quot;      },      {         &quot;id&quot;:6,         &quot;name&quot;:&quot;jack6&quot;,         &quot;email&quot;:&quot;123456@126.com&quot;,         &quot;sex&quot;:&quot;man&quot;,         &quot;address&quot;:&quot;shanghai&quot;      }   ],   &quot;pageable&quot;:{      &quot;sort&quot;:{         &quot;sorted&quot;:false,         &quot;unsorted&quot;:true,         &quot;empty&quot;:true      },      &quot;pageNumber&quot;:1,      &quot;pageSize&quot;:3,      &quot;offset&quot;:3,      &quot;paged&quot;:true,      &quot;unpaged&quot;:false   },   &quot;numberOfElements&quot;:3,   &quot;sort&quot;:{      &quot;sorted&quot;:false,      &quot;unsorted&quot;:true,      &quot;empty&quot;:true   },   &quot;size&quot;:3,   &quot;number&quot;:1,   &quot;first&quot;:false,   &quot;last&quot;:false,   &quot;empty&quot;:false}这时发现上面的 Page 返回结果少了，那么一共有多少条结果、多少页的数据呢？再比较一下第二种和第三种测试结果的执行 SQL：第二种执行的是普通的分页查询 SQL：查询分页数据Hibernate: select user0_.id as id1_0_, user0_.address as address2_0_, user0_.email as email3_0_, user0_.name as name4_0_, user0_.sex as sex5_0_ from user user0_ limit ?计算分页数据Hibernate: select count(user0_.id) as col_0_0_ from user user0_第三种执行的 SQL 如下：Hibernate: select user0_.id as id1_0_, user0_.address as address2_0_, user0_.email as email3_0_, user0_.name as name4_0_, user0_.sex as sex5_0_ from user user0_ limit ? offset ?通过对比可以看出，只查询偏移量，不计算分页数据，这就是 Page 和 Slice 的主要区别。接着看第四种测试结果。**第四种：返回 List 结果如下：**[ { &quot;id&quot;:1, &quot;name&quot;:&quot;jack1&quot;, &quot;email&quot;:&quot;123456@126.com&quot;, &quot;sex&quot;:&quot;man&quot;, &quot;address&quot;:&quot;shanghai&quot; }, { &quot;id&quot;:2, &quot;name&quot;:&quot;jack2&quot;, &quot;email&quot;:&quot;123456@126.com&quot;, &quot;sex&quot;:&quot;man&quot;, &quot;address&quot;:&quot;shanghai&quot; }]到这里，可以很简单地查询出来 ID=1 和 ID=2 的数据，没有分页信息。上面四种方法介绍了常见的多条数据返回结果的形式，单条的我就不多介绍了，相信你一看就懂，无非就是对 JDK8 的 Optional 的支持。比如支持了 Null 的优雅判断，再一个就是支持直接返回 Entity，或者一些存在 / 不存在的 Boolean 的结果和一些 count 条数的返回结果而已。Repository 对 Feature/CompletableFuture 异步返回结果的支持可以使用 Spring 的异步方法执行Repository查询，这意味着方法将在调用时立即返回，并且实际的查询执行将发生在已提交给 Spring TaskExecutor 的任务中，比较适合定时任务的实际场景。异步使用起来比较简单，直接加@Async 注解即可，如下所示：@AsyncFuture&amp;lt;User&amp;gt; findByFirstname(String firstname); (1)@AsyncCompletableFuture&amp;lt;User&amp;gt; findOneByFirstname(String firstname); (2)@AsyncListenableFuture&amp;lt;User&amp;gt; findOneByLastname(String lastname);(3)上述三个异步方法的返回结果，分别做如下解释： 第一处：使用 java.util.concurrent.Future 的返回类型； 第二处：使用 java.util.concurrent.CompletableFuture 作为返回类型； 第三处：使用 org.springframework.util.concurrent.ListenableFuture 作为返回类型。以上是对 @Async 的支持，关于实际使用需要注意以下三点内容： 在实际工作中，直接在 Repository 这一层使用异步方法的场景不多，一般都是把异步注解放在 Service 的方法上面，这样的话，可以有一些额外逻辑，如发短信、发邮件、发消息等配合使用； 使用异步的时候一定要配置线程池，这点切记，否则“死”得会很难看； 万一失败会怎么处理？关于事务是怎么处理的呢？这种需要重点考虑的对 Reactive 支持 flux 与 Mono到Spring Data Common里面对React还是有支持的，那为什么在JpaRespository里面没看到有响应的返回结果支持呢？其实Common里面提供的只是接口，而JPA里面没有做相关的Reactive 的实现，但是本身Spring Data Common里面对 Reactive 是支持的。引用一个Spring Data Common的子模块implementation ‘org.springframework.boot:spring-boot-starter-data-mongodb’ 来加载依赖，这时候我们打开 Repository 看 Hierarchy 就可以看到，这里多了一个 Mongo 的 Repsitory 的实现，天然地支持着 Reactive 这条线。返回结果支持总结processResult 的时候分别对 PageQuery、Stream、Reactiv 有了各自的判断。QueryExecutorConverters 里面对 JDK8、Guava、vavr 也做了各种支持。下表列出了 Spring Data JPA Query Method 机制支持的方法的返回值类型：最常见的 DTO 返回结果的支持方法Projections 的概念Spring JPA 对 Projections 扩展的支持，个人觉得这是个非常好的东西，从字面意思上理解就是映射，指的是和 DB 的查询结果的字段映射关系。一般情况下，返回的字段和 DB 的查询结果的字段是一一对应的；但有的时候，需要返回一些指定的字段，或者返回一些复合型的字段，而不需要全部返回。原来的做法是自己写各种 entity 到 view 的各种 convert 的转化逻辑，而 Spring Data 正是考虑到了这一点，允许对专用返回类型进行建模，有选择地返回同一个实体的不同视图对象。下面以 User 查询对象为例，看看怎么自定义返回 DTO：@Entity@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class User { @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String name; private String email; private String sex; private String address;}看上面的原始 User 实体代码，如果只想返回 User 对象里面的 name 和 email，有三种方法。第一种方法：新建一张表的不同 Entity首先，新增一个 Entity 类：通过 @Table 指向同一张表，这张表和 User 实例里面的表一样都是 user，完整内容如下：@Entity@Table(name = &quot;user&quot;)@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class UserOnlyNameEmailEntity { @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String name; private String email;}然后，新增一个 UserOnlyNameEmailEntityRepository，做单独的查询：package cn.happymaya.jpaguide;import org.springframework.data.jpa.repository.JpaRepository;public interface UserOnlyNameEmailEntityRepository extends JpaRepository&amp;lt;UserOnlyNameEmailEntity,Long&amp;gt; {}最后，测试用例里面的写法如下：@Testpublic void testProjections() { userRepository.save( User.builder() .id(1L) .name(&quot;jack12&quot;) .email(&quot;123456@126.com&quot;) .sex(&quot;man&quot;) .address(&quot;shanghai&quot;) .build() ); List&amp;lt;User&amp;gt; users= userRepository.findAll(); System.out.println(users); UserOnlyNameEmailEntity uName = userOnlyNameEmailEntityRepository.getOne(1L); System.out.println(uName);}输出结果：Hibernate: insert into user (address, email, name, sex, id) values (?, ?, ?, ?, ?)Hibernate: select user0_.id as id1_0_, user0_.address as address2_0_, user0_.email as email3_0_, user0_.name as name4_0_, user0_.sex as sex5_0_ from user user0_[User(id=1, name=jack12, email=123456@126.com, sex=man, address=shanghai)]Hibernate: select useronlyna0_.id as id1_0_0_, useronlyna0_.email as email3_0_0_, useronlyna0_.name as name4_0_0_ from user useronlyna0_ where useronlyna0_.id=?UserOnlyNameEmailEntity(id=1, name=jack12, email=123456@126.com)上述结果可以看到，当在 user 表里面插入了一条数据，而 userRepository 和 userOnlyNameEmailEntityRepository 查询的都是同一张表 user。 优点：简单、方便，很容易可以想到； 缺点：通过两个实体都可以进行 update 操作。如果同一个项目里面这种实体比较多，到时候就容易不知道是谁更新的，从而导致出 bug 不好查询，实体职责划分不明确。第二种方法：直接定义一个 UserOnlyNameEmailDto首先，新建一个 DTO 类来返回我们想要的字段，它是 UserOnlyNameEmailDto，用来接收 name、email 两个字段的值，具体如下：@Data@Builder@AllArgsConstructorpublic class UserOnlyNameEmailDto { private String name,email;}其次，在 UserRepository 里面做如下用法：public interface UserRepository extends JpaRepository&amp;lt;User,Long&amp;gt; { //测试只返回name和email的DTO UserOnlyNameEmailDto findByEmail(String email);}然后，测试用例里面写法如下：@Testpublic void testProjections() { userRepository.save( User.builder() .id(1L) .name(&quot;jack12&quot;) .email(&quot;123456@126.com&quot;) .sex(&quot;man&quot;) .address(&quot;shanghai&quot;) .build() ); UserOnlyNameEmailDto userOnlyNameEmailDto = userRepository.findByEmail(&quot;123456@126.com&quot;);System.out.println(userOnlyNameEmailDto);}最后，输出结果如下：Hibernate: select user0_.name as col_0_0_, user0_.email as col_1_0_ from user user0_ where user0_.email=?UserOnlyNameEmailDto(name=jack12, email=123456@126.com)这里需要注意的是，如果看源码的话，关键的 PreferredConstructorDiscoverer 类时会发现，UserDTO 里面只能有一个全参数构造方法。 优点：返回的结果不需要是个实体对象，对 DB 不能进行除了查询之外的任何操作； 缺点：有 set 方法还可以改变里面的值，构造方法不能更改，必须全参数。第三种方法：返回结果是一个 POJO 的接口返回不同字段的方式，这种方式与上面两种的区别是只需要定义接口，它的好处是只读，不需要添加构造方法，使用起来非常灵活，一般很难产生 Bug，实现方式如下。首先，定义一个 UserOnlyName 的接口：package cn.happymaya.jpaguide;public interface UserOnlyName { String getName(); String getEmail();}其次，UserRepository 写法如下：package cn.happymaya.jpaguide;import org.springframework.data.jpa.repository.JpaRepository;public interface UserRepository extends JpaRepository&amp;lt;User,Long&amp;gt; { /** * 接口的方式返回DTO * @param address * @return */ UserOnlyName findByAddress(String address);}然后，测试用例的写法如下：@Testpublic void testProjections() { userRepository.save( User.builder() .name(&quot;jack12&quot;) .email(&quot;123456@126.com&quot;) .sex(&quot;man&quot;) .address(&quot;shanghai&quot;) .build() ); UserOnlyName userOnlyName = userRepository.findByAddress(&quot;shanghai&quot;); System.out.println(userOnlyName);}最后，运行结果如下：Hibernate: select user0_.name as col_0_0_, user0_.email as col_1_0_ from user user0_ where user0_.address=?org.springframework.data.jpa.repository.query.AbstractJpaQuery$TupleConverter$TupleBackedMap@1d369521这个时候会 userOnlyName 接口成了一个代理对象，里面通过 Map 的格式包含了我们的要返回字段的值（如：name、email），用的时候直接调用接口里面的方法即可，如 userOnlyName.getName() 即可；这种方式的优点是接口为只读，并且语义更清晰。其中源码是如何实现的，我来说一个类，你可以通过 debug，看一下最终 DTO 和接口转化执行的 query 有什么不同.小技巧当写 userRepositor 的定义方法的时候，IDA 会为提供满足 JPA 语法的提示，这也是用 Spring Data JPA 的好处之一，因为这些一旦约定死了（这里是指遵守 JPA 协议），周边的工具会越来越成熟，其中 MyBatis 太灵活了，就会导致周边的工具没办法跟上。" }, { "title": "Spring Data Common 之 Repository", "url": "/posts/spring-data-common-repository/", "categories": "Java, Spring", "tags": "Spring Security, React", "date": "2020-09-05 13:33:00 +0000", "snippet": "Spring Data 对整个数据操作做了很好的封装，其中 Spring Data Common 定义了很多公用的接口和一些相对数据操作的公共实现（如分页排序、结果映射、Autiting 信息、事务等），而 Spring Data JPA 就是 Spring Data Common 的关系数据库的查询实现。Spring Data Common 的核心内容——Repository。Spring Data Common 的依赖关系Spring Data Common 的依赖关系如下图：通过上图的项目依赖，不难发现： 数据库连接用的是 JDBC，连接池用的是 HikariCP，强依赖 Hibernate； Spring Boot Starter Data JPA 依赖 Spring Data JPA； Spring Data JPA 依赖 Spring Data Commons。Repository 接口Repository 是 Spring Data Common 里面的顶级父类接口，操作 DB 的入口类。查看 Resposiory 源码Common 里面的 Resposiory 源码，如下所示：package org.springframework.data.repository;import org.springframework.stereotype.Indexed;@Indexedpublic interface Repository&amp;lt;T, ID&amp;gt; {}Resposiory 是 Spring Data 里面进行数据库操作顶级的抽象接口，里面什么方法都没有，但是如果任何接口继承它，就能得到一个 Repository，还可以实现 JPA 的一些默认实现方法。Spring 利用 Respository 作为 DAO 操作的 Type，以及利用 Java 动态代理机制就可以实现很多功能，比如为什么接口就能实现 DB 的相关操作？这就是 Spring 框架的高明之处。Spring 在做动态代理的时候，只要是它的子类或者实现类，再利用 T 类以及 T 类的 主键 ID 类型作为泛型的类型参数，就可以来标记出来、并捕获到要使用的实体类型，就能帮助使用者进行数据库操作。Repository 类层次关系用工具 Intellij Idea，打开类 Repository.class，然后依次【导航】 → 【Hierchy 类型】，会得到如下图所示的结果：通过该层次结构视图，就会明白基类 Repository 的用意，由此可知，存储库分为以下 4 个大类： ReactiveCrudRepository，这条线是响应式编程，主要支持当前 NoSQL 方面的操作，因为这方面大部分操作都是分布式的，所以由此可以看出 Spring Data 想统一数据操作的“野心”，即想提供关于所有 Data 方面的操作。目前 Reactive 主要有Cassandra、MongoDB、Redis 的实现 RxJava2CrudRepository，这条线是为了支持 RxJava 2 做的标准响应式编程的接口； CoroutineCrudRepository，这条继承关系链是为了支持 Kotlin 语法而实现的； CrudRepository， 这条继承关系链，是 JPA 相关的操作接口。Repository 继承关系图如下图所示：7 个大 Repository 接口： Repository(org.springframework.data.repository)，没有暴露任何方法； CrudRepository(org.springframework.data.repository)，简单的 Curd 方法； PagingAndSortingRepository(org.springframework.data.repository)，带分页和排序的方法； QueryByExampleExecutor(org.springframework.data.repository.query)，简单 Example 查询； JpaRepository(org.springframework.data.jpa.repository)，JPA 的扩展方法； JpaSpecificationExecutor(org.springframework.data.jpa.repository)，JpaSpecification 扩展查询； QueryDslPredicateExecutor(org.springframework.data.querydsl)，QueryDsl 的封装。两大 Repository 实现类： SimpleJpaRepository(org.springframework.data.jpa.repository.support)，JPA 所有接口的默认实现类； QueryDslJpaRepository(org.springframework.data.jpa.repository.support)，QueryDsl 的实现类。一个 Repository 的实例利用 UserRepository 继承 Repository 来实现对 User 的两个查询方法，如下：public interface UserRepository extends Repository&amp;lt;User, Long&amp;gt; { // 根据名称进行查询用户列表 List&amp;lt;User&amp;gt; findByName(String name); // 根据用户的邮箱和名称查询 List&amp;lt;User&amp;gt; findByEmailAndName(String email, String name);}CrudRepository 接口Repository 有了一定的掌握，它的直接子类 CurdRepository 接口提供了哪些方法，其中一部分如下： count(): long 查询总数返回 long 类型； void delete(T entity) 根据 entity 进行删除； void deleteAll(Iterable&amp;lt;? extends T&amp;gt; entities) 批量删除； void deleteAll() 删除所有；原理可以通过刚才的类关系查看，CrudRepository 的实现方法如下： //SimpleJpaRepository里面的deleteALL方法public void deleteAll() { for (T element : findAll()) { delete(element); }} 通过源码我们可以看出 SimpleJpaRepository 里面的 deleteAll 是利用 for 循环调用 delete 方法进行删除操作。CrudRepository 提供的方法： void deleteById(ID id)：根据主键删除，查看源码会发现，其是先查询出来再进行删除； boolean existsById(ID id)：根据主键判断实体是否存在； Iterable findAllById(Iterable ids); 根据主键列表查询实体列表； Iterable findAll(); 查询实体的所有列表； Optional findById(ID id); 根据主键查询实体，返回 JDK 1.8 的 Optional，这可以避免 null exception； S save(S entity); 保存实体方法，参数和返回结果可以是实体的子类； saveAll(Iterable entities) : 批量保存，原理和 save方法相同，我们去看实现的话，就是 for 循环调用上面的 save 方法。上面这些方法是 CrudRepository 对外暴露的常见的 Crud 接口，在对数据库进行 Crud 的时候就会运用到，如打算对 User 实体进行 Curd 操作，来看一下应该怎么写，如下所示：public interface UserRepository extends CrudRepository&amp;lt;User,Long&amp;gt; {}通过 UserRepository 继承 CrudRepository，这个时候的 UserRepository 就会有 CrudRepository 里面的所有方法。这里需要注意一下 save 和 deleteById 的实现逻辑，这两种方法是怎么实现方式如下：//新增或者保存public &amp;lt;S extends T&amp;gt; S save(S entity) { if (entityInformation.isNew(entity)) { em.persist(entity); return entity; } else { return em.merge(entity); }}//删除public void deleteById(ID id) { Assert.notNull(id, ID_MUST_NOT_BE_NULL); delete(findById(id).orElseThrow(() -&amp;gt; new EmptyResultDataAccessException(String.format(&quot;No %s entity with id %s exists!&quot;, entityInformation.getJavaType(), id), 1)));}在进行 Update、Delete、Insert 等操作之前。上面的源码，会通过 findById 先查询一下实体对象的 ID，然后再去对查询出来的实体对象进行保存操作。而如果在 Delete 的时候，查询到的对象不存在，则直接抛异常。这里特别强调了一下 Delete 和 Save 方法，是因为在实际工作中，看到有的同事画蛇添足：自己在做 Save 的时候先去 Find 一下，其实是没有必要的，Spring JPA 底层都考虑到了。当用任何第三方方法的时候，最好先查一下其源码和逻辑或者 API，然后再写出优雅的代码。关于 entityInformation.isNew（entity），如果当传递的参数里面没有 ID，则直接 insert；若当传递的参数里面有 ID，则会触发 select 查询。此方法会去看一下数据库里面是否存在此记录，若存在，则 update，否则 insert。PagingAndSortingRepository 接口PagingAndSortingRepository 接口，该接口也是 Repository 接口的子类，主要用于分页查询和排序查询。PagingAndSortingRepository 源码发现有两个方法，分别是用于分页和排序的时候使用的，如下所示：package org.springframework.data.repository;import org.springframework.data.domain.Page;import org.springframework.data.domain.Pageable;import org.springframework.data.domain.Sort;@NoRepositoryBeanpublic interface PagingAndSortingRepository&amp;lt;T, ID&amp;gt; extends CrudRepository&amp;lt;T, ID&amp;gt; { Iterable&amp;lt;T&amp;gt; findAll(Sort sort); （1） Page&amp;lt;T&amp;gt; findAll(Pageable pageable); （2）}其中： 第一个方法 findAll 参数是 Sort，是根据排序参数，实现不同的排序规则获取所有的对象的集合； 第二个方法 findAll 参数是 Pageable，是根据分页和排序进行查询，并用 Page 对返回结果进行封装。而 Pageable 对象包含 Page 和 Sort 对象。通过【Repository 继承关系图】和上的一大堆源码可以看到，PagingAndSortingRepository 继承了 CrudRepository，进而拥有了父类的方法，并且增加了分页和排序等对查询结果进行限制的通用的方法。PagingAndSortingRepository 和 CrudRepository 都是 Spring Data Common 的标准接口，那么实现类是什么呢？ 如果采用 JPA，那对应的实现类就是 Spring Data JPA 的 jar 包里面的 SimpleJpaRepository； 如果是其他 NoSQL的 实现如 MongoDB，那实现就在 Spring Data MongoDB 的 jar 里面的 MongoRepositoryImpl。PagingAndSortingRepository 使用案例第一步：定一个 UserRepository 类来继承 PagingAndSortingRepository 接口，实现对 User 的分页和排序操作，实现源码如下：package com.example.jpa.example1;import org.springframework.data.repository.PagingAndSortingRepository;public interface UserRepository extends PagingAndSortingRepository&amp;lt;User,Long&amp;gt; {}第二步：利用 UserRepository 直接继承 PagingAndSortingRepository 即可，而 Controller 里面就可以有如下用法了：/** * 验证排序和分页查询方法，Pageable的默认实现类：PageRequest * @return */@GetMapping(path = &quot;/page&quot;)@ResponseBodypublic Page&amp;lt;User&amp;gt; getAllUserByPage() { return userRepository.findAll(PageRequest.of(1, 20,Sort.by(new Sort.Order(Sort.Direction.ASC,&quot;name&quot;))));}/** * 排序查询方法，使用Sort对象 * @return */@GetMapping(path = &quot;/sort&quot;)@ResponseBodypublic Iterable&amp;lt;User&amp;gt; getAllUsersWithSort() { return userRepository.findAll(Sort.by(new Sort.Order(Sort.Direction.ASC,&quot;name&quot;)));}JpaRepository 接口上面的那些都是 Spring Data 为了兼容 NoSQL 而进行的一些抽象封装，而从 JpaRepository 开始是对关系型数据库进行抽象封装。从类图可以看出来它继承 PagingAndSortingRepository 类，也就继承了其所有方法，并且其实现类也是 SimpleJpaRepository。从类图上还可以看出 JpaRepository 继承和拥有了 QueryByExampleExecutor 的相关方法。JpaRepository 里面重点新增了批量删除，优化了批量删除的性能，类似于之前 SQL 的 batch 操作，并不是像上面的 deleteAll 来 for 循环删除。其中 flush() 和 saveAndFlush() 提供了手动刷新 session，把对象的值立即更新到数据库里面的机制。JPA 是 由 Hibernate 实现的，所以有 session 一级缓存的机制，当调用 save() 方法的时候，数据库里面是不会立即变化的。用 UserRepository 直接继承 JpaRepository，来实现 JPA 的相关方法，如下所示：public interface UserRepository extends JpaRepository&amp;lt;User,Long&amp;gt; {}这样 controller 里面就可以直接调用 JpaRepository 及其父接口里面的所有方法了。Repository 的实现类 SimpleJpaRepository关系数据库的所有 Repository 接口的实现类就是 SimpleJpaRepository，如果有些业务场景需要进行扩展了，可以继续继承此类，如 QueryDsl 的扩展（虽然不推荐使用了，但可以参考它的做法，自定义自己的 SimpleJpaRepository），如果能将此类里面的实现方法看透了，基本上 JPA 中的 API 就能掌握大部分内容。通过 Debug 视图看一下动态代理过程，会发现 UserRepository 的实现类是 Spring 启动的时候，利用 Java 动态代理机制帮我们生成的实现类，而真正的实现类就是 SimpleJpaRepository。通过上面【类的继承关系图】也可以知道 SimpleJpaRepository 是 Repository 接口、CrudRepository 接口、PagingAndSortingRepository 接口、JpaRepository 接口的实现。其中，SimpleJpaRepository 的部分源码如下：@Repository@Transactional(readOnly = true)public class SimpleJpaRepository&amp;lt;T, ID&amp;gt; implements JpaRepository&amp;lt;T, ID&amp;gt;, JpaSpecificationExecutor&amp;lt;T&amp;gt; { private static final String ID_MUST_NOT_BE_NULL = &quot;The given id must not be null!&quot;; private final JpaEntityInformation&amp;lt;T, ?&amp;gt; entityInformation; private final EntityManager em; private final PersistenceProvider provider; private @Nullable CrudMethodMetadata metadata; ...... @Transactional public void deleteAllInBatch() { em.createQuery(getDeleteAllQueryString()).executeUpdate(); } ......通过此类的源码，可以清晰地看出 SimpleJpaRepository 的实现机制，是通过 EntityManger 进行实体的操作，而 JpaEntityInforMation 里面存在实体的相关信息和 Crud 方法的元数据等。利用 Java 动态代理机制帮我们生成的实现类，那么关于动态代理的实现，可以在 RepositoryFactorySupport 设置一个断点，启动的时候，在断点处就会发现 UserRepository 的接口会被动态代理成 SimpleJapRepository 的实现。这里需要注意的是每一个 Repository 的子类，都会通过这里的动态代理生成实现类。Repository 接口给我的启发在接触了 Repository 的源码之后，在工作中遇到过一些类似需要抽象接口和写动态代理的情况，所以对于 Repository 的源码，我受到了一些启发：第一，上面的 7 个大 Repository 接口，在使用的时候可以根据实际场景，来继承不同的接口，从而选择暴露不同的 Spring Data Common 给我们提供的已有接口。这其实利用了 Java 语言的 interface 特性，在这里可以好好理解一下 interface 的妙用。第二，利用源码也可以很好地理解一下 Spring 中动态代理的作用，可以利用这种思想，在改善 MyBatis 的时候使用。" }, { "title": "Spring Data JPA - Defining Query Methods 的命名语法与参数", "url": "/posts/defining-query-methods/", "categories": "Java, Spring", "tags": "Spring Data JPA, Defining Query Methods", "date": "2020-09-04 15:33:00 +0000", "snippet": "Spring Data JPA 的最大特色是利用方法名定义查询方法（Defining Query Methods）来做 CRUD 操作。Spring Data JPA 的 Defining Query Methods（DQM）通过方法名和参数，可以很好地解决上面的问题，也能让方法名的语义更加清晰，开发效率也会提升很多。DQM 语法共有 2 种，可以实现上面的那些问题，具体如下： 一种是直接通过方法名就可以实现； 另一种是 @Query 手动在方法上定义。定义查询方法的配置和使用方法若想要实现 CRUD 的操作，常规做法是写一大堆 SQL 语句。但在 JPA 里面，只需要继承 Spring Data Common 里面的任意 Repository 接口或者子接口，然后直接通过方法名就可以实现，第 1 步，User 实体的 UserRepository 继承 Spring Data Common 里面的 Repository 接口：interface UserRepository extends CrudRepository&amp;lt;User, Long&amp;gt; { User findByEmailAddress(String emailAddress);}第 2 步，对于 Service 层就可以直接使用 UserRepository 接口：@Servicepublic class UserServiceImpl{ @Autowired UserRepository userRepository; public void testJpa() { userRepository.deleteAll(); userRepository.findAll(); userRepository.findByEmailAddress(String email); }}这个时候就可以直接调用 CrudRepository 里面暴露的所有接口方法，以及 UserRepository 里面定义的方法，不需要写任何 SQL 语句，也不需要写任何实现方法。通过上面的两步完成了 Defining Query Methods（DQM）的基本使用。另外一种情况：选择性暴露方法。然而，有时如果不想暴露 CrudRepository 里面的所有方法，那么可以直接继承我们认为需要暴露的那些方法的接口。假如 UserRepository 只想暴露 findOne 和 save，除了这两个方法之外不允许任何的 User 操作，其做法如下。选择性地暴露 CRUD 方法，直接继承 Repository（因为这里面没有任何方法），把 CrudRepository 里面的 save 和 findOne 方法复制到自己的 MyBaseRepository 接口即可，代码如下：@NoRepositoryBeaninterface MyBaseRepository&amp;lt;T, ID extends Serializable&amp;gt; extends Repository&amp;lt;T, ID&amp;gt; { T findOne(ID id); T save(T entity);}interface UserRepository extends MyBaseRepository&amp;lt;User, Long&amp;gt; { User findByEmailAddress(String emailAddress);}这样在 Service 层就只有 findOne、save、findByEmailAddress 这 3 个方法可以调用，不会有更多方法了，可以对 SimpleJpaRepository 里面任意已经实现的方法做选择性暴露。综上所述，得出以下 2 点结论： MyRepository Extends Repository 接口可以实现 Defining Query Methods 的功能； 继承其他 Repository 的子接口，或者自定义子接口，可以选择性地暴露 SimpleJpaRepository 里面已经实现的基础公用方法。在平时的工作中，可以通过方法名，或者定义方法名上面添加 @Query 注解两种方式来实现 CRUD 的目的。而 Spring 提供了两种切换方式。方法的查询策略设置目前在实际生产中还没有遇到要修改默认策略的情况，但必须要知道有这样的配置方法，做到心中有数，这样才能知道为什么方法名可以，@Query 也可以。通过 @EnableJpaRepositories 注解来配置方法的查询策略，详细配置方法如下：@EnableJpaRepositories(queryLookupStrategy= QueryLookupStrategy.Key.CREATE_IF_NOT_FOUND)其中，QueryLookupStrategy.Key 的值共 3 个，具体如下： Create：直接根据方法名进行创建，规则是根据方法名称的构造进行尝试，一般的方法是从方法名中删除给定的一组已知前缀，并解析该方法的其余部分。如果方法名不符合规则，启动的时候会报异常，这种情况可以理解为，即使配置了 @Query 也是没有用的； USE_DECLARED_QUERY：声明方式创建，启动的时候会尝试找到一个声明的查询，如果没有找到将抛出一个异常，可以理解为必须配置 @Query； CREATE_IF_NOT_FOUND：这个是默认的，除非有特殊需求，可以理解为这是以上 2 种方式的兼容版。先用声明方式（@Query）进行查找，如果没有找到与方法相匹配的查询，那用 Create 的方法名创建规则创建一个查询；这两者都不满足的情况下，启动就会报错。以 Spring Boot 项目为例，更改其配置方法如下：@EnableJpaRepositories(queryLookupStrategy= QueryLookupStrategy.Key.CREATE_IF_NOT_FOUND)public class Example1Application { public static void main(String[] args) { SpringApplication.run(Example1Application.class, args); }}以上就是方法的查询策略设置，很简单。Defining Query Method（DQM）语法该语法是：带查询功能的方法名由查询策略（关键字）+ 查询字段 + 一些限制性条件组成，具有语义清晰、功能完整的特性，实际工作中 80% 的 API 查询都可以简单实现。interface PersonRepository extends Repository&amp;lt;User, Long&amp;gt; { // and 的查询关系 List&amp;lt;User&amp;gt; findByEmailAddressAndLastname(EmailAddress emailAddress, String lastname); // 包含 distinct 去重，or 的 sql 语法 List&amp;lt;User&amp;gt; findDistinctPeopleByLastnameOrFirstname(String lastname, String firstname); // 根据 lastname 字段查询忽略大小写 List&amp;lt;User&amp;gt; findByLastnameIgnoreCase(String lastname); // 根据 lastname 和 firstname 查询 equal 并且忽略大小写 List&amp;lt;User&amp;gt; findByLastnameAndFirstnameAllIgnoreCase(String lastname, String firstname); // 对查询结果根据 lastname 排序，正序 List&amp;lt;User&amp;gt; findByLastnameOrderByFirstnameAsc(String lastname);  // 对查询结果根据 l List&amp;lt;User&amp;gt; findByLastnameOrderByFirstnameDesc(String lastname);}下面表格是一个在上面 DQM 方法语法里常用的关键字列表，方便快速查阅，并满足在实际代码中更加复杂的场景：| 关键字 | 案例 | JPQL表达 || —————– | ———————————————————— | ——————————————- || And | findByLastnameAndFirstname | …where x.lastname = ?1 and x.firstname = ?2 || Or | findByLastnameOrFirstname | ..where x.lastname = ?1 or x.firstname = ?2 || Is,Equals | findByFirstnamefindByFirstnamelsfindByFirstnameEquals | ..where x.firstname =?1 || Between | findByStartDateBetween | …where x.startDate between ?1 and ?2 || LessThan | findByAgeLessThan | …where x.age &amp;lt; ?1 || LessThanEqual | findByAgeLessThanEqual | …where x.age &amp;lt;= ?l || GreaterThan | findByAgeGreaterThan | …where x.age &amp;gt; ?1 || GreaterThanEqual | findByAgeGreaterThanEqual | …where x.age &amp;gt;= ?1 || After | findByStartDateAfter | ..where x.startDate &amp;gt; ?1 || Before | findByStartDateBefore | …where x.startDate &amp;lt; ?1 || IsNull | findByAgelsNull | …where x.age is null || IsNotNull,NotNull | findByAge(Is)NotNull | …where x.age not null || Like | findByFirstnameLike | ..where x.firstname like ?1 || NotLike | findByFirstnameNotLike | ..where x.firstname not like ?1 || StartingWith | findByFirstnameStartingWith | ..where x.firstname like ?1 (参数增加前缀%) || EndingWith | findByFirstnameEndingWith | ..where x.firstname like ?1 (参数增加后缀%) || Containing | findByFirstnameContaining | ..where x.firstname like ?1 (参数被%包裹) || OrderBy | findByAgeOrderByLastnameDesc | ..where x.age=?1 order by x.lastname desc || Not | findByLastnameNot | ..where x.lastname &amp;lt;&amp;gt; ?1 || In | findByAgeln(Collection ages) | …where x.age in ?l || NotIn | findByAgeNotln(Collection ages) | …where x.age not in ?l || True | findByActiveTrue() | …where x.active = true || False | findByActiveFalse() | ..where x.active = false || IgnoreCase | findByFirstnamelgnoreCase | ..where UPPER(x.firstame) = UPPER(?1) |综上，总结 3 点经验： 方法名的表达式通常是实体属性连接运算符的组合，如 And、or、Between、LessThan、GreaterThan、Like 等属性连接运算表达式，不同的数据库（NoSQL、MySQL）可能产生的效果不一样，如果遇到问题，打开 SQL 日志观察即可； IgnoreCase 可以针对单个属性（如 findByLastnameIgnoreCase(…)），也可以针对查询条件里面所有的实体属性忽略大小写（所有属性必须在 String 情况下，如 findByLastnameAndFirstnameAllIgnoreCase(…)) OrderBy 可以在某些属性的排序上提供方向（Asc 或 Desc），称为静态排序，也可以通过一个方便的参数 Sort 实现指定字段的动态排序的查询方法（如 repository.findAll(Sort.by(Sort.Direction.ASC, &quot;myField&quot;))）。上面的表格虽然大多是 find 开头的方法，除此之外，JPA 还支持 read、get、query、stream、count、exists、delete、remove 等前缀，如字面意思一样。来看看 count、delete、remove 的例子，其他前缀可以举一反三。实例代码如下：interface UserRepository extends CrudRepository&amp;lt;User, Long&amp;gt; { // 查询总数 long countByLastname(String lastname); // 根据一个字段进行删除操作，并返回删除行数 long deleteByLastname(String lastname); // 根据 Lastname 删除一堆 User,并返回删除的 User List&amp;lt;User&amp;gt; removeByLastname(String lastname);}有的时候随着版本的更新，也会有更多的语法支持，或者不同的版本语法可能也不一样，通过源码来看一下上面说的几种语法。可以到类 org.springframework.data.repository.query.parser.PartTree 查看相关源码的逻辑和处理方法。根据源码可以分析出来，query method 包含其他的表达式，比如 find、count、delete、exist 等关键字在 by 之前通过正则表达式匹配。由此可知，方法中的关键字不是乱填的，是枚举定义好的。接下来打开枚举类 Type 源码看下，比什么都清楚。public static enum Type { BETWEEN(2, new String[]{&quot;IsBetween&quot;, &quot;Between&quot;}), IS_NOT_NULL(0, new String[]{&quot;IsNotNull&quot;, &quot;NotNull&quot;}), IS_NULL(0, new String[]{&quot;IsNull&quot;, &quot;Null&quot;}), LESS_THAN(new String[]{&quot;IsLessThan&quot;, &quot;LessThan&quot;}), LESS_THAN_EQUAL(new String[]{&quot;IsLessThanEqual&quot;, &quot;LessThanEqual&quot;}), GREATER_THAN(new String[]{&quot;IsGreaterThan&quot;, &quot;GreaterThan&quot;}), GREATER_THAN_EQUAL(new String[]{&quot;IsGreaterThanEqual&quot;, &quot;GreaterThanEqual&quot;}), BEFORE(new String[]{&quot;IsBefore&quot;, &quot;Before&quot;}), AFTER(new String[]{&quot;IsAfter&quot;, &quot;After&quot;}), NOT_LIKE(new String[]{&quot;IsNotLike&quot;, &quot;NotLike&quot;}), LIKE(new String[]{&quot;IsLike&quot;, &quot;Like&quot;}), STARTING_WITH(new String[]{&quot;IsStartingWith&quot;, &quot;StartingWith&quot;, &quot;StartsWith&quot;}), ENDING_WITH(new String[]{&quot;IsEndingWith&quot;, &quot;EndingWith&quot;, &quot;EndsWith&quot;}), IS_NOT_EMPTY(0, new String[]{&quot;IsNotEmpty&quot;, &quot;NotEmpty&quot;}), IS_EMPTY(0, new String[]{&quot;IsEmpty&quot;, &quot;Empty&quot;}), NOT_CONTAINING(new String[]{&quot;IsNotContaining&quot;, &quot;NotContaining&quot;, &quot;NotContains&quot;}), CONTAINING(new String[]{&quot;IsContaining&quot;, &quot;Containing&quot;, &quot;Contains&quot;}), NOT_IN(new String[]{&quot;IsNotIn&quot;, &quot;NotIn&quot;}), IN(new String[]{&quot;IsIn&quot;, &quot;In&quot;}), NEAR(new String[]{&quot;IsNear&quot;, &quot;Near&quot;}), WITHIN(new String[]{&quot;IsWithin&quot;, &quot;Within&quot;}), REGEX(new String[]{&quot;MatchesRegex&quot;, &quot;Matches&quot;, &quot;Regex&quot;}), EXISTS(0, new String[]{&quot;Exists&quot;}), TRUE(0, new String[]{&quot;IsTrue&quot;, &quot;True&quot;}), FALSE(0, new String[]{&quot;IsFalse&quot;, &quot;False&quot;}), NEGATING_SIMPLE_PROPERTY(new String[]{&quot;IsNot&quot;, &quot;Not&quot;}), SIMPLE_PROPERTY(new String[]{&quot;Is&quot;, &quot;Equals&quot;}); ....}看源码就可以知道框架支持了哪些逻辑关键字，比如 NotIn、Like、In、Exists 等，有的时候比查文档和任何人写的博客都准确、还快。特定类型的参数：Sort 排序和 Pageable 分页Spring Data JPA 为了方便我们排序和分页，支持了两个特殊类型的参数：Sort 和 Pageable。Sort 在查询的时候可以实现动态排序，其源码：public Sort(Direction direction, String... properties) { this(direction, properties == null ? new ArrayList&amp;lt;&amp;gt;() : Arrays.asList(properties));}Sort 里面决定了哪些字段的排序方向（ASC 正序、DESC 倒序）。Pageable 在查询的时候可以实现分页效果和动态排序双重效果。Pageable 是一个接口，里面有常见的分页方法排序、当前页、下一行、当前指针、一共多少页、页码、**pageSize **等。下面代码定义了根据 Lastname 查询 User 的分页和排序的实例，此段代码是在 UserRepository 接口里面定义的方法：// 根据分页参数查询User，返回一个带分页结果的Page(下一课时详解)对象（方法一）Page&amp;lt;User&amp;gt; findByLastname(String lastname, Pageable pageable);// 根据分页参数返回一个Slice的user结果（方法二）Slice&amp;lt;User&amp;gt; findByLastname(String lastname, Pageable pageable);// 根据排序结果返回一个List（方法三）List&amp;lt;User&amp;gt; findByLastname(String lastname, Sort sort);// 根据分页参数返回一个List对象（方法四）List&amp;lt;User&amp;gt; findByLastname(String lastname, Pageable pageable); 方法一：允许将 org.springframework.data.domain.Pageable 实例传递给查询方法，将分页参数添加到静态定义的查询中，通过 Page 返回的结果得知可用的元素和页面的总数。这种分页查询方法可能是昂贵的（会默认执行一条 count 的 SQL 语句），所以用的时候要考虑一下使用场景； 方法二：返回结果是 Slice，因为只知道是否有下一个 Slice 可用，而不知道 count，所以当查询较大的结果集时，只知道数据是足够的，也就是说用在业务场景中时不用关心一共有多少页； 方法三：如果只需要排序，需在 org.springframework.data.domain.Sort 参数中添加一个参数，正如上面看到的，只需返回一个 List 也是有可能的； 方法四：排序选项也通过 Pageable 实例处理，在这种情况下，Page 将不会创建构建实际实例所需的附加元数据（即不需要计算和查询分页相关数据），而仅仅用来做限制查询给定范围的实体。由此可知，可以通过 PageRequest 里面提供的几个 of 静态方法（多态），分别构建页码、页面大小、排序等。我们来看下，在使用中的写法，如下所示：// 查询user里面的 lastname=jk 的第一页，每页大小是 20 条；并会返回一共有多少页的信息Page&amp;lt;User&amp;gt; users = userRepository.findByLastname(&quot;jk&quot;,PageRequest.of(1, 20));// 查询user里面的 lastname=jk 的第一页的 20 条数据，不知道一共多少条Slice&amp;lt;User&amp;gt; users = userRepository.findByLastname(&quot;jk&quot;,PageRequest.of(1, 20));// 查询出来所有的 user 里面的lastname=jk的User数据，并按照name正序返回ListList&amp;lt;User&amp;gt; users = userRepository.findByLastname(&quot;jk&quot;,new Sort(Sort.Direction.ASC, &quot;name&quot;))// 按照 createdAt 倒序，查询前一百条 User 数据List&amp;lt;User&amp;gt; users = userRepository.findByLastname(&quot;jk&quot;,PageRequest.of(0, 100, Sort.Direction.DESC, &quot;createdAt&quot;));上面讲解了分页和排序的应用场景，在实际工作中，如果遇到不知道参数怎么传递的情况，可以看一下源码，因为 Java 是类型安全的。限制查询结果 First 和 Top这是分页的另一种表达方式。有的时候想直接查询前几条数据，也不需要动态排序，那么就可以简单地在方法名字中使用 First 和 Top 关键字，来限制返回条数。userRepository 里面可以定义的一些限制返回结果的使用。在查询方法上加限制查询结果的关键字 First 和 Top。User findFirstByOrderByLastnameAsc();User findTopByOrderByAgeDesc();List&amp;lt;User&amp;gt; findDistinctUserTop3ByLastname(String lastname, Pageable pageable);List&amp;lt;User&amp;gt; findFirst10ByLastname(String lastname, Sort sort);List&amp;lt;User&amp;gt; findTop10ByLastname(String lastname, Pageable pageable);其中： 查询方法在使用 First 或 Top 时，数值可以追加到 First 或 Top 后面，指定返回最大结果的大小； 如果数字被省略，则假设结果大小为 1； 限制表达式也支持 Distinct 关键字； 支持将结果包装到 Optional 中； 如果将 Pageable 作为参数，以 Top 和 First 后面的数字为准，即分页将在限制结果中应用。First 和 Top 关键字的使用非常简单，可以让方法名语义更加清晰。@NonNull、@NonNullApi、@Nullable从 Spring Data 2.0 开始，JPA 新增了@NonNull，@NonNullApi，@Nullable，是对 null 的参数和返回结果做的支持。 @NonNullApi：在包级别用于声明参数，以及返回值的默认行为是不接受或产生空值的； @NonNull：用于不能为空的参数或返回值（在 @NonNullApi 适用的参数和返回值上不需要）； @Nullable：用于可以为空的参数或返回值。在自己的 Repository 所在 package 的 package-info.java 类里面做如下声明：@org.springframework.lang.NonNullApipackage com.myrespository;myrespository 下面的 UserRepository 实现如下：package com.myrespository;import org.springframework.lang.Nullable;interface UserRepository extends Repository&amp;lt;User, Long&amp;gt; { User getByEmailAddress(EmailAddress emailAddress); }这个时候当 emailAddress 参数为 null 的时候就会抛异常，当返回结果为 null 的时候也会抛异常。因为在 package 的 package-info.java里面指定了 NonNullApi，所有返回结果和参数不能为 Null。// 当添加@Nullable 注解之后，参数和返回结果这个时候就都会允许为 null 了；@NullableUser findByEmailAddress(@Nullable EmailAddress emailAdress);// 返回结果允许为 null,参数不允许为 null 的情况Optional&amp;lt;User&amp;gt; findOptionalByEmailAddress(EmailAddress emailAddress); 学习了 Defining Query Methods 的语法和其所表达的命名规范，在实际工作中，也可以将方法名（非常语义化的 respository 里面所定义方法命名规范）的强制约定规范运用到 controller 和 service 层，这样全部统一后，可以减少很多的沟通成本。Spring Data Common 里面的 repository 基类，否可以应用推广到 service 层呢？能否也建立一个自己的 baseService？来看下面的实战例子：public interface BaseService&amp;lt;T, ID&amp;gt; { Class&amp;lt;T&amp;gt; getDomainClass(); &amp;lt;S extends T&amp;gt; S save(S entity); &amp;lt;S extends T&amp;gt; List&amp;lt;S&amp;gt; saveAll(Iterable&amp;lt;S&amp;gt; entities); void delete(T entity); void deleteById(ID id); void deleteAll(); void deleteAll(Iterable&amp;lt;? extends T&amp;gt; entities); void deleteInBatch(Iterable&amp;lt;T&amp;gt; entities); void deleteAllInBatch(); T getOne(ID id); &amp;lt;S extends T&amp;gt; Optional&amp;lt;S&amp;gt; findOne(Example&amp;lt;S&amp;gt; example); Optional&amp;lt;T&amp;gt; findById(ID id); List&amp;lt;T&amp;gt; findAll(); List&amp;lt;T&amp;gt; findAll(Sort sort); Page&amp;lt;T&amp;gt; findAll(Pageable pageable); &amp;lt;S extends T&amp;gt; List&amp;lt;S&amp;gt; findAll(Example&amp;lt;S&amp;gt; example); &amp;lt;S extends T&amp;gt; List&amp;lt;S&amp;gt; findAll(Example&amp;lt;S&amp;gt; example, Sort sort); &amp;lt;S extends T&amp;gt; Page&amp;lt;S&amp;gt; findAll(Example&amp;lt;S&amp;gt; example, Pageable pageable); List&amp;lt;T&amp;gt; findAllById(Iterable&amp;lt;ID&amp;gt; ids); long count(); &amp;lt;S extends T&amp;gt; long count(Example&amp;lt;S&amp;gt; example); &amp;lt;S extends T&amp;gt; boolean exists(Example&amp;lt;S&amp;gt; example); boolean existsById(ID id); void flush(); &amp;lt;S extends T&amp;gt; S saveAndFlush(S entity);}模仿 JpaRepository 接口也自定义了一个自己的BaseService，声明了常用的CRUD操作，上面的代码是生产代码，可以作为参考。当然了也可以建立自己的 PagingAndSortingService、ComplexityService、SampleService 等来划分不同的 service接口，供不同目的 Service 子类继承。再来模仿一个 SimpleJpaRepository，来实现自己的 BaseService 的实现类。public class BaseServiceImpl&amp;lt;T, ID, R extends JpaRepository&amp;lt;T, ID&amp;gt;&amp;gt; implements BaseService&amp;lt;T, ID&amp;gt; { private static final Map&amp;lt;Class, Class&amp;gt; DOMAIN_CLASS_CACHE = new ConcurrentHashMap&amp;lt;&amp;gt;(); private final R repository; public BaseServiceImpl(R repository) { this.repository = repository; } @Override public Class&amp;lt;T&amp;gt; getDomainClass() { Class thisClass = getClass(); Class&amp;lt;T&amp;gt; domainClass = DOMAIN_CLASS_CACHE.get(thisClass); if (Objects.isNull(domainClass)) { domainClass = GenericsUtils.getGenericClass(thisClass, 0); DOMAIN_CLASS_CACHE.putIfAbsent(thisClass, domainClass); } return domainClass; } protected R getRepository() { return repository; } @Override public &amp;lt;S extends T&amp;gt; S save(S entity) { return repository.save(entity); } @Override public &amp;lt;S extends T&amp;gt; List&amp;lt;S&amp;gt; saveAll(Iterable&amp;lt;S&amp;gt; entities) { return repository.saveAll(entities); } @Override public void delete(T entity) { repository.delete(entity); } @Override public void deleteById(ID id) { repository.deleteById(id); } @Override public void deleteAll() { repository.deleteAll(); } @Override public void deleteAll(Iterable&amp;lt;? extends T&amp;gt; entities) { repository.deleteAll(entities); } @Override public void deleteInBatch(Iterable&amp;lt;T&amp;gt; entities) { repository.deleteInBatch(entities); } @Override public void deleteAllInBatch() { repository.deleteAllInBatch(); } @Override public T getOne(ID id) { return repository.getOne(id); } @Override public &amp;lt;S extends T&amp;gt; Optional&amp;lt;S&amp;gt; findOne(Example&amp;lt;S&amp;gt; example) { return repository.findOne(example); } @Override public Optional&amp;lt;T&amp;gt; findById(ID id) { return repository.findById(id); } @Override public List&amp;lt;T&amp;gt; findAll() { return repository.findAll(); } @Override public List&amp;lt;T&amp;gt; findAll(Sort sort) { return repository.findAll(sort); } @Override public Page&amp;lt;T&amp;gt; findAll(Pageable pageable) { return repository.findAll(pageable); } @Override public &amp;lt;S extends T&amp;gt; List&amp;lt;S&amp;gt; findAll(Example&amp;lt;S&amp;gt; example) { return repository.findAll(example); } @Override public &amp;lt;S extends T&amp;gt; List&amp;lt;S&amp;gt; findAll(Example&amp;lt;S&amp;gt; example, Sort sort) { return repository.findAll(example, sort); } @Override public &amp;lt;S extends T&amp;gt; Page&amp;lt;S&amp;gt; findAll(Example&amp;lt;S&amp;gt; example, Pageable pageable) { return repository.findAll(example, pageable); } @Override public List&amp;lt;T&amp;gt; findAllById(Iterable&amp;lt;ID&amp;gt; ids) { return repository.findAllById(ids); } @Override public long count() { return repository.count(); } @Override public &amp;lt;S extends T&amp;gt; long count(Example&amp;lt;S&amp;gt; example) { return repository.count(example); } @Override public &amp;lt;S extends T&amp;gt; boolean exists(Example&amp;lt;S&amp;gt; example) { return repository.exists(example); } @Override public boolean existsById(ID id) { return repository.existsById(id); } @Override public void flush() { repository.flush(); } @Override public &amp;lt;S extends T&amp;gt; S saveAndFlush(S entity) { return repository.saveAndFlush(entity); }}以上代码就是 BaseService 常用的 CURD 实现代码，这里面大部分也是直接调用 Repository 提供的方法。需要注意的是，当继承 BaseServiceImpl 的时候需要传递自己的 Repository，如下面实例代码：@Servicepublic class UserServiceImpl extends BaseServiceImpl&amp;lt;User, Long, UserRepository&amp;gt; implements UserService { public UserServiceImpl(UserRepository repository) { super(repository); } .....} 如何返回自定义 DTO 而不是 Entity !" }, { "title": "Spring Data JPA - @Query", "url": "/posts/query-annotation/", "categories": "Java, Spring", "tags": "Spring Security, React", "date": "2020-09-03 15:33:00 +0000", "snippet": "新增一个 @Query 的方法，快速体验一下 @Query 的使用方法，如下所示：package com.example.jpa.example1;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.Query;import org.springframework.data.repository.query.Param;public interface UserDtoRepository extends JpaRepository&amp;lt;User,Long&amp;gt; { //通过query注解根据name查询user信息 @Query(&quot;From User where name=:name&quot;) User findByQuery(@Param(&quot;name&quot;) String nameParam);}然后，新增一个测试类：package com.example.jpa.example1;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;@DataJpaTestpublic class UserRepositoryQueryTest { @Autowired private UserDtoRepository userDtoRepository; @Test public void testQueryAnnotation() { // 新增一条数据方便测试 // userDtoRepository.save(User.builder().name(&quot;jackxx&quot;).email(&quot;123456@126.com&quot;).sex(&quot;man&quot;).address(&quot;shanghai&quot;).build()); //调用上面的方法查看结果 User user2 = userDtoRepository.findByQuery(&quot;jack&quot;); System.out.println(user2); }}运行的结果如下：Hibernate: insert into user (address, email, name, sex, version, id) values (?, ?, ?, ?, ?, ?)Hibernate: select user0_.id as id1_0_, user0_.address as address2_0_, user0_.email as email3_0_, user0_.name as name4_0_, user0_.sex as sex5_0_, user0_.version as version6_0_ from user user0_ where user0_.name=?User(id=1, name=jack, email=123456@126.com, version=0, sex=man, address=shanghai)通过上面发现，这次不是通过方法名来生成查询语法，而是@Query 注解在其中起了作用，使 From User where name=:name&quot;JPQL 生效了。Query 的基本用法@Query 用法是使用 JPQL 为实体创建声明式查询方法。注解源码:@Retention(RetentionPolicy.RUNTIME)@Target({ElementType.METHOD, ElementType.ANNOTATION_TYPE})@QueryAnnotation@Documentedpublic @interface Query { /** * 指定JPQL的查询语句。（nativeQuery=true的时候，是原生的Sql语句） */ String value() default &quot; /** * 指定count的JPQL语句，如果不指定将根据query自动生成。（如果当nativeQuery=true的时候，指的是原生 * 的Sql语句） */ String countQuery() default &quot; /** * 根据哪个字段来count，一般默认即可。 */ String countProjection() default &quot; /** * 默认是false，表示value里面是不是原生的sql语句 */ boolean nativeQuery() default false /** * 可以指定一个query的名字，必须唯一的。 * 如果不指定，默认的生成规则是：{$domainClass}.${queryMethodName} */ String name() default &quot; /** * 可以指定一个count的query的名字，必须唯一的。 * 如果不指定，默认的生成规则是：{$domainClass}.${queryMethodName}.count */ String countName() default &quot;&quot;;}一般只需要关心 @Query 里面的 value 和 nativeQuery、countQuery 的值即可，因为其他的不常用。使用声明式 JPQL 查询有个好处，就是启动的时候就知道语法正确不正确。JPQL 语法。SELECT ... FROM ...[WHERE ...][GROUP BY ... [HAVING ...]][ORDER BY ...]它的语法结构类似 SQL，唯一的区别就是 JPQL FROM 后面跟的是对象，而 SQL 里面的字段对应的是对象里面的属性字段。update 和 delete 的语法结构：DELETE FROM ... [WHERE ...]UPDATE ... SET ... [WHERE ...]其中 “…” 省略的部分是实体对象名字和实体对象里面的字段名字。@Query 用法案例案例 1： 要在 Repository 的查询方法上声明一个注解，这里就是 @Query 注解标注的地方。public interface UserRepository extends JpaRepository&amp;lt;User, Long&amp;gt;{ @Query(&quot;select u from User u where u.emailAddress = ?1&quot;) User findByEmailAddress(String emailAddress);}案例 2： LIKE 查询，注意 firstname 不会自动加上“%”关键字。public interface UserRepository extends JpaRepository&amp;lt;User, Long&amp;gt; { @Query(&quot;select u from User u where u.firstname like %?1&quot;) List&amp;lt;User&amp;gt; findByFirstnameEndsWith(String firstname);}案例 3： 直接用原始 SQL，nativeQuery = true 即可。public interface UserRepository extends JpaRepository&amp;lt;User, Long&amp;gt; { @Query(value = &quot;SELECT * FROM USERS WHERE EMAIL_ADDRESS = ?1&quot;, nativeQuery = true) User findByEmailAddress(String emailAddress);}案例 4： 下面是**nativeQuery 的排序错误的写法，会导致无法启动。public interface UserRepository extends JpaRepository&amp;lt;User, Long&amp;gt; { @Query(value = &quot;select * from user_info where first_name=?1&quot;,nativeQuery = true List&amp;lt;UserInfoEntity&amp;gt; findByFirstName(String firstName,Sort sort);}案例 5： nativeQuery 排序的正确写法。@Query(value = &quot;select * from user_info where first_name=?1 order by ?2&quot;,nativeQuery = true)List&amp;lt;UserInfoEntity&amp;gt; findByFirstName(String firstName,String sort);//调用的地方写法last_name是数据里面的字段名，不是对象的字段名repository.findByFirstName(&quot;jackzhang&quot;,&quot;last_name&quot;);通过上面几个案例，看到了 @Query 的几种用法，就会明白排序、参数、使用方法、LIKE、原始 SQL 怎么写。@Query 的排序@Query中在用JPQL的时候，想要实现排序，方法上直接用 PageRequest 或者 Sort 参数都可以做到。在排序实例中，实际使用的属性需要与实体模型里面的字段相匹配，这意味着它们需要解析为查询中使用的属性或别名。我们看一下例子，这是一个state_field_path_expression JPQL的定义，并且 Sort 的对象支持一些特定的函数。案例 6： Sort and JpaSort 的使用，它可以进行排序。public interface UserRepository extends JpaRepository&amp;lt;User, Long&amp;gt; { @Query(&quot;select u from User u where u.lastname like ?1%&quot;) List&amp;lt;User&amp;gt; findByAndSort(String lastname, Sort sort); @Query(&quot;select u.id, LENGTH(u.firstname) as fn_len from User u where u.lastname like ?1%&quot;) List&amp;lt;Object[]&amp;gt; findByAsArrayAndSort(String lastname, Sort sort);}//调用方的写法，如下：repo.findByAndSort(&quot;lannister&quot;, new Sort(&quot;firstname&quot;));repo.findByAndSort(&quot;stark&quot;, new Sort(&quot;LENGTH(firstname)&quot;));repo.findByAndSort(&quot;targaryen&quot;, JpaSort.unsafe(&quot;LENGTH(firstname)&quot;));repo.findByAsArrayAndSort(&quot;bolton&quot;, new Sort(&quot;fn_len&quot;));@Query 的分页@Query 的分页分为两种情况，分别为： JPQL 的排序 nativeQuery 的排序。案例 7：直接用 Page 对象接受接口，参数直接用 Pageable 的实现类即可。public interface UserRepository extends JpaRepository&amp;lt;User, Long&amp;gt; { @Query(value = &quot;select u from User u where u.lastname = ?1&quot;) Page&amp;lt;User&amp;gt; findByLastname(String lastname, Pageable pageable);}// 调用者的写法repository.findByFirstName(&quot;jackzhang&quot;,new PageRequest(1,10));案例 8：@Query 对原生 SQL 的分页支持，并不是特别友好，因为这种写法比较“骇客”，可能随着版本的不同会有所变化。以 MySQL 为例。public interface UserRepository extends JpaRepository&amp;lt;UserInfoEntity, Integer&amp;gt;, JpaSpecificationExecutor&amp;lt;UserInfoEntity&amp;gt; { @Query(value = &quot;select * from user_info where first_name=?1 /* #pageable# */&quot;, countQuery = &quot;select count(*) from user_info where first_name=?1&quot;, nativeQuery = true) Page&amp;lt;UserInfoEntity&amp;gt; findByFirstName(String firstName, Pageable pageable);}//调用者的写法return userRepository.findByFirstName(&quot;jackzhang&quot;,new PageRequest(1,10, Sort.Direction.DESC,&quot;last_name&quot;));//打印出来的sqlselect * from user_info where first_name=? /* #pageable# */ order by last_name desc limit ?, ?这里需要注意：这个注释 / #pageable# / 必须有。另外，随着版本的变化，这个方法有可能会进行优化。此外还有一种实现方法，就是自己写两个查询方法，自己手动分页。@Param 用法@Param 注解指定方法参数的具体名称，通过绑定的参数名字指定查询条件，这样不需要关心参数的顺序。比较推荐这种做法，因为它比较利于代码重构。如果不用 @Param 也是可以的，参数是有序的，这使得查询方法对参数位置的重构容易出错。案例 9：根据 firstname 和 lastname 参数查询 user 对象public interface UserRepository extends JpaRepository&amp;lt;User, Long&amp;gt; { @Query(&quot;select u from User u where u.firstname = :firstname or u.lastname = :lastname&quot;) User findByLastnameOrFirstname(@Param(&quot;lastname&quot;) String lastname, @Param(&quot;firstname&quot;) String firstname);}案例 10： 根据参数进行查询，top 10 前面说的“query method”关键字照样有用，如下所示：public interface UserRepository extends JpaRepository&amp;lt;User, Long&amp;gt; { @Query(&quot;select u from User u where u.firstname = :firstname or u.lastname = :lastname&quot;) User findTop10ByLastnameOrFirstname(@Param(&quot;lastname&quot;) String lastname, @Param(&quot;firstname&quot;) String firstname);}在通过 @Query 定义自己的查询方法时，建议用 Spring Data JPA 的 name query 的命名方法，这样下来风格就比较统一了。@Query 之 Projections 应用返回指定 DTO在之前的例子的基础上新增一张表 UserExtend，里面包含身份证、学号、年龄等信息，最终实体变成如下模样：@Entity@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class UserExtend { //用户扩展信息表 @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private Long userId; private String idCard; private Integer ages; private String studentNumber;}@Entity@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class User { //用户基本信息表 @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String name; private String email; @Version private Long version; private String sex; private String address;}如果想定义一个 DTO 对象，里面只要 name、email、idCard，这种场景非常常见，但好多人使用的都不是最佳实践，这里介绍几种方式做一下对比。先看一下，刚学 JPA 的时候别手别脚的写法：public interface UserDtoRepository extends JpaRepository&amp;lt;User,Long&amp;gt; { /** * 查询用户表里面的name、email和UserExtend表里面的idCard * @param id * @return */ @Query(&quot;select u.name,u.email,e.idCard from User u,UserExtend e where u.id= e.userId and u.id=:id&quot;) List&amp;lt;Object[]&amp;gt; findByUserId(@Param(&quot;id&quot;) Long id);}通过下面的测试用例来取上面 findByUserId 方法返回的数据组结果值，再塞到 DTO 里面，代码如下：@Testpublic void testQueryAnnotation() { // 新增一条用户数据 userDtoRepository.save(User.builder().name(&quot;jack&quot;).email(&quot;123456@126.com&quot;).sex(&quot;man&quot;).address(&quot;shanghai&quot;).build()); //再新增一条和用户一对一的UserExtend数据 userExtendRepository.save(UserExtend.builder().userId(1L).idCard(&quot;shengfengzhenghao&quot;).ages(18).studentNumber(&quot;xuehao001&quot;).build()); // 查询想要的结果 List&amp;lt;Object[]&amp;gt; userArray = userDtoRepository.findByUserId(1L); System.out.println(String.valueOf(userArray.get(0)[0])+String.valueOf(userArray.get(0)[1])); UserDto userDto = UserDto.builder().name(String.valueOf(userArray.get(0)[0])).build(); System.out.println(userDto);}其实经验的丰富的老司机一看就知道这肯定不是最佳实践，这多麻烦呀，肯定会有更优解。那么再对此稍加改造，用 UserDto 接收返回结果。利用 class UserDto 获取想要的结果新建一个 UserDto 类的内容。package com.example.jpa.example1;import lombok.AllArgsConstructor;import lombok.Builder;import lombok.Data;@Data@Builder@AllArgsConstructorpublic class UserDto { private String name,email,idCard;}利用 @Query 在 Repository 里面怎么写。public interface UserDtoRepository extends JpaRepository&amp;lt;User, Long&amp;gt; { @Query(&quot;select new com.example.jpa.example1.UserDto(CONCAT(u.name,&#39;JK123&#39;),u.email,e.idCard) from User u,UserExtend e where u.id= e.userId and u.id=:id&quot;) UserDto findByUserDtoId(@Param(&quot;id&quot;) Long id);}利用 JPQL，new 了一个 UserDto；再通过构造方法，接收查询结果。其中会发现，用 CONCAT 的关键字做了一个字符串拼接，可以查看 JPQL 的 Oracal 文档，也可以通过源码来看支持的关键字有哪些。首先，打开 ParameterizedFunctionExpression 会发现 Hibernate 支持的关键字有这么多，都是 MySQL 数据库的查询关键字。写一个测试方法，调用上面的方法测试一下。@Testpublic void testQueryAnnotationDto() { userDtoRepository.save(User.builder().name(&quot;jack&quot;).email(&quot;123456@126.com&quot;).sex(&quot;man&quot;).address(&quot;shanghai&quot;).build());userExtendRepository.save(UserExtend.builder().userId(1L).idCard(&quot;shengfengzhenghao&quot;).ages(18).studentNumber(&quot;xuehao001&quot;).build()); UserDto userDto = userDtoRepository.findByUserDtoId(1L); System.out.println(userDto);}最后，运行一下测试用例，结果如下。这时会发现，按照预期操作得到了 UserDto 的结果。.Hibernate: insert into user (address, email, name, sex, version, id) values (?, ?, ?, ?, ?, ?)Hibernate: insert into user_extend (ages, id_card, student_number, user_id, id) values (?, ?, ?, ?, ?)Hibernate: select (user0_.name||&#39;JK123&#39;) as col_0_0_, user0_.email as col_1_0_, userextend1_.id_card as col_2_0_ from user user0_ cross join user_extend userextend1_ where user0_.id=userextend1_.user_id and user0_.id=?UserDto(name=jackJK123, email=123456@126.com, idCard=shengfengzhenghao)利用 UserDto 接口获得想要的结果新增一个 UserSimpleDto 接口来得到想要的 name、email、idCard 信息。package com.example.jpa.example1;public interface UserSimpleDto { String getName(); String getEmail(); String getIdCard();}其次，在 UserDtoRepository 里面新增一个方法，返回结果是 UserSimpleDto 接口。public interface UserDtoRepository extends JpaRepository&amp;lt;User, Long&amp;gt; { //利用接口DTO获得返回结果，需要注意的是每个字段需要as和接口里面的get方法名字保持一样 @Query(&quot;select CONCAT(u.name,&#39;JK123&#39;) as name,UPPER(u.email) as email ,e.idCard as idCard from User u,UserExtend e where u.id= e.userId and u.id=:id&quot;) UserSimpleDto findByUserSimpleDtoId(@Param(&quot;id&quot;) Long id);}然后，测试用例写法如下。@Testpublic void testQueryAnnotationDto() { userDtoRepository.save(User.builder().name(&quot;jack&quot;).email(&quot;123456@126.com&quot;).sex(&quot;man&quot;).address(&quot;shanghai&quot;).build());userExtendRepository.save(UserExtend.builder().userId(1L).idCard(&quot;shengfengzhenghao&quot;).ages(18).studentNumber(&quot;xuehao001&quot;).build()); UserSimpleDto userDto = userDtoRepository.findByUserSimpleDtoId(1L); System.out.println(userDto); System.out.println(userDto.getName()+&quot;:&quot;+userDto.getEmail()+&quot;:&quot;+userDto.getIdCard());}最后，执行可以得到如下结果。org.springframework.data.jpa.repository.query.AbstractJpaQuery$TupleConverter$TupleBackedMap@373c28e5jackJK123:123456@126.COM:shengfengzhenghao比起 DTO 不需要 new 了，并且接口只能读，那么我们返回的结果 DTO 的职责就更单一了，只用来查询。接口的方式是比较推荐的做法，因为它是只读的，对构造方法没有要求，返回的实际是 HashMap。@Query 动态查询解决方法首先，新增一个 UserOnlyName 接口，只查询 User 里面的 name 和 email 字段。package com.example.jpa.example1;// 获得返回结果public interface UserOnlyName { String getName(); String getEmail();}其次，在 UserDtoRepository 里面新增两个方法：一个是利用 JPQL 实现动态查询，一个是利用原始 SQL 实现动态查询。package com.example.jpa.example1;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.Query;import org.springframework.data.repository.query.Param;import java.util.List;public interface UserDtoRepository extends JpaRepository&amp;lt;User, Long&amp;gt; { /** * 利用JQPl动态查询用户信息 * @param name * @param email * @return UserSimpleDto接口 */ @Query(&quot;select u.name as name,u.email as email from User u where (:name is null or u.name =:name) and (:email is null or u.email =:email)&quot;) UserOnlyName findByUser(@Param(&quot;name&quot;) String name,@Param(&quot;email&quot;) String email); /** * 利用原始sql动态查询用户信息 * @param user * @return */ @Query(value = &quot;select u.name as name,u.email as email from user u where (:#{#user.name} is null or u.name =:#{#user.name}) and (:#{#user.email} is null or u.email =:#{#user.email})&quot;,nativeQuery = true) UserOnlyName findByUser(@Param(&quot;user&quot;) User user);然后，新增一个测试类，测试一下上面方法的结果。@Testpublic void testQueryDinamicDto() { userDtoRepository.save(User.builder().name(&quot;jack&quot;).email(&quot;123456@126.com&quot;).sex(&quot;man&quot;).address(&quot;shanghai&quot;).build());UserOnlyName userDto = userDtoRepository.findByUser(&quot;jack&quot;, null); System.out.println(userDto.getName() + &quot;:&quot; + userDto.getEmail()); UserOnlyName userDto2 = userDtoRepository.findByUser(User.builder().email(&quot;123456@126.com&quot;).build()); System.out.println(userDto2.getName() + &quot;:&quot; + userDto2.getEmail());}最后，运行结果如下。Hibernate: insert into user (address, email, name, sex, version, id) values (?, ?, ?, ?, ?, ?) : binding parameter [1] as [VARCHAR] - [shanghai] : binding parameter [2] as [VARCHAR] - [123456@126.com] : binding parameter [3] as [VARCHAR] - [jack] : binding parameter [4] as [VARCHAR] - [man] : binding parameter [5] as [BIGINT] - [0] : binding parameter [6] as [BIGINT] - [1]Hibernate: select user0_.name as col_0_0_, user0_.email as col_1_0_ from user user0_ where (? is null or user0_.name=?) and (? is null or user0_.email=?) : binding parameter [1] as [VARCHAR] - [jack] : binding parameter [2] as [VARCHAR] - [jack] : binding parameter [3] as [VARCHAR] - [null] : binding parameter [4] as [VARCHAR] - [null]jack:123456@126.comHibernate: select u.name as name,u.email as email from user u where (? is null or u.name =?) and (? is null or u.email =?) : binding parameter [1] as [VARBINARY] - [null] : binding parameter [2] as [VARBINARY] - [null] : binding parameter [3] as [VARCHAR] - [123456@126.com] : binding parameter [4] as [VARCHAR] - [123456@126.com]jack:123456@126.com注意：其中打印了一下 SQL 传入的参数，是为了更清楚参数都传入了什么值。上面的两个方法，分别采用了JPQL 的动态参数和 SPEL 的表达式方式获取参数。通过上面的实例可以看得出来，采用了 :email isnullor s.email = :email 这种方式来实现动态查询的效果。实际工作中也可以演变得很复杂。所以，再看一个实际工作中复杂一点的例子。通过原始 sql，根据动态条件 room 关联 room_record 来获取 room_record 的结果。通过 JPQL 动态参数查询 RoomRecord。@Query 注解亦可以获得想要的结果，nativeQuery 也可以获得想要的结果，选择如下： 能用方法名表示的，尽量用方法名表示，因为这样语义清晰、简单快速，基本上只要编译通过，一定不会有问题； 能用 @Query 里面的 JPQL 表示的，就用 JPQL，这样与 SQL 无关，万一哪天换数据库了，基本上代码不用改变； 最后实在没有办法了，可以选择 nativeQuery 写原始 SQL" }, { "title": "利用单元测试和集成测试", "url": "/posts/spring-data-jpa-test/", "categories": "Java, Spring", "tags": "Spring Security, React", "date": "2020-09-02 15:33:00 +0000", "snippet": "如果测试用例非常完备，是可以提升团队体效率的Spring Data JPA 单元测试实际工作中我们免不了要和 Repository 打交道，那么这层的测试用例应该怎么写呢？怎么才能提高开发效率呢？关于 JPA 的 Repository，下面我们分成两个部分来介绍：了解基本语法；分析最佳实践。Spring Data JPA Repository 的测试用例测试用例写法步骤如下。第一步：引入 test 的依赖，gradle 的语法如下所示。testImplementation &#39;com.h2database:h2&#39;testImplementation &#39;org.springframework.boot:spring-boot-starter-test&#39;第二步：利用项目里面的实体和 Repository，假设项目里面有 Address 和 AddressRepository，代码如下所示。第三步：新建 RepsitoryTest，@DataJpaTest 即可@DataJpaTestpublic class AddressRepositoryTest { @Autowired private AddressRepository addressRepository; //测试一下保存和查询 @Test public void testSave() { Address address = Address.builder().city(&quot;shanghai&quot;).build(); addressRepository.save(address); List&amp;lt;Address&amp;gt; address1 = addressRepository.findAll(); address1.stream().forEach(address2 -&amp;gt; System.out.println(address2); }}通过上面的测试用例可以看到，直接添加了 @DataJpaTest 注解，然后利用 Spring 的注解 @Autowired，引入了 spring context 里面管理的 AddressRepository 实例。换句话说，我们在这里面使用了集成测试，即直接连接的数据库来完成操作。第四步：直接运行上面的测试用例，可以得到如下图所示的结果。通过测试结果，可以发现： 测试方法默认都会开启一个事务，测试完了之后就会进行回滚； 执行了 insert 和 select 两种操作； 如果开启了 Session Metrics 的日志的话，也可以观察出来其发生了一次 connection。通过这个案例，可以知道 Repository 的测试用例写起来还是比较简单的，其中主要利用了 @DataJpaTest 的注解Repository 的测试" }, { "title": "Spring Data JPA", "url": "/posts/spring-data-jpa-first/", "categories": "Java, Spring", "tags": "Spring Security, React", "date": "2020-09-01 15:33:00 +0000", "snippet": "Spring Boot 和 Spring Data JPA第一步：利用 IDEA 和 SpringBoot 2.3.3 快速创建一个演示项目选择 Spring Boot 的依赖： Lombok：帮我们创建一个简单的 Entity 的 POJO，主要用来省去创建 GET 和 SET 方法； Spring Web：MVC 必备组件； Spring Data JPA：重头戏，这是本课时的重点内容； H2 Database：内存数据库； Spring Boot Actuator：监控我们项目状态使用。然后通过下图操作界面选择上面的依赖，如下图所示：第二步：通过 IDEA 的图形化界面，一路单击 Next 按钮，然后单击 Finsh 按钮，得到一个工程：第三步：新增 3 个类来完成对 User 的 CURD第一个类：新增 User.java，它是一个实体类，用来做 User 表的映射的，如下所示：public class User { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private String name; private String email;}第二个类：新增 UserRepository，它是 DAO 层，用来操作实体 User 进行增删改成操作，如下所示：public interface UserRepository extends JpaRepository&amp;lt;User, Long&amp;gt; {}第三个类：新增 UserController，它是 Controller，用来创建 Rest 的 API 的接口的，如下所示：package cn.happymaya.jpaguide.controller;import cn.happymaya.jpaguide.dao.UserRepository;import cn.happymaya.jpaguide.domain.User;import org.springframework.data.domain.Page;import org.springframework.data.domain.Pageable;import org.springframework.http.MediaType;import org.springframework.web.bind.annotation.*;import java.awt.*;@RestController@RequestMapping(path = &quot;/api/v1&quot;)public class UserController { private UserRepository userRepository; public UserController(UserRepository userRepository) { this.userRepository = userRepository; } /** * 保存用户 * @param user 用户对象 * @return 返回用户对象 */ @PostMapping(path = &quot;user&quot;, consumes = {MediaType.APPLICATION_ATOM_XML_VALUE}) public User addNewUser(@RequestBody User user) { return userRepository.save(user); } /** * 根据分页信息查询用户 * @param request 请求数据 * @return 返回用户信息 */ @GetMapping(path = &quot;users&quot;) @ResponseBody public Page&amp;lt;User&amp;gt; getAllUsers(Pageable request) { return userRepository.findAll(request); }}最终，项目结构变成如下图所示的模样：上图中，appliaction.properties 里面的内容是空的，到现在三步搞定，其他什么都不需要配置，直接点击 JpaApplication 这个类，就可启动项目了。第四步：调用项目里面的 User 相关的 API 接口测试一下新增一个 JpaApplication.http 文件，内容如下：POST /api/v1/user HTTP/1.1Host: 127.0.0.1:8080Content-Type: application/jsonCache-Control: no-cache{&quot;name&quot;:&quot;jack&quot;,&quot;email&quot;:&quot;123@126.com&quot;}#######GET http://127.0.0.1:8080/api/v1/users?size=3&amp;amp;page=0###运行之后，结果如下：POST http://127.0.0.1:8080/api/v1/userHTTP/1.1 200 Content-Type: application/jsonTransfer-Encoding: chunkedDate: Sat, 22 Aug 2020 02:48:43 GMTKeep-Alive: timeout=60Connection: keep-alive{ &quot;id&quot;: 4, &quot;name&quot;: &quot;jack&quot;, &quot;email&quot;: &quot;123@126.com&quot;}Response code: 200; Time: 30ms; Content length: 44 bytesGET http://127.0.0.1:8080/api/v1/users?size=3&amp;amp;page=0HTTP/1.1 200 Content-Type: application/jsonTransfer-Encoding: chunkedDate: Sat, 22 Aug 2020 02:50:20 GMTKeep-Alive: timeout=60Connection: keep-alive{ &quot;content&quot;: [ { &quot;id&quot;: 1, &quot;name&quot;: &quot;jack&quot;, &quot;email&quot;: &quot;123@126.com&quot; }, { &quot;id&quot;: 2, &quot;name&quot;: &quot;jack&quot;, &quot;email&quot;: &quot;123@126.com&quot; }, { &quot;id&quot;: 3, &quot;name&quot;: &quot;jack&quot;, &quot;email&quot;: &quot;123@126.com&quot; } ], &quot;pageable&quot;: { &quot;sort&quot;: { &quot;sorted&quot;: false, &quot;unsorted&quot;: true, &quot;empty&quot;: true }, &quot;offset&quot;: 0, &quot;pageNumber&quot;: 0, &quot;pageSize&quot;: 3, &quot;unpaged&quot;: false, &quot;paged&quot;: true }, &quot;totalPages&quot;: 2, &quot;last&quot;: false, &quot;totalElements&quot;: 4, &quot;size&quot;: 3, &quot;number&quot;: 0, &quot;numberOfElements&quot;: 3, &quot;sort&quot;: { &quot;sorted&quot;: false, &quot;unsorted&quot;: true, &quot;empty&quot;: true }, &quot;first&quot;: true, &quot;empty&quot;: false}Response code: 200; Time: 59ms; Content length: 449 bytes通过以上案例，Spring Data JPA 可以做数据的 CRUD 操作。JPA 整合 MySQL 数据库切换 MySQL 数据源上面的例子，采用的是默认 H2 数据源的方式，现在调整一下上面的代码，以 MySQL 作为数据源。第一处改动，application.properties 内容如下：第二处改动，删除 H2 数据源，新增 MySQL 数据库驱动：调整完毕之后，重启这个项目，以同样的方式测试上面的两个接口依然 OK。其实这个时候可以发现一件事情，那就是没有手动去创建任何表，JPA 自动帮我创建了数据库的 DDL，并新增了 User 表，所以用 JPA 之后创建表的工作就不会那么复杂了，只需要把实体写好就可以了。Spring Data JPA 测试用例的写法关注 Repository 的测试用例的写法，Controller 和 Service 等更复杂的测试不考虑。第一步，在 Test 目录里增加 UserRepositoryTest 类：@DataJpaTestpublic class UserRepositoryTest { @Autowired private UserRepository userRepository; @Test public void testSaveUser() { User user = userRepository.save(User.builder().name(&quot;jackxx&quot;).email(&quot;123456@126.com&quot;).build()); Assert.assertNotNull(user); List&amp;lt;User&amp;gt; users= userRepository.findAll(); System.out.println(users); Assert.assertNotNull(users); }}整体认识 JPA主流 ORM 框架比对下表是市场上比较流行的 ORM 框架，这里罗列了 MyBatis、Hibernate、Spring Data JPA 等，并对比了下它们的优缺点：| ORM框架 | 优点 | 缺点 || :————-: | :———————————————————-: | :———————————————————-: || MyBatis | MyBatis 本是 Apache 的一个开源项目 iBatis；2010年由 Apache Software Foundation 迁移到了Google Code，改名为MyBatis其着力于 POJO 与 SQL 之间的映射关系，可以进行更为细致的SQL编写操作使用起来十分灵活、上手简单、容易掌握，所以深受开发者的喜欢目前市场占有率最高，比较适合互联应用公司的API场景 | 工作量比较大需要各种配置文件和 SQL 语句 || Hibernate | Hibernate 是一个开放源代码的对象关系映射框架，它对 JDBC 进行了非常轻量级的对象封装，使得Jva程序员可以随心所欲地使用对象编程思维来操纵数据库对象有自己的生命周期，着力点为对象与对象之间关系有自己的 HQL 查询语言，所以数据库移植性很好Hibernate 是完备的ORM框架，是符合 JPA 规范的，有自己的缓存机制 | 上手比较难比较适合企业级的应用系统开发 || Spring Data JPA | 可以理解为JPA规范的再次封装抽象，底层还是使用了 Hibernate 的JPA技术实现，引用JPQL（Java Persistence Query Language)查询语言，属于Spring的整个生态体系的一部分由于 Spring Boot 和 Spring Cloud 在市场上比铰流行，Spring Data JPA也逐渐进入我们的视野，他们有机的整体，使用起来比较方便，加快了开发的效率，使开发者不需要关心和配置更多的东西，完全可以沉浸在Spring的完整生态标准的实现下上手简单、开发效率高，又对对象的支持比较好,有很大的灵活性，市场的认可度越来越高 | 入门简单上手比较快但想要精通就需要了解很多知识 || OpenJPA | 这是Apache组织提供的开源项目，它实现了EJB3.0中的JPA标准为开发者提供功能强大、使用简单的持久化数据管理框架 | 功能、性能、普及性等方面需要加大力度所以使用的人不是特别多 || QueryDSL | QueryDSL 是个对ORM框架扩展的操作，提供了一种通用的Util和方法用来构建不同查询参数的API,提供统一的抽象；目前QueryDSL支持的框架包括JPA、JDO、SQL、Java Collections、RDF、Lucene、Hibernate等 | |Java Persistence API 和开源实现JPA 是 JDK 5.0 新增的协议，通过相关持久层注解（@Entity 里面的各种注解）来描述对象和关系型数据里面的表映射关系，并将 Java 项目运行期的实体对象，通过一种Session持久化到数据库中。想象一下，一旦协议有了，大家都遵守了此种协议进行开发，那么周边开源产品就会大量出现，比如 Spring Data Rest 就基于这套标准，进而对 Entity 的操作再进行封装，从而可以得到更加全面的 Rest 协议的 API接口。再比如 JSON API（https://jsonapi.org/）协议，就是雅虎的大牛基于 JPA 的协议的基础，封装制定的一套 RESTful 风格、JSON 格式的 API 协议，那么一旦 JSON API 协议成了标准，就会有很多周边开源产品出现。比如很多 JSON API 的客户端、现在比较流行的 Ember 前端框架，就是基于 Entity 这套 JPA 服务端的协议，在前端解析 API 协议，从而可以对普通 JSON 和 JSON API 的操作进行再封装。所以规范是一件很有意思的事情，突然之间世界大变样，很多东西都一样了，我们的思路就需要转换了。JPA 的分类一套 API 标准定义了一套接口，在 javax.persistence 的包下面，用来操作实体对象，执行 CRUD 操作，而实现的框架（Hibernate）替代完成所有的事情，从烦琐的 JDBC 和 SQL 代码中解脱出来，更加聚焦自己的业务代码，并且使架构师架构出来的代码更加可控。定义了一套基于对象的 SQL：Java Persistence Query Language（JPQL），像 Hibernate 一样，通过写面向对象（JPQL）而非面向数据库的查询语言（SQL）查询数据，避免了程序与数据库 SQL 语句耦合严重，比较适合跨数据源的场景（一会儿 MySQL，一会儿 Oracle 等）。ORM（Object/Relational Metadata）对象注解映射关系，JPA 直接通过注解的方式来表示 Java 的实体对象及元数据对象和数据表之间的映射关系，框架将实体对象与 Session 进行关联，通过操作 Session 中不通实体的状态，从而实现数据库的操作，并实现持久化到数据库表中的操作，与 DB 实现同步。JPA 协议地址JPA 的开源实现JPA 的宗旨是为 POJO 提供持久化标准规范，可以集成在 Spring 的全家桶使用，也可以直接写独立 application 使用。任何用到 DB 操作的场景，都可以使用，极大地方便开发和测试，所以 JPA 的理念已经深入人心了。Spring Data JPA、Hibernate 3.2+、TopLink 10.1.3 以及 OpenJPA、QueryDSL 都是实现 JPA 协议的框架，他们之间的关系结构如下图所示：俗话说得好：“未来已经来临，只是尚未流行”，大神资深开发用 Spring Data JPA，编程极客者用 JPA；而普通 Java 开发者，不想去挑战的 Java“搬砖者”用 Mybatis。Spring Data 认识1. Spring Data 介绍Spring Data 项目是从 2010 年开发发展起来的，Spring Data 利用一个大家熟悉的、一致的、基于注解的数据访问编程模型，做一些公共操作的封装，它可以轻松地让开发者使用数据库访问技术，包括关系数据库、非关系数据库（NoSQL）。同时又有不同的数据框架的实现，保留了每个底层数据存储结构的特殊特性。Spring Data Common 是 Spring Data 所有模块的公共部分，该项目提供了基于 Spring 的共享基础设施，它提供了基于 repository 接口以 DB 操作的一些封装，以及一个坚持在 Java 实体类上标注元数据的模型。Spring Data 不仅对传统的数据库访问技术如 JDBC、Hibernate、JDO、TopLick、JPA、MyBatis 做了很好的支持和扩展、抽象、提供方便的操作方法，还对 MongoDb、KeyValue、Redis、LDAP、Cassandra 等非关系数据的 NoSQL 做了不同的实现版本，方便我们开发者触类旁通。其实这种接口型的编程模式可以很好地学习 Java 的封装思想，实现对操作的进一步抽象，也可以把这种思想运用在自己公司写的 Framework 上面。2. Spring Data 的子项目有哪些主要项目（Main Modules）： Spring Data Commons，相当于定义了一套抽象的接口，下一课时我们会具体介绍 Spring Data Gemfire Spring Data JPA，关注的重点，对 Spring Data Common 的接口的 JPA 协议的实现 Spring Data KeyValue Spring Data LDA Spring Data MongoD Spring Data RES Spring Data Redi Spring Data for Apache Cassandr Spring Data for Apache Solr社区支持的项目（Community Modules）： Spring Data Aerospik Spring Data Couchbas Spring Data DynamoD Spring Data Elasticsearc Spring Data Hazelcas Spring Data Jes Spring Data Neo4 Spring Data Vault其他（Related Modules）： Spring Data JDBC Extension Spring for Apache Hadoo Spring Content除了上面这些，还有很多开源社区版本，比如 Spring Data、MyBatis 等" }, { "title": "Docker &amp; CI/CD 下（15）", "url": "/posts/docker-CI-CD-2/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker, DevOps, CI/CD", "date": "2020-08-22 08:33:00 +0000", "snippet": "昨天使用 Docker + Jenkins + GitLab 搭建 CI/CD 环境，今天使用已经构建好的环境来实际构建和部署一个应用。构建和部署一个应用的流程可以分为五部分。 我们首先需要配置 GitLab SSH 访问公钥，使得我们可以直接通过 SSH 拉取或推送代码到 GitLab。 接着将代码通过 SSH 上传到 GitLab。 再在 Jenkins 创建构建任务，使得 Jenkins 可以成功拉取 GitLab 的代码并进行构建。 然后配置代码变更自动构建流程，使得代码变更可以触发自动构建 Docker 镜像。 最后配置自动部署流程，镜像构建完成后自动将镜像发布到测试或生产环境。步骤如下：1 配置 GitLab SSH 访问公钥为能够让 Jenkins 顺利从 GitLab 拉取代码，需要先生成 ssh 密钥。可以使用 ssh-keygen 命令来生成 2048 位的 ras 密钥。在 Linux 上执行如下命令：$ ssh-keygen -o -t rsa -b 2048 -C &quot;email@example.com&quot;# 输入上面命令后系统会提示密钥保存的位置等信息，只需要按回车即可。Generating public/private rsa key pair.Enter file in which to save the key (/home/centos/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /home/centos/.ssh/id_rsa.Your public key has been saved in /home/centos/.ssh/id_rsa.pub.The key fingerprint is:SHA256:A+d0NQQrjxV2h+zR3BQIJxT23puXoLi1RiTKJm16+rg email@example.comThe key&#39;s randomart image is:+---[RSA 2048]----+|  =XB=o+o|| ..=B+o .||  . + +. o   ||   = B .o .  ||  o S +  o . || . * .... . +||  =  ..o   +.|| ...  o..   .|| E=. ... |+----[SHA256]-----+执行完上述命令后 ，$HOME/.ssh/ 目录下会自动生成两个文件 id_rsa.pub 文件为公钥文件 id_rsa 文件为私钥文件然后通过 cat 命令来查看公钥文件内容：$ cat $HOME/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDljSlDNHnUr4ursYISKXK5j2mWTYnt100mvYeJCLpr6tpeSarGyr7FnTc6sLM721plU2xq0bqlFEU5/0SSvFdLTht7bcfm/Hf31EdAuIqZuy/guP06ijpidfX6lVDxLWx/sO3Wbj3t7xgj4sfCFTiv+OOFP0NxKr5wy+emojm6KIaXkhjbPeJDgph5bvluFnKAtesMUkdhceAdN9grE3nkBOnwWw6G4dCtbrKt2o9wSyzgkDwPjj2qjFhcE9571/61/Nr8v9iqSHvcb/d7WZ0Qq7a2LYds6hQkpBg2RCDDJA16fFVs8Q5eNCpDQwGG3IbhHMUwvpKDf0OYrS9iftc5 email@example.com然后将公钥文件拷贝到 GitLab 的个人设置 -&amp;gt; SSH Keys 中，点击添加按钮，将公钥添加到 GitLab 中。2 上传服务代码到 GitLab这里，使用 Golang 编写了一个 HTTP 服务，代码如下：package mainimport ( &quot;fmt&quot; &quot;net/http&quot;)func hello(w http.ResponseWriter, req *http.Request) { fmt.Fprintf(w, &quot;hello\\n&quot;)}func headers(w http.ResponseWriter, req *http.Request) { for name, headers := range req.Header { for _, h := range headers { fmt.Fprintf(w, &quot;%v: %v\\n&quot;, name, h) } }}func main() { http.HandleFunc(&quot;/hello&quot;, hello) http.HandleFunc(&quot;/headers&quot;, headers) http.ListenAndServe(&quot;:8090&quot;, nil)}然后编写一个 Dockerfile，利用多阶段构建写的代码编译，并将编译后的二进制文件拷贝到 scratch（scratch 是一个空镜像，用于构建其他镜像，体积非常小）的基础镜像中。Dockerfile 的内容如下：FROM golang:1.14 as builderWORKDIR /go/src/github.com/wilhelmguo/devops-demo/COPY main.go .RUN CGO_ENABLED=0 GOOS=linux go build -o /tmp/http-server .FROM scratchWORKDIR /root/COPY --from=builder /tmp/http-server .CMD [&quot;./http-server&quot;]编写完 Go HTTP 文件和 Dockerfile 文件后，代码目录内容如下：$ ls -lhtotal 24-rw-r--r--  1 root  root   243B Nov  3 22:03 Dockerfile-rw-r--r--  1 root  root26B Nov  3 22:06 README.md-rw-r--r--  1 root  root   441B Nov  3 22:03 main.go然后，在 GitLab 上创建一个 hello 项目，并将代码上传。项目创建完成后，GitLab 会自动跳转到项目详情页面。3 创建 Jenkins 任务 在 Jenkins 中添加一个自由风格的任务。 点击确定，然后到源码管理选择 Git，填写 GitLab 项目的 URL 此时 Jenkins 会提示没有访问 GitLab 的相关权限，需要点击添加按钮将私钥添加到 Jenkins 中用以鉴权。 由于部署 GitLab 的宿主机 ssh 默认端口为 22，为了避免与宿主机的 ssh 端口冲突，我们的 GitLab ssh 端口配置为 2222，因此 Jenkins 连接 GitLab 的 URL 中需要包含端口号 2222， 配置格式为 ssh://git@172.20.1.6:2222/root/hello.git。 选择添加的密钥类型为 “SSH Username with private key”，Username 设置为 jenkins，然后将私钥粘贴到 Private Key 输入框中，点击添加即可。 添加完成后，认证名称选择 jenkins 后，红色报错提示就会消失。这证明此时 Jenkins 和 GitLab 已经认证成功，可以成功从 GitLab 拉取代码了。 下面使用 shell 脚本来构建我们的应用镜像，在构建中增加一个 Shell 类型的构建步骤，并且填入以下信息 # 第一步，登录镜像仓库# USER 可替换为目标镜像仓库的用户名# PASSWORD 可替换为镜像仓库的密码$ docker login -u {USER} -p  {PASSWORD}# 第二步，使用 docker build 命令构建镜像$ docker build -t lagoudocker/devops-demo . # 第三步, 使用 docker push 命令推送镜像$ docker push lagoudocker/devops-demo  完成后点击保存，此时任务已经成功添加到 Jenkins 中 回到任务首页，点击构建按钮即可开始构建 第一次构建需要下载依赖的基础镜像，这个过程可能比较慢。不过构建过程中，可以点击控制台查看构建输出的内容4 配置自动构建 点击上一步创建的任务 点击配置，进入任务配置界面，到【构建触发器】下勾选 GitLab 相关的选项 点击 Generate 按钮生成一个 GitLab 回调 Jenkins 的 token 记录下 Jenkins 的回调地址和生成的 token 信息。 在 GitLab 项目设置中，选择 Webhooks， 将 Jenkins 的回调地址和 token 信息添加到 Webhooks 的配置中，点击添加即可。后面每次提交都会触发自动构建。为了实现根据 git 的 tag 自动构建相应版本的镜像，需要修改 Jenkins 构建步骤中的 shell 脚本为以下内容：# 需要推送的镜像名称IMAGE_NAME=&quot;happymaya/devops-demo&quot; # 获取当前构建的版本号GIT_VERSION=`git describe --always --tag`# 生成完整的镜像 URL 变量，用于构建和推送镜像REPOSITORY=docker.io/${IMAGE_NAME}:${GIT_VERSION} # 构建Docker镜像 docker build -t $REPOSITORY -f Dockerfile . # 登录镜像仓库，username 跟 password 为目标镜像仓库的用户名和密码docker login --username=xxxxx --password=xxxxxx docker.io# 推送 Docker 镜像到目标镜像仓库docker push $REPOSITORY 好了，到此已经完成了 GitLab -&amp;gt; Jenkins -&amp;gt; Docker 镜像仓库的自动构建和推送。当推送代码到 GitLab 中时，会自动触发 Webhooks，然后 GitLab 根据配置的 Webhooks 调用 Jenkins 开始构建镜像，镜像构建完成后自动将镜像上传到我们的镜像仓库。5 配置自动部署镜像构建完成后，还需要将镜像发布到测试或生产环境中将镜像运行起来。发布到环境的过程可以设置为自动发布。每当推送代码到 master 中时，即开始自动构建镜像，并将构建后的镜像发布到测试环境中。在镜像构建过程中，实际上 Jenkins 是通过执行编写的 shell 脚本完成的，要想实现镜像构建完成后自动在远程服务器上运行最新的镜像，需要借助一个 Jenkins 插件 Publish Over SSH，这个插件可以完成自动登录远程服务器，并执行一段脚本，将服务启动。下面是使用这个插件的步骤：第一步，在 Jenkins 中安装 Publish Over SSH 插件。 在 Jenkins 系统管理，插件管理中，搜索 Publish Over SSH，然后点击安装并重启 Jenkins 服务。第二步，配置 Publish Over SSH 插件。 插件安装完成后，在 Jenkins 系统管理的系统设置下，找到 Publish Over SSH 功能模块，添加远程服务器节点，这里我使用密码验证的方式添加一台服务器。配置好后，我们可以使用测试按钮测试服务器是否可以正常连接，显示Success 代表服务器可以正常连接，测试连接成功后，点击保存按钮保存配置。第三步，修改之前 shell 任务中脚本， 添加部署相关的内容：# 需要推送的镜像名称IMAGE_NAME=&quot;lagoudocker/devops-demo&quot; # 获取当前构建的版本号GIT_VERSION=`git describe --always --tag`# 生成完整的镜像 URL 变量，用于构建和推送镜像REPOSITORY=docker.io/${IMAGE_NAME}:${GIT_VERSION} # 构建Docker镜像 docker build -t $REPOSITORY -f Dockerfile . # 登录镜像仓库，username 跟 password 为目标镜像仓库的用户名和密码docker login --username={USER} --password={PASSWORD} docker.io# 推送 Docker 镜像到目标镜像仓库docker push $REPOSITORY mkdir -p ./shell &amp;amp;&amp;amp; echo \\ &quot;docker login --username={USER} --password={PASSWORD} \\n&quot;\\ &quot;docker pull $REPOSITORY\\n&quot;\\ &quot;docker kill hello \\n&quot;\\ &quot;docker run --rm --name=hello -p 8090:8090 -d $REPOSITORY&quot; &amp;gt;&amp;gt; ./shell/release在 docker push 命令后，增加一个输出 shell 脚本到 release 文件的命令，这个脚本会发送到远端的服务器上并执行，通过执行这个脚本文件可以在远端服务器上，拉取最新镜像并且重新启动容器第四步，配置远程执行。在 Jenkins 的 hello 项目中，点击配置，在执行步骤中点击添加Send files or execute commands over SSH的步骤，选择之前添加的服务器，并且按照以下内容填写相关信息 Source file 是要传递的 shell 脚本信息，这里填写上面生成的 shell 脚本文件即可 Remove prefix 是需要过滤的目录，这里填写 shell Remote directory 为远程执行脚本的目录最后点击保存，保存配置即可。配置完成后，就完成了推送代码到 GitLab，Jenkins 自动构建镜像，之后推送镜像到镜像仓库，最后自动在远程服务器上拉取并重新部署容器。 如果生产环境中使用的 Kubernetes 管理服务，可以在 Jenkins 中安装 Kubernetes 的插件，然后构建完成后直接发布镜像到 Kubernetes 集群中。 Jenkins 有 Kubernetes 的插件，可以调用 Kubernetes 接口发布到 Kubernetes 中 Flutter 相关的环境搭建可以参考 Flutter 官网" }, { "title": "Docker &amp; CI/CD 上（14）", "url": "/posts/docker-CI-CD-1/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker, DevOps, CI/CD", "date": "2020-08-21 06:33:00 +0000", "snippet": "DevOps 的思想可以缩短上线周期并且提高软件迭代速度，而 CI/CD 则是 DevOps 思想中最重要的部分。具体来说，CI/CD 是一种通过在应用开发阶段，引入自动化的手段来频繁地构建应用，并且向客户交付应用的方法。它的核心理念是持续开发、持续部署以及持续交付，它还可以让自动化持续交付贯穿于整个应用生命周期，使得开发和运维统一参与协同支持。下面我们来详细了解下 CI/CD 。什么是 CI/CDCI 持续集成（Continuous Integration）随着软件功能越来越复杂，一个大型项目要想在规定时间内顺利完成，需要多位开发人员协同开发。但是，如果每个人都负责开发自己的代码，然后集中在某一天将代码合并在一起（称为“合并日”）。会发现，代码可能会有很多冲突和编译问题，而这个处理过程十分烦琐、耗时，并且需要每一位小伙伴确认代码是否被覆盖，代码是否完整。这种情况显然不是想要看到的，这时持续集成（CI）就可以很好地帮助解决这个问题。CI 持续集成要求开发人员频繁地（甚至是每天）将代码提交到共享分支中。一旦开发人员的代码被合并，将会自动触发构建流程来构建应用，并通过触发自动化测试（单元测试或者集成测试）来验证这些代码的提交，确保这些更改没有对应用造成影响。如果发现提交的代码在测试过程中或者构建过程中有问题，则会马上通知研发人员确认，修改代码并重新提交。通过将以往的定期合并代码的方式，改变为频繁提交代码并且自动构建和测试的方式，可以帮助及早地发现问题和解决冲突，减少代码出错。传统 CI 流程的实现十分复杂，无法做到标准化交付，而当应用容器化后，应用构建的结果就是 Docker 镜像。代码检查完毕没有缺陷后合并入主分支。此时启动构建流程，构建系统会自动将我们的应用打包成 Docker 镜像，并且推送到镜像仓库。CD 持续交付（Continuous Delivery）每次完成代码的测试和构建后，需要将编译后的镜像快速发布到测试环境，这时持续交付登场了。持续交付要求实现自动化准备测试环境、自动化测试应用、自动化监控代码质量，并且自动化交付生产环境镜像。在以前，测试环境的构建是非常耗时的，并且很难保证测试环境和研发环境的一致性。但现在，借助于容器技术，可以很方便地构建出一个测试环境，并且可以保证开发和测试环境的一致性，这样不仅可以提高测试效率，还可以提高敏捷性。容器化后的应用交付过程是这样的： 将测试的环境交由 QA 来维护， 当确定好本次上线要发布的功能列表时，将不同开发人员开发的 feature 分支的代码合并到 release 分支 由 QA 来将构建镜像部署到测试环境，结合自动测试和人工测试、自动检测和人工记录，形成完整的测试报告 把测试过程中遇到的问题交由开发人员修改 开发修改无误后再次构建测试环境进行测试 测试没有问题后，自动交付生产环境的镜像到镜像仓库CD 持续部署（Continuous Deployment）CD 不仅有持续交付的含义，还代表持续部署。经测试无误打包完生产环境的镜像后，我们需要把镜像部署到生产环境，持续部署是最后阶段，它作为持续交付的延伸，可以自动将生产环境的镜像发布到生产环境中。部署业务首先需要我们有一个资源池，实现资源自动化供给，而且有的应用还希望有自动伸缩的能力，根据外部流量自动调整容器的副本数，而这一切在容器云中都将变得十分简单。可以想象，如果有客户提出了反馈，通过持续部署在几分钟内，就能在更改完代码的情况下，将新的应用版本发布到生产环境中（假设通过了自动化测试），这时我们就可以实现快速迭代，持续接收和整合用户的反馈，将用户体验做到极致。搭建 DevOps 环境的工具非常多，这里我选择的工具为 Jenkins、Docker 和 GitLab。 Gitlab 是由 Gitlab Inc. 开发的一款基于 Git 的代码托管平台，它的功能和 GitHub 类似，可以帮助存储代码。除此之外，GitLab 还具有在线编辑 wiki、issue 跟踪等功能，另外最新版本的 GitLab 还集成了 CI/CD 功能，不过这里仅仅使用 GitLab 的代码存储功能， CI/CD 还是交给我们的老牌持续集成工具 Jenkins 来做。Docker + Jenkins+ GitLab 搭建 CI/CD 系统软件安装环境如下。 操作系统：CentOS 7 Jenkins：tls 长期维护版 Docker：18.06 GitLab：13.3.8-ce.0第一步：安装 Docker略第二步：安装 GitLabGitLab 官方提供了 GitLab 的 Docker 镜像，因此我们只需要执行以下命令就可以快速启动一个 GitLab 服务了$ docker run -d \\--hostname localhost \\-p 8080:80 -p 2222:22 \\--name gitlab \\--restart always \\--volume /tmp/gitlab/config:/etc/gitlab \\--volume /tmp/gitlab/logs:/var/log/gitlab \\--volume /tmp/gitlab/data:/var/opt/gitlab \\gitlab/gitlab-ce:13.3.8-ce.0这个启动过程可能需要几分钟的时间。当服务启动后我们就可以通过 http://localhost:8080 访问到我们的 GitLab 服务了。 第一次登陆，GitLab 会要求设置管理员密码，输入管理员密码后点击确认即可 然后，GitLab 会自动跳转到登录页面。 最后，输入默认管理员用户名：admin@example.com 和上一步设置的密码，点击登录即可登录到系统中至此，GitLab 已经安装成功。第三步：安装 JenkinsJenkins 官方提供了 Jenkins 的 Docker 镜像，使用 Jenkins 镜像就可以一键启动一个 Jenkins 服务。命令如下：# docker run -d --name=jenkins \\-p 8888:8080 \\-u root \\--restart always \\-v /var/run/docker.sock:/var/run/docker.sock \\-v /usr/bin/docker:/usr/bin/docker \\-v /tmp/jenkins_home:/var/jenkins_home \\jenkins/jenkins:lts## 将 docker.sock 和 docker 二进制挂载到了 Jenkins 容器中，## 是为了让 Jenkins 可以直接调用 docker 命令来构建应用镜像。Jenkins 默认密码，会在容器启动后，打印在容器的日志中，可以通过以下命令找到 Jenkins 的默认密码，星号之间的类似于一串 UUID 的随机串就是默认密码。$ docker logs -f jenkinsunning from: /usr/share/jenkins/jenkins.warwebroot: EnvVars.masterEnvVars.get(&quot;JENKINS_HOME&quot;)2020-10-31 16:13:06.472+0000 [id=1] INFO org.eclipse.jetty.util.log.Log#initialized: Logging initialized @292ms to org.eclipse.jetty.util.log.JavaUtilLog2020-10-31 16:13:06.581+0000 [id=1] INFO winstone.Logger#logInternal: Beginning extraction from war file2020-10-31 16:13:08.369+0000 [id=1] WARNING o.e.j.s.handler.ContextHandler#setContextPath: Empty contextPath... 省略部分启动日志Jenkins initial setup is required. An admin user has been created and a password generated.Please use the following password to proceed to installation:***************************************************************************************************************************************************************************************Jenkins initial setup is required. An admin user has been created and a password generated.Please use the following password to proceed to installation:fb3499944e4845bba9d4b7d9eb4e3932This may also be found at: /var/jenkins_home/secrets/initialAdminPassword***************************************************************************************************************************************************************************************This may also be found at: /var/jenkins_home/secrets/initialAdminPassword2020-10-31 16:17:07.577+0000 [id=28] INFO jenkins.InitReactorRunner$1#onAttained: Completed initialization2020-10-31 16:17:07.589+0000 [id=21] INFO hudson.WebAppMain$3#run: Jenkins is fully up and running之后，通过访问 http://localhost:8888 就可以访问到 Jenkins 服务了。然后将日志中的密码粘贴到密码框即可，之后 Jenkins 会自动初始化，我们根据引导，安装推荐的插件即可。选择好安装推荐的插件后，Jenkins 会自动开始初始化一些常用插件。插件初始化完后，创建管理员账户和密码，输入用户名、密码和邮箱等信息，然后点击保存并完成即可。这里，确认 Jenkins 地址，就可以进入到 Jenkins 主页了。然后在系统管理 -&amp;gt; 插件管理 -&amp;gt; 可选插件处，搜索 GitLab 和 Docker ，分别安装相关插件即可，以便 Jenkins 服务和 GitLab 以及 Docker 可以更好地交互。等待插件安装完成， 重启 Jenkins ，我 Jenkins 环境就准备完成了。现在，Docker + Jenkins + GitLab 环境已经准备完成，后面只需要把代码推送到 GitLab 中，并做相关的配置即可实现推送代码自动构建镜像和发布。" }, { "title": "Docker &amp; DevOps（13）", "url": "/posts/docker-DevOps/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker, DevOps", "date": "2020-08-21 02:33:00 +0000", "snippet": "DevOps 作为一个热门的概念，近几年被提及的频率也越来越高。有些人说它是一种方法论，有些人说它是一堆工具，有些人说它是企业的一种管理模式。那么，DevOps 究竟是什么呢？Docker 在 DevOps 中又扮演了什么角色呢？DevOps 的前生今世1964 年，世界上的第一台计算机诞生，那时的计算机主要用于军事领域。计算机的运行离不开程序，那时负责编程的人员被称之为“程序员”。由于那时的程序比较简单，很多工作可以一个人完成，所以早期的计算软件交付流程是这样的：设计—开发—自测—发布—部署—维护。如图下图所示：随着计算机的发展和普及，越来越多的人接触到了计算机，这时的计算机也开始逐渐应用于商业领域，市场上出现了越来越多的办公、游戏等“软件”，也有越来越多的人开始从事软件开发这个行业，而这些软件开发者也有了更加专业的称呼“软件开发工程师”。后来，又随着计算机软件规模的增大，软件也越来越复杂，这时一个人已经无法完成一个软件完整的生命周期管理。一个软件的开发需要各个团队的分工配合，同时职能划分也需要更加细化，整个软件管理流程中除了软件工程师外又增加了测试工程师和运维工程师。分工之后软件开发流程如下: 研发工程师做代码设计和开发 测试工程师做专业的测试工作 运维工程师负责将软件部署并负责维护软件如图下图所示：这种软件开发模式被称为瀑布模型，这种模式将软件生命周期划分为： 制定计划 需求分析 软件设计 程序编写 软件测试 运行维护等六个基本活动并且规定了它们自上而下、相互衔接的固定次序，如瀑布流水一样，逐级的下降。瀑布模型的模式十分理想化，它假定用户需求十分明确，开发时间十分充足，且项目是单向迭代的。但随着互联网的出现，软件迭代速度越来越快，软件开发越来越“敏捷”，这时候大名鼎鼎的“敏捷开发”出现了，敏捷开发把大的时间点变成细小的时间点，快速迭代开发，软件更新速度也越来越快。敏捷开发对传统的开发、测试、运维模式提出了新的挑战，要求更快的开发速度和更高的软件部署频率。而运维工程师信奉的则是稳定性压倒一切，不希望软件频繁变更而引发新的问题。于是乎，敏捷开发和运维工程师之间的矛盾便诞生了。敏捷开发使得开发和运维工程师之间的矛盾变得越来越深，为了解决这个问题，DevOps 诞生了。DevOps 是研发工程师（Development）和运维工程师（Operations）的组合。下面是维基百科对 DevOps 的定义： DevOps（Development 和 Operations 的组合词）是一种重视“软件开发人员（Dev）”和“IT 运维技术人员（Ops）”之间沟通合作的文化、运动或惯例。透过自动化“软件交付”和“架构变更”的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠。DevOps 的整体目标是促进开发和运维人员之间的配合，并且通过自动化的手段缩短软件的整个交付周期，提高软件的可靠性。微服务、容器与 DevOps软件开发早期，业务模型简单，很多功能都放在一个服务中，这时的服务称之为单体服务。然而随着业务功能越来越复杂，发现这种单体服务功能过于复杂，容易牵一发而动全身，导致开发维护成本很高，软件迭代成本也越来越高。这时，软件开发者开始将单体服务拆分为多个小型服务，每一个小型服务独立负责一项任务，各个小型服务之间通过某种方式（RPC 或者 HTTP）相互调用，然后将不同的服务可以分发给不同的业务团队来开发，各个业务团队可以选择适合自己的编程语言来进行开发，这就是微服务。想要微服务实现更快的迭代和更加可靠的稳定性，一定是离不开一个一体化的 DevOps 平台，DevOps 的目标是构建一个稳定可靠的软件生命周期管理环境。所以它不仅可以节省很多研发、测试和运维成本，还可以极大地提高软件迭代速度，可以说微服务要想顺利实施，离不开 DevOps 的思想作为指导。在 Docker 技术出现之前，人们通常更加关注如何做好 CI（Continuous Integration，持续集成）/CD（Continuous Delivery持续交付）以及 IAAS（基础设施即服务），这时我们称之为 DevOps 1.0 时代。随着 Docker 技术的诞生，开始迎来了 DevOps 2.0 时代，DevOps 所有的这些需求都与 Docker 所提供的能力极其匹配 首先 Docker 足够轻量，可以帮助微服务实现快速迭代 其次 Docker 可以很方便地构建任何语言的运行环境，顺利地使用多种语言来开发的服务 最后 Docker 可以更好地隔离开发环境和生产环境。可以说 Docker 几乎满足了微服务的所有需求，Docker 为 DevOps 提供了很好的基础支撑。这时的研发和运维都开始关注软件统一交付的格式和软件生命周期的管理，而不像之前一样研发只关注“打包前”，而运维只关注“打包后”的模式，DevOps 无论是研发环境还是生产环境都开始围绕 Docker 进行构建。综上所述，微服务、Docker 与 DevOps 三者之间的关系，如上图所示。 云平台作为底层基础，采用 Docker 技术将服务做容器化部署，使用资源管理和调度平台（例如 Kubernetes 或 Swarm）来自动化地管理容器 DevOps 平台在云基础平台之上，通过流程自动化以及工具自动化的手段，为可持续集成和交付提供能力支持。 有了云平台和 DevOps 的支撑，微服务才能够发挥更大的作用，使得我们的业务更加成熟和稳定。容器如何助力 DevOpsDocker 可以在 DevOps 各个阶段发挥重要作用。例如： 在开发阶段提供统一的开发环境 在持续集成阶段快速构建应用 在部署阶段快速发布或更新生产环境的应用具体如下：开发流程开发人员可以在本地或者开发机上快速安装一个 Docker 环境，然后使用 Docker 可以快速启动和部署一个复杂的开发环境。相比传统的配置开发环境的方式，不仅大大提升了开发环境部署的效率，同时也保证了不同开发人员的环境一致。集成流程通过编写 Dockerfile 可以将业务容器化，然后将 Dockerfile 提交到代码仓库中，在做持续集成的过程中基于已有的 Dockerfile 来构建应用镜像，可以极大提升持续集成的构建速度。这主要是因为 Docker 镜像使用了写时复制（Copy On Write）和联合文件系统（Union FileSystem）的机制。Docker 镜像分层存储，相同层仅会保存一份，不同镜像的相同层可以复用，比如 Golang 容器在一次构建停止后，镜像已经存在于构建机上了，当我们开始新一轮的测试时，可以直接复用已有的镜像层，大大提升了构建速度。部署流程镜像仓库的存在使得 Docker 镜像分发变得十分简单，当镜像构建完成后，无论在哪里只需要执行 docker pull 命令就可以快速地将镜像拉取到本地并且启动我们的应用，这使得应用的创建或更新更快、更高效。另外，Docker 结合 Kubernetes 或者其他容器管理平台，可以轻松地实现蓝绿发布等流程，当升级应用观察到流量异常时，可以快速回滚到稳定版本。DevOps 工具介绍工欲善其事，必先利其器，要想顺利落地 DevOps，工具的选择十分重要，下面我们来看下除了Docker 外还有哪些工具可以帮助我们顺利地构建 DevOps 平台。GitGit 是一种分布式的版本控制工具， 是目前使用最广泛的 DevOps 工具之一。Git 相比于其他版本控制工具，它可以实现离线代码提交，它允许我们提交代码时未连接到 Git 服务器，等到网络恢复再将我们的代码提交到远程服务器。Git 非常容易上手，并且占用空间很小，相比于传统的版本控制工具（例如：Subversion、CVS 等）性能非常优秀，它可以帮助我们快速地创建分支，使得团队多人协作开发更加方便。目前全球最大的在线 Git 代码托管服务是 GitHub，GitHub 提供了代码在线托管服务，可以帮助我们快速地将 DevOps 工作流集成起来。除了 GitHub 外，还有很多在线代码托管服务，例如 GitLab、Bitbucket 等。JenkinsJenkins 是开源的 CI/CD 构建工具，Jenkins 采用插件化的方式来扩展它的功能，它拥有非常丰富的插件，这些插件可以帮助我们实现构建、部署、自动化等流程。它还拥有强大的生态系统，这些生态系统可以很方便地与 Docker 和 Kubernetes 集成。Jenkins 几乎可以和所有的 DevOps 工具进行集成。AnsibleAnsible 是一个配置管理工具。Ansible 可以帮助我们自动完成一些重复的 IT 配置管理，应用程序部署等任务，还可以帮助我们放弃编写繁杂的 shell 脚本，仅仅做一些 YAML 的配置，即可实现任务下发和管理工作。并且 Ansible 的每一行命令都是幂等的，它允许我们多次重复执行相同的脚本并且得到的结果都是一致的。KubernetesKubernetes 是当前最流行的容器编排工具之一，Docker 帮助我们解决了容器打包和镜像分发的问题，而 Kubernetes 则帮助我们解决了大批量容器管理和调度的问题，它可以打通从研发到上线的整个流程，使得 DevOps 落地更加简单方便。总结DevOps 虽然已经被提及很多年，但是一直没有很好的落地。直到 2013 年 Docker 的诞生，才使得 DevOps 这个理念又重新火了起来，因为 Docker 为我们解决了应用的构建、分发和隔离的问题，才使得 DevOps 落地变得更加简单。DevOps 提倡小规模和增量的服务发布方式，并且 DevOps 还指导尽量避免开发大单体（把所有的功能都集成到一个服务中）应用。这一切，都与 Docker 所能提供的能力十分匹配。因此，Docker 是非常重要的 DevOps 工具。" }, { "title": "Docker 实现镜像多阶级构建（12）", "url": "/posts/docker-multi-order-image-construction/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker", "date": "2020-08-18 15:33:00 +0000", "snippet": "Docker 镜像是分层的，并且每一层镜像都会额外占用存储空间。一个 Docker 镜像层数越多，该镜像占用存储空间则会越多。镜像构建最重要的一个原则就是保持镜像体积尽可能小，实现这个目标通常从两个方面入手： 基础镜像体积应该尽量小 尽量减少 Dockerfile 的行数（因为 Dockerfile 的每一条指令都会生成一个镜像层）在 Docker 早期版本中，对于编译型语言（例如 C、Java、Go）的镜像构建，只能将应用的编译环境和运行环境的准备，全部都放到一个 Dockerfile 中，这导致构建出来的镜像体积很大，从而增加了镜像的存储和分发成本，这显然与镜像构建原则不符。为了减小镜像体积，需要借助一个额外的脚本，将镜像的编译过程和运行过程分开。 编译阶段：负责将代码编译成可执行对象；运行时构建阶段：准备应用程序运行的依赖环境，然后将编译后的可执行对象拷贝到镜像中以 Go 语言开发的一个 HTTP 服务为例，代码如下：package mainimport ( &quot;fmt&quot; &quot;net/http&quot;)func hello(w http.ResponseWriter, req *http.Request) { fmt.Fprintf(w, &quot;hello world!\\n&quot;)}func main() { http.HandleFunc(&quot;/&quot;, hello) http.ListenAndServe(&quot;:8080&quot;, nil)}将这个 Go 服务构建成镜像分为两个阶段：代码的编译阶段和镜像构建阶段。构建镜像时，镜像中需要包含 Go 语言编译环境，应用的编译阶段可以使用 Dockerfile.build 文件来构建镜像。Dockerfile.build 的内容如下：FROM golang:1.13WORKDIR /go/src/github.com/wilhelmguo/multi-stage-demo/COPY main.go .RUN CGO_ENABLED=0 GOOS=linux go build -o http-server .Dockerfile.build 可以帮助把代码编译成可以执行的二进制文件，使用以下 Dockerfile 构建一个运行环境：FROM alpine:latest WORKDIR /root/COPY http-server .CMD [&quot;./http-server&quot;] 然后，将应用的编译和运行环境的准备步骤，都放到一个 build.sh 脚本文件中，内容如下：# 声明 shell 文件#!/bin/sh# 输出开始构建信息echo Building http-server:build# 使用 Dockerfile.build 文件来构建一个临时镜像 http-server:builddocker build -t http-server:build . -f Dockerfile.build# 使用 Dokcerfile.build 镜像创建一个名为 builder 的容器，该容器包含编译后的 http-server 二进制文件docker create --name builder http-server:build # 使用docker cp命令从 builder 容器中拷贝 http-server 文件到当前构建目录下，并且删除名称为 builder 的临时容器docker cp builder:/go/src/github.com/wilhelmguo/multi-stage-demo/http-server ./http-server docker rm -f builder# 输出开始构建镜像信息echo Building http-server:latest# 构建运行时镜像，然后删除临时文件 http-serverdocker build -t http-server:latest .rm ./http-server总结，上面的代码使用 Dockerfile.build 文件来编译应用程序，使用 Dockerfile 文件来构建应用的运行环境。然后通过创建一个临时容器，把编译后的 http-server 文件拷贝到当前构建目录中，然后再把这个文件拷贝到运行环境的镜像中，最后指定容器的启动命令为 http-server.使用这种方式虽然可以实现分离镜像的编译和运行环境，但是还需要额外引入一个 build.sh 脚本文件，而且构建过程中，还需要创建临时容器 builder 拷贝编译后的 http-server 文件，这使得整个构建过程比较复杂，并且整个构建过程也不够透明。为了解决这种问题， Docker 在 17.05 推出了多阶段构建（multistage-build）的解决方案。使用多阶段构建Docker 允许在 Dockerfile 中使用多个 FROM 语句，而每个 FROM 语句都可以使用不同基础镜像。最终生成的镜像，是以最后一条 FROM 为准，所以可以在一个 Dockerfile 中声明多个 FROM，然后选择性地将一个阶段生成的文件拷贝到另外一个阶段中，从而实现最终的镜像只保留我们需要的环境和文件。多阶段构建的主要使用场景是分离编译环境和运行环境。接下来，使用多阶段构建的特性，将上述未使用多阶段构建的过程精简成如下 Dockerfile：FROM golang:1.13WORKDIR /go/src/github.com/wilhelmguo/multi-stage-demo/COPY main.go .RUN CGO_ENABLED=0 GOOS=linux go build -o http-server .FROM alpine:latest WORKDIR /root/COPY --from=0 /go/src/github.com/wilhelmguo/multi-stage-demo/http-server .CMD [&quot;./http-server&quot;] 上面的 Dockerfile 主要分为两步，如下：第一步，编译代码FROM golang:1.13WORKDIR /go/src/github.com/wilhelmguo/multi-stage-demo/COPY main.go .RUN CGO_ENABLED=0 GOOS=linux go build -o http-server .第二步，构建运行时镜像FROM alpine:latest WORKDIR /root/COPY --from=0 /go/src/github.com/wilhelmguo/multi-stage-demo/http-server .CMD [&quot;./http-server&quot;]使用第二个 FROM 命令表示镜像构建的第二阶段，使用 COPY 指令拷贝编译后的文件到 alpine 镜像中，–from=0 表示从第一阶段构建结果中拷贝文件到当前构建阶段。最后，只需要使用以下命令，即可实现整个镜像的构建：docker build -t http-server:latest .构建出来的镜像与未使用多阶段构建之前构建的镜像大小一致，为了验证这一结论，分别使用这两种方式来构建镜像，最后对比一下镜像构建的结果。镜像构建对比使用多阶段构建前后的代码参考Github$ mkdir /go/src/github.com/wilhelmguo$ cd /go/src/github.com/wilhelmguo$ git clone https://github.com/wilhelmguo/multi-stage-demo.git代码克隆完成后，首先切换到without-multi-stage分支：$ cd without-multi-stage$ git checkout without-multi-stage这个分支是未使用多阶段构建技术构建镜像的代码，执行 build.sh 文件构建镜像：$  chmod +x build.sh &amp;amp;&amp;amp; ./build.shBuilding http-server:buildSending build context to Docker daemon  96.26kBStep 1/4 : FROM golang:1.131.13: Pulling from library/golangd6ff36c9ec48: Pull complete c958d65b3090: Pull complete edaf0a6b092f: Pull complete 80931cf68816: Pull complete 813643441356: Pull complete 799f41bb59c9: Pull complete 16b5038bccc8: Pull complete Digest: sha256:8ebb6d5a48deef738381b56b1d4cd33d99a5d608e0d03c5fe8dfa3f68d41a1f8Status: Downloaded newer image for golang:1.13 ---&amp;gt; d6f3656320feStep 2/4 : WORKDIR /go/src/github.com/wilhelmguo/multi-stage-demo/ ---&amp;gt; Running in fa3da5ffb0c0Removing intermediate container fa3da5ffb0c0 ---&amp;gt; 97245cbb773fStep 3/4 : COPY main.go . ---&amp;gt; a021d2f2a5bbStep 4/4 : RUN CGO_ENABLED=0 GOOS=linux go build -o http-server . ---&amp;gt; Running in b5c36bb67b9cRemoving intermediate container b5c36bb67b9c ---&amp;gt; 76c0c88a5cf7Successfully built 76c0c88a5cf7Successfully tagged http-server:build4b0387b270bc4a4da570e1667fe6f9baac765f6b80c68f32007494c6255d9e5bbuilderBuilding http-server:latestSending build context to Docker daemon  7.496MBStep 1/4 : FROM alpine:latestlatest: Pulling from library/alpinedf20fa9351a1: Already exists Digest: sha256:185518070891758909c9f839cf4ca393ee977ac378609f700f60a771a2dfe321Status: Downloaded newer image for alpine:latest ---&amp;gt; a24bb4013296Step 2/4 : WORKDIR /root/ ---&amp;gt; Running in 0b25ffe603b8Removing intermediate container 0b25ffe603b8 ---&amp;gt; 80da40d3a0b4Step 3/4 : COPY http-server . ---&amp;gt; 3f2300210b7bStep 4/4 : CMD [&quot;./http-server&quot;] ---&amp;gt; Running in 045cea651ddeRemoving intermediate container 045cea651dde ---&amp;gt; 5c73883177e7Successfully built 5c73883177e7Successfully tagged http-server:latest经过一段时间的等待，镜像就构建完成了。镜像构建完成后，使用docker image ls命令查看一下刚才构建的镜像大小：$ docker image ls http-serverREPOSITORY          TAG                 IMAGE ID            CREATED             SIZEhttp-server         latest              5c73883177e7        3 minutes ago       13MBhttp-server         build               76c0c88a5cf7        3 minutes ago       819MB可以看到，http-server:latest 镜像只有 13M，而编译镜像 http-server:build 则为 819M，虽然编写了很复杂的脚本 build.sh，但是这个脚本也确实将镜像体积减小了很多下面，将代码切换到多阶段构建分支：$ git checkout with-multi-stageSwitched to branch &#39;with-multi-stage&#39;为了避免镜像名称重复，将多阶段构建的镜像命名为 http-server-with-multi-stage:latest ，并且禁用缓存，避免缓存干扰构建结果，构建命令如下：$ docker build --no-cache -t http-server-with-multi-stage:latest .Sending build context to Docker daemon  96.77kBStep 1/8 : FROM golang:1.13 ---&amp;gt; d6f3656320feStep 2/8 : WORKDIR /go/src/github.com/wilhelmguo/multi-stage-demo/ ---&amp;gt; Running in 640da7a92a62Removing intermediate container 640da7a92a62 ---&amp;gt; 9c27b4606da0Step 3/8 : COPY main.go . ---&amp;gt; bd9ce4af24cbStep 4/8 : RUN CGO_ENABLED=0 GOOS=linux go build -o http-server . ---&amp;gt; Running in 6b441b4cc6b7Removing intermediate container 6b441b4cc6b7 ---&amp;gt; 759acbf6c9a6Step 5/8 : FROM alpine:latest ---&amp;gt; a24bb4013296Step 6/8 : WORKDIR /root/ ---&amp;gt; Running in c2aa2168acd8Removing intermediate container c2aa2168acd8 ---&amp;gt; f026884acda6 Step 7/8 : COPY --from=0 /go/src/github.com/wilhelmguo/multi-stage-demo/http-server . ---&amp;gt; 667503e6bc14Step 8/8 : CMD [&quot;./http-server&quot;] ---&amp;gt; Running in 15c4cc359144Removing intermediate container 15c4cc359144 ---&amp;gt; b73cc4d99088Successfully built b73cc4d99088Successfully tagged http-server-with-multi-stage:latest镜像构建完成后，同样使用docker image ls命令查看一下镜像构建结果：$ docker image ls http-server-with-multi-stage:latestREPOSITORY                     TAG                 IMAGE ID            CREATED             SIZEhttp-server-with-multi-stage   latest              b73cc4d99088        2 minutes ago       13MB可以看到，使用多阶段构建的镜像大小与上一步构建的镜像大小一致，都为 13M。但是使用多阶段构建后，大大减少构建步骤，使得构建过程更加清晰可读。多阶段构建的其他使用方式多阶段构建除了上面的使用方式，还有更多其他的使用方式，这些使用方式，可以使得多阶段构建实现更多的功能。为构建阶段命名默认情况下，每一个构建阶段都没有被命名，可以通过 FROM 指令出现的顺序来引用这些构建阶段。构建阶段的序号是从 0 开始的。然而，为了提高 Dockerfile 的可读性，需要为某些构建阶段起一个名称，这样即便后面对 Dockerfile 中的内容进程重新排序或者添加了新的构建阶段，其他构建过程中的 COPY 指令也不需要修改。上面的 Dockerfile 可以优化成如下内容：FROM golang:1.13 AS builderWORKDIR /go/src/github.com/wilhelmguo/multi-stage-demo/COPY main.go .RUN CGO_ENABLED=0 GOOS=linux go build -o http-server .FROM alpine:latest WORKDIR /root/COPY --from=builder /go/src/github.com/wilhelmguo/multi-stage-demo/http-server .CMD [&quot;./http-server&quot;]第一个构建阶段，使用 AS 指令将这个阶段命名为 builder。然后在第二个构建阶段使用 –from=builder 指令，即可从第一个构建阶段中拷贝文件，使得 Dockerfile 更加清晰可读。停止在特定的构建阶段有时候，构建阶段非常复杂，想在代码编译阶段进行调试，但是多阶段构建默认构建 Dockerfile 的所有阶段，为了减少每次调试的构建时间，可以使用 target 参数来指定构建停止的阶段。例如，只想在编译阶段调试 Dockerfile 文件，可以使用如下命令：$ docker build --target builder -t http-server:latest .在执行docker build命令时添加 target 参数，可以将构建阶段停止在指定阶段，从而方便我们调试代码编译过程。使用现有镜像作为构建阶段使用多阶段构建时，不仅可以从 Dockerfile 中已经定义的阶段中拷贝文件，还可以使用COPY --from指令从一个指定的镜像中拷贝文件，指定的镜像可以是本地已经存在的镜像，也可以是远程镜像仓库上的镜像。例如，当我们想要拷贝 nginx 官方镜像的配置文件到我们自己的镜像中时，可以在 Dockerfile 中使用以下指令：COPY --from=nginx:latest /etc/nginx/nginx.conf /etc/local/nginx.conf从现有镜像中拷贝文件还有一些其他的使用场景。例如，有些工具没有我们使用的操作系统的安装源，或者安装源太老，需要自己下载源码并编译这些工具，但是这些工具可能依赖的编译环境非常复杂，而网上又有别人已经编译好的镜像。这时就可以使用COPY --from指令从编译好的镜像中将工具拷贝到自己的镜像中，很方便地使用这些工具了。 使用多阶段构建的唯一限制条件是 Docker 版本必须高于 17.05 go 这种编译完可以直接把二进制文件拿走运行，python 这种只能再镜像中安装 python 运行环境了，java 可以把 jdk 做为编译环境，然后把编译完的包放到 jre 环境中即可，只有 jre 的环境镜像体积会小很多。" }, { "title": "Docker Kubernetes（11）", "url": "/posts/docker-kubernetes/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker, Kubernetes", "date": "2020-08-15 15:33:00 +0000", "snippet": "Docker 虽然在容器领域有着不可撼动的地位，然而在容器的编排领域，却有着另外一个事实标准，那就是 Kubernetes。Kubernetes 的历史说起 Kubernetes，这一切得从云计算说起。云计算是 2006 年由 Google 提起的，近些年被提及的频率也越来越高。云计算从起初的概念，演变为现在的 AWS、阿里云等实实在在的云产品（主要是虚拟机和相关的网络、存储服务），经变得非常成熟和稳定。正当大家以为云计算领域已经变成了以虚拟机为代表的云平台时，Docker 在 2013 年横空出世。Docker 提出了镜像、仓库等核心概念，规范了服务的交付标准，使得复杂服务的落地变得更加简单。之后 Docker 又定义了 OCI 标准，可以说在容器领域 Docker 已经成了事实的标准。然而 Docker 诞生只是帮助定义了开发和交付标准，如果想要在生产环境中大批量的使用容器，还离不开的容器的编排技术。在 2014 年 6 月 7 日，Kubernetes（Kubernetes 简称为 K8S，8 代表 ubernete 8个字母） 的第一个 commit（提交）拉开了容器编排标准定义的序幕。Kubernetes 是舵手的意思，把 Docker 比喻成一个个集装箱，Kubernetes 正是运输这些集装箱的舵手。早期的Kubernetes 主要参考 Google 内部的 Borg 系统，Kubernetes 刚刚诞生时，提出了 Pod、Sidecar 等概念，这些都是 Google 内部积累的精华。经过将近一年的沉淀和积累，Kubernetes 于 2015 年 7 月 21 日对外发布了第一个正式版本 v1.0，正式走入了大众的视线。Kubernetes 架构Kubernetes 采用声明式 API 来工作，所有组件的运行过程都是异步的，整个工作过程大致为： 用户声明想要的状态， Kubernetes 各个组件相互配合并且努力达到用户想要的状态。Kubernetes 采用典型的主从架构，分为 Master 和 Node 两个角色。 Mater 是 Kubernetes 集群的控制节点，负责整个集群的管理和控制功能。 Node 为工作节点，负责业务容器的生命周期管理。整体架构如下图：更多 Kubernetes 的了解去官方阅读 Kubernetes 的文档Master 节点Master 节点负责对集群中所有容器的调度，各种资源对象的控制，以及响应集群的所有请求。Master 节点包含三个重要的组件： kube-apiserver kube-scheduler kube-controller-managerkube-apiserver负责提供 Kubernetes 的 API 服务，所有的组件都需要与 kube-apiserver 交互获取或者更新资源信息，它是 Kubernetes Master 中最前端组件。kube-apiserver 的所有数据都存储在 etcd 中 etcd 采用 Go 语言编写的高可用 Key-Value 数据库，由 CoreOS 开发 etcd 虽然不是 Kubernetes 的组件，但是在 Kubernetes 中却扮演着至关重要的角色，是 Kubernetes 的数据大脑 etcd 的稳定性直接关系着 Kubernetes 集群的稳定性，因此生产环境中 etcd 一定要部署多个实例以确保集群的高可用。kube-scheduler用于监听未被调度的 Pod，然后根据一定调度策略将 Pod 调度到合适的 Node 节点上运行。kube-controller-manager负责维护整个集群的状态和资源的管理。例如： 多个副本数量的保证， Pod 的滚动更新等。每种资源的控制器都是一个独立 为了保证 Kubernetes 集群的高可用，Master 组件需要部署在多个节点上，由于 Kubernetes 所有数据都存在于 etcd 中，Etcd 是基于 Raft 协议实现，因此生产环境中 Master 通常建议至少三个节点（如果你想要更高的可用性，可以使用 5 个或者 7 个节点）。Node 节点Node 节点是 Kubernetes 的工作节点，负责运行业务容器。Node 节点主要包含两个组件： kubelet kube-proxykubelet 是在每个工作节点运行的代理 负责管理容器的生命周期 通过监听分配到自己运行的主机上的 Pod 对象，确保这些 Pod 处于运行状态，并定期检查 Pod 的运行状态，将 Pod 的运行状态更新到 Pod 对象中kube-proxy 是在每个工作节点的网络插件，实现了 Kubernetes 的 Service 的概念 通过维护集群上的网络规则，实现集群内部通过负载均衡的方式访问到后端的容器Kubernetes 的成功不仅得益于其优秀的架构设计，更加重要的是 Kubernetes 提出了很多核心的概念，这些核心概念构成了容器编排的主要模型。Kubernetes 核心概念 概念 解释 集群 一组被 Kubernetes 统一管理和调度的节点，被 Kubernetes 纳管的节点可以是物理机或者虚拟机。集群其中一部分节点作为 Master 节点，负责集群状态的管理和协调，另一部分作为 Node 节点，负责执行具体的任务，实现用户服务的启停等功能 标签（Label） 一组键值对，每个资源对象都会拥有此字段。Kubernetes 中使用 Label 对资源进行标记，然后根据 Label 对资源进行分类和筛选 命名空间（Namespace） Kubernetes 中通过命名空间来实现资源的虚拟化隔离，将一组相关联的资源放到同一个命名空间内，避免不同租户的资源发生命名冲突，从逻辑上实现了多租户的资源隔离。 容器组（Pod） 最小调度单位，由一个或多个容器组成，一个 Pod 内的容器共享相同的网络命名空间和存储卷。Pod 是真正的业务进程的载体，在 Pod 运行前，Kubernetes 会先启动一个 Pause 容器开辟一个网络命名空间，完成网络和存储相关资源的初始化，然后再运行业务容器。 部署（Deployment） 一组 Pod 的抽象，通过 Deployment 控制器保障用户指定数量的容器副本正常运行，并且实现了滚动更新等高级功能，当我们需要更新业务版本时，Deployment 会按照我们指定策略自动的杀死旧版本的 Pod 并且启动新版本的 Pod。 状态副本集（StatefulSet） 和 Deployment 类似，也是一组 Pod 的抽象，但是 StatefulSet 主要用于有状态应用的管理，StatefulSet 生成的 Pod 名称是固定且有序的，确保每个 Pod 独一无二的身份标识。 守护进程集（DaemonSet） 确保每个 Node 节点上运行一个 Pod，当集群有新加入的 Node 节点时，Kubernetes 会自动帮助我们在新的节点上运行一个 Pod。一般用于日志采集，节点监控等场景。 任务（Job） 创建一个 Pod 并且保证 Pod 的正常退出，如果 Pod 运行过程中出现了错误，Job 控制器可以创建新的 Pod，直到 Pod 执行成功或者达到指定重试次数。 服务（Service） Service 是一组 Pod 访问配置的抽象。由于 Pod 的地址是动态变化的，不能直接通过 Pod 的 IP 去访问某个服务，Service 通过在主机上配置一定的网络规则，实现通过一个固定的地址访问一组 Pod。 配置集（ConfigMap） 存放业务的配置信息，使用 Key-Value 的方式，存放于 Kubernetes 中，将配置数据和应用程序代码分开 加密字典（Secret） 存放业务的敏感配置信息，类似于 ConfigMap，使用 Key-Value 的方式存在于 Kubernaters 中，主要用于存放密码和证书等敏感信息 Kubernetes 的安装Kubernetes 目前支持在多种环境下安装，可以在公有云，私有云，甚至裸金属中安装 Kubernetes。以 Linux 平台为例，通过 minikube 来演示，如何快速安装和启动一个 Kubernetes 集群 minikube 是官方提供的一个快速搭建本地 Kubernetes 集群的工具，主要用于本地开发和调试。 在其他平台使用 minikube 安装 Kubernetes，请参考官网安装教程。在使用 minikube 安装 Kubernetes 之前，先确保已经正确安装并且启动 Docker。第一步，安装 minikube 和 kubectl。首先执行以下命令安装 minikube$ curl -LO https://github.com/kubernetes/minikube/releases/download/v1.13.1/minikube-linux-amd64$ sudo install minikube-linux-amd64 /usr/local/bin/minikubeKubectl 是 Kubernetes 官方的命令行工具，实现对 Kubernetes 集群的管理和控制。使用以下命令来安装 kubectl：$ curl -LO https://dl.k8s.io/v1.19.2/kubernetes-client-linux-amd64.tar.gz$ tar -xvf kubernetes-client-linux-amd64.tar.gzkubernetes/kubernetes/client/kubernetes/client/bin/kubernetes/client/bin/kubectl$ sudo install kubernetes/client/bin/kubectl /usr/local/bin/kubectl第二步，安装 Kubernetes 集群，执行以下命令使用 minikube 安装 Kubernetes 集群：$ minikube start执行完上述命令后，minikube 会自动帮助创建并启动一个 Kubernetes 集群。命令输出如下，当命令行输出 Done 时，代表集群已经部署完成。第三步，检查集群状态。集群安装成功后，使用以下命令检查 Kubernetes 集群是否成功启动。$ kubectl cluster-infoKubernetes master is running at https://172.17.0.3:8443KubeDNS is running at https://172.17.0.3:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.执行 kubectl cluster-info 命令后，输出 “Kubernetes master is running” 表示的集群已经成功运行。 172.17.0.3 为演示环境机器的 IP 地址，这个 IP 会根据你的实际 IP 地址而变化。创建第一个应用集群搭建好后，使用 Kubernetes 来创建第一个应用。这里使用 Deployment 来定义应用的部署信息，使用 Service 暴露应用到集群外部，从而使得应用可以从外部访问到。第一步，创建 deployment.yaml 文件，并且定义启动的副本数（replicas）为 3。apiVersion: apps/v1kind: Deploymentmetadata:  name: hello-worldspec:  replicas: 3  selector:    matchLabels:      app: hello-world  template:    metadata:      labels:        app: hello-world    spec:      containers:      - name: hello-world        image: wilhelmguo/nginx-hello:v1        ports: - containerPort: 80第二步，发布部署文件到 Kubernetes 集群中$ kubectl create -f deployment.yaml部署发布完成后，可以使用 kubectl 来查看一下 Pod 是否被成功启动。$ kubectl get pod -o wideNAME   READY   STATUS    RESTARTS   AGE     IP    NODE       NOMINATED NODE   READINESS GATEShello-world-57968f9979-xbmzt   1/1     Running   0     3m19s   172.18.0.7   minikube   &amp;lt;none&amp;gt;    &amp;lt;none&amp;gt;hello-world-57968f9979-xq5w4   1/1     Running   0     3m18s   172.18.0.5   minikube   &amp;lt;none&amp;gt;    &amp;lt;none&amp;gt;hello-world-57968f9979-zwvgg   1/1     Running   0     4m14s   172.18.0.6   minikube   &amp;lt;none&amp;gt;    &amp;lt;none&amp;gt;这里可以看到 Kubernetes 创建了 3 个 Pod 实例。第三步，创建 service.yaml 文件，将服务暴露出去，内容如下：apiVersion: v1kind: Servicemetadata:  name: hello-worldspec:  type: NodePort  ports:  - port: 80    targetPort: 80  selector: app: hello-world然后执行如下命令在 Kubernetes 中创建 Service：kubectl create -f service.yaml服务创建完成后，Kubernetes 会随机分配一个外部访问端口，可以通过以下命令查看服务信息：$ kubectl  get service -o wideNAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE   SELECTORhello-world   NodePort    10.101.83.18   &amp;lt;none&amp;gt;        80:32391/TCP   12s   app=hello-worldkubernetes    ClusterIP   10.96.0.1      &amp;lt;none&amp;gt;        443/TCP        40m   &amp;lt;none&amp;gt;由于集群使用 minikube 安装，要想集群中的服务可以通过外部访问，还需要执行以下命令：$ minikube service hello-world输出如下：可以看到 minikube 将服务暴露在了 32391 端口上，通过 http://{U-IP}:32391 可以访问到已启动的服务，如下图所示。" }, { "title": "Docker Swarm（10）", "url": "/posts/docker-Swarm/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker, Docker Swarm", "date": "2020-08-14 15:33:00 +0000", "snippet": "Docker Compose 是 Docker 的单节点引擎工具，它能够在单一节点上管理和编排多个容器。当服务和容器数量较小时可以使用 Docker Compose 来管理容器。然而随着业务规模越来越大，容器规模也逐渐增大时，数量庞大的容器管理将带来许多挑战。Docker 官方为了解决多容器管理的问题推出了 Docker Swarm ，可以用它来管理规模更大的容器集群。Swarm 的前生今世2014 年 Docker 在容器界越来越火，这时容器的编排工具 Mesos 和 Kubernetes 开始崭露头角。此时，Docker 公司也开始筹划容器的编排和集群管理工具，推出了自己的通信协议项目 Beam。后来，通过改进 Beam，Beam 成为一个允许使用 Docker API 来控制的一种分布式系统，之后项目被重命名为 libswarm在 2014 年 11 月，Docker 公司又对 libswarm 进行了重新设计，支持了远程调用 API，并且被重新命名为 Swarm。称之为 Swarm V1在 2016 年，为了解决中央服务可扩展性的问题，Docker 团队重新设计了 Swarm，并称之为 Swarm V2此时的 Docker Swarm 已经可以支持超过 1000 多个节点的集群规模，并且 Docker 团队在发布 Docker 1.12 版本时，将 Docker Swarm 默认集成到了 Docker 引擎中由于 Swarm 是 Docker 官方推出的容器集群管理工具，因此 Swarm 最大的优势之一就是原生支持 Docker API，带来了极大的便利，原来的 Docker 用户可以很方便地将服务迁移到 Swarm 中来。与此同时，Swarm 还内置了对 Docker 网络插件的支持，因此可以很方便地部署需要跨主机通信的容器集群。其实 Swarm 的优点远远不止这些，还有很多，例如以下优点。 分布式： Swarm 使用Raft（一种分布式一致性协议）协议来做集群间数据一致性保障，使用多个容器节点组成管理集群，从而避免单点故障。 安全： Swarm 使用 TLS 双向认证来确保节点之间通信的安全，它可以利用双向 TLS 进行节点之间的身份认证，角色授权和加密传输，并且可以自动执行证书的颁发和更换。 简单： Swarm 的操作非常简单，并且除 Docker 外基本无其他外部依赖，而且从 Docker 1.12 版本后， Swarm 直接被内置到了 Docker 中，可以说真正做到了开箱即用。Swarm 的这些优点得益于它优美的架构设计。Swarm 的架构Swarm 的架构整体分为： 管理节点（Manager Nodes） 工作节点（Worker Nodes）整体架构如下图：管理节点： 管理节点，负责接受用户请求，用户请求中包含，用户定义的容器运行状态描述 然后 Swarm 负责调度和管理容器，并且努力达到用户所期望的状态工作节点： 工作节点运行执行器（Executor）负责执行具体的容器管理任务（Task） 例如容器的启动、停止、删除等操作 管理节点和工作节点的角色并不是一成不变的，可以手动将工作节点转换为管理节点，也可以将管理节点转换为工作节点。Swarm 核心概念在真正使用 Swarm 之前，需要了解几个 Swarm 核心概念，这些核心概念可以帮助更好地学习和理解 Swarm 设计理念。Swarm 集群Swarm 集群是一组被 Swarm 统一管理和调度的节点，被 Swarm纳管的节点可以是物理机或者虚拟机。 其中一部分节点作为管理节点，负责集群状态的管理和协调 另一部分作为工作节点，负责执行具体的任务来管理容器，实现用户服务的启停等功能。节点Swarm 集群中的每一台物理机或者虚拟机称为节点。节点按照工作职责分为管理节点和工作节点，管理节点由于需要使用 Raft 协议来协商节点状态，生产环境中通常建议将管理节点的数量设置为奇数个，一般为 3 个、5 个或 7 个。服务服务是为了支持容器编排所提出的概念，是一系列复杂容器环境互相协作的统称。一个服务的声明通常包含: 容器的启动方式 启动的副本数 环境变量 存储 配置 网络等一系列配置用户通过声明一个服务，将它交给 Swarm，Swarm 负责将用户声明的服务实现。服务分为全局服务（global services）和副本服务（replicated services）。 全局服务 每个工作节点上都会运行一个任务 类似于 Kubernetes 中的 Daemonset。 副本服务 按照指定的副本数在整个集群中调度运行 任务任务是集群中的最小调度单位，它包含一个真正运行中的 Docker 容器。当管理节点，根据服务中声明的副本数，将任务调度到节点时，任务则开始在该节点启动和运行。当节点出现异常时，任务会运行失败。此时调度器会把失败的任务，重新调度到其他正常的节点上正常运行，以确保运行中的容器副本数满足用户所期望的副本数服务外部访问由于容器的 IP 只能在集群内部访问到，而且容器又是用后马上销毁，这样容器的 IP 也会动态变化。因此容器集群内部的服务，想要被集群外部的用户访问到，服务必须映射到主机上的固定端口。Swarm 使用入口负载均衡（ingress load balancing）的模式将服务暴露在主机上，该模式下，每一个服务会被分配一个公开端口（PublishedPort），可以指定使用某个未被占用的公开端口，也可以让 Swarm 自动分配一个。Swarm 集群的公开端口，可以从集群内的任意节点上访问到，当请求达到集群中的一个节点时，如果该节点没有要请求的服务，则会将请求转发到实际运行该服务的节点上，从而响应用户的请求。公有云的云负载均衡器（cloud load balancers）可以利用这一特性将流量导入到集群中的一个或多个节点，从而实现利用公有云的云负载均衡器将流量导入到集群中的服务搭建 Swarm 集群使用 Swarm 集群有如下一些要求： Docker 版本大于 1.12，推荐使用最新稳定版 Docker； 主机需要开放一些端口（TCP：2377 UDP:4789 TCP 和 UDP:7946）。我通过四台机器来搭建一个 Swarm 集群，演示的节点规划如下： 节点名称 节点 IP 角色 swarm-manager 192.168.31.100 manger swarm-node1 192.168.31.101 node swarm-node2 192.168.31.102 node swarm-node3 192.168.31.103 node 生产环境中推荐使用至少三个 manager 作为管理节点。初始化集群Docker 1.12 版本后， Swarm 已经默认集成到 Docker 中，因此可以直接使用 Docker 命令来初始化 Swarm，集群初始化的命令格式如下：docker swarm init --advertise-addr &amp;lt;YOUR-IP&amp;gt; advertise-addr 一般用于主机有多块网卡的情况，如果主机只有一块网卡，可以忽略此参数在管理节点上，通过以下命令初始化集群：$ docker swarm initSwarm initialized: current node (1ehtnlcf3emncktgjzpoux5ga) is now a manager.To add a worker to this swarm, run the following command:docker swarm join --token SWMTKN-1-1kal5b1iozbfmnnhx3kjfd3y6yqcjjjpcftrlg69pm2g8hw5vx-8j4l0t2is9ok9jwwc3tovtxbp 192.168.31.100:2377To add a manager to this swarm, run &#39;docker swarm join-token manager&#39; and follow the instructions.加入工作节点按照第一步集群初始化后输出的提示，只需要复制其中的命令即可，然后在剩余的三台工作节点上分别执行如下命令：$ docker swarm join --token SWMTKN-1-1kal5b1iozbfmnnhx3kjfd3y6yqcjjjpcftrlg69pm2g8hw5vx-8j4l0t2is9ok9jwwc3tovtxbp 192.168.31.100:2377This node joined a swarm as a worker.默认加入的节点为工作节点，如果是生产环境，可以使用docker swarm join-token manager命令来查看如何加入管理节点：$ docker swarm join-to ken managerTo add a manager to this swarm, run the following command:    docker swarm join --token SWMTKN-1-1kal5b1iozbfmnnhx3kjfd3y6yqcjjjpcftrlg69pm2g8hw5vx-8fq89jxo2axwggryvom5a337t 192.168.31.100:2377 复制 Swarm 输出的结果即可加入管理节点到集群中 管理节点的数量必须为奇数，生产环境推荐使用3个、5个或7个管理节点来管理 Swarm 集群（因为要选主，超过一半的投票才可以被选为主节点，偶数节点会出现一半一半的情况）节点查看节点添加完成后，使用以下命令可以查看当前节点的状态：$ docker node lsID                          HOSTNAME      STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION1ehtnlcf3emncktgjzpoux5ga * swarm-manager Ready  Active       Leader         19.03.12pn7gdm847sfzydqhcv3vma97y * swarm-node1   Ready  Active                      19.03.124dtc9pw5quyjs5yf25ccgr8uh * swarm-node2   Ready  Active                      19.03.12est7ww3gngna4u7td22g9m2k5 * swarm-node3   Ready  Active                      19.03.12到此，一个包含 1 个管理节点，3 个工作节点的 Swarm 集群已经搭建完成。使用 Swarm集群搭建完成后，可以在 Swarm 集群中创建服务了，Swarm 集群中常用的服务部署方式有以下两种。通过 docker service 命令创建服务使用 docker service create 命令可以创建服务，创建服务的命令如下：$ docker service create --replicas 1 --name hello-world nginx24f9ng83m9sq4ml3e92k4g5byoverall progress: 1 out of 1 tasks1/1: running   [==================================================&amp;gt;]verify: Service converged此时已经创建好了一个服务，使用 docker service ls 命令可以查看已经启动的服务：$ docker service lsID           NAME        MODE       REPLICAS IMAGE        PORTS24f9ng83m9sq hello-world replicated 1/1      nginx:latest当不再需要这个服务了，可以使用 docker service rm 命令来删除服务：$ docker service rm hello-worldhello-world此时 hello-world 这个服务已经成功地从集群中删除。了解更多的 docker service 命令的相关操作，可以参考这里。生产环境中，推荐使用 docker-compose 模板文件来部署服务，这样服务的管理会更加方便并且可追踪，而且可以同时创建和管理多个服务，更加适合生产环境中依赖关系较复杂的部署模式。通过 docker stack 命令创建服务在上文中成功的使用该模板文件创建并启动了 MySQL 服务和 WordPress 两个服务。这里将上文中的 docker-compose 模板文件略微改造一下：version: &#39;3&#39;services:   mysql:     image: mysql:5.7     volumes: mysql_data:/var/lib/mysql     restart: always     environment:       MYSQL_ROOT_PASSWORD: root       MYSQL_DATABASE: mywordpress       MYSQL_USER: mywordpress       MYSQL_PASSWORD: mywordpress   wordpress:     depends_on: mysql     image: wordpress:php7.4     deploy:       mode: replicated       replicas: 2     ports: &quot;8080:80&quot;     restart: always     environment:       WORDPRESS_DB_HOST: mysql:3306       WORDPRESS_DB_USER: mywordpress       WORDPRESS_DB_PASSWORD: mywordpress       WORDPRESS_DB_NAME: mywordpressvolumes:    mysql_data: {}在服务模板文件中添加了 deploy 指令，并且指定使用副本服务（replicated）的方式启动两个 WordPress 实例。准备好启动 WordPress 服务的配置后，在 /tmp 目下新建 docker-compose.yml 文件，并且写入以上的内容，然后使用以下命令启动服务：$ docker stack deploy -c docker-compose.yml wordpressIgnoring unsupported options: restartCreating network wordpress_defaultCreating service wordpress_mysqlCreating service wordpress_wordpress执行完以上命令后，成功启动了两个服务： MySQL 服务，默认启动了一个副本。 WordPress 服务，根据 docker-compose 模板的定义启动了两个副本。下面用 docker service ls 命令查看一下当前启动的服务$ docker service lsID            NAME               MODE       REPLICAS IMAGE            PORTSv8i0pzb4e3tc wordpress_mysql     replicated 1/1      mysql:5.796m8xfyeqzr5 wordpress_wordpress replicated 2/2      wordpress:php7.4 *:8080-&amp;gt;80/tcp可以看到，Swarm 已经为成功启动了一个 MySQL 服务，并且启动了两个 WordPress 实例。WordPress 实例通过 8080 端口暴露在了主机上.通过访问集群中的任意节点的 IP 加 8080 端口即可访问到 WordPress 服务。例如，访问http://192.168.31.101:8080 即可成功访问到搭建的 WordPress 服务。总结Docker Swarm 是一个用来定义复杂应用的集群编排工具，可以把多台主机组成一个 Swarm 集群，并且帮助管理和调度复杂的容器服务。由于 Swarm 已经被内置于 Docker 中，因此 Swarm 的安装和使用也变得非常简单。" }, { "title": "Docker Compose（09）", "url": "/posts/docker-compose/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker", "date": "2020-08-11 15:33:00 +0000", "snippet": "Docker Compose 的前世今生Docker Compose 的前身是 Orchard 公司开发的 Fig，2014 年 Docker 收购了 Orchard 公司，然后将 Fig 重命名为 Docker Compose。现阶段 Docker Compose 是 Docker 官方的单机多容器管理系统 本质是一个 Python 脚本，通过解析用户编写的 yaml 文件，调用 Docker API 实现动态的创建和管理多个容器。安装 Docker ComposeDocker Compose 可以安装在 macOS、 Windows 和 Linux 系统中。其中在 macOS 和 Windows 系统下 ，Docker Compose 都是随着 Docker 的安装一起安装好的，因此，着重总结如何在 Linux 系统下安装 Docker Compose。Linux 系统下安装 Docker Compose在安装 Docker Compose 之前，先确保的机器已经正确运行了 Docker，如果还没有安装 Docker，请参考官方网站安装 Docker。要在 Linux 平台上安装 Docker Compose，需要到 Compose 的 Github 页面下载对应版本的安装包。这里以 1.27.3 版本为例，安装一个 Docker Compose，如下： 使用 curl 命令（一种发送 http 请求的命令行工具）下载 Docker Compose 的安装包： $ sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.27.3/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose 如果安装其他版本的 Docker Compose，将 1.27.3 替换直接替换安装的版本即可 修改 Docker Compose 执行权限 $ sudo chmod +x /usr/local/bin/docker-compose 检查 Docker Compose 是否安装成功 $ docker-compose --versiondocker-compose version 1.27.3, build 1110ad01 当执行完上述命令后，如果 Docker Compose 输出了当前版本号，就表示 Docker Compose 已经安装成功。 Docker Compose 安装成功后，就可以很方便地使用它了。在使用 Docker Compose 之前，首先需要先编写 Docker Compose 模板文件，因为 Docker Compose 运行的时候是根据 Docker Compose 模板文件中的定义来运行的。编写 Docker Compose 模板文件在使用 Docker Compose 启动容器时， Docker Compose 会默认使用 docker-compose.yml 文件， docker-compose.yml 文件的格式为 yaml（类似于 json，一种标记语言）Docker Compose 模板文件一共有三个版本： v1、v2 和 v3。目前最新的版本为 v3，也是功能最全面的一个版本，下面我主要围绕 v3 版本介绍一下如何编写 Docker Compose 文件。Docker Compose 文件主要分为三部分： services（服务）、networks（网络） 和 volumes（数据卷）。 services（服务）：服务定义了容器启动的各项配置，就像 docker run 命令时传递的容器启动的参数一样，指定了容器应该如何启动，例如容器的启动参数，容器的镜像和环境变量等。 networks（网络）：网络定义了容器的网络配置，就像 docker network create 命令创建网络配置一样。 volumes（数据卷）：数据卷定义了容器的卷配置，就像执行 docker volume create 命令创建数据卷一样。一个典型的 Docker Compose 文件结构如下：version: &quot;3&quot;services: nginx: ## ... 省略部分配置networks: frontend: backend:volumes: db-data:编写 Service 配置services 下，首先需要定义服务名称，假如服务是 nginx 服务，可以定义 service 名称为 nginx，格式如下：version: &quot;3.8&quot;services: nginx: 服务名称定义完毕后，需要在服务名称的下一级定义当前服务的各项配置，使得服务可以按照配置正常启动。常用的 16 种 service 配置如下：build 用于构建 Docker 镜像，类似于 docker build 命令 build 可以指定 Dockerfile 文件路径，然后根据 Dockerfile 命令来构建文件build: ## 构建执行的上下文目录 context: . ## Dockerfile 名称 dockerfile: Dockerfile-namecap_add、cap_drop 指定容器可以使用到哪些内核能力（capabilities）cap_add: - NET_ADMINcap_drop: - SYS_ADMINcommand 用于覆盖容器默认的启动命令，它和 Dockerfile 中的 CMD 用法类似# 方式一command: sleep 3000# 方式二command: [&quot;sleep&quot;, &quot;3000&quot;]container_name 用于指定容器启动时容器的名称container_name: nginxdepends_on 用于指定服务间的依赖关系，这样可以先启动被依赖的服务。例如，服务依赖数据库服务 db，可以指定 depends_on 为 dbversion: &quot;3.8&quot;services: my-web: build: . depends_on: - db db: image: mysqldevices 挂载主机的设备到容器中devices: - &quot;/dev/sba:/dev/sda&quot;dns 自定义容器中的 dns 配置devices: - &quot;/dev/sba:/dev/sda&quot;dns_search 配置 dns 的搜索域dns_search: - svc.cluster.com - svc1.cluster.comentrypoint： 覆盖容器的 entrypoint 命令# 方式一entrypoint: sleep 3000# 方式二entrypoint: [&quot;sleep&quot;, &quot;3000&quot;]env_file： 指定容器的环境变量文件 启动时会把该文件中的环境变量值注入容器中env_file: - ./dbs.envenv 文件的内容格式如下：KEY_ENV=valuesenvironment 指定容器启动时的环境变量```yamlenvironment: KEY_ENV=values``` image 指定容器镜像的地址 image: busybox:latest pid 共享主机的进程命名空间 像在主机上直接启动进程一样，可以看到主机的进程信息 image: busybox:latest ports 暴露端口信息，使用格式为 HOST:CONTAINER， 前面填写要映射到主机上的端口，后面填写对应的容器内的端口```yamlports: “1000” “1000-1005” “8080:8080” “8888-8890:8888-8890” “2222:22” “127.0.0.1:9999:9999” “127.0.0.1:3000-3005:3000-3005” “6789:6789/udp”``` networks 这是服务要使用的网络名称，对应顶级的 networks 中的配置。volume 不仅可以挂载主机数据卷到容器中，也可以直接挂载主机的目录到容器中 使用方式类似于使用 docker run 启动容器时添加 -v 参数 不仅长语法外，还支持短语法的书写方式# 长语法version: &quot;3&quot;services: db: image: mysql:5.6 volumes: - type: volume source: /var/lib/mysql target: /var/lib/mysql# 短语法version: &quot;3&quot;services: db: image: mysql:5.6 volumes: - /var/lib/mysql:/var/lib/mysql编写 Volume 配置如果多个容器间实现共享数据卷，则需要在外部声明数据卷，然后在容器里声明使用数据卷。例如在两个服务间共享日志目录，则使用以下配置：version: &quot;3&quot;services: my-service1: image: service:v1 volumes: - type: volume source: logdata target: /var/log/mylog my-service2: image: service:v2 volumes: - type: volume source: logdata target: /var/log/mylogvolumes: logdata:编写 Network 配置Docker Compose 文件顶级声明的 networks 允许你创建自定义的网络，类似于 docker network create 命令。例如声明一个自定义 bridge 网络配置，并且在服务中使用它，使用格式如下：docker-compose [-f &amp;lt;arg&amp;gt;...] [options] [--] [COMMAND] [ARGS...]其中 options 是 docker-compose 的参数，支持的参数和功能说明如下：  -f, --file FILE             指定 docker-compose 文件，默认为 docker-compose.yml  -p, --project-name NAME     指定项目名称，默认使用当前目录名称作为项目名称  --verbose                   输出调试信息  --log-level LEVEL           日志级别 (DEBUG, INFO, WARNING, ERROR, CRITICAL)  -v, --version               输出当前版本并退出  -H, --host HOST             指定要连接的 Docker 地址  --tls                       启用 TLS 认证  --tlscacert CA_PATH         TLS CA 证书路径  --tlscert CLIENT_CERT_PATH  TLS 公钥证书问价  --tlskey TLS_KEY_PATH       TLS 私钥证书文件  --tlsverify                 使用 TLS 校验对端  --skip-hostname-check       不校验主机名  --project-directory PATH    指定工作目录，默认是 Compose 文件所在路径COMMAND 为 docker-compose 支持的命令。支持的命令如下：  build              构建服务  config             校验和查看 Compose 文件  create             创建服务  down               停止服务，并且删除相关资源  events             实时监控容器的时间信息  exec               在一个运行的容器中运行指定命令  help               获取帮助  images             列出镜像  kill               杀死容器  logs               查看容器输出  pause              暂停容器  port               打印容器端口所映射出的公共端口  ps                 列出项目中的容器列表  pull               拉取服务中的所有镜像  push               推送服务中的所有镜像  restart            重启服务  rm                 删除项目中已经停止的容器  run                在指定服务上运行一个命令  scale              设置服务运行的容器个数  start              启动服务  stop               停止服务  top                限制服务中正在运行中的进程信息  unpause            恢复暂停的容器  up                 创建并且启动服务  version            打印版本信息并退出使用 Docker Compose 管理 WordPress启动 WordPress(1) 创建项目目录。在 /tmp 目录下创建一个 WordPress 的目录，这个目录将作为工作目录$ mkdir /tmp/wordpress(2) 进入工作目录$ cd /tmp/wordpress(3) 创建 docker-compose.yml 文件$ touch docker-compose.yml然后写入以下内容version: &#39;3&#39;services:mysql:image: mysql:5.7volumes:- mysql_data:/var/lib/mysqlrestart: alwaysenvironment:MYSQL_ROOT_PASSWORD: rootMYSQL_DATABASE: mywordpressMYSQL_USER: mywordpressMYSQL_PASSWORD: mywordpresswordpress:depends_on:- mysqlimage: wordpress:php7.4ports:- &quot;8080:80&quot;restart: alwaysenvironment:WORDPRESS_DB_HOST: mysql:3306WORDPRESS_DB_USER: mywordpressWORDPRESS_DB_PASSWORD: mywordpressWORDPRESS_DB_NAME: mywordpressvolumes: mysql_data: {}(4) 启动 MySQL 数据库和 WordPress 服务$ docker-compose up -dStarting wordpress_mysql_1 ... doneStarting wordpress_wordpress_1 ... done执行完以上命令后，Docker Compose 首先会启动一个 MySQL 数据库，按照 MySQL 服务中声明的环境变量来设置 MySQL 数据库的用户名和密码。然后等待 MySQL 数据库启动后，再启动 WordPress 服务。WordPress 服务启动后，就可以通过 http://localhost:8080 访问它了，然后按照提示一步一步设置就可以拥有属于自己的专属博客系统了。停止 WordPress如果不需要 WordPress 服务了，可以使用 docker-compose stop 命令来停止已启动的服务$ docker-compose stopStopping wordpress_wordpress_1 ... doneStopping wordpress_mysql_1     ... done" }, { "title": "Docker 监控（08）", "url": "/posts/docker-actour/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker", "date": "2020-08-09 15:33:00 +0000", "snippet": "生产环境中监控容器的运行状况十分重要，往往通过监控可以随时掌握容器的运行状态，做到线上隐患和问题早发现，早解决。虽然传统的物理机和虚拟机监控已经有了比较成熟的监控方案，但是容器的监控面临着更大的挑战，因为容器的行为和本质与传统的虚拟机是不一样的，总的来说，容器具有以下特性： 容器是短期存活的，并且可以动态调度； 容器的本质是进程，而不是一个完整操作系统； 由于容器非常轻量，容器的创建和销毁也会比传统虚拟机更加频繁。Docker 容器的监控方案有很多，除了 Docker 自带的docker stats命令，还有很多开源的解决方案，例如 sysdig、cAdvisor、Prometheus 等，都是非常优秀的监控工具。docker stats 命令使用 Docker 自带的 docker stats 命令可以很方便地看到主机上所有容器的 CPU、内存、网络 IO、磁盘 IO、PID 等资源的使用情况。下面我们可以具体操作看看。首先在主机上使用以下命令启动一个资源限制为 1 核 2G 的 nginx 容器：$ docker run --cpus=1 -m=2g --name=nginx  -d nginx容器启动后，可以使用 docker stats 命令查看容器的资源使用状态:$ docker stats nginx通过 docker stats 命令可以看到容器的运行状态如下：CONTAINER     CPU %   MEM USAGE/LIMIT  MEM %  NET I/O         BLOCK I/O       PIDSf742a467b6d8  0.00%   1.387 MiB/2 GiB  0.07%  656 B / 656 B   0 B / 9.22 kB   2从容器的运行状态可以看出，docker stats命令确实可以获取并显示 Docker 容器运行状态。但是它的缺点也很明显，因为它只能获取本机数据，无法查看历史监控数据，没有可视化展示面板。因此，生产环境中我们通常使用另一种容器监控解决方案 cAdvisor。CAdvisorcAdvisor 是谷歌开源的一款通用的容器监控解决方案。cAdvisor 不仅可以采集机器上所有运行的容器信息，还提供了基础的查询界面和 HTTP 接口，更方便与外部系统结合。所以，cAdvisor很快成了容器指标监控最常用组件，并且 Kubernetes 也集成了 cAdvisor 作为容器监控指标的默认工具。CAdvisor 的安装与使用以 cAdvisor 0.37.0 版本为例，演示一下 cAdvisor 的安装与使用。cAdvisor 官方提供了 Docker 镜像，只需要拉取镜像并且启动镜像即可。(1) 使用以下命令启动 cAdvisor$ docker run \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:ro \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --volume=/dev/disk/:/dev/disk:ro \\ --publish=8080:8080 \\ --detach=true \\ --name=cadvisor \\ --privileged \\ --device=/dev/kmsg \\ lagoudocker/cadvisor:v0.37.0(2) 通过上面的命令，cAdvisor 已经成功启动，然后通过访问 http://localhost:8080 访问到 cAdvisor 的 Web 界面 cAdvisor 不仅可以监控容器的资源使用情况，还可以监控主机的资源使用情况。使用 CAdvisor 查看主机监控访问 http://localhost:8080/containers/ 地址，在首页可以看到主机的资源使用情况，包含 CPU、内存、文件系统、网络等资源，如下图所示：使用 CAdvisor 查看容器监控查看主机上运行的容器资源使用情况，可以访问 http://localhost:8080/docker/，这个页面会列出 Docker 的基本信息和运行的容器情况，如下图所示：在上图中的 Subcontainers 下会列出当前主机上运行的所有容器，点击其中一个容器即可查看该容器的详细运行状态，如下图所示：总体来说，使用 cAdvisor 监控容器具有以下特点： 同时采集物理机和容器的状态 展示监控历史数据监控原理Docker 是基于 Namespace、Cgroups 和联合文件系统实现的。其中 Cgroups 不仅可以用于容器资源的限制，还可以提供容器的资源使用率。无论何种监控方案的实现，底层数据都来源于 Cgroups。Cgroups 的工作目录为/sys/fs/cgroup，/sys/fs/cgroup目录下包含了 Cgroups 的所有内容。Cgroups包含很多子系统，可以用来对不同的资源进行限制。例如对CPU、内存、PID、磁盘 IO等资源进行限制和监控。为了更详细的了解 Cgroups 的子系统，通过 ls -l 命令查看/sys/fs/cgroup文件夹，可以看到很多目录：$ sudo ls -l /sys/fs/cgroup/total 0dr-xr-xr-x 5 root root  0 Jul  9 19:32 blkiolrwxrwxrwx 1 root root 11 Jul  9 19:32 cpu -&amp;gt; cpu,cpuacctdr-xr-xr-x 5 root root  0 Jul  9 19:32 cpu,cpuacctlrwxrwxrwx 1 root root 11 Jul  9 19:32 cpuacct -&amp;gt; cpu,cpuacctdr-xr-xr-x 3 root root  0 Jul  9 19:32 cpusetdr-xr-xr-x 5 root root  0 Jul  9 19:32 devicesdr-xr-xr-x 3 root root  0 Jul  9 19:32 freezerdr-xr-xr-x 3 root root  0 Jul  9 19:32 hugetlbdr-xr-xr-x 5 root root  0 Jul  9 19:32 memorylrwxrwxrwx 1 root root 16 Jul  9 19:32 net_cls -&amp;gt; net_cls,net_priodr-xr-xr-x 3 root root  0 Jul  9 19:32 net_cls,net_priolrwxrwxrwx 1 root root 16 Jul  9 19:32 net_prio -&amp;gt; net_cls,net_priodr-xr-xr-x 3 root root  0 Jul  9 19:32 perf_eventdr-xr-xr-x 5 root root  0 Jul  9 19:32 pidsdr-xr-xr-x 5 root root  0 Jul  9 19:32 systemd这些目录代表了 Cgroups 的子系统，Docker 会在每一个 Cgroups 子系统下创建 docker 文件夹。这些目录代表了 Cgroups 的子系统，Docker 会在每一个 Cgroups 子系统下创建 docker 文件夹。由此，容器监控数据来源于 Cgroups 即可。获取容器的内存限制以 memory 子系统（memory 子系统是Cgroups 众多子系统的一个，主要用来限制内存使用，Cgroups 会在第十课时详细讲解）为例，讲解一下监控组件是如何获取到容器的资源限制和使用状态的（即容器的内存限制）。首先在主机上使用以下命令启动一个资源限制为 1 核 2G 的 nginx 容器：$ docker run --name=nginx --cpus=1 -m=2g --name=nginx  -d nginx## 这里输出的是容器 ID51041a74070e9260e82876974762b8c61c5ed0a51832d74fba6711175f89ede1 注意：如果已经创建过名称为 nginx 的容器，先使用 docker rm -f nginx 命令删除已经存在的 nginx 容器。容器启动后，通过命令行的输出可以得到容器的 ID，同时 Docker 会在 /sys/fs/cgroup/memory/docker 目录下以容器 ID 为名称创建对应的文件夹。下面是 /sys/fs/cgroup/memory/docker目录下的文件：$ sudo ls -l /sys/fs/cgroup/memory/dockertotal 0drwxr-xr-x 2 root root 0 Sep  2 15:12 51041a74070e9260e82876974762b8c61c5ed0a51832d74fba6711175f89ede1-rw-r--r-- 1 root root 0 Sep  2 14:57 cgroup.clone_children--w--w--w- 1 root root 0 Sep  2 14:57 cgroup.event_control-rw-r--r-- 1 root root 0 Sep  2 14:57 cgroup.procs-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.failcnt--w------- 1 root root 0 Sep  2 14:57 memory.force_empty-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.kmem.failcnt-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.kmem.limit_in_bytes-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.kmem.max_usage_in_bytes-r--r--r-- 1 root root 0 Sep  2 14:57 memory.kmem.slabinfo-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.kmem.tcp.failcnt-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.kmem.tcp.limit_in_bytes-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.kmem.tcp.max_usage_in_bytes-r--r--r-- 1 root root 0 Sep  2 14:57 memory.kmem.tcp.usage_in_bytes-r--r--r-- 1 root root 0 Sep  2 14:57 memory.kmem.usage_in_bytes-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.limit_in_bytes-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.max_usage_in_bytes-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.memsw.failcnt-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.memsw.limit_in_bytes-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.memsw.max_usage_in_bytes-r--r--r-- 1 root root 0 Sep  2 14:57 memory.memsw.usage_in_bytes-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.move_charge_at_immigrate-r--r--r-- 1 root root 0 Sep  2 14:57 memory.numa_stat-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.oom_control---------- 1 root root 0 Sep  2 14:57 memory.pressure_level-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.soft_limit_in_bytes-r--r--r-- 1 root root 0 Sep  2 14:57 memory.stat-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.swappiness-r--r--r-- 1 root root 0 Sep  2 14:57 memory.usage_in_bytes-rw-r--r-- 1 root root 0 Sep  2 14:57 memory.use_hierarchy-rw-r--r-- 1 root root 0 Sep  2 14:57 notify_on_release-rw-r--r-- 1 root root 0 Sep  2 14:57 tasks由上可以看到，容器 ID 的目录下有很多文件，其中 memory.limit_in_bytes 文件代表该容器内存限制大小，单位为 byte，使用 cat 命令（cat 命令可以查看文件内容）查看一下文件内容：$ sudo cat /sys/fs/cgroup/memory/docker/51041a74070e9260e82876974762b8c61c5ed0a51832d74fba6711175f89ede1/memory.limit_in_bytes2147483648这里可以看到 memory.limit_in_bytes 的值为2147483648，转换单位后正好为 2G，正符合启动容器时的内存限制 2G。通过 memory 子系统的例子，可以知道监控组件通过读取 memory.limit_in_bytes 文件即可获取到容器内存的限制值。获取容器的内存使用状态内存使用情况存放在 memory.usage_in_bytes 文件里，同样也使用 cat 命令查看一下文件内容：$ sudo cat /sys/fs/cgroup/memory/docker/51041a74070e9260e82876974762b8c61c5ed0a51832d74fba6711175f89ede1/memory.usage_in_bytes4259840可以看到当前内存的使用大小为 4259840 byte，约为 4 M。了解了内存的监控。网络的监控数据来源是从 /proc/{PID}/net/dev 目录下读取的，其中 PID 为容器在主机上的进程 ID。下面我们首先使用 docker inspect 命令查看一下上面启动的 nginx 容器的 PID，命令如下：$ docker inspect nginx |grep Pid            &quot;Pid&quot;: 27348,            &quot;PidMode&quot;: &quot;&quot;,            &quot;PidsLimit&quot;: 0,可以看到容器的 PID 为 27348，使用 cat 命令查看一下 /proc/27348/net/dev 的内容：$ sudo cat /proc/27348/net/devInter-|   Receive                                                |  Transmit face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed    lo:       0       0    0    0    0     0          0         0        0       0    0    0    0     0       0          0  eth0:       0       0    0    0    0     0          0         0        0       0    0    0    0     0       0          0/proc/27348/net/dev 文件记录了该容器里每一个网卡的流量接收和发送情况，以及错误数、丢包数等信息。可见容器的网络监控数据都是定时从这里读取并展示的。总之，容器的监控原理其实就是定时读取 Linux 主机上相关的文件并展示给用户CAdvisor 的不足cAdvisor 虽然可以临时存储一段历史监控数据，并且提供了一个简版的监控面板，在大规模的容器集群中，cAdvisor 有明显的不足： cAdvisor只能监控某一台主机及其上面的容器，虽然有可视化和存储，但是大规模情况下不集中，无聚合解决办法是： cadvisor 结合 prometheus 一起使用。因为 cAdvisor 是提供监控数据的，Prometheus 是负责采集的数据的，这两个作用是不一样的，生产集群中一般都是 cAdvisor 配合 Prometheus 一起使用（或者使用 grafana +promethues ）。" }, { "title": "Docker 安全（07）", "url": "/posts/docker-safe/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker", "date": "2020-08-08 15:33:00 +0000", "snippet": "Docker 是基于 Linux 内核的 Namespace 技术实现资源隔离的，所有的容器都共享主机的内核。其实这与以虚拟机为代表的云计算时代还是有很多区别的，比如虚拟机有着更好的隔离性和安全性，而容器的隔离性和安全性则相对较弱。Docker 与虚拟机区别虚拟机虚拟机是通过管理系统(Hypervisor)模拟出 CPU、内存、网络等硬件，然后在这些模拟的硬件上创建客户内核和操作系统。这样做的好处是： 虚拟机有自己的内核和操作系统， 硬件都是通过虚拟机管理系统模拟出来的，用户程序无法直接使用到主机的操作系统和硬件资源因此虚拟机对隔离性和安全性有着更好的保证。Docker 容器而 Docker 容器是通过 Linux 内核的 Namespace 技术实现了文件系统、进程、设备以及网络的隔离，然后再通过 Cgroups 对 CPU、 内存等资源进行限制，最终实现了容器之间相互不受影响，由于容器的隔离性仅仅依靠内核来提供，因此容器的隔离性也远弱于虚拟机。既然虚拟机安全性这么好，为什么我们还要用容器呢？这是因为容器与虚拟机相比，容器具有以下的优点： 性能损耗非常小 镜像也非常小 在业务快速开发和迭代的今天，容器秒级的启动等特性也非常匹配业务快速迭代的业务场景Docker 容器的安全问题Docker 自身安全Docker 作为一款容器引擎，本身存在一些安全漏洞，CVE 目前已经记录了多项与 Docker 相关的安全漏洞，主要有：权限提升、信息泄露等几类安全问题。具体 Docker 官方记录的安全问题可以参考这里。 CVE 的维基百科定义：CVE 是公共漏洞和暴露（英语：CVE, Common Vulnerabilities and Exposures）又称常见漏洞与披露，是一个与信息安全有关的数据库，收集各种信息安全弱点及漏洞并给予编号以便于公众查阅。此数据库现由美国非营利组织 MITRE 所属的 National Cybersecurity FFRDC 所营运维护镜像安全由于 Docker 容器是基于镜像创建并启动，因此镜像的安全直接影响到容器的安全。具体影响镜像安全的总结如下： 镜像软件存在安全漏洞：由于容器需要安装基础的软件包，如果软件包存在漏洞，则可能会被不法分子利用并且侵入容器，影响其他容器或主机安全。 仓库漏洞：无论是 Docker 官方的镜像仓库还是我们私有的镜像仓库，都有可能被攻击，然后篡改镜像，当我们使用镜像时，就可能成为攻击者的目标对象。 用户程序漏洞：用户自己构建的软件包可能存在漏洞或者被植入恶意脚本，这样会导致运行时提权影响其他容器或主机安全。Linux 内核隔离性不够尽管目前 Namespace 已经提供了非常多的资源隔离类型，但是仍有部分关键内容没有被完全隔离，其中包括一些系统的关键性目录（如 /sys、/proc 等），这些关键性的目录可能会泄露主机上一些关键性的信息，让攻击者利用这些信息对整个主机甚至云计算中心发起攻击。而且仅仅依靠 Namespace 的隔离是远远不够的，因为一旦内核的 Namespace 被突破，使用者就有可能直接提权获取到主机的超级权限，从而影响主机安全。所有容器共享主机内核由于同一宿主机上所有容器共享主机内核，所以攻击者可以利用一些特殊手段导致内核崩溃，进而导致主机宕机影响主机上其他服务。既然容器有这么多安全上的问题，应该如何做才能够既享受到容器的便利性同时也可以保障容器安全呢？解决容器的安全问题？Docker 自身安全性改进事实上，Docker 从 2013 年诞生到现在，在安全性上面已经做了非常多的努力。目前 Docker 在默认配置和默认行为下是足够安全的。Docker 自身是基于 Linux 的多种 Namespace 实现的，其中有一个很重要的 Namespace 叫作 User Namespace。User Namespace 主要是用来做容器内用户和主机的用户隔离的。在过去容器里的 root 用户就是主机上的 root 用户，如果容器受到攻击，或者容器本身含有恶意程序，在容器内就可以直接获取到主机 root 权限。Docker 从 1.10 版本开始，使用 User Namespace 做用户隔离，实现了容器中的 root 用户映射到主机上的非 root 用户，从而大大减轻了容器被突破的风险。因此，尽可能地使用 Docker 最新版本就可以得到更好的安全保障。保障镜像安全为保障镜像安全，可以在私有镜像仓库安装镜像安全扫描组件，对上传的镜像进行检查，通过与 CVE 数据库对比，一旦发现有漏洞的镜像及时通知用户或阻止非安全镜像继续构建和分发。同时为了我们使用的镜像足够安全，在拉取镜像时，要确保只从受信任的镜像仓库拉取，并且与镜像仓库通信一定要使用 HTTPS 协议。加强内核安全和管理由于仅仅依赖内核的隔离可能会引发安全问题，因此对于内核的安全应该更加重视。可以从以下几个方面进行加强。宿主机及时升级内核漏洞宿主机内核应该尽量安装最新补丁，因为更新的内核补丁往往有着更好的安全性和稳定性。使用 Capabilities 划分权限Capabilities 是 Linux 内核的概念，Linux 将系统权限分为了多个 Capabilities，它们都可以单独地开启或关闭，Capabilities 实现了系统更细粒度的访问控制。容器和虚拟机在权限控制上还是有一些区别的，在虚拟机内我们可以赋予用户所有的权限，例如设置 cron 定时任务、操作内核模块、配置网络等权限。而容器则需要针对每一项 Capabilities 更细粒度的去控制权限，例如： cron 定时任务可以在容器内运行，设置定时任务的权限也仅限于容器内部； 由于容器是共享主机内核的，因此在容器内部一般不允许直接操作主机内核； 容器的网络管理在容器外部，这就意味着一般情况下，我们在容器内部是不需要执行ifconfig、route等命令的 。由于容器可以按照需求逐项添加 Capabilities 权限，因此在大多数情况下，容器并不需要主机的 root 权限，Docker 默认情况下也是不开启额外特权的。最后，在执行docker run命令启动容器时，如非特殊可控情况，--privileged参数不允许设置为 true，其他特殊权限可以使用 --cap-add 参数，根据使用场景适当添加相应的权限。使用安全加固组件Linux 的 SELinux、AppArmor、GRSecurity 组件都是 Docker 官方推荐的安全加固组件。下面我对这三个组件做简单介绍。 SELinux (Secure Enhanced Linux): 是 Linux 的一个内核安全模块，提供了安全访问的策略机制，通过设置 SELinux 策略可以实现某些进程允许访问某些文件。 AppArmor: 类似于 SELinux，也是一个 Linux 的内核安全模块，普通的访问控制仅能控制到用户的访问权限，而 AppArmor 可以控制到用户程序的访问权限。 GRSecurity: 是一个对内核的安全扩展，可通过智能访问控制，提供内存破坏防御，文件系统增强等多种防御形式。这三个组件可以限制一个容器对主机的内核或其他资源的访问控制。目前，容器报告的一些安全漏洞中，很多都是通过对内核进行加强访问和隔离来实现的。资源限制在生产环境中，建议每个容器都添加相应的资源限制。下面给出一些执行docker run命令启动容器时可以传递的资源限制参数： --cpus 限制 CPU 配额 -m, --memory 限制内存配额 --pids-limit 限制容器的 PID 个数例如想要启动一个 1 核 2G 的容器，并且限制在容器内最多只能创建 1000 个 PID，启动命令如下：$ docker run -it --cpus=1 -m=2048m --pids-limit=1000 busybox sh推荐在生产环境中限制 CPU、内存、PID 等资源，这样即便应用程序有漏洞，也不会导致主机的资源完全耗尽，最大限度降低安全风险。使用安全容器容器有着轻便快速启动的优点，虚拟机有着安全隔离的优点，有没有一种技术可以兼顾两者的优点，做到既轻量又安全呢？答案是有，那就是安全容器。安全容器是相较于普通容器的，安全容器与普通容器的主要区别在于，安全容器中的每个容器都运行在一个单独的微型虚拟机中，拥有独立的操作系统和内核，并且有虚拟化层的安全隔离。安全容器目前推荐的技术方案是 Kata Containers，Kata Container 并不包含一个完整的操作系统，只有一个精简版的 Guest Kernel 运行着容器本身的应用，并且通过减少不必要的内存，尽量共享可以共享的内存来进一步减少内存的开销。另外，Kata Container 实现了 OCI 规范，可以直接使用 Docker 的镜像启动 Kata 容器，具有开销更小、秒级启动、安全隔离等许多优点。总结 Docker 容器的安全问题 解决办法 Docker 作为一款容器引擎，本身存在权限提升、信息泄露等安全问题 尽量使用 Docker 最新版本，以便得到更好的安全保障 镜像软件也存在像仓库漏洞、用户程序漏洞等安全漏洞 在私有镜像仓库安装镜像扫描组件，对上传的镜像进行检查，通过与 CVE 数据库对比，一旦发现有漏洞的镜像及时通知用户或阻止非安全镜像继续构建和分发 Linux 内核 Namespace 隔离不够，有部分关键内容没有被完全隔离 宿主机内核应尽量安装最新补丁。比如：使用 Capabilities 划分权限、 使用 SELinux 、AppArmor、GRSecurity 等安全组件加强安全、每个容器都要限制资源使用 所有容器共享主机内核，攻击者可以利用一些特殊手段导致内核崩溃，进而导致主机宕机影响主机上其他服务 使用安全容器（例如 Kata Containers、Google 的 gVistor，通过实现Linux的API，拦截容器对底层Linux接口的调用，实现容器安全的目标） " }, { "title": "Docker Dockerfile（06）", "url": "/posts/dockerfile/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker", "date": "2020-08-07 15:43:00 +0000", "snippet": "生产实践中一定优先使用 Dockerfile 的方式构建镜像。 因为使用 Dockerfile 构建镜像可以带来很多好处： 易于版本化管理，Dockerfile 本身是一个文本文件，方便存放在代码仓库做版本管理，可以很方便找到各个版本之间的变更历史 过程可追溯，Dockerfile 的每一行指令代表一个镜像层，根据 Dockerfile 的内容即可很明确地查看镜像的完整构建过程 屏蔽构建环境异构，使用 Dockerfile 构建镜像无须考虑构建环境，基于相同 Dockerfile 无论在哪里运行，构建结果都一致虽然有这么多好处，但是如果 Dockerfile 使用不当也会引发很多问题。比如： 镜像构建时间过长，甚至镜像构建失败 镜像层数过多，导致镜像文件过大因此，本文对在生产环境中编写最优的 Dockerfile 的经验总结。Dockerfile 书写原则遵循以下 Dockerfile 书写原则，不仅可以使得自己的 Dockerfile 简洁明了，让协作者清楚地了解镜像的完整构建流程，还可以减少镜像的体积，加快镜像构建的速度和分发速度。单一职责容器的本质是进程，一个容器代表一个进程，因此不同功能的应用应该尽量拆分为不同的容器，每个容器只负责单一业务进程。提供注释信息Dockerfile 也是一种代码，应该保持良好的代码编写习惯，晦涩难懂的代码尽量添加注释，让协作者可以一目了然地知道每一行代码的作用，并且方便扩展和使用。保持容器最小化避免安装无用的软件包，比如在一个 nginx 镜像中，不需要安装 vim 、gcc 等开发编译工具。这样不仅可以加快容器构建速度，而且可以避免镜像体积过大合理选择基础镜像容器的核心是应用，因此只要基础镜像能够满足应用的运行环境即可。例如一个Java类型的应用运行时只需要JRE，并不需要JDK，因此基础镜像只需要安装JRE环境即可。使用 .dockerignore 文件在使用git时，可以使用.gitignore文件忽略一些不需要做版本管理的文件。同理，使用.dockerignore文件允许在构建时，忽略一些不需要参与构建的文件，从而提升构建效率。.dockerignore的定义类似于.gitignore。.dockerignore的本质是文本文件，Docker 构建时可以使用换行符来解析文件定义，每一行可以忽略一些文件或者文件夹。具体使用方式如下： 规则 含义 # # 开头的表示注释，# 后面所有内容将会被忽略 /tmp 匹配当前目录下任何以 tmp 开头的文件或者文件夹 *.md 匹配以 .md 为后缀的任意文件 tem? 匹配以 tem 开头并且以任意字符结尾的文件，？代表任意一个字符 !README.md ! 表示排除忽略。例如 .dockerignore 定义如下：*.md \\n !README.md，表示除了 README.md 文件外所有以 .md 结尾的文件。 尽量构建缓存Docker 构建过程中，每一条 Dockerfile 指令都会提交为一个镜像层，下一条指令都是基于上一条指令构建的。如果构建时发现要构建的镜像层的父镜像层已经存在，并且下一条命令使用了相同的指令，即可命中构建缓存。Docker 构建时判断是否需要使用缓存的规则如下： 从当前构建层开始，比较所有的子镜像，检查所有的构建指令是否与当前完全一致，如果不一致，则不使用缓存 一般情况下，只需要比较构建指令即可判断是否需要使用缓存，但是有些指令除外（例如ADD和COPY） 对于ADD和COPY指令不仅要校验命令是否一致，还要为即将拷贝到容器的文件计算校验和（根据文件内容计算出的一个数值，如果两个文件计算的数值一致，表示两个文件内容一致 ），命令和校验和完全一致，才认为命中缓存因此，基于 Docker 构建时的缓存特性，可以把不轻易改变的指令放到 Dockerfile 前面（例如安装软件包），而可能经常发生改变的指令放在 Dockerfile 末尾（例如编译应用程序）。例如，定义一些环境变量并且安装一些软件包，可以按照如下顺序编写 Dockerfile：FROM centos:7# 设置环境变量指令放前面ENV PATH /usr/local/bin:$PATH# 安装软件指令放前面RUN yum install -y make# 把业务软件的配置,版本等经常变动的步骤放最后...按照上面原则编写的 Dockerfile 在构建镜像时，前面步骤命中缓存的概率会增加，可以大大缩短镜像构建时间。正确设置时区从 Docker Hub 拉取的官方操作系统镜像大多数都是 UTC 时间（世界标准时间）。如果想要在容器中使用中国区标准时间（东八区），则要根据使用的操作系统修改相应的时区信息，下面是几种常用操作系统的修改方式：Ubuntu 和Debian 系统Ubuntu 和Debian 系统可以向 Dockerfile 中添加以下指令：RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeRUN echo &quot;Asia/Shanghai&quot; &amp;gt;&amp;gt; /etc/timezoneCentOS系统CentOS 系统则向 Dockerfile 中添加以下指令：RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime使用国内软件源加快镜像构建速度由于常用的官方操作系统镜像基本都是国外的，软件服务器大部分也在国外，所以构建镜像的时候想要安装一些软件包可能会非常慢。这里以 CentOS 7 为例，介绍一下如何使用 163 软件源（国内有很多大厂，例如阿里、腾讯、网易等公司都免费提供的软件加速源）加快镜像构建。首先在容器构建目录创建文件 CentOS7-Base-163.repo，文件内容如下：# CentOS-Base.repo## The mirror system uses the connecting IP address of the client and the# update status of each mirror to pick mirrors that are updated to and# geographically close to the client. You should use this for CentOS updates# unless you are manually picking other mirrors.## If the mirrorlist= does not work for you, as a fall back you can try the # remarked out baseurl= line instead.##[base]name=CentOS-$releasever - Base - 163.com#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;amp;arch=$basearch&amp;amp;repo=osbaseurl=http://mirrors.163.com/centos/$releasever/os/$basearch/gpgcheck=1gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7#released updates[updates]name=CentOS-$releasever - Updates - 163.com#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;amp;arch=$basearch&amp;amp;repo=updatesbaseurl=http://mirrors.163.com/centos/$releasever/updates/$basearch/gpgcheck=1gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7#additional packages that may be useful[extras]name=CentOS-$releasever - Extras - 163.com#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;amp;arch=$basearch&amp;amp;repo=extrasbaseurl=http://mirrors.163.com/centos/$releasever/extras/$basearch/gpgcheck=1gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7#additional packages that extend functionality of existing packages[centosplus]name=CentOS-$releasever - Plus - 163.combaseurl=http://mirrors.163.com/centos/$releasever/centosplus/$basearch/gpgcheck=1enabled=0gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7然后在 Dockerfile 中添加如下指令：COPY CentOS7-Base-163.repo /etc/yum.repos.d/CentOS7-Base.repo执行完上述步骤后，再使用yum install命令安装软件时就会默认从 163 获取软件包，这样可以大大提升构建速度。最小化镜像层数在构建镜像时尽可能地减少 Dockerfile 指令行数。例如要在 CentOS 系统中安装make和net-tools两个软件包，应该在 Dockerfile 中使用以下指令：RUN yum install -y make net-tools而不应该写成这样：RUN yum install -y makeRUN yum install -y net-toolsDockerfile 指令书写建议针对常用的指令，书写建议如下：RUNRUN指令在构建时将会生成一个新的镜像层并且执行 RUN 指令后面的内容。使用RUN指令时应该尽量遵循以下原则： 当RUN指令后面跟的内容比较复杂时，建议使用反斜杠（\\） 结尾并且换行； RUN指令后面的内容尽量按照字母顺序排序，提高可读性。例如，在官方的 CentOS 镜像下安装一些软件，一个建议的 Dockerfile 指令如下：FROM centos:7RUN yum install -y automake \\ curl \\ python \\                   vimCMD和 ENTRYPOINTCMD 和 ENTRYPOINT 指令都是容器运行的命令入口，这两个指令使用中有很多相似的地方，但是也有一些区别。这两个指令的相同点： 第一种为 CMD/ENTRYPOINT[&quot;command&quot; , &quot;param&quot;]。这种格式是使用 Linux 的 exec 实现的， 一般称为 exec 模式，这种书写格式为 CMD/ENTRYPOINT 后面跟 json 数组，也是Docker 推荐的使用格式。 另外一种格式为 CMD/ENTRYPOINTcommand param，这种格式是基于 shell 实现的， 通常称为 shell模式。当使用 shell 模式时，Docker 会以 /bin/sh -c command 的方式执行命令。使用 exec 模式启动容器时，容器的 1 号进程就是 CMD/ENTRYPOINT 中指定的命令。而使用 shell 模式启动容器时相当于把启动命令放在了 shell 进程中执行，等效于执行/bin/sh -c &quot;task command&quot; 命令。因此 shell 模式启动的进程在容器中实际上并不是 1 号进程。这两个指令的区别： Dockerfile 中如果使用了 ENTRYPOINT 指令，启动 Docker 容器时需要使用 --entrypoint 参数才能覆盖 Dockerfile 中的 ENTRYPOINT 指令 ，而使用 CMD 设置的命令则可以被 docker run 后面的参数直接覆盖。 ENTRYPOINT 指令可以结合 CMD 指令使用，也可以单独使用，而 CMD 指令只能单独使用。如果希望镜像足够灵活，推荐使用CMD指令。如果镜像只执行单一的具体程序，并且不希望用户在执行docker run时覆盖默认程序，建议使用ENTRYPOINT。最后再强调一下，无论使用CMD还是ENTRYPOINT，都尽量使用exec模式。ADD 和 COPYADD 和 COPY 指令功能类似，都是从外部往容器内添加文件。但是 COPY 指令只支持基本的文件和文件夹拷贝功能，ADD 则支持更多文件来源类型，比如自动提取 tar 包，并且可以支持源文件为 URL 格式。那么在日常应用中，更推荐使用 COPY 指令，因为 COPY 指令更加透明，仅支持本地文件向容器拷贝，而且使用 COPY 指令可以更好地利用构建缓存，有效减小镜像体积。当要使用 ADD 向容器中添加 URL 文件时，请尽量考虑使用其他方式替代。例如想要在容器中安装 memtester（一种内存压测工具），应该避免使用以下格式：ADD http://pyropus.ca/software/memtester/old-versions/memtester-4.3.0.tar.gz /tmp/RUN tar -xvf /tmp/memtester-4.3.0.tar.gz -C /tmpRUN make -C /tmp/memtester-4.3.0 &amp;amp;&amp;amp; make -C /tmp/memtester-4.3.0 install下面是推荐写法：RUN wget -O /tmp/memtester-4.3.0.tar.gz http://pyropus.ca/software/memtester/old-versions/memtester-4.3.0.tar.gz \\&amp;amp;&amp;amp; tar -xvf /tmp/memtester-4.3.0.tar.gz -C /tmp \\&amp;amp;&amp;amp; make -C /tmp/memtester-4.3.0 &amp;amp;&amp;amp; make -C /tmp/memtester-4.3.0 installwork为了使构建过程更加清晰明了，推荐使用 WORKDIR 来指定容器的工作路径，应该尽量避免使用 RUN cd /work/path &amp;amp;&amp;amp; do some work 这样的指令。最后给出几个常用软件的官方 Dockerfile 示例链接： Go Nginx Hy分离编译环境和运行环境编写编译型语言（例如 Golang、Java）的 Dockerfile 时，将分离编译环境（编译运行程序所需要的环境）和运行环境（应用程序正常运行所依赖的环境。例如编译 Java 应用程序需要 JDK 环境，而真正运行的 Java 程序则仅需要 JRE 环境即可），使得镜像体积尽可能小。把 编译环境 单独打包镜像，只提供编译好的二进制（注意运行 CPU 架构）。运行环境单独做镜像，只考虑基础运行环境的配置。好处有： 程序发布单独版本管理，运行环境也单独组合 运行环境打补丁升级等等，不用影响程序发布" }, { "title": "Docker 仓库（05）", "url": "/posts/docker-private-repo/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker", "date": "2020-08-06 11:33:00 +0000", "snippet": "仓库是什么？仓库（Repository）是存储和分发 Docker 镜像的地方。镜像仓库类似于代码仓库，Docker Hub 的命名来自 GitHub，Github 是常用的代码存储和分发的地方。同样 Docker Hub 是用来提供 Docker 镜像存储和分发的地方。注册服务器（Registry）和仓库（Repository）的区别： 注册服务器是存放仓库的实际服务器，仓库则可以理解为一个具体的项目或者目录 注册服务器可以包含很多个仓库，每个仓库又可以包含多个镜像例如：docker.io/centos，docker.io 是注册服务器，centos 是仓库名仓库类型，镜像仓库分为公共镜像仓库和私有镜像仓库。公共镜像仓库公共镜像仓库一般是 Docker 官方或者其他第三方组织（阿里云，腾讯云，网易云等）提供的，允许所有人注册和使用的镜像仓库。Docker Hub 是全球最大的镜像市场，目前已经有超过 10w 个容器镜像，这些容器镜像主要来自软件供应商、开源组织和社区。大部分的操作系统镜像和软件镜像都可以直接在 Docker Hub 下载并使用。以 Docker Hub ，总结公共镜像仓库分发和存储镜像。 首先访问 Docker Hub官网，点击注册按钮进入注册账号界面； 注册完成后，点击创建仓库，新建一个仓库用于推送镜像；创建好仓库后，就可以推送本地镜像到新创建的仓库中。下面通过一个实例，演示如何推送镜像到自己的仓库中。第一步，使用以下命令拉去 happymaya 镜像$ docker pull happymayaUsing default tag: latestlatest: Pulling from library/happymayaDigest: sha256:4f47c01fa91355af2865ac10fef5bf6ec9c7f42ad2321377c21e844427972977Status: Image is up to date for busybox:latestdocker.io/library/happymaya:latest第二步，使用 docker login 命令登录镜像服务器（只有已经登录的用户才可以推送镜像到仓库）$ docker loginLogin with your Docker ID to push and pull images from Docker Hub. If you don&#39;t have a Docker ID, head over to https://hub.docker.com to create one.Username: superhscPassword:Login Succeeded docker login命令默认会请求 Docker Hub，如果想登录第三方镜像仓库或者自建的镜像仓库，在docker login后面加上注册服务器即可。例如：登录访问阿里云镜像服务器，则使用docker login registry.cn-beijing.aliyuncs.com，输入阿里云镜像服务的用户名密码即可。第三步，先把镜像“重命名”，才能正确推送到自己创建的镜像仓库中，使用 docker tag命令将镜像“重命名”第四步，镜像“重命名”后使用docker push命令就可以推送镜像到自己创建的仓库中此时，happymaya 这个镜像就被推送到自定义的镜像仓库了。还可以新建其他的镜像仓库，然后将自己构建的镜像推送到仓库中。搭建私有仓库启动本地仓库有时候，出于安全或保密的需求，需要搭建一个自己的镜像仓库Docker 官方提供了开源的镜像仓库 Distribution，并且镜像存放在 Docker Hub 的 Registry 仓库下供我们下载。可以使用如下命令启动一个本地镜像仓库：$ docker run -d -p 5000:5000 --name registry registry:2.7Unable to find image &#39;registry:2.7&#39; locally2.7: Pulling from library/registrycbdbe7a5bc2a: Pull complete47112e65547d: Pull complete46bcb632e506: Pull completec1cc712bcecd: Pull complete3db6272dcbfa: Pull completeDigest: sha256:8be26f81ffea54106bae012c6f349df70f4d5e7e2ec01b143c46e2c03b9e551dStatus: Downloaded newer image for registry:2.7d7e449a8a93e71c9a7d99c67470bd7e7a723eee5ae97b3f7a2a8a1cf25982cc3然后使用 docker ps 命令查看下刚才启动的容器：$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd7e449a8a93e registry:2.7 &quot;/entrypoint.sh /etc…&quot; 50 seconds ago Up 49 seconds 0.0.0.0:5000-&amp;gt;5000/tcp registry此时就拥有了一个私有镜像仓库，访问地址为 http://localhost:5000 .推送镜像到本地仓库第一步，使用docker tag命令把 happymaya 镜像”重命名”为localhost:5000/happymaya$ docker tag busybox localhost:5000/happymaya此时 Docker 为 busybox 镜像创建了一个别名 localhost:5000/happymaya，localhost:5000 为主机名和端口，Docker 将会把镜像推送到这个地址。第二步，使用 docker push 推送镜像到本地仓库$ docker push localhost:5000/happymayaThe push refers to repository [localhost:5000/happymaya]514c3a3e64d4: Layer already existslatest: digest: sha256:400ee2ed939df769d4681023810d2e4fb9479b8401d97003c710d0e20f7c49c6 size: 527第三步，验证从本地镜像仓库拉取镜像（1）删除本地的happymaya和localhost:5000/happymaya镜像$ docker rmi happymaya localhost:5000/happymayaUntagged: happymaya:latestUntagged: busybox@sha256:4f47c01fa91355af2865ac10fef5bf6ec9c7f42ad2321377c21e844427972977Untagged: localhost:5000/happymaya:latestUntagged: localhost:5000/happymaya@sha256:400ee2ed939df769d4681023810d2e4fb9479b8401d97003c710d0e20f7c49c6（2）查看本地 happymaya 镜像$ docker image ls happymayaREPOSITORY TAG IMAGE ID CREATED SIZE可以看到此时本地已经没有 happymaya 这个镜像了。（3）从本地镜像仓库拉取 happymaya 镜像：$ docker pull localhost:5000/happymaya Using default tag: latestlatest: Pulling from happymaya Digest: sha256:400ee2ed939df769d4681023810d2e4fb9479b8401d97003c710d0e20f7c49c6Status: Downloaded newer image for localhost:5000/happymaya :latestlocalhost:5000/happymaya :latest（4） 最后再使用 docker image ls happymaya命令，这时可以看到已经成功从私有镜像仓库拉取happymaya镜像到本地了持久化镜像存储容器是无状态的。上面私有仓库的启动方式可能会导致镜像丢失，因为没有把仓库的数据信息持久化到主机磁盘上，这在生产环境中是无法接受的。可以使用以下命令将镜像持久化到主机目录：$ docker run -v /var/lib/registry/data:/var/lib/registry -d -p 5000:5000 --name registry registry:2.7上面启动registry的命令中加入了-v /var/lib/registry/data:/var/lib/registry -v 的含义是把 Docker 容器的某个目录或文件挂载到主机上，保证容器被重建后数据不丢失 -v 参数冒号前面为主机目录，冒号后面为容器内目录。 事实上，registry 的持久化存储除了支持本地文件系统还支持很多种类型，例如： S3 Google Cloud Platform Microsoft Azure Blob Storage Service 等多种存储类型。。 此时，虽然可以本地访问和拉取，但是如果想在另外一台机器上，是无法通过 Docker 访问到这个镜像仓库的。因为 Docker 要求非 localhost 访问的镜像仓库必须使用 HTTPS，此时就需要构建外部可访问的镜像仓库（或者配置 insecure-registry）。构建外部可访问的镜像仓库要构建一个支持 HTTPS 访问的安全镜像仓库，需要满足以下两个条件： 拥有一个合法的域名，并且可以正确解析到镜像服务器； 从证书颁发机构（CA）获取一个证书。在准备好域名和证书后，就可以部署的镜像服务器了。这里以regisry.happymaya.io这个域名为例，步骤如下：（1）准备存放证书的目录 /var/lib/registry/certs（2）把申请到的证书私钥和公钥分别放到该目录下， 假设申请到的证书文件分别为regisry.happymaya.io.crt和regisry.happymaya.io.key（3）停止并删除已启动的仓库容器$ docker stop registry &amp;amp;&amp;amp; docker rm registry（4）使用以下命令启动新的镜像仓库$ docker run -d \\ --name registry \\ -v &quot;/var/lib/registry/data:/var/lib/registry \\ -v &quot;/var/lib/registry/certs:/certs \\ -e REGISTRY_HTTP_ADDR=0.0.0.0:443 \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/regisry.lagoudocker.io.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/regisry.lagoudocker.io.key \\ -p 443:443 \\ registry:2.7这里，使用 -v 参数把镜像数据持久化在/var/lib/registry/data目录中，同时把主机上的证书文件挂载到了容器的 /certs 目录下，同时通过 -e 参数设置 HTTPS 相关的环境变量参数，最后让仓库在主机上监听 443 端口。（5）仓库启动后，就可以远程推送镜像了$ docker tag busybox regisry.lagoudocker.io/busybox$ docker push regisry.lagoudocker.io/busyboxHarborDocker 官方开源的镜像仓库Distribution仅满足了镜像存储和管理的功能，用户权限管理相对较弱，并且没有管理界面。如果想要构建一个企业的镜像仓库，Harbor 是一个非常不错的解决方案。 Harbor 是一个基于 Distribution 项目开发的一款企业级镜像管理软件 拥有 RBAC （基于角色的访问控制）、管理用户界面以及审计等非常完善的功能 目前已经从 CNCF 毕业，这代表它已经有了非常高的软件成熟度 生产中使用的企业较多，社区更加活跃一些Harbor 的使命是成为 Kubernetes 信任的云原生镜像仓库。 Harbor 需要结合 Kubernetes 才能发挥其最大价值，可以在的 Harbor 官网了解更多。 当使用 Docker Hub 拉取镜像很慢的时候，如何加快镜像的拉取速度 ?参考文档 Docker 拉取镜像速度慢怎么破？" }, { "title": "Docker 容器的操作（04）", "url": "/posts/docker-basic-operation/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker", "date": "2020-08-05 15:33:00 +0000", "snippet": "容器（Container）是什么？ 容器是基于镜像创建的可运行实例，并且单独存在 一个镜像可以创建出多个容器 容器的本质是进程，启动需要一个不能退出的命令作为主进程运行容器化环境时，实际是在容器内部创建该文件的读写副本， 将会添加一个容器层，该层允许修改镜像的整个副本。容器的生命周期容器的生命周期分为 5 种： created：初建状态，通过 docker create命令生成的容器状态为初建状态 running：运行状态，初建状态通过docker start命令可以转化为运行状态 stopped：停止状态 运行状态通过docker stop命令转化为停止状态 停止状态的容器通过docker start转化为运行状态 paused： 暂停状态 运行状态的容器也可以通过docker pause命令转化为暂停状态 处于暂停状态的容器可以通过docker unpause转化为运行状态 deleted：删除状态【虚状态】，通过 docker delete删除 处于初建状态、运行状态、停止状态、暂停状态的容器都可以直接删除。容器的操作容器的操作可以分为五个步骤 创建并启动容器 终止容器 进入容器 删除容器 导入和导出容器创建并启动容器容器十分轻量，可以随时创建和删除它。使用docker create命令来创建容器，例如：$ docker create -it --name=busybox busyboxUnable to find image &#39;busybox:latest&#39; locallylatest: Pulling from library/busybox61c5ed1cbdf8: Pull completeDigest: sha256:4f47c01fa91355af2865ac10fef5bf6ec9c7f42ad2321377c21e844427972977Status: Downloaded newer image for busybox:latest2c2e919c2d6dad1f1712c65b3b8425ea656050bd5a0b4722f8b01526d5959ec6$ docker ps -a | grep busybox2c2e919c2d6d  busybox  &quot;sh&quot; 34 seconds ago   Created    busybox如果使用docker create命令创建的容器处于停止状态，可以使用docker start命令来启动它，如下所示：$ docker start busybox$ docker psCONTAINER ID IMAGE     COMMAND  CREATED     STATUS       PORTS      NAMESd6f3d364fad3 busybox   &quot;sh&quot;     16 seconds ago  Up 8 seconds      busybox容器启动有两种方式： 使用docker start命令基于已经创建好的容器直接启动 。 使用docker run命令直接基于镜像新建一个容器并启动，相当于先执行docker create命令从镜像创建容器，然后再执行docker start命令启动容器。当使用docker run创建并启动容器时，Docker 后台执行的流程为： Docker 会检查本地是否存在 busybox 镜像，如果镜像不存在则从 Docker Hub 拉取 busybox 镜像； 使用 busybox 镜像创建并启动一个容器； 分配文件系统，并且在镜像只读层外创建一个读写层； 从 Docker IP 池中分配一个 IP 给容器； 执行用户的启动命令运行镜像。上述命令中: -t 参数的作用是分配一个伪终端 -i 参数则可以终端的 STDIN 打开 同时使用 -it 参数可以让进入交互模式。在交互模式下，用户可以通过所创建的终端来输入命令，例如：$ ps auxPID   USER     TIME  COMMAND1 root      0:00 sh6 root      0:00 ps aux可以看到容器的 1 号进程为 sh 命令，在容器内部并不能看到主机上的进程信息，因为容器内部和主机是完全隔离的。同时由于 sh 是 1 号进程，意味着如果通过 exit 退出 sh，那么容器也会退出。所以对于容器来说，杀死容器中的主进程，则容器也会被杀死。终止容器想停止运行中的容器，可以使用docker stop命令。命令格式为 docker stop [-t|--time[=10]]。该命令的过程是： 向运行中的容器发送 SIGTERM 信号，如果容器内 1 号进程接受并能够处理 SIGTERM，则等待 1 号进程处理完毕后退出 如果等待一段时间后，容器仍然没有退出，则会发送 SIGKILL 强制终止容器。$ docker stop busyboxbusybox查看停止状态的容器信息，使用 docker ps -a 命令。$ docker ps -aCONTAINERID   IMAGE   COMMAND CREATED         STATUS    PORTS         NAMES28d477d3737a  busybox &quot;sh&quot;    26 minutes ago  Exited (137) About a minute ago  busybox重启容器，使用命令 docker restart$ docker restart busyboxbusybox$ docker psCONTAINER ID  IMAGE     COMMAND   CREATED          STATUS       PORTS   NAMES28d477d3737a  busybox   &quot;sh&quot;      32 minutes ago   Up 3 seconds         busybox进入容器处于运行状态的容器可以通过docker attach、docker exec、nsenter等多种方式进入容器。使用docker attach命令进入容器使用 docker attach ，进入容器，如下所示。$ docker attach busybox# ps auxPID   USER     TIME  COMMAND1 root      0:00 sh7 root      0:00 ps aux需要注意： 当同时使用docker attach命令同时在多个终端运行时，所有的终端窗口将同步显示相同内容 当某个命令行窗口的命令阻塞时，其他命令行窗口同样也无法操作。 由于docker attach命令不够灵活，因此一般不会使用docker attach进入容器。使用 docker exec 命令进入容器Docker 从 1.3 版本开始，提供了更加方便地进入容器的命令docker exec，通过docker exec -it CONTAINER的方式进入到一个已经运行中的容器，如下所示。$ docker exec -it busybox sh$ ps auxPID   USER     TIME  COMMAND 1 root     0:00 sh 7 root     0:00 sh 12 root     0:00 ps aux删除容器删除容器命令的使用方式如：docker rm [OPTIONS] CONTAINER [CONTAINER...]如果要删除一个停止状态的容器，可以使用docker rm命令删除。docker rm busybox如果要删除正在运行中的容器，必须添加 -f (或 –force) 参数， Docker 会发送 SIGKILL 信号强制终止正在运行的容器docker rm -f busybox导出导入容器导出容器使用docker export CONTAINER命令导出一个容器到文件，不管此时该容器是否处于运行中的状态。导出容器前，先进入容器，创建一个文件，过程如下。 进入容器创建文件 $ docker exec -it busybox shcd /tmp &amp;amp;&amp;amp; touch test 然后执行导出命令 $ docker export busybox &amp;gt; busybox.tar 执行以上命令后会在当前文件夹下生成 busybox.tar 文件，可以将该文件拷贝到其他机器上，通过导入命令实现容器的迁移。 导入容器使用docker import命令导入，执行完docker import后变为本地镜像，最后再使用docker run命令启动该镜像，这样就实现了容器的迁移。导入容器的命令格式为： docker import [OPTIONS] file|URL [REPOSITORY[:TAG]]。首先，使用docker import命令导入上一步导出的容器$ docker import busybox.tar busybox:test此时，busybox.tar 被导入成为新的镜像，镜像名称为 busybox:test 。然后，使用docker run命令启动并进入容器，查看上一步创建的临时文件$ docker run -it busybox:test sh$ ls /tmp/test可以看到之前在 /tmp 目录下创建的 test 文件也被迁移过来了。这样就通过docker export和docker import命令配合实现了容器的迁移。 save 和 load 操作是针对镜像的，export 和 import 是针对容器的操作 容器的文件系统要设计成写时复制(如图 1 所示)，而不是每一个容器都单独拷贝一份镜像文件吗？" }, { "title": "Docker 镜像的操作（03）", "url": "/posts/docker-mirror-operation/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker", "date": "2020-08-02 13:33:00 +0000", "snippet": "镜像 一个只读的 Docker 容器模板，包含启动容器所需要的所有文件系统结构和内容 一个特殊的文件系统，它提供了容器运行时所需的程序、软件库、资源、配置等静态数据。即镜像不包含任何动态数据，镜像内容在构建后不会被改变。镜像的操作镜像的操作分为： 拉取镜像，使用docker pull命令拉取远程仓库的镜像到本地 重命名镜像，使用docker tag命令“重命名”镜像 查看镜像，使用docker image ls或docker images命令查看本地已经存在的镜像 删除镜像，使用docker rmi命令删除无用镜像 构建镜像，构建镜像有两种方式。第一种方式是使用docker build命令基于 Dockerfile 构建镜像，也是我比较推荐的镜像构建方式；第二种方式是使用docker commit命令基于已经运行的容器提交为镜像拉取镜像命令：docker pull，命令格式为： docker pull [Registry]/[Repository]/[Image]:[Tag]。 Registry：注册服务器（默认会从 docker.io 拉取镜像，如果有自己的镜像仓库，可以把 Registry 替换为自己的注册服务器） Reponsitory：镜像仓库（通常把一组相关联的镜像归为一个镜像仓库，library 为 Docker 默认的镜像仓库) image: 镜像名称 Tag：镜像的标签，如果不指定拉取镜像的标签，默认为latest。栗子：需要获取一个 busybox 镜像，可以执行以下命令： busybox 是一个集成了数百个 Linux 命令（例如 curl、grep、mount、telnet 等）的精简工具箱，只有几兆大小，被誉为 Linux 系统的瑞士军刀。我经常会使用 busybox 做调试来查找生产环境中遇到的问题。$ docker pull busyboxUsing default tag: latestlatest: Pulling from library/busybox61c5ed1cbdf8: Pull completeDigest: sha256:4f47c01fa91355af2865ac10fef5bf6ec9c7f42ad2321377c21e844427972977Status: Downloaded newer image for busybox:latestdocker.io/library/busybox:latest$ docker 执行docker pull busybox命令，都是先从本地搜索，如果本地搜索不到busybox镜像则从 Docker Hub 下载镜像。查看镜像Docker 镜像查看使用 docker images 或者docker image ls命令。 docker images命令列出本地所有的镜像$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest 4bb46517cac3 9 days ago 133MBnginx 1.15 53f3fd8007f7 15 months ago 109MBbusybox latest 018c9d7b792b 3 weeks ago 1.22MB docker image ls命令来查询指定的镜像$ docker image ls busyboxREPOSITORY TAG IMAGE ID CREATED SIZEbusybox latest 018c9d7b792b 3 weeks ago 1.22MB docker images命令列出所有镜像，然后使用grep命令进行过滤$ docker images |grep busyboxbusybox latest 018c9d7b792b 3 weeks ago 1.22MB重命名镜像命令：docker tag自定义镜像名称或者推送镜像到其他镜像仓库命令格式：docker tag [SOURCE_IMAGE][:TAG] [TARGET_IMAGE][:TAG]栗子：$ docker tag busybox:latest mybusybox:latest执行完docker tag命令后，可以使用查询镜像命令查看一下镜像列表：$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEbusybox latest 018c9d7b792b 3 weeks ago 1.22MBmybusybox latest 018c9d7b792b 3 weeks ago 1.22MB可以看到，镜像列表中多了一个mybusybox的镜像。但细心的同学可能已经发现，busybox和mybusybox这两个镜像的 IMAGE ID 是完全一样的。为什么呢？实际上它们指向了同一个镜像文件，只是别名不同而已。删除镜像命令：docker rmi或docker image rm命令删除镜像举例：使用以下命令删除mybusybox镜像$ docker rmi mybusybox Untagged: mybusybox:latest 此时，再次使用docker images命令查看下机器上的镜像列表。$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest 018c9d7b792b 3 weeks ago 1.22MB 通过上面的输出，可以看到mybusybox镜像已经被删除。构建镜像构建镜像主要有两种方式： 从运行中的容器提交为镜像； 使用docker commit命令 从 Dockerfile 构建镜像 使用docker build命令 最重要最常用的镜像构建方式 Dockerfile 包含了用户所有构建命令的文本 从运行中的容器提交为镜像命令：docker commit 使用命令创建一个名为 busybox 的容器并进入 busybox 容器； $ docker run --rm --name=busybox -it busybox sh 执行完上面的命令后，当前窗口会启动一个 busybox 容器并且进入容器中。在容器中，执行以下命令创建一个文件并写入内容： $ touch hello.txt &amp;amp;&amp;amp; echo &quot;I love Docker. &quot; &amp;gt; hello.txt 新打开另一个命令行窗口，运行以下命令提交镜像： $ docker commit busybox busybox:hellosha256:cbc6406aaef080d1dd3087d4ea1e6c6c9915ee0ee0f5dd9e0a90b03e2215e81c 最后使用docker image ls命令查看镜像：此时就看到主机上新生成了 busybox:hello 这个镜像。从 Dockerfile 构建镜像命令：docker build使用 Dockerfile 构建的镜像具有以下特性： Dockerfile 的每一行命令都会生成一个独立的镜像层，并且拥有唯一的 ID Dockerfile 的命令是完全透明的，通过查看 Dockerfile 的内容，就可以知道镜像是如何一步步构建的 Dockerfile 是纯文本的，方便跟随代码一起存放在代码仓库并做版本管理Dockerfile 常用的指令：# 基于 `centos:7`这个镜像来构建自定义镜像# 每个 Dockerfile 的第一行除了注释都必须以 FROM 开头FROM centos:7# 拷贝贝本地文件 `nginx.repo` 文件到容器内的 `/etc/yum.repos.d` 目录下# 这里拷贝 `nginx.repo` 文件是为了添加 nginx 的安装源COPY nginx.repo /etc/yum.repos.d/nginx.repo# RUN yum install -y nginx# 声明容器内业务（nginx）使用 80 端口对外提供服务ENV HOST=mynginx# 定义容器的启动命令，命令格式为 json 数组# 设置了容器的启动命令为 nginx # 添加了 nginx 的启动参数 -g &#39;daemon off;&#39; ，使得 nginx 以前台的方式启动CMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]上面这个 Dockerfile 的例子基本涵盖了常用的镜像构建指令。镜像的实现原理 Docker 镜像是由一系列镜像层（layer）组成的， 每一层代表了镜像构建过程中的一次提交下面以一个镜像构建的 Dockerfile 来说明镜像是如何分层的。# 基于 busybox 创建一个镜像层FROM busybox# 拷贝本机 test 文件到镜像内COPY test /tmp/test# 在 /tmp 文件夹下创建一个目录 testdirRUN mkdir /tmp/testdir为验证镜像存储结构，使用docker build命令在上面 Dockerfile 所在目录构建一个镜像：$ docker build -t busybox .这里的 Docker 使用的是 overlay2 文件驱动，进入到/var/lib/docker/overlay2目录下使用tree .命令查看产生的镜像文件：$ tree .# 以下为 tree . 命令输出内容|-- 3e89b959f921227acab94f5ab4524252ae0a829ff8a3687178e3aca56d605679| |-- diff # 这一层为基础层，对应上述 Dockerfile 第一行，包含 busybox 镜像所有文件内容，例如 /etc,/bin,/var 等目录... 此次省略部分原始镜像文件内容| `-- link |-- 6591d4e47eb2488e6297a0a07a2439f550cdb22845b6d2ddb1be2466ae7a9391| |-- diff # 这一层对应上述 Dockerfile 第二行，拷贝 test 文件到 /tmp 文件夹下，因此 diff 文件夹下有了 /tmp/test 文件| | `-- tmp| | `-- test| |-- link| |-- lower| `-- work|-- backingFsBlockDev|-- bec6a018080f7b808565728dee8447b9e86b3093b16ad5e6a1ac3976528a8bb1| |-- diff  # 这一层对应上述 Dockerfile 第三行，在 /tmp 文件夹下创建 testdir 文件夹，因此 diff 文件夹下有了 /tmp/testdir 文件夹| | `-- tmp| | `-- testdir| |-- link| |-- lower| `-- work...通过上面的目录结构可以看到，Dockerfile 的每一行命令，都生成了一个镜像层，每一层的 diff 夹下只存放了增量数据，如图 2 所示。分层的结构使得 Docker 镜像非常轻量，每一层根据镜像的内容都有一个唯一的 ID 值，当不同的镜像之间有相同的镜像层时，便可以实现不同的镜像之间共享镜像层的效果。总结一下， Docker 镜像是静态的分层管理的文件组合 镜像底层的实现依赖于联合文件系统（UnionFS）总结镜像操作命令： 拉取镜像，使用 docker pull 命令拉取远程仓库的镜像到本地 重命名镜像，使用 docker tag 命令“重命名”镜像 查看镜像，使用 docker image ls 或 docker images 命令查看本地已经存在的镜像 删除镜像，使用 docker rmi 命令删除无用镜像 构建镜像，构建镜像有两种方式。第一种方式是使用 docker build 命令基于 Dockerfile 构建镜像，也是我比较推荐的镜像构建方式；第二种方式是使用 docker commit 命令基于已经运行的容器提交为镜像镜像的实现原理：镜像是由一系列的镜像层（layer ）组成，每一层代表了镜像构建过程中的一次提交，当我们需要修改镜像内的某个文件时，只需要在当前镜像层的基础上新建一个镜像层，并且只存放修改过的文件内容。分层结构使得镜像间共享镜像层变得非常简单和方便。" }, { "title": "Docker 的核心概念（02）", "url": "/posts/docker-core-idea/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker", "date": "2020-08-02 11:33:00 +0000", "snippet": "核心概念Docker 的操作围绕镜像、容器、仓库三大核心概念。镜像 通俗的讲，镜像是一个只读的文件和文件夹组合 镜像包含容器运行时所需要的所有基础文件和配置信息 镜像是容器启动的基础，是容器启动的先决条件。如果想要使用一个镜像，有两种方式： 自己创建镜像。通常情况下，一个镜像是基于一个基础镜像构建的，可以在基础镜像上添加一些用户自定义的内容。例如基于 CentOS 镜像创作自己的业务镜像，首先安装 nginx 服务，然后部署应用程序，最后做一些自定义配置，这样一个业务镜像就好了 从功能镜像仓库拉去别人制作好的镜像容器 通俗的讲，容器是镜像的运行实体 容器是带有运行时需要的可写文件层（镜像是静态的只读文件） 容器中的进程属于运行状态（容器运行着真正的应用进程） 容器有初建、运行、停止、暂停和删除五个种状态。容器本质上是主机上运行的一个进程.与直接运行在主机上进程的本质区别是：容器有自己独立的命名空间隔离和资源限制（在容器内部，无法看到主机上的进程、环境变量以及网络等信息）仓库 通俗的讲，Docker 的镜像仓库类似于代码仓库，用来存储和分发 Docker 镜像 镜像仓库分为公共镜像仓库和私有镜像仓库目前，Docker Hub 是 Docker 官方的公开镜像仓库，它不仅有很多应用或者操作系统的官方镜像，还有很多组织或者个人开发的镜像供我们免费存放、下载、研究和使用。除了公开镜像仓库，也可以构建自己的私有镜像仓库。镜像、容器、仓库三者之间的联系从上图 1 来看： 镜像是容器的基石，一个镜像可以创建多个容器 容器是镜像运行的实体 仓库就是用来存放的分发镜像。Docker 架构在了解 Docker 架构之前，先的了解以下容器的发展历史​容器的发展史 起初容器技术随着 Docker 的出现变得炙手可热 随后所有公司都在积极拥抱容器技术，此时，出现了除 Docker 容器之外的其它容器技术，如 CoreOS 的 rkt、lxc 等。容器技术百花齐放 容器技术的标准到底是什么？容器标准应该由谁来制定？ 诸公司各自为营 此时还伴随着的编排技术之争（编排技术有三大主力 Docker Swarm、Kubernetes 和 Mesos，Swarm 毋庸置疑，肯定愿意把 Docker 作为唯一的容器运行时，但是 Kubernetes 和 Mesos 就不同意了，因为它们不希望调度的形式过度单一。）在这样的背景下，最终爆发了容器大战。OCI 也正是在这样的背景下应运而生。 OCI 全称为开放容器标准（Open Container Initiative）， 一个轻量级、开放的治理结构 OCI 组织在 Linux 基金会的大力支持下，于 2015 年 6 月份正式注册成立。基金会旨在为用户围绕工业化容器的格式和镜像运行时，制定一个开放的容器标准。 目前主要有两个标准文档：容器运行时标准 （runtime spec）和容器镜像标准（image spec）。 正是由于容器的战争，才导致 Docker 不得不在战争中改变一些技术架构。最终形成了下图所示的技术架构。从上图可以看到，Docker 整体架构采用 C/S（客户端 / 服务器）模式，主要由客户端和服务端两大部分组成。 客户端负责发送操作指令 服务端负责接收和处理指令。客户端和服务端通信有多种方式，既可以在同一台机器上通过UNIX套接字通信，也可以通过网络连接远程通信。Docker 客户端 Docker 客户端是一种泛称 与服务端交互的方式： docker 命令（Docker 用户与 Docker 服务端交互） 直接请求 REST API 的方式与 Docker 服务端交互 使用各种编程语言的 SDK 与 Docker 服务端交互（前社区维护着 Go、Java、Python、PHP 等数十种语言的 SDK，足以满足你的日常需求） Docker 服务端 Docker 服务端是 Docker 所有后台服务的统称。 其中 dockerd 是一个非常重要的后台管理进程。负责响应和处理来自 Docker 客户端的请求，然后将客户端的请求转化为 Docker 的具体操作。例如镜像、容器、网络和挂载卷等具体对象的操作和管理。 Docker 从诞生到现在，服务端经历了多次架构重构。 起初，服务端的组件是全部集成在 docker 二进制里。 从 1.11 版本开始， dockerd 已经成了独立的二进制（此时的容器也不是直接由 dockerd 来启动了，而是集成了 containerd、runC 等多个组件）、 虽然 Docker 的架构在不停重构，但是各个模块的基本功能和定位并没有变化。 与一般的 C/S 架构系统一样，Docker 服务端模块负责与 Docker 客户端交互，并管理 Docker 的容器、镜像、网络等资源Docker 重要组件以 Docker 的 18.09.2 版本为例，看下 Docker 的工具和组件。在 Docker 安装路径下执行 ls 命令可以看到以下与 docker 有关的二进制文件。-rwxr-xr-x 1 root root 27941976 Dec 12 2019 containerd-rwxr-xr-x 1 root root 4964704 Dec 12 2019 containerd-shim-rwxr-xr-x 1 root root 15678392 Dec 12 2019 ctr-rwxr-xr-x 1 root root 50683148 Dec 12 2019 docker-rwxr-xr-x 1 root root 764144 Dec 12 2019 docker-init-rwxr-xr-x 1 root root 2837280 Dec 12 2019 docker-proxy-rwxr-xr-x 1 root root 54320560 Dec 12 2019 dockerd-rwxr-xr-x 1 root root 7522464 Dec 12 2019 runc​这里面由 Docker 两个至关重要的组件：runc 和 containerd​ runc Docker 官方安装 OCI 容器运行时标准的实现 用来运行容器的轻量级工具，是真正用来运行的容器 containered Docker 服务端的核心组件 是从 dockerd 中剥离出来的 它的诞生完全遵循 OCI 遍，是容器标准化后的掺入 通过 containerd-shim 启动并管理 runC 真正的管理容器的生命周期 通过上图，可以看到： dockerd 通过 gRPC 与 containerd 通信 由于 dockerd 与真正的容器运行是 runC 之间有了 containerd 这一 OCI 标准，使得 dockerd 可以确保接口向下兼容 gRPC 是一种远程服务调用。想了解更多信息可以参考https://grpc.iocontainerd-shim 的意思是垫片，类似于拧螺丝时夹在螺丝和螺母之间的垫片。containerd-shim 的主要作用是将 containerd 和真正的容器进程解耦，使用 containerd-shim 作为容器进程的父进程，从而实现重启 dockerd 不影响已经启动的容器进程。 Docker 各组件之间的关系 首先通过以下命令来启动一个 happymaya 容器 $ docker run -d happymaya sleep 3600 容器启动后，通过以下命令查看 dockerd 的 PID $ sudo ps aux | grep dockerdroot      4147  0.3  0.2 1447892 83236 ?       Ssl  Jul09 245:59 /usr/bin/dockerd 为验上图中 Docker 各组件之间的调用关系，使用 pstree 命令查看进程父子关系, $ sudo pstree -l -a -A 4147dockerd  |-containerd --config /var/run/docker/containerd/containerd.toml --log-level info  |   |-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/d14d20507073e5743e607efd616571c834f1a914f903db6279b8de4b5ba3a45a -address /var/run/docker/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc  |   |   |-sleep 3600 事实上，dockerd 启动的时候， containerd 就随之启动了，dockerd 与 containerd 一直存在。当执行 docker run 命令（通过 happymaya 镜像创建并启动容器）时，containerd 会创建 containerd-shim 【containerd-shim 是真正容器的进程的父进程，这么做为了不让真正的容器进程作为 containerd 的子进程，从而可以实现重启 containerd 而不影响已经运行的容器】充当 “垫片”进程，然后启动容器的真正进程 sleep 3600 。这个过程和架构图是完全一致的 为什么 Docker 公司要把containerd拆分并捐献给社区吗？containerd 捐赠拆分， 在一定程度上让开发者更容易的去接触到一些特性，使得‘标准’这个概念也更加深刻吧" }, { "title": "Docker 的安装（01）", "url": "/posts/docker-installation/", "categories": "虚拟化技术, Docker 落地笔记", "tags": "Docker", "date": "2020-08-01 02:33:00 +0000", "snippet": "Docker 安装Docker 能做什么Docker 的官方定义是： Package Software into Standardized Units for Deveoplement, Shipment and Deployment.由官方定义可知，Docker 是用于开发、**发布和部署的标准化单元很多地方，将 Docker 类比集装箱。 大船上，各种货物要想被整齐摆放并且相互不受到影响，只需要把各种货物进行集装箱标准化。有了集装箱，就不需要专门运输水果或者化学用品的船。把各种货物通过集装箱打包，然后统一放到一艘船上运输。 Docker 要做的就是把各种软件打包成一个集装箱（镜像），然后分发。 每个镜像运行在独立的容器中，因此在运行的时候相互隔离CentOS 下安装 Docker Docker 是跨平台解决方案 Docker 支持在当前主流的各大平台安装使用 包括 Ubuntu、RHEL、CentOS、Debian 等 Linux 发行版 包括 OSX、Microsoft Windows 等非 Linux 平台下 Linux 是 Docker 的原生支持平台，故推荐在 Linux 上使用 Docker 由于使用 CentOS 较多，因此主要针对 CentOS 平台下安装和使用 操作系统要求 CentOS 7 以上的发行版本 建议使用 overlay2 存储驱动程序卸载已有的 Docker 如果已经安装过旧版本的 Docker ，执行以下命令卸载旧版本的 Docker $ sudo yum remove docker \\ docker-clinet \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine \\ 安装 Docker 首先添加 Docker 安装源，添加 Docker 安装源的命令如下： $ sudo yum-config-manager \\ --add -repo \\ https://download.docker.com/linux/centos/docker-ce.repo 然后从已经配好的源安装和更新 Docker 正常情况下，直接安装最新版本的 Docker 即可，因为最新版本的 Docker 有更好的稳定性和安全性，使用以下命令安装最新本的 Docker： $ sudo yum install docker-ce docker-ce-cli containerd.io 如果安装指定版本的 Docker ，使用以下命令： $ sudo yum install docker-ce --showduplicates | sort -r$ sudo yum install docker-ce-&amp;lt;VERSION_STRING&amp;gt; docker-ce-cli-&amp;lt;VERSION_SPRING&amp;gt; containl&#39;l&#39;l&#39;l&#39;l&#39;l&#39;l&#39;l&#39;l&#39;l&#39;l&#39;l&#39;l&#39;l&#39;l&#39;l&#39;l&#39;l&#39;lerd.io 安装完成后，使用以下命令启动 docker $ sudo systemctl start docker 这里有一个国际惯例，安装完成后，使用以下命令启动一个 hello world 的容器： $ sudo docker run hello-world 运行上述命令: Dokcer 首先会检查本地是否有 Helllo Word 这个镜像，如果发现本地没有这个镜像，Docker 就会去 Docker Hub 官方仓库下载此镜像， 然后运行它， 最后，看到该镜像输出 “Hello from Docker” 并退出 安装完成后，默认 docker 命令只能以 root 用户执行。如果想要允许普通用户执行 docker 命令，需要执行以下命令 sudo groupadd docker &amp;amp;&amp;amp; sudo gpasswd -a ${USER} docker &amp;amp;&amp;amp; sudo systemctl restart docker ，执行完命令后，退出当前命令行窗口并打开新的窗口即可 容器技术原理 提起容器就不得不说 chroot，因为 chroot 是最早的容器雏形 Chroot 意味着切换目录，我们可以把任何目录更改为当前进程的根目录，这与容器非常类型chroot chroot 是在 Unix 和 Linux 系统的一个操作，针对正在运作的软件进程和它的子进程，改变它外显的根目录。一个运行在这个环境下，经由 chroot 设置根目录的程序，它不能够对这个指定根目录之外的文件进行访问动作，不能读取，也不能更改它的内容。 通俗地说，Chroot 可以改变某进程的根目录，使这个程序不能访问目录之外的其它目录，这个跟在一个容易中是很相似的栗子 ： 首先在当前目录下创建一个 rootfs 目录： $ mkdir rootfs 为了方便，使用现成的 busybox 镜像来创建一个系统，命令如下： $ cd rootfs$ docker export $(docker create busybox) -o busybox.tar$ tar -xf busybox.tar 执行完上面的命令后，在 rootfs 目录下，会得到一些目录和文件，使用 ls 命令查看一下 rootfs 目录下的内容： $ lsbin busybox.tar dev etc home proc root sys tmp usr var 可以看到 rootfs 目录下初始了一些目录 下面通过一条命令 chroot /home/centos/rootfs /bin/sh 见证 chroot 的神奇。启动一个 sh 进程，并且将 /home/centos/roots 作为 sh 进程的根目录。 此时，命令窗口已经处于上述命令启动 sh 进程中，在当前 sh 命令窗口下，使用 ls 命令查看一个当前的进行，看是否真的与主机上的其它目录隔离开了 / # /bin/ls / bin busybox.tar dev etc home proc root sys tmp usr var 这里可以看到当前进程的根目录已经变成了主机上的 /home/centos/rootfs 目录。这样就实现了当前进程与主机的隔离。到此为止，一个目录隔离的容器就完成了。 但是，此时还不能称之为一个容器，为什么呢？你可以在上一步（、 使用 chroot 启动命令行窗口）执行以下命令，查看如下路由信息： /etc # /bin/ip routedefault via 172.20.1.1 dev eth0172.17.0.0/16 dev docker0 scope link src 172.17.0.1172.20.1.0/24 dev eth0 scope link src 172.20.1.3 执行 ip route 命令后，你可以看到网络信息并没有隔离，实际上进程等信息此时也并未隔离。要想实现一个完整的容器，我们还需要 Linux 的其他三项技术： Namespace、Cgroups 和联合文件系统。 Docker 是利用 Linux 的 Namespace 、Cgroups 和联合文件系统三大机制来保证实现的， 所以它的原理是使用 Namespace 做主机名、网络、PID 等资源的隔离，使用 Cgroups 对进程或者进程组做资源（例如：CPU、内存等）的限制，联合文件系统用于镜像构建和容器运行环境。 NamespaceNamespace 是 Linux 内核的一项功能，该功能对内核资源进行隔离，使得容器中的进程都可以在单独的命名空间中运行，并且只可以访问当前容器命名空间的资源。Namespace 可以隔离进程 ID、主机名、用户 ID、文件名、网络访问和进程间通信等相关资源。Docker 主要用到以下五种命名空间。 pid namespace：用于隔离进程 ID。 net namespace：隔离网络接口，在虚拟的 net namespace 内用户可以拥有自己独立的 IP、路由、端口等。 mnt namespace：文件系统挂载点隔离。 ipc namespace：信号量,消息队列和共享内存的隔离。 uts namespace：主机名和域名的隔离。CgroupsCgroups 是一种 Linux 内核功能，可以限制和隔离进程的资源使用情况（CPU、内存、磁盘 I/O、网络等）。在容器的实现中，Cgroups 通常用来限制容器的 CPU 和内存等资源的使用。联合文件系统联合文件系统，又叫 UnionFS，是一种通过创建文件层进程操作的文件系统，因此，联合文件系统非常轻快。Docker 使用联合文件系统为容器提供构建层，使得容器可以实现写时复制以及镜像的分层构建和存储。常用的联合文件系统有 AUFS、Overlay 和 Devicemapper 等。" }, { "title": "怎么提高查询性能", "url": "/posts/07-mysql-improve-query-performance/", "categories": "Database, MySQL", "tags": "MySQL, Index Design", "date": "2020-07-12 02:53:22 +0000", "snippet": "MySQL 查询优化器SELECT 执行过程查询优化器处理 SQL 的全过程。以 SELECT 的 SQL 的执行过程为例，如下图所示： 客户端发送一条 SELECT 查询给服务器； 服务器先检查查询缓存，如果命中缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段； 服务器进行 SQL 解析、预处理、再由查询优化器生成对应的执行计划； MySQL 根据优化器生成的执行计划，调用存储引擎的 API 来执行查询； 将结果返回给客户端，同时也会放入查询缓存中。优化栗子MySQL 查询优化器并不是那么完美！！！MySQL 采用基于开销的优化器，以确定处理查询的最佳方式，也就是说执行查询之前，都会先选择一条自以为最优的方案。在很多情况下，MySQL 能够计算最佳的可能查询计划，但在某些情况下，MySQL 没有关于数据的足够信息，或者是提供太多的相关数据信息，它所采用的可能并非就是事实上的最优方案。两个例子来说明。案例一：为什么 Range 执行效率差 ？Range 类查询的执行计划流程： 根据查询条件计算所有的 possible keys； 计算全表扫描代价（cost_all）； 计算最小的索引范围访问代价（这一步很关键，直接决定了 Range 的查询效率），它有三步： 对于每一个 possible keys（可选索引），调用 records_in_ranges 函数计算范围中的rows； 根据 rows，计算二级索引访问代价； 获取 cost 最小的二级索引访问代价（cost_range）。 选择执行最小化访问代价的执行计划。如果 cost_all &amp;lt;= cost_range，则全表扫描，否则索引范围扫描。对于这个例子： Range 使用了 records_in_range 函数估算每个值范围的 rows，结果依赖于possible_keys； possible_keys 越多，随机 IO 代价越大，Range 查询效率越差。所以，索引不是越多越好，相反，应该尽量减少 possible_keys，减少 records_in_range 从而减少 IO 的消耗。有两个工具： 用 pt-index-usage 工具来删除冗余索引； 用 pt-duplicate-key-checker 工具来删除重复索引。案例二这样的一个表结构（优化后的），优化前有一个索引 idx_global_id。图中的这条 SQL 语句的 where 条件包括一个 sub_id 的等值查询和一个 global_id 的范围查询。执行一次需要 2.37 秒。从下一页的执行计划中，我们可以看到虽然查询优化器使用了唯一索引 uniq_subid_globalid，但是由于 idx_global_id 的干扰，实际只使用了前面的 4 个长度就 access，剩余 8 个长度都被 filter 了。从优化后的执行计划中可以看到，使用了 force index 来强制使用唯一索引。正如上文列举的，相似的命令还有 ignore index 忽略索引，straght_join 强制优化器按特定的顺序使用数据表，high_priority 或 low_priority 两个命令来控制 SQL 的执行优先权。ICP？什么是 MRR？什么是 BKA？ICPICP 是 Index Condition Pushdown 的简称，是 MySQL 使用索引从表中检索行数据的一种优化方式。目的是减少从基表中全记录读取操作的数量，从而降低 IO 操作。在没有开启 ICP 之前，存储引擎会通过遍历索引查找基表中的行，然后返回给 MySQL Server 层，再去为这些数据行进行 where 后的条件过滤。开启 ICP 之后，如果部分 where 条件能使用索引中的字段，MySQL Server 会把这部分下推到存储引擎层，存储引擎通过索引过滤，把满足的行从表中读取出来。ICP 能减少引擎层访问基表的次数和 MySQL Server 访问存储引擎的次数。对于 InnoDB 表来说，ICP 只适用于辅助索引。MRRMRR 是 Multi-Range Read 的简称，是 MySQL 优化器将随机 IO转化为顺序 IO 以降低查询过程中 IO 开销的一种手段。MRR 的适用场景是辅助索引，如 INDEX(key1)，查询 key1 在 n 到 m 范围内的数据。使用限制就是 MRR，MRR 适用于 range、ref、eq_ref 的查询。BKA 和 BNLBKA 是 Batched Key Access 的简称，是 MySQL 优化器提高表 join 性能的一种手段，它是一种算法。而 BNL 是 Block Nested Loop 的简称，它是默认的处理表 join 的方式和算法。二者有的区别：• BNL 比 BKA 出现的早，BKA 直到 MySQL 5.6 版本才出现，而 BNL 至少在 MySQL 5.1 版本中就存在了；• BNL 主要用于被 join 的表上无索引时；• BKA 只在被 join 表上有索引时可以使用，那么就在行提交给被 join 的表之前，对这些行按照索引字段进行排序，因此减少了随机 IO，排序才是两者最大的区别，但如果被 join 的表没有索引呢？那么就只能使用 BNL 了。使用 BKA 需要注意一些问题，比如： BKA 的适用场景支持 inner join、outer join、semi-join operations、including nested outer joins 等； BKA 有两个使用限制： 一个是使用 BKA 特性，必须启用 MRR 特性； 二是 BKA 主要适用于 join 的表上有索引可使用的情况，否则只能使用 BNL。 MySQL 执行计划分析“三步曲”MySQL 的查询优化器有时候也会“犯糊涂”，当有慢查询或者执行 SQL 遇到瓶颈时，分析这类问题时可以参考 MySQL 执行计划分析“三步曲”。 查看 SQL 执行计划 explain SQL； desc 表名； show create table 表名。 通过 Profile 定位 QUERY 代价消耗 set profiling=1； 执行 SQL； show profiles; 获取 Query_ID； show profile for query Query_ID，查看详细的 profile 信息。 通过 Profile 定位 QUERY 代价消耗 set session optimizer_trace=’enabled=on’； 执行 SQL； 查询 information_schema.optimizer_trace 表，获取 SQL 查询计划树； set session optimizer_trace=‘enabled=off’，==开启此项影响性能，记得用后关闭== MySQL 执行计划查询分析对于 MySQL 的慢查询，相关参数和分析工具。相关参数MySQL 可以通过设置一些参数，将运行时间长或者非索引查找的 SQL 记录到慢查询文件中。可以分析慢查询文件中的 SQL，有针对性的进行优化： 参数 slow_query_log，表示是否开启慢查询日志，ON 或者 1 表示开启，OFF 或者 0 表示关闭； 参数 long_query_time，设置慢查询的阈值，MySQL 5.7 版本支持微秒级； 参数 slow_query_log_file，慢查询文件的存放路径； 参数 log_queries_not_using_indexes，表示是否将非索引查找的 SQL 也记录到慢查询文件中； 参数 log_throttle_queries_not_using_indexes，表示每分钟记录到慢查询文件中未使用索引的 SQL 语句上限，0 表示没限制； 参数 max_execution_time，用来控制 SELECT 语句的最大执行时间，单位毫秒，超过此值MySQL 自动 kill 掉该查询。如上图所示，是一个慢查询的例子，通过这个例子可以看到慢查询文件中记录了哪些信息。包括： 慢 SQL 产生的时间，SQL 源自的 IP 和对应的数据库用户名，以及访问的数据库名称； 查询的总耗时，被 lock 的时间，结果集行数，扫描的行数，以及字节数等，还有具体的 SQL 语句。分析工具 explain； Mysqldumpslow，官方慢查询分析工具； pt-query-digest，Percona 公司开源的慢查询分析工具，它是用于分析 MySQL 慢查询的一个常用工具： 先对查询语句的条件进行参数化； 然后对参数化以后的查询进行分组统计，统计出各查询的执行时间、次数、占比等； 同时把分析结果输出到文件中； 也可以结合 Anemometer 工具将慢查询平台化展示。 vc-mysql-sniffer，第三方的慢查询抓取工具； pt-kill，Percona 公司开源的慢查询 kill 工具，常用于生产环境的过载保护。社区还开源了许多优秀的慢查询优化工具或系统，如：Inception、SQLAdvisor 和 Soar 等，优化 SQL 规则SQL 设计和优化需要考虑的几个方面： 全表扫描还是索引扫描。对于小表来说，二者 IO 调用次数和返回时间相差不大；但对于大表，如果全表扫描，那么查询返回的时间就会很长，就需要使用索引扫描加快查询速度。但并不是要求 DBA 根据每一种查询条件组合都要创建索引，索引过多也会降低写入和修改的速度，而且如果导致表数据和索引数据比例失调，也不利于后期的正常维护； 如何创建索引，在哪些列上建立索引适合业务需求？一般情况下，你可以在选择度高的列上创建索引，也可以在 status 列上创建索引。创建索引时，要注意避免冗余索引，除非一些特殊情况外。如 index(a,b,c) 和 index(a)，其中 a 的单列索引就是冗余索引； 创建索引以后，尽量不要过频修改。业务可以根据现有的索引情况合理使用索引，而不是每次都去修改索引。能在索引中完成的查找，就不要回表查询。比如 SELECT 某个具体字段，就有助于实现覆盖索引从而降低 IO 次数，达到优化 SQL 的目的。 多表关联的 SQL，在关联列上要有索引且字段类型一致，这样 MySQL 在进行嵌套循环连接查找时可以使用索引，且不会因为字段类型不匹配，而发生隐式转换进而导致无法使用索引的情况发生。 在现实情况中，经常会出现 SQL 中关联列字段类型不一致或者传入的参数类型与字段类型不匹配的情况，这样就会导致无法使用索引，在优化 SQL 时需要重点排查这种情况。 另外索引列上使用函数也不会涉及索引。 多表关联时，尽量让结果集小的表作为驱动表，注意是结果集小的表，不是小表。 在日常中发现全模糊匹配的查询，由于 MySQL 的索引是 B+ 树结构，所以当查询条件为全模糊时，例如‘%**%’，索引无法使用，这时需要通过添加其他选择度高的列或者条件作为一种补充，从而加快查询速度。 当然也可以通过强制 SQL 进行全索引扫描，但这种方式不好，尽量不要在 SQL 中添加 hints。对于这种全模糊匹配的场景，可以放到 ES 或者 solr 中解决。 尽量不要使用子查询，对子查询产生的临时表再扫描时将无索引可查询，只能进行全表扫描，并且 MySQL 对于出现在 from 中的表无所谓顺序，对于 where 中也无所谓顺序，这也是可以优化 SQL 的地方。 另外 order by/group by 的 SQL 涉及排序，尽量在索引中包含排序字段，并让排序字段的排序顺序与索引列中的顺序相同，这样可以避免排序或减少排序次数。 复杂查询还是简单查询？貌似总会面临这样的疑问和选择。不要总想着用一个SQL 解决所有事情，可以分步骤来进行，MySQL 也十分擅长处理短而简单的 SQL，总体耗时会更短，而且也不会产生臃肿的 SQL，让人难以理解和优化。 MySQL 自身优化 SQLMySQL 自身也对 SQL 自动进行了优化处理。MySQL 能够处理的优化类型有下面这些。 重新定义表的关联顺序。多表关联查询时，MySQL 日益强大的优化器会自动选择驱动表，以及表的连接顺序，基于 cost 规则极大减少 SQL 执行的时间； 使用等价变化规则。MySQL 可以合并或减少一些比较，还可以移除一些恒成立或恒不成立的判断； 优化 count()、min() 和 max()。索引和列是否可为空通常可以帮助 MySQL 优化这类表达式，如查找最小值只需找到索引树最左边的第一条记录即可。常用的 SQL 编写规范如下所示： SELECT 只获取必要的字段，禁止使用 SELECT *。这样能减少网络带宽消耗，有效利用覆盖索引，表结构变更对程序基本无影响； 用 IN 代替 OR。SQL 语句中 IN 包含的值不宜过多，应少于 1000 个。过多会使随机 IO 增大，影响性能。 禁止使用 order by rand()。order by rand() 会为表增加几个伪列，然后用 rand() 函数为每一行数据计算 rand() 值，最后基于该行排序，这通常都会生成磁盘上的临时表，因此效率非常低。建议先使用 rand() 函数获得随机的主键值，然后通过主键获取数据； SQL 中避免出现 now()、rand()、sysdate()、current_user() 等不确定结果的函数。在语句级复制场景下，引起主从数据不一致；不确定值的函数，产生的 SQL 语句无法使用 QUERY CACHE； 重要 SQL 必须被索引：update、delete 的 where 条件列、order by、group by、distinct 字段、多表 join 字段； 禁止使用 % 前导查询，例如：like “%abc”，⽆法利⽤到索引； 禁止使⽤负向查询，例如：not in、!=、&amp;lt;&amp;gt;、not like； 使⽤ EXPLAIN 判断 SQL 语句是否合理使用索引，尽量避免 extra 列出现：Using File Sort、Using Temporary 等； 减少与数据库交互次数，尽量采用批量 SQL 语句； 获取⼤量数据时，建议分批次获取数据，每次获取数据少于 5000 条，结果集应⼩于 1M； 拆分复杂 SQL 为多个 小SQL，避免⼤事务。简单的 SQL 容易使用到 MySQL 的 QUERY CACHE；减少锁表时间特别是 MyISAM；可以使用多核 CPU。栗子根据 Extra 列优化根据 Extra 列的输出内容进行优化的案例，如下图所示。优化方法就是创建索引，避免排序和临时表的开销。根据 ROWS 列优化如下图：优化前的 SQL 执行计划中 ROWS 是 181899 行，耗时 0.51 秒。咋一看来，还挺满意的，但是放在具体的业务场景中，尤其是业务并发量大，机器性能一般的情况下，业务高峰期是一个巨大的坑，随着业务并发量的增长，性能呈现急剧恶化，这种情况依然需要优化。通过添加 endtime 索引来达到优化效果的，优化后，执行计划中的 ROWS 变成了 134 行，基本上是“秒回”。优化后效果见下图：这个案例中使用的优化思路是：结合业务模式和数据分布来优化具体的 SQL 语句；重点分析 ROWS 列的输出，对各查询条件扫描行数进行对比，找出最优的索引策略。group by 和 order by 的优化优化方式见上文【SQL 规则】 GROUP BY 实质为排序和去重（distinct）的组合，通常它会结合一些聚合函数如 sum、count、avg、max、min 等来使用，根据一个或多个列对结果集进行分组，查看执行计划中的 Extra 列的输出内容，可以看到有 Using temporary; Using filesort。 ORDER BY 对结果集进行排序时，查看执行计划中的 Extra 列的输出内容，也可以看到有 Using temporary; Using filesort。分组和排序都涉及排序，故尽量在索引中包含排序字段，并让排序字段的排序顺序与索引列中的顺序相同，这样可以避免排序或减少排序次数。比如 where a=? order by b,c，就可以创建一个索引 (a,b,c)。如果执行计划中出现 using filesort，这时就要重点关注索引字段和顺序了。Limit 分页优化对于分页问题，经常会被问到的问题，SQL 语句如下：SELECT * FROM TABLE ORDER BY col2 limit 80000,20;Limit 有 start、offset 两个值，分页 SQL 的耗时随着 start 的增大而增加。原始分页查询的耗时是 2.87 秒，然后用子查询和表连接两种方式进行 SQL 改写后，查询的耗时都只有 0.5 秒左右。优化思路就是： SQL 尽量走索引，避免排序，减少不必要的物理 IO； 每页展示数据确定起始范围，取符合条件取 N 条记录即可； 由于传递的是主键（具有唯一特性），可以快速定位，获取数据。count 优化下面的四条 SELECT 语句： select count(*) from table … ; select count(1) from table … ; select count(primary key) from table … ; select count(index key) from table …;哪个执行效率最高呢？这个问题需要具体问题具体分析，不能一概而论。以 SELECT count(1) 这条 SQL 为例：优化前和优化后，执行效率相差 2 倍。就添加了一个索引。优化思路是： 选择索引 key_len 最短的二级索引效率高，不要使用全表扫描（PK 聚族索引会全表扫描），因为索引 key_len 越短，读取页面越少，进而 IO_COST 越小。Bad SQL 案例下面是一些 Bad SQL 的栗子，如下图所示: SQL 影响 select/update/delete 语句中没有 where 或者 where 中没有索引字段 会造成全表扫描，update 和 delete 语句还会加表级锁，阻塞其他并发的 SQL delete from table 删除全表数据。delete 会产生大量 undo 和 redo ，执行时间很长，可用 truncate table 来代替 select from table where col_vchar=123 产生隐式转换，col_vchar 字段上默认会加一个to_number的函数，无法使用索引，全表扫描 select from table where DATE(ctime)=curdate() 索引字段上有显式函数，无法使用索引，全表扫描 select from like ‘%**%‘ 全模糊匹配 select from tabl,tab2 where tab1.col_int=col_vchar 两个表关联查询，关联列的字段类型不匹配，一个varchar，一个int，无法使用索引 col_utf8mb4=col_utf8 关联列类型都是 varchar, 但字符集不同，无法使用索引 select/update/delete 语句中没有 where 或者 where 中无索引字段，这样的 SQL 都没有使用索引，会造成全表扫描，update 和 delete 语句还会加表级锁，阻塞其他并发的 SQL； select from table where col_vchar=123，这个 SQL 产生了隐式转换，col_vchar 字段是一个 varchar 类型，但传入的值却是数字类型，这样 col_vchar 字段上会默认加一个 to_number 的函数，无法使用索引，全表扫描，你需要重点注意一下这样看着简单但很容易被忽略的 SQL ； col_utf8mb4=col_utf8，大家要注意这种情况，尽量两个表的关联列类型都是 varchar，但字符集不相同，同样无法使用索引，你在日常优化 SQL 和设计表结构时一定要注意，尽量采用 utf8mb4 为默认字符集，如果两个表字符集不同，则需要提前变更一下。实际业务中优化数据库访问要做到以下几点： DBA 要参与重⼤项⽬的数据库⽅案选型和设计，以及审核数据库表结构和 SQL 语句； DBA 设计批量导入/导出数据的方案并监控过程和影响； 大量的更新/删除操作控制频度，例如每秒操作 2000 行以下； 对于后台管理或者其他统计类的只读场景，可以做读写分离，利用从库分担写库的读压力； 使⽤ prepared statement 和绑定变量，可以提升性能并避免 SQL 注入； 程序应有捕获 SQL 异常的处理机制，必要时通过rollback显式回滚。 尽量少使用 distinct、order by、group by、union 等 SQL，排序需求可以放到前端； 大事务或者长查询的需求根据业务特点拆分； 杜绝程序中在处理事务时夹杂 RPC，会造成资源长时间不释放。有很多锁超时、并发数上涨都是由于事务中有 RPC 造成的； 关注软件本身的优化同时，也需要关注硬件的性能指标和优化，以及硬件的发展方向。MySQL 属于 IO 密集型的应用，对存储硬件的 IO 性能要求比较高，在高并发的场景中，建议使用 PCI-e。应用层性能优化除了对 MySQL 优化外，有很多时候，MySQL优化已经到了极致，需要站在更高的角度来进行“上层建筑”的规划和设计，比如要知道常用的缓存技术 Memcached、Redis、Codis、Pika、Aerospike 等，还需要知道常用的消息队列 Kafka、RabbitMQ、Redis Stream 数据结构等，还需要了解常用的全文索引工具 Elasticsearch、Solr、sphinx 等，大数据系统 HBase、Spark、Druid 等等。" }, { "title": "索引设计", "url": "/posts/06-mysql-index-desgin/", "categories": "Database, MySQL", "tags": "MySQL, Index Design", "date": "2020-07-11 09:53:22 +0000", "snippet": "索引的定义 数据库索引是一种数据结构，以额外写入和存储空间为代价来提高数据库表上数据检索操作的速度。 通俗来说，索引类似于书的目录，根据其中记录的页码可以快速找到所需的内容。 ——《维基百科》MySQL 官方对索引（Index）的定义是存储引擎用于快速查找记录的一种数据结构： 索引是物理数据页，数据库页大小（Page Size）决定了一个页可以存储多少个索引行，以及需要多少页来存储指定大小的索引； 索引可以加快检索速度，但同时也降低索引列插入、删除、更新的速度，索引维护需要代价。索引设计的理论知识有： 二分查找； 哈希表； B+Tree.二分查找法二分查找法也叫作折半查找法，它是在有序数组中查找指定数据的搜索算法。 维基百科对二分查找法的定义： Set $L$ to $0$ and $R$ to $n -1$ . If $L &amp;gt; R$, the search terminates as unsuccessful . Set $m$ (the position of the middle element) to the ==floor== of $\\frac{L+R}{2}$ ，which is the greatest integer less than or equal to $\\frac{L+R}{2}$ If $A_m &amp;lt; T$ , set $L$ to $m+1$ and go to step 2. If $A_m &amp;gt; T$ , set $R$ to $m -1$ and go to step 2. Now $A_m &amp;gt; T$, the search is done; return m. 优点 等值查询； 范围查询性能优秀。缺点更新数据、新增数据、删除数据维护成本高。栗子 有序数组 [1-71] 有 17 个值，即在有序数组 $[A_0-A_{16}] $ 中希望找到 $Target(7)$ 所在的位置，首选确定下标 $L$ 为 0，下标 $R$ 为 16，下标 m 为 floor [$\\frac{L+R}{2}$]，即向下取整数。第一次查询$下标\\ L=0, R = 16, m = floor[\\frac{0+16}{2}] = 8$获得 A8 的值为 14，因为 A8(14) &amp;gt;Target(7) 则设置 R=m-1=7，如下图所示。第二次查询$下标\\ L=0, R = 7, m = floor[\\frac{0+7}{2}] = 3$获得 $A_3$ 的值为 6，$A_3(6) &amp;lt; Target(7)$ 则设置下标 $L=m+1=4$ ，如下图所示：第三次查询$下标\\ L=4, R = 7, m = floor[\\frac{4+7}{2}] = 5$ ，获得 $A_5$ 的值为 8，$A_5(8) = Target(7)$，则设置下标 $R = m - 1 = 4$，如下图所示：第四次查询$下标\\ L=4, R = 4, m = floor[\\frac{4+2}{2}] = 4$ ，获得 $A_4$ 的值为 7，$A_4(7) = Target(7)$，查询结束，如下图所示：此次查询经过 4 次二分查找后找到目标数据 7，如果在查询过程中出现下标 $L &amp;gt; R $ 的情况，则表示目标元素不在有序数组内，结束查询。二分查找是索引实现的理论基础，索引原理数据库查询是数据库的核心功能，而索引又是作为加速查询的重要技术手段。对于索引数据结构的选择，本质是贴合当前数据读写的硬件环境，选择一个优秀的数据结构。进行数据存储和遍历。在数据库中，大部分索引都是通过 ==B+Tree== 来实现的。当然也涉及其他数据结构，在 MySQL 中除了 ==B+Tree== 索引外，还需要关注==Hash 索引==。Hash 索引哈希表是数据库中哈希索引的基础，根据键值 &amp;lt;key,value&amp;gt; 存储数据的结构。简单说： 哈希表是使用哈希函数，将索引列计算到桶或槽的数组， 实际存储是根据哈希函数，将 key 换算成确定的存储位置，并将 value 存放到该数组位置上； 访问时，只需要输入待查找的 key，即可通过哈希函数计算，得出确定的存储位置并读取数据。如下图所示：姓名作为 key，通过哈希函数对姓名字段数据进行计算，得到哈希码并存放到桶或槽的数组中，同时存放指向真实数据行的指针作为 value，形成哈希表。哈希索引的实现 数据库中哈希索引是基于哈希表实现， 对于==哈希索引列==的数据通过 Hash 算法计算，得到对应==索引列的哈希码==形成==哈希表==，由==哈希码及哈希码指向的真实数据行的指针==组成了==哈希索引==； 哈希索引的应用场景是只在对==哈希索引列的等值查询==才有效。如下图所示：根据表中的 name 字段构建 Hash 索引，通过 Hash 算法对每一行 name 字段的数据进行计算，得出 Hash 码。由 Hash 码及 Hash 码指向真实数据行的指针组成了哈希索引。优点因为哈希索引只存储哈希值和行指针，不存储实际字段值，所以其结构紧凑，查询速度也非常快，在无哈希冲突的场景下访问哈希索引一次即可命中。缺点 哈希索引只适用于等值查询，包括 =、IN()、&amp;lt;=&amp;gt; （安全等于， select null &amp;lt;=&amp;gt; null 和 select null=null 是不一样的结果) ； 不支持范围查询； 另外，哈希索引的性能跟哈希冲突数量成反比，哈希冲突越多其维护代价越大性能越低。Hash 碰撞处理Hash 碰撞是指，不同索引列值计算出相同的哈希码，如上图所示， 表中 name 字段为 John Smith 和 Sandra Dee 两个不同值根据 Hash 算法计算出来的哈希码都是 152，这就表示出现了 Hash 碰撞。对于 Hash 碰撞通用的处理方法是使用链表： 将 Hash 冲突碰撞的元素形成一个链表，发生冲突时在链表上进行二次遍历找到数据。Hash 碰撞跟选择的 Hash 算法有关系，为了减少 Hash 碰撞的概率： 优先选择避免 Hash 冲突的 Hash 算法，例如： 使用 Percona Server 的函数 FNV64() ，其哈希值为 64 位，出现 Hash 冲突的概率要比 CRC32 小很多； 其次是考虑性能，优先选择数字类型的 Hash 算法，因为字符串类型的 Hash 算法不仅浪费空间而且不方便进行比较。常见的 CRC32、SHA1 和 MD5 Hash 函数生成的返回值如下图所示：因此，Hash 算法优先级：==`FNV64 &amp;gt; CRC32 （大数据量下 Hash 冲突概率较大）&amp;gt; MD5 &amp;gt; SHA1。==MySQL 如何使用 Hash在 MySQL 中使用 Hash 索引主要是分为： Memory 存储引擎，原生支持的 Hash 索引 ； InnoDB 自适应哈希索引； NDB 集群的哈希索引 3 类。如上图所示，Memory 存储引擎创建表时即可原生显式创建并使用 Hash 索引。InnoDB 存储引擎，虽然不能原生显示创建 Hash 索引，但是可以伪造哈希索引来加速定值查询的性能。例如：为超长文本（如网站 URL）进行 Hash 计算后的字段 url_hash 创建 B+Tree 索引，获得 Hash 索引的功能。关于哈希索引，InnoDB 提供了 InnoDB 自适应哈希索引的强大功能，InnoDB 自适应哈希索引是为了提升查询效率，InnoDB 存储引擎会监控表上各个索引页的查询，当 InnoDB 注意到某些索引值访问非常频繁时，会在内存中基于 B+Tree 索引再创建一个哈希索引，使得内存中的 B+Tree 索引具备哈希索引的功能，即能够快速定值访问频繁访问的索引页。当满足下面三个条件时，InnoDB 为整个 block 构建 AHI 记录项： 分析使用自适应哈希索引（AHI）可以成功查询的次数是否超过 block 上记录数的1/16； btr_search_info::n_hash_potential大于或等于BTR_SEARCH_BUILD_LIMIT (100)，表示为 SQL 查询能够连续 100 次成功使用 AHI； 尚未为当前 block 构造索引或者当前 block 上已经构建了 AHI 索引且 block-&amp;gt;n_hash_helps 大于 page 上记录数的两倍或者当前 block上 推荐的前缀索引列发生了变化 。为什么要为 B+Tree 索引页二次创建自适应哈希索引呢： 因为 B+Tree 索引的查询效率取决于 B+Tree 的高度，在数据库系统中通常 B+Tree 的高度为 3～4 层，所以访问数据需要做 3～4 次的查询。 而 Hash 索引访问通常一次查找就能定位数据（无 Hash 碰撞的情况），其等值查询场景 Hash 索引的查询效率要优于 B+Tree。自适应哈希索引的建立使得 InnoDB 存储引擎能自动根据索引页访问的频率和模式自动地为某些热点页建立哈希索引来加速访问。另外 InnoDB 自适应哈希索引的功能，用户只能选择开启或关闭功能，无法进行人工干涉。功能开启后可以通过 Show Engine Innodb Status 看到当前自适应哈希索引的使用情况：Hash table size 276707， node heap has 0 buffer(s)0.00 Hash searches/s， 0.00 non-Hash searches/s// 可以看到 Hash table 的大小，使用情况及每秒使用 AHI 和非 AHI 搜索的情况。B+ 索引在数据库中大部分索引都是通过 B+Tree 来实现的。对于 B+Tree 具体的定义可以参考《数据结构》等相关书籍。在 MySQL 数据库中讨论索引时，如果没有明确指定类型，则默认是指使用 B+Tree 数据结构进行存储，其说法等价于 B+Tree、B-Tree、BTREE（看到创建索引语句为 BTREE 也不要惊讶，等同于 B+Tree）。如下图所示为一个简单的、标准的 B+tree，每个节点有 K 个键值和 K+1 个指针。对于 MySQL 存储引擎而言，其实际使用的 B+Tree 索引是为了满足： 数据读写性能； 适配磁盘访问模式优化后的数据结构，每一个叶子节点都包含指向下一个叶子节点的指针。在 MySQL 中，索引是在存储引擎层而非服务器层实现的，所以不同存储引擎层支持的索引类型可以不同。例如： 虽然 MyISAM 和 InnoDB 的索引都是使用 B+Tree 实现的，但是其实际数据存储结构有不少差异； 下图中 B+Tree 示例一共 2 层，图中每个页面都已经被随机编号（编号可以认定为页面号），其中页面号为 20 的页面是 B+Tree 的根页面（根页面通常是存放在内存中的），根页面存储了 &amp;lt;key+pageno&amp;gt;，pageno 是指向具体叶子节点的页面号。其他页面都是叶子节点，存放了具体的数据 &amp;lt;key+data&amp;gt; B+Tree 索引能够快速访问数据，就是因为存储引擎可以不再需要通过全表扫描来获取数据，而是从索引的根结点（通常在内存中）开始进行二分查找，根节点的槽中都存放了指向子节点的指针，存储引擎根据这些指针能够快速遍历数据。例如：通过页面号为 20 的根节点可以快速得知 Key&amp;lt;10 的数据在 pageno 33 的页面，key在 [10,16) 范围的数据在 pageno 56 的页面。叶子节点存放的 &amp;lt;key+data&amp;gt; ，对于真正要存放哪些数据还得取决于该 B+Tree 是聚簇索引（Clustered Index）还是辅助索引（Secondary Index）。聚簇索引和辅助索引聚簇索引 聚簇索引是一种数据存储方式，它表示表中的数据按照主键顺序存储，是索引组织表； InnoDB 的聚簇索引是按照主键顺序构建 B+Tree，B+Tree 的叶子节点就是行记录，数据行和主键值紧凑地存储在一起。 这也意味着 InnoDB 的主键索引就是数据表本身，它按主键顺序存放了整张表的数据； 聚簇索引占用的空间就是整个表数据量的大小； InnoDB 只能创建一个聚簇索引（假想下如果能支持多个聚簇索引，那就意味着一张表按不同排序规则冗余存储多份全表数据了）； 相比索引组织表，还有一种堆表类型，堆表是根据数据写入的顺序直接存储在磁盘上的。对于堆表而言，其主键和辅助索引唯一的区别就是键值是否唯一，两者都是根据索引列排序构建 B+Tree 的，在每个叶子节点加上指向堆表的行指针（row data pointer） 。堆表在各类数据库中也被广泛使用，MyISAM 存储引擎的表就是堆表。辅助索引 InnoDB 辅助索引（也叫作二级索引）只是根据索引列构建 B+Tree，但在 B+Tree 的每一行都存了主键信息，加速回表操作。； 二级索引会比聚簇索引小很多， 通常创建辅助索引就是为了提升查询效率。 可以创建多个辅助索引。索引类型MySQL 中索引是在存储引擎层而非服务器层实现的，所以不同存储引擎层支持的索引类型可以不同。在 MySQL 中不同存储引擎间支持的常见索引类型有： 哈希索引（Memory/InnoDB adaptive Hash index/NDB）； B+Tree 索引（MyISAM/InnoDB）； 全文索引（MyISAM/InnoDB）； 空间索引（MyISAM R-Tree）； 分形树索引（TokuDB Fractal Tree Index）。如下表所示： 索引类型 存储引擎 哈希索引 Memory/InnoDB Adaptive Hash Index/NDB B+Tree 索引 MylSAM/InnoDB 全文索引 MylSAM/InnoDB 空间索引 MylSAM R-Tree 分形树索引 TokuDB Fractal Tree Index 在 MySQL InnoDB 中索引通常可以分为两大类： 主键索引（即聚簇索引）； 辅助索引（非聚簇索引） 。对于没有指定主键的表，InnoDB 会自己选择合适字段为主键，其选择顺序如下： 显式主键； 第一个唯一索引（要求唯一索引所有列都非 NULL）； 内置的 6 字节 ROWID。优先使⽤ UNSIGNED 自增列显示创建主键！根据索引列个数和功能描述不同索引也可以分为： 联合索引，指在多个字段联合组建索引的。 覆盖索引，当通过索引即可查询到所有记录，不需要回表到聚簇索引时，这类索引也叫作覆盖索引； 主键查询是天然的覆盖索引，联合索引可以是覆盖索引。看 SQL 语句是否使用到覆盖索引： 通常在查看执行计划时， Extra 列为 Using index 则表示优化器使用了覆盖索引。优先考虑使用覆盖索引，这是因为如果 SQL 需要查询辅助索引中不包含的数据列时，就需要先通过辅助索引查找到主键值，然后再回表通过主键查询到其他数据列（即回表查询），需要查询两次。而覆盖索引能从索引中直接获取查询需要的所有数据，从⽽避免回表进行二次查找，节省IO，效率较⾼。例如：--- 如果 uid 不是主键，那可以将索引添加为 index(uid，email)，以获得查询性能提升。SELECT email，uid FROM user_email WHERE uid=xx;索引使用技巧主要有： ==谓词==，就是条件表达式，通俗讲就是过滤字段，如下面的 SQL 语句： select * from city where city=&quot;Beijing&quot; and last_update=&quot;2019-08-01&quot;; 可以拆解为： 简单谓词：city 和 last_update 组合谓词：city and last_update ==过滤因子==，谓词之后就可以计算谓词的过滤因子： 直接描述了谓词的选择性 表示满足谓词条件的记录行数所占比例 过滤因子越小意味着能过滤越多数据，需要在这类谓词字段上创建索引； 过滤因子的计算算法，就是满足谓词条件的记录行数除以表总行数： $简单谓词的过滤因子 = \\frac{谓词结果集的数量}{表总行数}$ $组合谓词的过滤因子 = 谓词 \\ 1 \\ 的过滤因子 * 谓词 \\ 2 \\ 的过滤因子$ ==基数（Cardinality）==，是某个键值去重后的行数， 索引列不重复记录数量的预估值，MySQL 优化器会依赖于它。 ==选择率==，$ \\frac{count(distinct city)}{count(*)}$，越接近 1 ，越适合创建索引，例如主键和唯一键的选择率都是 1。 ==回表==，指无法通过索引扫描访问所有数据，需要回到主表进行数据扫描并返回。 栗子借助谓词、过滤因子、基数、选择率以及回表可以创建高效索引。用一个例子来看下，如何快速根据 SQL 语句计算谓词、过滤因子、基数和选择率。根据 SQL 语句可以快速得到谓词信息： 简单谓词 city 和 last_update； 组合谓词 city and last_update。计算每个谓词信息的过滤因子，过滤因子越小表示选择性越强，字段越适合创建索引。例如： $city \\ 的过滤因子 = \\frac{谓词 city 结果集的数量 }{表总行数}= \\frac{select \\ count() \\ from \\ city \\ where \\ city \\ = \\ ‘BeiJing’ } {select \\ count() \\ from \\ city } = 0.2$； $last_update \\ 的过滤因子 = \\frac{谓词 last_update \\ 结果集的数量}{表总行数} = \\frac{select \\ count() \\ from \\ city \\ where \\ last_update \\ = \\ ‘2019-08-01’}{select \\ count() \\ from \\ city \\ } = \\ 0.1$； $组合谓词 \\ = \\ city \\ 过滤因子 * last_update \\ 过滤因子 = 0.2 * 0.1 = 0.02 $，组合谓词的过滤因子为 2%，即只有表总行数的 2% 匹配过滤条件，可以考虑创建组合索引 (city，last_update)。除谓词信息、过滤因子外，字段基数和选择率信息可以帮助了解字段数据的分布情况。统计信息MySQL InnoDB 的==统计信息==参考==基数 Cardinality==的信息。Cardinality 能快速告知字段的选择性，高选择性字段有利于创建索引。优化器在选择执行计划时会依赖该信息，通常这类信息也叫作统计信息，数据库中对于统计信息的采集是在存储引擎层进行的。执行下面命令会看到 Cardinality，同时也会触发 MySQL 数据库对 Cardinaltiy 值的统计：show index from table_name;除此之外，还有三种更新策略： 触发统计：Cardinality 统计信息更新发生在 INSERT 和 UPDATE 时，InnoDB 存储引擎内部更新的 Cardinality 信息的策略为：表中超过1/16的数据发生变化：stat_modified_counter &amp;gt; 2000 000 000 （20亿）； 采样统计（sample）：为了减少统计信息更新造成的资源消耗，数据库对Cardinality 通过采样来完成统计信息更新，每次随机获取 innodb_stats_persistent_sample_pages 页的数量进行 Cardinality 统计； 手动统计：alter table table_name engine=innodb 或 analyze table table_name，当发现优化器选择错误的执行计划或没有走理想的索引时，执行 SQL 语句来手动统计信息有时是一种有效的方法。由于采样统计的信息是随机获取 8 个（8 是由 ==innodb_stats_transient_sample_pages== 参数指定）页面数进行分析，这就意味着下一次随机的 8 个页面可能是其他页面，其采集页面的 Carinality 也不同。因此当表数据无变化时也会出现 Cardinality 发生变化的情况，如下图所示：关于统计信息的采集，涉及如下主要参数： ==information_schema_stats_expiry:86400==，Cardinality 存放过期时间，设置为 0 表示实时获取统计信息，严重影响性能，建议设置默认值并通过手动刷新统计信息； ==innodb_stats_auto_recalc:ON==，是否自动更新统计信息，默认即可； ==innodb_stats_include_delete_marked :OFF==，计算持久化统计信息时 InnoDB 是否包含 Delete-marked 记录，默认即可； ==innodb_stats_method:nulls_equal==，用来判断如何对待索引中出现的 NULL 值记录，默认为 nulls_equal，表示将 NULL 值记录视为相等的记录； ==innodb_stats_on_metadata==， 默认值 OFF，执行 SQL 语句 ANALYZE TABLE、SHOW TABLE STATUS、SHOQ INDEX，以及访问 INFORMATION_SCHEMA 架构下表 tables和statistics 时，是否重新计算索引的 Cardinality 值； ==innodb_stats_persistent:ON==，表示通过 ANALYZE TABLE 计算得到的 Cardinality值存放到磁盘上； ==innodb_stats_persistent_sample_pages:20==，表示 ANALYZE TABLE 更新Cardinality 值时采样页的数量； ==innodb_stats_transient_sample_pages:8==，表示每次统计 Cardinality 时采样页的数量，默认为 8。索引使用细节 首先是创建索引后，如何确认 SQL 语句是否走索引了？ 创建索引后通过查看执行 SQL 语句的执行计划，即可知道 SQL 语句是否走索引； 执行计划重点关注跟索引相关的关键项有： type ==possible_keys==，表示查询可能使用的索引 ==key==，表示真正实际使用的索引 ==key_len==，表示使用索引字段的长度 ref ==Extra== ，Extra 显示 use index 时就表示该索引是覆盖索引，通常性能排序的结果是 usd index &amp;gt; use where &amp;gt; use filsort，如下图所示： key_len当索引选择组合索引时，通过计算 key_len 来了解有效索引长度对索引优化也是非常重要的。key_len 表示得到结果集所使用的选择索引的长度[字节数]，不包括 order by，也就是说如果 order by 也使用了索引则 key_len 不计算在内。key_len 计算规则从两个方面考虑： 索引字段的数据类型，根据索引字段的定义可以分为变长和定长两种数据类型： 定长数据类型，比如 char、int、datetime，需要有是否为空的标记，这个标记需要占用 1 个字节； 变长数据类型，比如 Varchar，除了是否为空的标记外，还需要有长度信息，需要占用 2 个字节；（备注：当字段定义为非空的时候，是否为空的标记将不占用字节）。 表、字段使用的字符集： 不同的字符集计算的 key_len 不一样，例如，GBK 编码的是一个占用 2 个字节大小的字符，UTF8 编码的是一个占用 3 个字节大小的字符。 举例说明：在四类字段上创建索引后的 key_len 的计算： Varchr(10) 变长字段且允许 NULL:10(Character Set：utf8=3，gbk=2，latin1=1)+1（标记是否为 NULL 需要 1 个字节）+ 2（变长字段存储长度信息需要 2 个字节）； Varchr(10) 变长字段且不允许 NULL:10(Character Set：utf8=3，gbk=2，latin1=1)+2（变长字段存储长度信息需要2个字节），非空不再需要占用字节来标记是否为空； Char(10) 固定字段且允许 NULL:10(Character Set：utf8=3，gbk=2，latin1=1)+1（标记是否为 NULL 需要 1 个字节）； Char(10) 固定字段且不允许 NULL:10(Character Set：utf8=3，gbk=2，latin1=1)，非空不再需要占用字节来标记是否为空。 最左前缀匹配原则通过 key_len 计算也帮助了解索引的最左前缀匹配原则。最左前缀匹配原则，是指在使用 B+Tree 联合索引进行数据检索时，MySQL 优化器会读取谓词（过滤条件），并按照联合索引字段创建顺序一直向右匹配，直到遇到范围查询或非等值查询后停止匹配，此字段之后的索引列不会被使用，这时计算 key_len 可以分析出联合索引实际使用了哪些索引列。设计性能索引创建一个 test 表。 在 a、b、c 上创建索引，执行表中的 SQL 语句，快速定位语句孰好孰坏。首先分析 key_len， 因为 a、b、c 不允许 NULL 的 varchar(50)，那么，每个字段的 key_len 为 50×4+2=202，整个联合索引的 key_len 为 202×3=606。 序号 SQL 语句 是否索引 结果 sql1 select * from test where a=? and b=? and c=?; key: idx_a_b_c key_len: 606 Extra: Using index 可以使用覆盖索引，性能好 sql2 select * from test where a=? and b=? and c=? order by c; key: idx_a_b_c key_len: 606 Extra: Using index 可以使用覆盖索引同时可以避免排序，性能好 sql3 select * from test where b=? and c=?; key: idx_a_b_c key_len: 606 Extra: Using where, Using index 可以使用覆盖索引，但是需要根据 where 字句进行过滤 sql4 select * from test where a=? order by c; key: idx_a_b_c key_len: 606 Extra: Using index, Using filesort 可以使用部分索引 a，但无法避免排序，性能差 sql5 select * from test order by a, b, c; key: idx_a_b_c key_len: 606 Extra: Using index 完全使用覆盖索引，同时可以避免排序，性能好 sql6 select * from test order by a, b, c desc; key: idx_a_b_c key_len: 606 Extra: filesort 可以使用覆盖索引，但无法避免排序，这是因为 MySQL InnoDB 创建索引时默认asc升序，索引无法自动倒序排序； sql7 select * from test where a in (?,?) and b in (?,?) and c=?; key: idx_a_b_c key_len: 606 Extra: Using where, Using index 可以使用覆盖索引，但是需要根据 where 子句进行过滤（非定值查询） 在实际设计高性能索引时，可以结合前面的内容，按照如下步骤进行分析。 定位由于索引不合适或缺少索引而导致的慢查询： 通常在业务建库建表时就需要提交业务运行相关的 SQL 给 DBA 审核； 也可以借助Arkcontrol Arkit 来自动化审核。比如，慢查询日志分析，抓出运行慢的 SQL 进行分析； 也可以借助第三方工具例如 Arkcontrol 慢查询分析系统进行慢查询采集和分析； 在分析慢查询时进行参数最差输入，同时，对 SQL 语句的谓词进行过滤因子、基数、选择率和 SQL 查询回表情况的分析。 设计索引: 设计索引的目标是让查询语句运行得足够快，同时让表、索引维护也足够快； 例如，使用业务不相关自增字段为主键，减缓页分裂、页合并等索引维护成本，加速性能； 也可以使用第三方工具进行索引设计，例如 Arkcontrol SQL 优化助手，会给出设计索引的建议。 创建索引策略： 优先为搜索列、排序列、分组列创建索引，必要时加入查询列创建覆盖索引； 计算字段列基数和选择率，选择率越接近于 1 越适合创建索引； 索引选用较小的数据类型（整型优于字符型），字符串可以考虑前缀索引； 不要建立过多索引，优先基于现有索引调整顺序；参与比较的字段类型保持匹配并创建索引。 调优索引： 分析执行计划； 更新统计信息（Analyze Table）； Hint 优化，方便调优（FORCE INDEX、USE INDEX、IGNORE INDEX、STRAIGHT_JOIN）； 检查连接字段数据类型、字符集； 避免使用类型转换； 关注 optimizer_switch，重点关注索引优化特性 ： MRR（Multi-Range Read） MRR 优化是为了减少磁盘随机访问，将随机 IO 转化为顺序 IO 的数据访问，其方式是将查询得到辅助索引的键值放到内存中进行排序，通常是按照主键或 RowID 进行排序，当需要回表时直接根据主键或 RowID 排序顺序访问实际的数据文件，加速 SQL 查询。 ICP（Index Condition Pushdown） ICP 优化也是对索引查询的优化特性，MySQL 根据索引查询到数据后会优先应用 where 条件进行数据过滤，即无法使用索引过滤的 where 子句，其过滤由之前 Server 层的数据过滤下推到了存储引擎层，可以减少上层对记录的检索，提高数据库的整体性能。 创建索引的规范MySQL 创建索引规范如下： 命名规范， 内部统一。 索引维护的成本，单张表的索引数量不超过 5 个，单个索引中的字段数不超过 5 个。 表必需有主键，推荐使⽤ UNSIGNED 自增列作为主键。表不设置主键时 InnoDB 会默认设置隐藏的主键列，不便于表定位数据同时也会增大 MySQL 运维成本（例如主从复制效率严重受损、pt 工具无法使用或正确使用）。 唯一键由 3 个以下字段组成，并且在字段都是整形时，可使用唯一键作为主键。其他情况下，建议使用自增列或发号器作主键。 禁止冗余索引、禁止重复索引，索引维护需要成本，新增索引时优先考虑基于现有索引进行 rebuild，例如 (a,b,c)和 (a,b)，后者为冗余索引可以考虑删除。重复索引也是如此，例如索引(a)和索引(a,主键ID) 两者重复，增加运维成本并占用磁盘空间，按需删除冗余索引。 联表查询时，JOIN 列的数据类型必须相同，并且要建⽴索引。 不在低基数列上建⽴索引，例如“性别”。 在低基数列上创建的索引查询相比全表扫描不一定有性能优势，特别是当存在回表成本时。 选择区分度（选择率）大的列建立索引。组合索引中，区分度（选择率）大的字段放在最前面。 对过长的 Varchar 段建立索引。建议优先考虑前缀索引，或添加 CRC32 或 MD5 伪列并建⽴索引。 合理创建联合索引，(a,b,c) 相当于 (a) 、(a,b) 、(a,b,c)。 合理使用覆盖索引减少IO，避免排序。 " }, { "title": "设计高性能的数据库表", "url": "/posts/05-mysql-database-table-desgin/", "categories": "Database, MySQL", "tags": "MySQL, Table Design", "date": "2020-07-11 07:53:22 +0000", "snippet": "范式与反范式表设计是高性能数据库的基础。要设计出高性能的库表结构，必须要提到数据库范式，范式是基础规范，反范式是针对性设计。范式范式是关系数据库理论的基础，也是在设计数据库结构过程中，需要遵循的规则和指导方法。数据库的设计范式是数据库设计所需要满足的规范。只有理解数据库的设计范式，才能设计出高效率、优雅的数据库，否则可能会设计出低效的库表结构。目前关系数据库有六种范式： 第一范式（1NF） 第二范式（2NF） 第三范式（3NF） 巴斯-科德范式（BCNF，Boyce Codd Normal Form） 第四范式（4NF） 第五范式（5NF，还又称完美范式）。满足最低要求的叫第一范式，简称 1NF。在第一范式基础上进一步满足一些要求的为第二范式，简称 2NF。其余依此类推。各种范式呈递次规范，越高的范式数据库冗余越小。通常所用到的只是前三个范式，即：第一范式（1NF），第二范式（2NF），第三范式（3NF）。第一范式第一范式无重复的列，表中的每一列都是不可再拆分的基本数据项，强调列的原子性.。在实际场景中，一个联系人有家庭电话和公司电话，那么以“姓名、性别、电话”为表头的表结构就没有达到 1NF。要符合 1NF 只需把电话列拆分，让表头变为姓名、性别、家庭电话、公司电话即可。第二范式第二范式属性完全依赖于主键。 首先需要满足 1NF； 此外，还需要包含两部分内容： 表必须有一个主键； 没有包含在主键中的列，必须完全依赖于主键，而不能只依赖于主键的一部分。即要求实体的属性完全依赖（指不能存在仅依赖主关键字一部分的属性）于主关键字。 第三范式第三范式属性不传递依赖于其他非主属性， 首先需要满足 2NF； 另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在，非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况。第二范式和第三范式的区别 第二范式：非主键列是否依赖主键（包括一列通过某一列间接依赖主键），要是有依赖关系就是第二范式； 第三范式：非主键列是否直接依赖主键，不能是那种通过传递关系的依赖。要是符合这种依赖关系就是第三范式。综上，3NF 是 2NF 的子集，2NF 是 1NF 的子集。设计符合 2NF 的表接下来以订单信息表为例，总结设计一个符合 2NF 表的过程。 首先，原始的订单信息表，如下图所示： 订单编号 商品编号 商品名称 数量 单位 价格 客户 所属单位 001 1 产品一 1 个 100 张三 XXX 002 2 产品二 2 个 200 张三 XXX 003 3 产品三 1 个 50 李四 XXX 图中，以订单编号和商品编号作为联合主键，商品名称、单位、价格等信息不与主键相关，只与编号相关，违反了第二范式。 应该对订单信息表进行拆分，商品信息单独一张表，订单项目一张表，如下所示，拆分分成 3 张表： 包含客户信息的订单信息表： 订单编号 客户 所属单位 001 张三 XXX 002 李四 XXX 包含商品详情的商品信息表： 商品编号 商品名称 单位 价格 1 产品一 个 100 2 产品三 个 200 3 产品三 个 50 包含订单详情的订单详情表： 订单编号 商品编号 数量 001 1 1 001 2 2 002 3 1 范式的优点 避免数据冗余，减少维护数据完整性的麻烦； 减少数据库的空间； 数据变更速度快； 范式的优缺点。 范式的缺点 按照范式的规范设计的表，等级越高的范式设计出来的表数量越多。 获取数据时，表关联过多，性能较差。 表的数量越多，查询所需要的时间越多。也就是说所用的范式越高，对数据操作的性能越低。 反范式范式是普适的规则，满足大多数的业务场景的需求。对于一些特殊的业务场景，范式设计的表，无法满足性能的需求。此时，就需要根据业务场景，在范式的基础之上进行灵活设计，也就是反范式设计。反范式设计主要从三方面考虑： 业务场景； 相应时间； 字段冗余。反范式设计就是用空间来换取时间，提高业务场景的响应时间，减少多表关联。主要的优点如下： 允许适当的数据冗余，业务场景中需要的数据几乎都可以在一张表上显示，避免关联； 可以设计有效的索引。范式与反范式异同范式化模型： 数据没有冗余，更新容易； 当表的数量比较多，查询数据需要多表关联时，会导致查询性能低下。反范式化模型： 冗余将带来很好的读取性能，因为不需要 join 很多表； 虽然需要维护冗余数据，但是对磁盘空间的消耗是可以接受的。MySQL 使用原则和设计规范MySQL 虽然具有很多特性，并提供了很多功能，但是有些特性会严重影响它的性能，比如： 在数据库里进行计算； 写大事务； 大 SQL； 存储大字段等。基本使用原则发挥 MySQL 的最佳性能，需要遵循 3 个基本使用原则： 首先是需要让 MySQL 回归存储的基本职能：MySQL 数据库只用于数据的存储，不进行数据的复杂计算，不承载业务逻辑，确保存储和计算分离； 其次是查询数据时，尽量单表查询，减少跨库查询和多表关联； 还有就是要杜绝大事务、大 SQL、大批量、大字段等一系列性能杀手： 大事务，运行步骤较多，涉及的表和字段较多，容易造成资源的争抢，甚至形成死锁。一旦事务回滚，会导致资源占用时间过长； 大 SQL，复杂的 SQL 意味着过多的表的关联，MySQL 数据库处理关联超过 3 张表以上的 SQL 时，占用资源多，性能低下； 大批量，意味着多条 SQL 一次性执行完成，必须确保进行充分的测试，并且在业务低峰时段或者非业务时段执行； 大字段，blob、text 等大字段，尽量少用。必须要用时，尽量与主业务表分离，减少对这类字段的检索和更新。 基本设置规则 必须指定默认存储引擎为 InnoDB，并且禁用 MyISAM 存储引擎。 随着 MySQL 8.0 版本的发布，所有的数据字典表都已经转换成了 InnoDB，MyISAM 存储引擎已成为了历史！ 默认字符集 UTF8mb4。 以前版本的 UTF8 是 UTF8mb3，未包含个别特殊字符，新版本的 UTF8mb4 包含所有字符，官方强烈建议使用此字符集； 关闭区分大小写功能。 设置 lower_case_tables_name=1，即可关闭区分大小写功能，即大写字母 T 和小写字母 t 一样。 系统中区分大小写的库表，转换为不区分大小写的库表的操作步骤如下（因为要修改底层数据，还是比较麻烦的）： MySQL dump 导出数据库； 修改参数 lower_case_tables_name=1； 导入备份数据时，必须停止数据库，停止业务，影响非常大； 开启 per-table 表空间，开启后，每张业务表会单独创建一个独立于系统表空间的表空间，便于空间的回收，数据的迁移。MySQL 数据库提供的功能很全面，但并不是所有的功能性能都高效： 存储过程、触发器、视图、event： 为了存储计算分离，这类功能尽量在程序中实现！！！ 这些功能非常不完整，调试、排错、监控都非常困难，相关数据字典也不完善，存在潜在的风险； 在生产数据库中，禁止使用； lob、text、enum、set： 这些字段类型，在 MySQL 数据库的检索性能不高，很难使用索引进行优化； 如果必须使用这些功能，一般采取特殊的结构设计，或者与程序结合使用其他的字段类型替代； 比如：set 可以使用整型（0，1，2，3）、注释功能和程序的检查功能集合替代。 规范命名统一的规范命名，可以增加可读性，减少隐式转换。命名规范如下，命名时的字符取值范围为：a~z，0~9 和 _（下画线）： 所有表名小写，不允许驼峰式命名； 允许使用 -（横线）和 （空格）；如下图所示，当使用 -（横线），后台默认会转化成 @002d； 不允许使用其他特殊字符作为名称，减少潜在风险。 库名命名规则数据库库名的命名规则必须遵循“见名知意”的原则，即库名规则为：“数据库类型代码 + 项目简称 + 识别代码 + 序号”。这样包含了更多的业务信息，比如： 出入系统业务生产库：AOCT、AOCT1、AOCT2； 出入系统业务开发库：AOCTDEV、AOCTDEV1、AOCTDEV2； 出入系统业务测试库：AOCTTEST、AOCTTEST1、AOCTTEST2； 只有一个数据库，则不加序号，否则末尾增加序号； 生产库不加识别代码，否则需要增加识别代码 DEV 或 TEST； 如果只作历史库，则只需要：项目简称 +H+ 序号； 图例为常用的识别代码。程序账号与数据库名称保持一致。如果所有的程序账号都是 root@‘%’，密码也一样，很容易错连到其他的数据库，造成误操作。表名命名规则表名的命名规则分为： 单表仅使用 a~z、_； 分表名称为表名_编号； 业务表名代表用途、内容：子系统简称_业务含义_后缀。常见业务表类型有： 临时表，tmp； 备份表，bak； 字典表，dic； 日志表，log。字段命名规则字段名精确，遵循“见名知意”的原则，格式：名称_后缀。避免普遍简单、有歧义的名称： 用户表中，用户名的字段为 UserName 比 Name 更好； 布尔型的字段，以助动词（has/is）开头： 用户是否有留言 hasmessage，用户是否通过检查 ischecked 等。 常见后缀如下： 流水号/无意义主键，后缀为 id，比如 task_id； 时间，后缀为 time，insert_time。索引命名规则索引命名格式，主要为了区分哪些对象是索引： 前缀_表名（或缩写）_字段名（或缩写）； 主键必须使用前缀pk_； UNIQUE 约束必须使用前缀uk_； 普通索引必须使用前缀“idx_”。数据库规范库表字段的命名，能够提高数据库的易读性，为数据库表设计打下基础。表设计的一些规则 显式指定需要的属性；创建表时显示指定字符集、存储引擎、注释信息等 不同系统之间，统一规范； 不同表之间的相同字段或者关联字段，字段类型/命名要保持一致；库表字符集和前端程序、中间件必须保持一致的 UTF8mb4。 InnoDB 表的注意事项 主键列，UNSIGNED 整数，使用 auto_increment；禁止手动更新 auto_increment，可以删除； 必须添加 comment 注释； 必须显示指定的 engine。 表必备三字段：id、 xxx_create、 xxx_modified： id 为主键，类型为 unsigned bigint 等数字类型； xxx_create、xxx_modified 的类型均为 datetime 类型，分别记录该条数据的创建时间、修改时间。 备份表/临时表等常见表的设计规范 备份表，表名必须添加 bak 和日期，主要用于系统版本上线时，存储原始数据，上线完成后，必须及时删除； 临时表，用于存储中间业务数据，定期优化，及时降低表碎片； 日志类表，首先考虑不入库，保存成文件，其次如果入库，明确其生命周期，保留业务需求的数据，定期清理； 大字段表，把主键字段和大字段，单独拆分成表，并且保持与主表主键同步，尽量减少大字段的检索和更新； 大表，根据业务需求，从垂直和水平两个维度进行拆分： 垂直拆分：按列关联度。 水平拆分： 按照时间、地域、范围等； 冷热数据（历史数据归档）。 字段设计要求 根据业务场景需求，选择合适的类型，最短的长度；确保字段的宽度足够用，但也不要过宽； 所有字段必须为 NOT NULL，空值则指定 default 值，空值难以优化，查询效率低。比如： 人的年龄用 unsigned tinyint（范围 0~255，人的寿命不会超过 255 岁）； 海龟就必须是 smallint，但如果是太阳的年龄，就必须是 int； 如果是所有恒星的年龄都加起来，那么就必须使用 bigint; 表字段数少而精，尽量不加冗余列； 单实例表个数必须控制在 2000 个以内。 单表分表个数必须控制在 1024 个以内。 单表字段数上限控制在 20~50 个。禁用 ENUM、SET 类型。兼容性不好，性能差。解决方案： 使用 TINYINT，在 COMMENT 信息中标明被枚举的含义，如下： `is_disable` TINYINT UNSIGNED DEFAULT &#39;0&#39; COMMENT &#39;0:启用 1:禁用 2:异常’。 禁用列为 NULL MySQL 难以优化 NULL 列； NULL 列加索引，需要额外空间； 含 NULL 复合索引无效。 解决方案：在列上添加 NOT NULL DEFAULT 缺省值。禁止 VARBINARY、BLOB 存储图片、文件等。 禁止在数据库中存储大文件，例如照片，可以将大文件存储在对象存储系统中，数据库中存储路径。不建议使用 TEXT/BLOB 处理性能差； 行长度变长； 全表扫描代价大。 解决方案：拆分成单独的表。存储字节大小存储字节越小，占用空间越小。尽量选择合适的整型，如下图所示： 主键列，无负数，建议使用 INT UNSIGNED 或者 BIGINT UNSIGNED； 预估字段数字取值会超过 42 亿，使用 BIGINT 类型； 短数据使用 TINYINT 或 SMALLINT，比如：人类年龄，城市代码； 使用 UNSIGNED 存储非负数值，扩大正数的范围。int(3)/int(5) 区别如下图所示： 正常显示没有区别； 3 和 5 仅是最小显示宽度而已； 有 zerofill 等扩展属性时则显示有区别。浮点数与定点数区别浮点数与定点数区别，如下图所示。 浮点数：float、double（或 real）。 定点数：decimal（或 numberic）。 从上图中可以观察到： 浮点数存在误差问题； 尽量避免进行浮点数比较； 对货币等对精度敏感的数据，应该使用定点数。 N 解释字符集都为 UTF8mb4，中文存储占三个字节，而数据或字母，则只占一个字节。下面看一下字符类型中 N 的解释： CHAR(N) 和 VARCHAR(N) 的长度 N，不是字节数，而是字符数； username 列可以存多少个汉字，占用多少个字节； username 最多能存储 40 个字符，占用 120 个字节。Char 与 Varchar 类型存储字符串长度相同的全部使用 Char 类型；字符长度不相同的使用 Varchar 类型，不预先分配存储空间，长度不要超过 255。Char 和 Varchar 占用空间的对比，如下图所示。Varchar 值存储为 1 字节或 2 字节长度前缀加数据。如果值不超过 255 个字节，则列使用一个字节长度；如果值可能需要超过 255 个字节，则列使用两个字节长度。为什么超过 255 个字节时，必须使用两个字节长度。\\[2^8=256, 1 个字节是 8 位\\]案例IP 处理 一般使用 Char(15) 进行存储，但是当进行查找和统计时，字符类型不是很高效。 MySQL 数据库内置了两个 IP 相关的函数 INET_ATON()、INET_NTOA()，可以实现 IP 地址和整数的项目转换。 因此，使用 INT UNSIGNED（占用 4 个字节）存储 IP，非 Char(15)。占 15 个字节。下图所示，IP：192.168.0.1 与整数之间的转换。将 IP 的存储从字符型转换成整形，转化后数字是连续的，提高了查询性能，使查询更快，占用空间更小。TIMESTAMP 处理同样的方法，使用 MySQL 内置的函数(FROM_UNIXTIME()，UNIX_TIMESTAMP())，可以将日期转化为数字，用 INT UNSIGNED 存储日期和时间。下图示所示，时间 2007-11-30 10:30:19 与整数之间的转换，转化后数字是连续的，占用空间更小，并且可以使用索引提升查询性能。本案例展示的是，不当的数字类型，导致表无法插入新数据，如下图所示：当使用 load data 进行批量加载数据时，会导致 1467 错误。根据分析，导致 1467 错误是由于 auto_increment 的值，超过了 int 类型的取值范围。原因分析部分显示，max(seq_id) 为 2147477751，而 int 的范围为 -2147483648~ 2147483647，还剩余空间 5896，而程序需要导入 1 万行，所以报错。解决办法将 int 改为 bigint 或者将数据分表。表大小及使用频率设计表时，必须考虑表的大小和使用频率，避免由于取值范围过小，导致程序运行失败。对于 InnoDB 表，要求创建一个与业务无关的主键，比如：每张表以 id 列为主键。但是 id 列非常常见，完全无法表达更深层次的意思，特别是在做两张表的联合查询时，它们都有相同的 id 主键的情况下。如果程序用的是列名，该如何区分 Accounts 表的 id 和 Bugs 的 id 呢？如下图所示，列名 id 并不会使查询变得更加清晰。但如果列名叫作 bug_id 或者 account_id，事情就会变得更加简单。使用主键来定位唯一一条记录，因此主键的列名就应该更加便于理解，如下图所示。在缺陷跟踪数据库中，使用 Products 表中的 product_id 主键列来关联产品和对应的联系人。每个账号可能对应很多产品，每个产品又引用了一个联系人，因此产品和帐号之间是多对一的关系随着项目日趋成熟，一个产品可能会有多个联系人，除了多对一的关系外，还需要支持产品到账号的一对多的关系。Products 表中的一行数据必须要存储多个联系人。为了把数据库表结构的改动控制在最小范围内，定将 account_id 的类型修改为 Varchar，这样可以在该列中存储多个账号 id，每个账号 id 之间用逗号分隔。这样的设计似乎是可行的，没有创建额外的表和列，仅仅改变了一个字段的数据类型。然而，这样的设计所必须承受的性能和数据完整性问题。所有外键都合并在一个单元格内，查询会变成异常困难。只能通过正则表达式进行模糊匹配，不但可能会返回错误的结果，而且无法使用索引提高性能。例如：查询指定产品的账号时，联合两张表将不能使用任何索引。这样的查询必然会对两张表进行全表扫描，并创建一个交叉结果集，然后使用正则表达式遍历每一行联合后的数据进行匹配。出于性能优化方面的考虑，可能在数据库的结果中需要使用反范式的设计。上述 Products 表中将列表存储为以逗号分隔的字符串，就是反范式的一个实例。这个设计只是简化了存储，但是性能低下。因此谨慎使用反范式的数据库设计。尽可能地使用规范化的数据库设计。根据业务需求，设计合理的反范式，解决方案是：创建一个交叉表。将 account_id 存储在一张单独的表中，而不是存储在 Products 表中，从而确保每个独立的 account 值都可以占据一行。这张新表 Contacts，实现了 Products 和 Accounts 的多对多关系。当一张表有指向两张表的外键时，称这种表为交叉表，它实现了两张表之间的多对多关系。这意味着每个产品都可以通过交叉表和多个账号关联；同样地，一个账号也可以通过交叉表和多个产品关联。当“查询指定产品的账号”时，就可以直接使用下面的联合查询语句高效实现。总结 以高性能为目标，库表设计以范式为主，根据特殊业务场景使用反范式，允许必要的空间换时间。 规范数据库的使用原则，统一规范命名，减少性能隐患，减少隐式转换。 高性能表设计的原则：合适的字段、合适的长度、NOT NULL。 从不同角度思考 IP、timestamp 的转换，拓宽设计思路。 规范的命名可提高可读性，反范式设计可提高查询性能。 " }, { "title": "大幅成倍提升 Redis 处理性能", "url": "/posts/cache-redis-11/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2020-07-06 16:33:22 +0000", "snippet": "主线程Redis 自问世以来，广受好评，应用广泛。但相比， Memcached 单实例压测 TPS 可以高达百万，线上可以稳定跑 20~40 万而言，Redis 的单实例压测 TPS 不过 10~12 万，线上一般最高也就 2~4 万，仍相差一个数量级。Redis 慢的主要原因是单进程单线程模型。虽然一些重量级操作也进行了分拆，如 RDB 的构建在子进程中进行，文件关闭、文件缓冲同步，以及大 key 清理都放在 BIO 线程异步处理，但还远远不够。线上 Redis 处理用户请求时，十万级的 client 挂在一个 Redis 实例上，所有的事件处理、读请求、命令解析、命令执行，以及最后的响应回复，都由主线程完成，纵然是 Redis 各种极端优化，巧妇难为无米之炊，一个线程的处理能力始终是有上限的。当前服务器 CPU 大多是 16 核到 32 核以上，Redis 日常运行主要只使用 1 个核心，其他 CPU 核就没有被很好的利用起来，Redis 的处理性能也就无法有效地提升。而 Memcached 则可以按照服务器的 CPU 核心数，配置数十个线程，这些线程并发进行 IO 读写、任务处理，处理性能可以提高一个数量级以上。IO 线程面对性能提升困境，虽然 Redis 作者不以为然，认为可以通过多部署几个 Redis 实例来达到类似多线程的效果。但多实例部署则带来了运维复杂的问题，而且单机多实例部署，会相互影响，进一步增大运维的复杂度。为此，社区一直有种声音，希望 Redis 能开发多线程版本。因此，Redis 即将在 6.0 版本引入多线程模型，当前代码在 unstable 版本中，6.0 版本预计在明年发版。Redis 的多线程模型，分为主线程和 IO 线程。因为处理命令请求的几个耗时点，分别是请求读取、协议解析、协议执行，以及响应回复等。所以 Redis 引入 IO 多线程，并发地进行请求命令的读取、解析，以及响应的回复。而其他的所有任务，如事件触发、命令执行、IO 任务分发，以及其他各种核心操作，仍然在主线程中进行，也就说这些任务仍然由单线程处理。这样可以在最大程度不改变原处理流程的情况下，引入多线程。命令处理流程Redis 6.0 的多线程处理流程如图所示。主线程负责监听端口，注册连接读事件。当有新连接进入时，主线程 accept 新连接，创建 client，并为新连接注册请求读事件。当请求命令进入时，在主线程触发读事件，主线程此时并不进行网络 IO 的读取，而将该连接所在的 client 加入待读取队列中。Redis 的 Ae 事件模型在循环中，发现待读取队列不为空，则将所有待读取请求的 client 依次分派给 IO 线程，并自旋检查等待，等待 IO 线程读取所有的网络数据。所谓自旋检查等待，也就是指主线程持续死循环，并在循环中检查 IO 线程是否读完，不做其他任何任务。只有发现 IO 线程读完所有网络数据，才停止循环，继续后续的任务处理。一般可以配置多个 IO 线程，比如配置 4~8 个，这些 IO 线程发现待读取队列中有任务时，则开始并发处理。每个 IO 线程从对应列表获取一个任务，从里面的 client 连接中读取请求数据，并进行命令解析。当 IO 线程完成所有的请求读取，并完成解析后，待读取任务数变为 0。主线程就停止循环检测，开始依次执行 IO 线程已经解析的所有命令，每执行完毕一个命令，就将响应写入 client 写缓冲，这些 client 就变为待回复 client，这些待回复 client 被加入待回复列表。然后主线程将这些待回复 client，轮询分配给多个 IO 线程。然后再次自旋检测等待。然后 IO 线程再次开始并发执行，将不同 client 的响应缓冲写给 client。当所有响应全部处理完后，待回复的任务数变为 0，主线程结束自旋检测，继续处理后续的任务，以及新的读请求。Redis 6.0 版本中新引入的多线程模型，主要是指可配置多个 IO 线程，这些线程专门负责请求读取、解析，以及响应的回复。通过 IO 多线程，Redis 的性能可以提升 1 倍以上。多线程方案优劣虽然多线程方案能提升1倍以上的性能，但整个方案仍然比较粗糙。首先所有命令的执行仍然在主线程中进行，存在性能瓶颈。然后所有的事件触发也是在主线程中进行，也依然无法有效使用多核心。而且，IO 读写为批处理读写，即所有 IO 线程先一起读完所有请求，待主线程解析处理完毕后，所有 IO 线程再一起回复所有响应，不同请求需要相互等待，效率不高。最后在 IO 批处理读写时，主线程自旋检测等待，效率更是低下，即便任务很少，也很容易把 CPU 打满。整个多线程方案比较粗糙，所以性能提升也很有限，也就 1~2 倍多一点而已。要想更大幅提升处理性能，命令的执行、事件的触发等都需要分拆到不同线程中进行，而且多线程处理模型也需要优化，各个线程自行进行 IO 读写和执行，互不干扰、等待与竞争，才能真正高效地利用服务器多核心，达到性能数量级的提升。" }, { "title": "Redis 处理容易超时的系统调用的方案", "url": "/posts/cache-redis-10/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2020-07-05 16:33:22 +0000", "snippet": "BIO 线程简介Redis 在运行过程中，不可避免的会产生一些运行慢的、容易引发阻塞的任务，如将内核中的文件缓冲同步到磁盘中、关闭文件，都会引发短时阻塞，还有一些大 key，如一些元素数高达万级或更多的聚合类元素，在删除时，由于所有元素需要逐一释放回收，整个过程耗时也会比较长。而 Redis 的核心处理线程是单进程单线程模型，所有命令的接受与处理、数据淘汰等都在主线程中进行，这些任务处理速度非常快。如果核心单线程还要处理那些慢任务，在处理期间，势必会阻塞用户的正常请求，导致服务卡顿。为此，Redis 引入了 BIO 后台线程，专门处理那些慢任务，从而保证和提升主线程的处理能力。Redis 的 BIO 线程采用生产者-消费者模型。主线程是生产者，生产各种慢任务，然后存放到任务队列中。BIO 线程是消费者，从队列获取任务并进行处理。如果生产者生产任务过快，队列可用于缓冲这些任务，避免负荷过载或数据丢失。如果消费者处理速度很快，处理完毕后就可以安静的等待，不增加额外的性能开销。再次，有新任务时，主线程通过条件变量来通知 BIO 线程，这样 BIO 线程就可以再次执行任务。BIO 处理任务Redis 启动时，会创建三个任务队列，并对应构建 3 个 BIO 线程，三个 BIO 线程与 3 个任务队列之间一一对应。BIO 线程分别处理如下 3 种任务： close 关闭文件任务。rewriteaof 完成后，主线程需要关闭旧的 AOF 文件，就向 close 队列插入一个旧 AOF 文件的关闭任务。由 close 线程来处理； fysnc 任务。Redis 将 AOF 数据缓冲写入文件内核缓冲后，需要定期将系统内核缓冲数据写入磁盘，此时可以向 fsync 队列写入一个同步文件缓冲的任务，由 fsync 线程来处理； lazyfree 任务。Redis 在需要淘汰元素数大于 64 的聚合类数据类型时，如列表、集合、哈希等，就往延迟清理队列中写入待回收的对象，由 lazyfree 线程后续进行异步回收。BIO 处理流程BIO 线程的整个处理流程如图所示。当主线程有慢任务需要异步处理时。就会向对应的任务队列提交任务。提交任务时，首先申请内存空间，构建 BIO 任务。然后对队列锁进行加锁，在队列尾部追加新的 BIO 任务，最后尝试唤醒正在等待任务的 BIO 线程。BIO 线程启动时或持续处理完所有任务，发现任务队列为空后，就会阻塞，并等待新任务的到来。当主线程有新任务后，主线程会提交任务，并唤醒 BIO 线程。BIO 线程随后开始轮询获取新任务，并进行处理。当处理完所有 BIO 任务后，则再次进入阻塞，等待下一轮唤醒。" }, { "title": "MySQL 锁", "url": "/posts/04-mysql-lock/", "categories": "Database, MySQL", "tags": "MySQL, Lock, 页级锁, 表级锁, 行级锁", "date": "2020-07-05 02:53:22 +0000", "snippet": "MySQL 锁分类在 MySQL 中有三种级别的锁：页级锁、表级锁、行级锁。 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。 会发生在：MyISAM、memory、InnoDB、BDB 等存储引擎中。 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度最高。会发生在：InnoDB 存储引擎。 页级锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。会发生在：BDB 存储引擎。 三种级别的锁分别对应存储引擎关系如下图所示：注意：MySQL 中的表锁包括读锁和写锁。只需记住这个表锁模式兼容矩阵即可。InnoDB 中的锁在 MySQL InnoDB 存储引擎中，锁分为行锁和表锁。其中行锁包括两种锁： 共享锁（S）：多个事务可以一起读，共享锁之间不互斥，共享锁会阻塞排它锁； 排他锁（X）：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB 还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁。表锁又分为三种： 意向共享锁（IS）：事务计划给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的 IS 锁； 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的 IX 锁； 自增锁（AUTO-INC Locks）：特殊表锁，自增长计数器通过该“锁”来获得子增长计数器最大的计数值。在加行锁之前必须先获得表级意向锁，否则等待 innodb_lock_wait_timeout 超时后根据innodb_rollback_on_timeout 决定是否回滚事务。InnoDB 自增锁在 MySQL InnoDB 存储引擎中，我们在设计表结构的时候，通常会建议添加一列作为自增主键。这里就会涉及一个特殊的锁：自增锁（即：AUTO-INC Locks），它属于表锁的一种，在 INSERT 结束后立即释放。我们可以执行 show engine innodb status\\G 来查看自增锁的状态信息。在自增锁的使用过程中，有一个核心参数，需要关注，即 innodb_autoinc_lock_mode,它有0、1、2 三个值。保持默认值就行。具体的含义可以参考官方文档，这里不再赘述，如下图所示：InnoDB 锁关系矩阵如下图所示，其中：+ 表示兼容，- 表示不兼容。InnoDB 行锁InnoDB 行锁是通过对索引数据页上的记录（record）加锁实现的。主要实现算法有 3 种：Record Lock、Gap Lock 和 Next-key Lock。Record Lock 锁：单个行记录的锁（锁数据，不锁 Gap）。Gap Lock 锁：间隙锁，锁定一个范围，不包括记录本身（不锁数据，仅仅锁数据前面的Gap）。Next-key Lock 锁：同时锁住数据，并且锁住数据前面的 Gap。排查 InnoDB 锁问题排查 InnoDB 锁问题通常有 2 种方法。打开 innodb_lock_monitor 表，注意使用后记得关闭，否则会影响性能。在 MySQL 5.5 版本之后，可以通过查看 information_schema 库下面的 innodb_locks、innodb_lock_waits、innodb_trx 三个视图排查 InnoDB 的锁问题。InnoDB 加锁行为下面举一些例子分析 InnoDB 不同索引的加锁行为。分析锁时需要跟隔离级别联系起来，我们以 RR 为例，主要是从四个场景分析。主键 + RR。唯一键 + RR。非唯一键 + RR。无索引 + RR。下面讲解第一种情况：主键 + RR，如下图所示。假设条件是：update t1 set name=‘XX’ where id=10。id 为主键索引。加锁行为：仅在 id=10 的主键索引记录上加 X锁。第二种情况：唯一键 + RR，如下图所示。假设条件是：update t1 set name=‘XX’ where id=10。id 为非唯一索引。加锁行为：先通过 id=10 在 key(id) 上定位到第一个满足的记录，对该记录加 X 锁，而且要在 (6,c)~(10,b) 之间加上 Gap lock，为了防止幻读。然后在主键索引 name 上加对应记录的X 锁；再通过 id=10 在 key(id) 上定位到第二个满足的记录，对该记录加 X 锁，而且要在(10,b)~(10,d)之间加上 Gap lock，为了防止幻读。然后在主键索引 name 上加对应记录的X 锁；最后直到 id=11 发现没有满足的记录了，此时不需要加 X 锁，但要再加一个 Gap lock： (10,d)~(11,f)。第四种情况：无索引 + RR，如下图所示。假设条件是：update t1 set name=‘XX’ where id=10。id 为非唯一索引。加锁行为：先通过 id=10 在 key(id) 上定位到第一个满足的记录，对该记录加 X 锁，而且要在 (6,c)~(10,b) 之间加上 Gap lock，为了防止幻读。然后在主键索引 name 上加对应记录的X 锁；再通过 id=10 在 key(id) 上定位到第二个满足的记录，对该记录加 X 锁，而且要在(10,b)~(10,d)之间加上 Gap lock，为了防止幻读。然后在主键索引 name 上加对应记录的X 锁；最后直到 id=11 发现没有满足的记录了，此时不需要加 X 锁，但要再加一个 Gap lock： (10,d)~(11,f)。第四种情况：无索引 + RR，如下图所示。假设条件是：update t1 set name=‘XX’ where id=10。id 列无索引。加锁行为：表里所有行和间隙均加 X 锁。至此，我们分析了四种索引在 RR 隔离级别下的加锁行为，那么在 RC 隔离级别下的加锁行为又是怎样的呢？这个问题留给你自己去思考，答案将在下一节课中给出。在前文中，我们有提到分析锁问题的三个视图，在实际的使用中，可以在数据库发生阻塞的时候，将这三个视图做联合查询来帮助获取详细的锁信息，帮助快速定位找出造成死锁的元凶和被害者，以及具体的事务。InnoDB 死锁在 MySQL 中死锁不会发生在 MyISAM 存储引擎中，但会发生在 InnoDB 存储引擎中，因为 InnoDB 是逐行加锁的，极容易产生死锁。那么死锁产生的四个条件是什么呢？互斥条件：一个资源每次只能被一个进程使用；请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放；不剥夺条件：进程已获得的资源，在没使用完之前，不能强行剥夺；循环等待条件：多个进程之间形成的一种互相循环等待资源的关系。在发生死锁时，InnoDB 存储引擎会自动检测，并且会自动回滚代价较小的事务来解决死锁问题。但很多时候一旦发生死锁，InnoDB 存储引擎的处理的效率是很低下的或者有时候根本解决不了问题，需要人为手动去解决。既然死锁问题会导致严重的后果，那么在开发或者使用数据库的过程中，如何避免死锁的产生呢？这里给出一些建议：加锁顺序一致；尽量基于 primary 或 unique key 更新数据。单次操作数据量不宜过多，涉及表尽量少。减少表上索引，减少锁定资源。相关工具：pt-deadlock-logger。资源争用下面分享一个基于资源争用导致死锁的情况，如下图所示。session1 首先拿到 id=1 的锁，session2 同期拿到了 id=5 的锁后，两者分别想拿到对方持有的锁，于是产生死锁。元数据锁下面分享一个 Metadata lock（即元数据锁）导致的死锁的情况，如下图所示。session1 和 session2 都在抢占 id=1 和 id=6 的元数据的资源，产生死锁。查看 MySQL 数据库中死锁的相关信息，可以执行 show engine innodb status\\G 来进行查看，重点关注 “LATEST DETECTED DEADLOCK” 部分。给大家一些开发建议来避免线上业务因死锁造成的不必要的影响。更新 SQL 的 where 条件时尽量用索引；加锁索引准确，缩小锁定范围；减少范围更新，尤其非主键/非唯一索引上的范围更新。控制事务大小，减少锁定数据量和锁定时间长度 （innodb_row_lock_time_avg）。加锁顺序一致，尽可能一次性锁定所有所需的数据行。本课时到这里就全部结束了，今天主要讲了 MySQL 的事务及其特性、并发事务带来的问题、事务的隔离级别、多版本并发控制 MVCC、InnoDB 锁分类、InnoDB 锁算法、InnoDB 死锁及其优化建议。00:00 高性能MySQL实战" }, { "title": "MySQL 事务", "url": "/posts/03-mysql-transaction/", "categories": "Database, MySQL", "tags": "MySQL, 事务, Transcation", "date": "2020-07-05 02:53:22 +0000", "snippet": "事务是指作为单个逻辑工作单元执行的一系列操作，这些操作要么全做，要么全不做，是一个不可分割的工作单元。一个逻辑工作单元要成为事务，在关系型数据库管理系统中，必须满足 4 个特性，即所谓的 ACID：原子性、一致性、隔离性和持久性： 一致性：事务开始之前和事务结束之后，数据库的完整性限制未被破坏； 原子性：事务的所有操作，要么全部完成，要么全部不完成，不会结束在某个中间环节； 持久性：事务完成之后，事务所做的修改进行持久化保存，不会丢失； 隔离性：当多个事务并发访问数据库中的同一数据时，所表现出来的相互关系。ACID 及它们之间的关系如下图所示，比如 4 个特性中有 3 个与 WAL 有关系，都需要通过 Redo、Undo 日志来保证等：一致性一致性其实包括两部分内容，分别是约束一致性和数据一致性： 约束一致性：很容易想到数据库中创建表结构时所指定的外键、Check、唯一索引等约束。可惜在 MySQL 中，是不支持 Check 的，只支持另外两种，所以约束一致性就非常容易理解了； 数据一致性：一个综合性的规定，或者说是一个把握全局的规定。因为它是由原子性、持久性、隔离性共同保证的结果，而不是单单依赖于某一种技术。原子性原子性就是两个“要么”，即要么改了，要么没改。也就是说用户感受不到一个正在改的状态。MySQL 是通过 WAL（Write Ahead Log）技术来实现这种效果的。原子性和 WAL 到底的关系两者的关系非常大。栗子： 如果事务提交了，那改了的数据就生效了，如果此时 Buffer Pool 的脏页没有刷盘，如何来保证改了的数据生效呢？就需要使用 Redo 日志恢复出来的数据； 如果事务没有提交，且 Buffer Pool 的脏页被刷盘了，那这个本不应该存在的数据如何消失呢？就需要通过 Undo 来实现了，Undo 又是通过 Redo 来保证的，所以最终原子性的保证还是靠 Redo 的 WAL 机制实现的。持久性所谓持久性，就是指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的，接下来的操作或故障不应该对其有任何影响。事务的原子性可以保证一个事务要么全执行，要么全不执行的特性，这可以从逻辑上保证用户看不到中间的状态。但持久性是如何保证的呢？一旦事务提交，通过原子性，即便是遇到宕机，也可以从逻辑上将数据找回来后再次写入物理存储空间，这样就从逻辑和物理两个方面保证了数据不会丢失，即保证了数据库的持久性。隔离性隔离性，指的是一个事务的执行不能被其他事务干扰，即一个事务内部的操作及使用的数据对其他的并发事务是隔离的。锁和多版本控制就符合隔离性。并发事务控制单版本控制-锁锁用独占的方式来保证在只有一个版本的情况下事务之间相互隔离，所以锁可以理解为单版本控制。在 MySQL 事务中，锁的实现与隔离级别有关系，在 RR（Repeatable Read）隔离级别下，MySQL 为了解决幻读的问题，以牺牲并行度为代价，通过 Gap 锁来防止数据的写入，而这种锁，因为其并行度不够，冲突很多，经常会引起死锁。现在流行的 Row 模式可以避免很多冲突甚至死锁问题，所以推荐默认使用 Row + RC（Read Committed）模式的隔离级别，可以很大程度上提高数据库的读写并行度。多版本控制-MVCC多版本控制也叫作 MVCC，是指在数据库中，为了实现高并发的数据访问，对数据进行多版本处理，并通过事务的可见性来保证事务能看到自己应该看到的数据版本。那个多版本是如何生成的呢？每一次对数据库的修改，都会在 Undo 日志中记录当前修改记录的事务号及修改前数据状态的存储地址（即 ROLL_PTR），以便在必要的时候可以回滚到老的数据版本。例如，一个读事务查询到当前记录，而最新的事务还未提交，根据原子性，读事务看不到最新数据，但可以去回滚段中找到老版本的数据，这样就生成了多个版本。多版本控制很巧妙地将稀缺资源的独占互斥转换为并发，大大提高了数据库的吞吐量及读写性能。特性背后的技术原理原子性背后的技术每一个写事务，都会修改 Buffer Pool，从而产生相应的 Redo 日志，这些日志信息会被记录到 ib_logfiles 文件中。因为 Redo 日志是遵循 Write Ahead Log 的方式写的，所以事务是顺序被记录的。在 MySQL 中，任何 Buffer Pool 中的页被刷到磁盘之前，都会先写入到日志文件中，这样做有两方面的保证。如果 Buffer Pool 中的这个页没有刷成功，此时数据库挂了，那在数据库再次启动之后，可以通过 Redo 日志将其恢复出来，以保证脏页写下去的数据不会丢失，所以必须要保证 Redo 先写。因为 Buffer Pool 的空间是有限的，要载入新页时，需要从 LRU 链表中淘汰一些页，而这些页必须要刷盘之后，才可以重新使用，那这时的刷盘，就需要保证对应的 LSN 的日志也要提前写到 ib_logfiles 中，如果没有写的话，恰巧这个事务又没有提交，数据库挂了，在数据库启动之后，这个事务就没法回滚了。所以如果不写日志的话，这些数据对应的回滚日志可能就不存在，导致未提交的事务回滚不了，从而不能保证原子性，所以原子性就是通过 WAL 来保证的。持久性背后的技术如下图所示，一个“提交”动作触发的操作有：binlog 落地、发送 binlog、存储引擎提交、flush_logs， check_point、事务提交标记等。这些都是数据库保证其数据完整性、持久性的手段。那这些操作如何做到持久性呢？前面讲过，通过原子性可以保证逻辑上的持久性，通过存储引擎的数据刷盘可以保证物理上的持久性。这个过程与前面提到的 Redo 日志、事务状态、数据库恢复、参数 innodb_flush_log_at_trx_commit 有关，还与 binlog 有关。这里多提一句，在数据库恢复时，如果发现某事务的状态为 Prepare，则会在 binlog 中找到对应的事务并将其在数据库中重新执行一遍，来保证数据库的持久性。隔离性背后的技术InnoDB 支持的隔离性有 4 种，隔离性从低到高分别为：读未提交、读提交、可重复读、可串行化。读未提交（RU，Read Uncommitted）。它能读到一个事务的中间过程，违背了 ACID 特性，存在脏读的问题，所以基本不会用到，可以忽略。读提交（RC，Read Committed）。它表示如果其他事务已经提交，那么我们就可以看到，这也是一种最普遍适用的级别。但由于一些历史原因，可能 RC 在生产环境中用的并不多。可重复读（RR，Repeatable Read），是目前被使用得最多的一种级别。其特点是有 Gap 锁、目前还是默认的级别、在这种级别下会经常发生死锁、低并发等问题。可串行化，这种实现方式，其实已经并不是多版本了，又回到了单版本的状态，因为它所有的实现都是通过锁来实现的。具体说到隔离性的实现方式，我们通常用 Read View 表示一个事务的可见性。前面讲到 RC 级别的事务可见性比较高，它可以看到已提交的事务的所有修改。而 RR 级别的事务，则没有这个功能，一个读事务中，不管其他事务对这些数据做了什么修改，以及是否提交，只要自己不提交，查询的数据结果就不会变。这是如何做到的呢？随着时间的推移，读提交每一条读操作语句都会获取一次 Read View，每次更新之后，都会获取数据库中最新的事务提交状态，也就可以看到最新提交的事务了，即每条语句执行都会更新其可见性视图。而反观下面的可重复读，这个可见性视图，只有在自己当前事务提交之后，才去更新，所以与其他事务是没有关系的。这里需要提醒大家的是：在 RR 级别下，长时间未提交的事务会影响数据库的 PURGE 操作，从而影响数据库的性能，所以可以对这样的事务添加一个监控。最后我们来讲下可串行化的隔离级别，前面已经提到了，可串行化是通过锁来实现的，所以实际上并不是多版本控制，它的特点也很明显：读锁、单版本控制、并发低。一致性背后的技术一致性可以归纳为数据的完整性。根据前文可知，数据的完整性是通过其他三个特性来保证的，包括原子性、隔离性、持久性，而这三个特性，又是通过 Redo/Undo 来保证的，正所谓：合久必分，分久必合，三足鼎力，三分归晋，数据库也是，为了保证数据的完整性，提出来三个特性，这三个特性又是由同一个技术来实现的，所以理解 Redo/Undo 才能理解数据库的本质。如上图所示，逻辑上的一致性，包括唯一索引、外键约束、check 约束，这属于业务逻辑范畴，这里就不做赘述了。MVCC 实现原理MySQL InnoDB 存储引擎，实现的是基于多版本的并发控制协议——MVCC，而不是基于锁的并发控制。MVCC 最大的好处是读不加锁，读写不冲突。在读多写少的 OLTP（On-Line Transaction Processing）应用中，读写不冲突是非常重要的，极大的提高了系统的并发性能，这也是为什么现阶段几乎所有的 RDBMS（Relational Database Management System），都支持 MVCC 的原因。快照读与当前读在 MVCC 并发控制中，读操作可以分为两类：快照读（Snapshot Read）与当前读 （Current Read） 快照读：读取的是记录的可见版本（有可能是历史版本），不用加锁； 当前读：读取的是记录的最新版本，并且当前读返回的记录，都会加锁，保证其他事务不会再并发修改这条记录。注意：MVCC 只在 Read Commited 和 Repeatable Read 两种隔离级别下工作。如何区分快照读和当前读呢？ 可以简单的理解为： 快照读：简单的 select 操作，属于快照读，不需要加锁。 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。 MVCC 多版本实现举一个“事务对某行记录更新的过程”的案例来讲解 MVCC 中多版本的实现。假设 F1～F6 是表中字段的名字，1～6 是其对应的数据。后面三个隐含字段分别对应该行的隐含ID、事务号和回滚指针，如下图所示。 隐含 ID（DB_ROW_ID），6 个字节，当由 InnoDB 自动产生聚集索引时，聚集索引包括这个 DB_ROW_ID 的值； 事务号（DB_TRX_ID），6 个字节，标记了最新更新这条行记录的 Transaction ID，每处理一个事务，其值自动 +1； 回滚指针（DB_ROLL_PT），7 个字节，指向当前记录项的 Rollback Segment 的 Undo log记录，通过这个指针才能查找之前版本的数据。具体的更新过程，简单描述如下： 假如这条数据是刚 INSERT 的，可以认为 ID 为 1，其他两个字段为空。 当事务 1 更改该行的数据值时，会进行如下操作，如下图所示： 用排他锁锁定该行；记录 Redo log； 把该行修改前的值复制到 Undo log，即图中下面的行； 修改当前行的值，填写事务编号，使回滚指针指向 Undo log 中修改前的行 接下来，与事务 1 相同，此时 Undo log 中有两行记录，并且通过回滚指针连在一起。因此，如果 Undo log 一直不删除，则会通过当前记录的回滚指针回溯到该行创建时的初始内容，所幸的是在 InnoDB 中存在 purge 线程，它会查询那些比现在最老的活动事务还早的 Undo log，并删除它们，从而保证 Undo log 文件不会无限增长，如下图所示。 并发事务问题及解决方案随着数据库并发事务处理能力的大大增强，数据库资源的利用率也会大大提高，从而提高了数据库系统的事务吞吐量，可以支持更多的用户并发访问。但并发事务处理也会带来一些问题，如：脏读、不可重复读、幻读。下面一一解释其含义。脏读一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象的叫作”脏读”（Dirty Reads）。不可重复读一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫作“ 不可重复读”（Non-Repeatable Reads）。幻读一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”（Phantom Reads）。解决方案产生的这些问题，MySQL 数据库是通过事务隔离级别来解决的。举例说明“脏读”和“不可重复读”的问题。MySQL 中默认的事务隔离级别是 RR，这里设置成 RC 隔离级别，此时提交事务 B 修改 id=1 的数据之后，事务 A 进行同样的查询操作，后一次和前一次的查询结果不一样，这就是不可重复读（重新读取产生的结果不一样了）。这里事务 A 读到了事务 B 提交的数据，即是“脏读”。上文讲解了不可重复读的情况，下面我们来看看在RR隔离级别下的情况。当 teacher_id=1时，事务 A 先进行一次读取操作，事务 B 中间修改了 id=1 的数据并提交，事务 C 也插入了一条数据并提交。事务 A 第二次读到的数据和第一次完全相同。所以说它是可重读的。这里我们举个例子来说明“幻读”的问题。行锁可以防止不同事务版本的数据在修改提交时造成数据冲突的情况。但如何避免别的事务插入数据造成的问题呢。我们先来看看在 RC 隔离级别下的处理过程。如下图所示，事务 A 修改了所有 teacher_id=30 的数据，但是当事务 B INSERT 新数据后，事务 A 发现莫名其妙的多了一行 teacher_id=30 的数据， 而且没有被之前的 UPDATE语句所修改，这就是“当前读”的幻读问题。跟上面的例子一样，也是在 RC 事务隔离级别下，这时事务 B INSERT 了一条数据，并提交，而事务 A 读到了事务 B 新插入的数据。这也是幻读，如下图所示。这里就需要重点注意不可重复读和幻读的区别了。前面讲了它们的含义，这个提醒大家的是：不可重复读重点在于 UPDATA 和 DELETE，而幻读的重点在于 INSERT。它们之间最大的区别是如何通过锁机制来解决它们产生的问题。这里说的锁只是使用悲观锁机制。那么在 RR 隔离级别下，事务 A 在 UPDATE 后加锁，事务 B 无法插入新数据，这样事务 A在 UPDATE 前后读的数据保持一致，避免了幻读。跟上面的案例一样，也是在 RR 事务隔离级别下，事务 A 在 UPDATE 后加锁，对于其他两个事务，事务 B 和事务 C 的 INSERT 操作，就必须等事务 A 提交后，才能继续执行。这里就用到了“锁”，这里使用的是 Gap 锁，后面会详细讲解。它和上面的情况一样，解决了“幻读”的发生，如下图所示。" }, { "title": "MySQL 存储引擎", "url": "/posts/02-mysql-storage-engine/", "categories": "Database, MySQL", "tags": "MySQL, 存储引擎, Storage Engine", "date": "2020-07-04 02:53:22 +0000", "snippet": "存储引擎是 MySQL 中具体与文件打交道的子系统。是根据 MySQL AB 公司提供的文件访问层抽象接口定制的一种文件访问机制，这种机制就叫作存储引擎。常用的存储引擎： 远古时期的 MyISAM； 支持事务的 InnoDB； 内存类型的 Memory； 归档类型的 Archive； 列式存储的 Infobright； 一些新兴的存储引擎： 以 RocksDB 为底层基础的 MyRocks 和 RocksDB； 以分形树索引组织存储的 TokuDB 在 MySQL 5.6 版本之前，默认的存储引擎都是 MyISAM，但 5.6 版本以后默认的存储引擎就是 InnoDB 了。InnoDB 存储引擎架构InnoDB 存储引擎的具体架构如下图所示： 上半部分是实例层（计算层），位于内存中； 下半部分是物理层，位于文件系统中实例层实例层分为线程和内存。InnoDB 重要的线程有 Master Thread，Master Thread 是 InnoDB 的主线程，负责调度其他各线程。Master Thread 的优先级最高, 其内部包含几个循环：主循环（loop）、后台循环（background loop）、刷新循环（flush loop）、暂停循环（suspend loop）。Master Thread 会根据其内部运行的相关状态在各循环间进行切换。大部分操作在主循环（loop）中完成，其包含 1s 和 10s 两种操作。1s 操作主要包括如下。日志缓冲刷新到磁盘（这个操作总是被执行，即使事务还没有提交）。最多可能刷 100 个新脏页到磁盘。执行并改变缓冲的操作。若当前没有用户活动，可能切换到后台循环（background loop）等。10s 操作主要包括如下。最多可能刷新 100 个脏页到磁盘。合并至多 5 个被改变的缓冲（总是）。日志缓冲刷新到磁盘（总是）。删除无用的 Undo 页（总是）。刷新 100 个或者 10 个脏页到磁盘（总是）产生一个检查点（总是）等。buf_dump_thread 负责将 buffer pool 中的内容 dump 到物理文件中，以便再次启动 MySQL 时，可以快速加热数据。page_cleaner_thread 负责将 buffer pool 中的脏页刷新到磁盘，在 5.6 版本之前没有这个线程，刷新操作都是由主线程完成的，所以在刷新脏页时会非常影响 MySQL 的处理能力，在5.7 版本之后可以通过参数设置开启多个 page_cleaner_thread。purge_thread 负责将不再使用的 Undo 日志进行回收。read_thread 处理用户的读请求，并负责将数据页从磁盘上读取出来，可以通过参数设置线程数量。write_thread 负责将数据页从缓冲区写入磁盘，也可以通过参数设置线程数量，page_cleaner 线程发起刷脏页操作后 write_thread 就开始工作了。redo_log_thread 负责把日志缓冲中的内容刷新到 Redo log 文件中。insert_buffer_thread 负责把 Insert Buffer 中的内容刷新到磁盘。实例层的内存部分主要包含 InnoDB Buffer Pool，这里包含 InnoDB 最重要的缓存内容。数据和索引页、undo 页、insert buffer 页、自适应 Hash 索引页、数据字典页和锁信息等。additional memory pool 后续已不再使用。Redo buffer 里存储数据修改所产生的 Redo log。double write buffer 是 double write 所需的 buffer，主要解决由于宕机引起的物理写入操作中断，数据页不完整的问题。物理层物理层在逻辑上分为系统表空间、用户表空间和 Redo日志。系统表空间里有 ibdata 文件和一些 Undo，ibdata 文件里有 insert buffer 段、double write段、回滚段、索引段、数据字典段和 Undo 信息段。用户表空间是指以 .ibd 为后缀的文件，文件中包含 insert buffer 的 bitmap 页、叶子页（这里存储真正的用户数据）、非叶子页。InnoDB 表是索引组织表，采用 B+ 树组织存储，数据都存储在叶子节点中，分支节点（即非叶子页）存储索引分支查找的数据值。Redo 日志中包括多个 Redo 文件，这些文件循环使用，当达到一定存储阈值时会触发checkpoint 刷脏页操作，同时也会在 MySQL 实例异常宕机后重启，InnoDB 表数据自动还原恢复过程中使用。内存和物理结构用户读取或者写入的最新数据都存储在 Buffer Pool 中，如果 Buffer Pool 中没有找到则会读取物理文件进行查找，之后存储到 Buffer Pool 中并返回给 MySQL Server。Buffer Pool 采用LRU 机制，具体的内存队列和刷新机制建议你课后学习了解下，这里不详细讲述。Buffer Pool 决定了一个 SQL 执行的速度快慢，如果查询结果页都在内存中则返回结果速度很快，否则会产生物理读（磁盘读），返回结果时间变长，性能远不如存储在内存中。但我们又不能将所有数据页都存储到 Buffer Pool 中，比如物理 ibd 文件有 500GB，我们的机器不可能配置能容得下 500GB 数据页的内存，因为这样做成本很高而且也没必要。在单机单实例情况下，我们可以配置 Buffer Pool 为物理内存的 60%~80%，剩余内存用于 session 产生的 sort 和 join 等，以及运维管理使用。如果是单机多实例，所有实例的buffer pool总量也不要超过物理内存的80%。开始时我们可以根据经验设置一个 Buffer Pool 的经验值，比如 16GB，之后业务在 MySQL 运行一段时间后可以根据 show global status like ‘%buffer_pool_wait%’ 的值来看是否需要调整 Buffer Pool 的大小。Redo log 是一个循环复用的文件集，负责记录 InnoDB 中所有对 Buffer Pool的物理修改日志，当 Redo log文件空间中，检查点位置的 LSN 和最新写入的 LSN 差值（checkpoint_age）达到 Redo log 文件总空间的 75% 后，InnoDB 会进行异步刷新操作，直到降至 75% 以下，并释放 Redo log 的空间；当 checkpoint_age 达到文件总量大小的 90% 后，会触发同步刷新，此时 InnoDB 处于挂起状态无法操作。这样我们就看到 Redo log 的大小直接影响了数据库的处理能力，如果设置太小会导致强行 checkpoint 操作频繁刷新脏页，那我们就需要将 Redo log 设置的大一些，5.6 版本之前 Redo log 总大小不能超过 3.8GB，5.7 版本之后放开了这个限制。那既然太小影响性能，是不是设置得越大越好呢，这个问题留给你课后自己思考。事务提交时 log buffer 会刷新到 Redo log 文件中，具体刷新机制由参数控制，你可以课后学习并根据自身业务特点进行配置。若参数 innodb_file_per_table=ON，则表示用户建表时采用用户独立表空间，即一个表对应一组物理文件，.frm 表定义文件和 .ibd 表数据文件。当然若这个参数设置为 OFF，则表示用户建表存储在 ibdata 文件中，不建议采用共享表空间，这样会导致 ibdata 文件过大，而且当表删除后空间无法回收。独立表空间可以在用户删除大量数据后回收物理空间，执行一个 DDL 就可以将表空间的高水位降下来了。新版本特性主要是 MySQL 5.7 版本和 8.0 版本的一些新特点。 MySQL 5.7 版本新特性如下： 将 Undo 从共享表空间 ibdata 文件中分离出来； 可以在安装 MySQL 时由用户自行指定文件大小和数量； 增加了 temporary 临时表空间，里面存储着临时表或临时查询结果集的数据。 Buffer Pool 大小可以动态修改，无需重启数据库实例，DBA 的福音。 MySQL 8.0 版本新特性如下： 将 InnoDB 表的数据字典和 Undo 都从共享表空间 ibdata 中彻底分离出来了，以前需要ibdata 文件中数据字典与独立表空间 ibd 文件中数据字典一致才行，8.0 版本就不需要了； temporary 临时表空间也可以配置多个物理文件，而且均为 InnoDB 存储引擎并能创建索引，这样加快了处理的速度； 用户可以像 Oracle 数据库那样设置一些表空间，每个表空间对应多个物理文件，每个表空间可以给多个表使用，但一个表只能存储在一个表空间中。 InnoDB 和 MyISAM功能对比InnoDB 和 MyISAM 的功能对比如下所示： InnoDB 支持 ACID 的事务 4 个特性，而 MyISAM 不支持； InnoDB 支持 4 种事务隔离级别，默认是可重复读 Repeatable Read 的，MyISAM 不支持； InnoDB 支持 crash 安全恢复，MyISAM 不支持； InnoDB 支持外键，MyISAM 不支持； nnoDB 支持行级别的锁粒度，MyISAM 不支持，只支持表级别的锁粒度； InnoDB 支持 MVCC，MyISAM 不支持； InnoDB 表最大还可以支持 64TB，支持聚簇索引、支持压缩数据存储，支持数据加密，支持查询/索引/数据高速缓存，支持自适应 hash 索引、空间索引，支持热备份和恢复等，如下图所示： 性能对比在性能对比上，InnoDB 完胜 MyISAM，如下图所示： 读写混合模式下，随着 CPU 核数的增加，InnoDB 的读写能力呈线性增长， 在测试用例里，最高可达近 9000 的 TPS，但 MyISAM 因为读写不能并发，它的处理能力跟核数没关系，呈一条水平线，TPS 低于 500。 只读模式下，随着 CPU 核数的增加，InnoDB 的读写能力呈线性增长，最高可达近 14000 的 TPS，但 MyISAM 的处理能力不到 3000 TPS。 以上测试仅为说明 InnoDB 比 MyISAM 的处理能力强大，具体 TPS 测试数据跟硬件和测试条件不同而有很大差异。InnoDB 存储引擎特性核心特性InnoDB 存储引擎的核心特性包括： MVCC 锁 锁算法和分类 事务 表空间和数据页 内存线程 状态查询如下所示：ARIES 三原则ARIES 三原则，是指 Write Ahead Logging（WAL）: 先写日志后写磁盘，日志成功写入后事务就不会丢失，后续由 checkpoint 机制来保证磁盘物理文件与 Redo 日志达到一致性； 利用 Redo 记录变更后的数据，即 Redo 记录事务数据变更后的值； 利用 Undo 记录变更前的数据，即 Undo 记录事务数据变更前的值，用于回滚和其他事务多版本读。命令show engine innodb status\\G 的结果里面有详细的 InnoDB 运行态信息，分段记录的，包括: 内存 线程 信号 锁 事务多使用这个命令，出现问题时从中能分析出具体原因和解决方案。" }, { "title": "MySQL 体系结构", "url": "/posts/01-mysql-system/", "categories": "Database, MySQL", "tags": "MySQL", "date": "2020-07-04 01:33:22 +0000", "snippet": "MySQL 数据库的体系结构，如下图所示：通过上图，MySQL 体系结构由 Client Connectors 层、MySQL Server 层及存储引擎层组成。Client Connectors 层负责处理客户端的连接请求，与客户端创建连接。目前 MySQL 几乎支持所有的连接类型，例如常见的 JDBC、Python、Go 等。MySQL Server 层MySQL Server 层主要包括： Connection Pool：负责处理和存储数据库与客户端创建的连接；一个线程负责管理一个连接；包括： 用户认证模块（用户登录身份的认证） 鉴权及安全管理（用户执行操作权限校验） Service &amp;amp; utilities： 管理服务&amp;amp;工具集，包括备份恢复、安全管理、集群管理服务和工具。 SQL interface：负责接收客户端发送的各种 SQL 语句，比如 DML、DDL 和存储过程等。 Parser 解析器：Parser 解析器会对 SQL 语句进行语法解析生成解析树。 Optimizer 查询优化器：查询优化器会根据解析树生成执行计划，并选择合适的索引，然后按照执行计划执行 SQL 语言并与各个存储引擎交互。 Caches 缓存：包括各个存储引擎的缓存部分，比如： InnoDB 存储的 Buffer Pool； MyISAM 存储引擎的 key buffer 等 Caches 中也会缓存一些权限，也包括一些 Session 级别的缓存。 存储引擎层存储引擎包括： MyISAM； InnoDB； 支持归档的 Archive ； 内存的 Memory 等。MySQL是插件式的存储引擎，只要正确定义与 MySQL Server 交互的接口，任何引擎都可以访问MySQL，这也是 MySQL 流行的原因之一。物理存储层存储引擎底部是物理存储层，是文件的物理存储层，包括： 二进制日志； 数据文件； 错误日志； 慢查询日志； 全日志； redo/undo 日志等。MySQL 的交互过程SQL SELECT如下图所示：过程如下： 通过【客户端/服务器】通信协议与 MySQL 建立连接； 查询缓存，是 MySQL 的一个可优化查询的地方，如果开启了 Query Cache 且在查询缓存过程中查询到完全相同的 SQL 语句，则将查询结果直接返回给客户端；如果没有开启Query Cache 或者没有查询到完全相同的 SQL 语句则会由解析器进行语法语义解析，并生成解析树； 预处理器生成新的解析树； 查询优化器生成执行计划； 查询执行引擎执行 SQL 语句，此时查询执行引擎会根据 SQL 语句中表的存储引擎类型，以及对应的 API 接口与底层存储引擎缓存或者物理文件的交互情况，得到查询结果，由MySQL Server 过滤后将查询结果缓存并返回给客户端。若开启了 Query Cache，这时也会将SQL 语句和结果完整地保存到 Query Cache 中，以后若有相同的 SQL 语句执行则直接返回结果。SQL INSERTSQL UPDATE" }, { "title": "Redis 是如何淘汰 key", "url": "/posts/cache-redis-08/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2020-06-30 14:23:22 +0000", "snippet": "淘汰原理系统线上运行中，内存总是昂贵且有限的，在数据总量远大于 Redis 可用的内存总量时，为了最大限度的提升访问性能，Redis 中只能存放最新最热的有效数据。当 key 过期后，或者 Redis 实际占用的内存超过阀值后，Redis 就会对 key 进行淘汰，删除过期的或者不活跃的 key，回收其内存，供新的 key 使用。Redis 的内存阀值是通过 maxmemory 设置的，而超过内存阀值后的淘汰策略，是通过 maxmemory-policy 设置的，具体的淘汰策略后面会进行详细介绍。Redis 会在 2 种场景下对 key 进行淘汰，第一种是在定期执行 serverCron 时，检查淘汰 key；第二种是在执行命令时，检查淘汰 key。第一种场景，Redis 定期执行 serverCron 时，会对 DB 进行检测，清理过期 key。清理流程如下。首先轮询每个 DB，检查其 expire dict，即带过期时间的过期 key 字典，从所有带过期时间的 key 中，随机选取 20 个样本 key，检查这些 key 是否过期，如果过期则清理删除。如果 20 个样本中，超过 5 个 key 都过期，即过期比例大于 25%，就继续从该 DB 的 expire dict 过期字典中，再随机取样 20 个 key 进行过期清理，持续循环，直到选择的 20 个样本 key 中，过期的 key 数小于等于 5，当前这个 DB 则清理完毕，然后继续轮询下一个 DB。在执行 serverCron 时，如果在某个 DB 中，过期 dict 的填充率低于 1%，则放弃对该 DB 的取样检查，因为效率太低。如果 DB 的过期 dict 中，过期 key 太多，一直持续循环回收，会占用大量主线程时间，所以 Redis 还设置了一个过期时间。这个过期时间根据 serverCron 的执行频率来计算，5.0 版本及之前采用慢循环过期策略，默认是 25ms，如果回收超过 25ms 则停止，6.0 非稳定版本采用快循环策略，过期时间为 1ms。第二种场景，Redis 在执行命令请求时。会检查当前内存占用是否超过 maxmemory 的数值，如果超过，则按照设置的淘汰策略，进行删除淘汰 key 操作。淘汰方式Redis 中 key 的淘汰方式有两种，分别是同步删除淘汰和异步删除淘汰。在 serverCron 定期清理过期 key 时，如果设置了延迟过期配置 lazyfree-lazy-expire，会检查 key 对应的 value 是否为多元素的复合类型，即是否是 list 列表、set 集合、zset 有序集合和 hash 中的一种，并且 value 的元素数大于 64，则在将 key 从 DB 中 expire dict 过期字典和主 dict 中删除后，value 存放到 BIO 任务队列，由 BIO 延迟删除线程异步回收；否则，直接从 DB 的 expire dict 和主 dict 中删除，并回收 key、value 所占用的空间。在执行命令时，如果设置了 lazyfree-lazy-eviction，在淘汰 key 时，也采用前面类似的检测方法，对于元素数大于 64 的 4 种复合类型，使用 BIO 线程异步删除，否则采用同步直接删除。淘汰策略Redis 提供了 8 种淘汰策略对 key 进行管理，而且还引入基于样本的 eviction pool，来提升剔除的准确性，确保 在保持最大性能 的前提下，剔除最不活跃的 key。eviction pool 主要对 LRU、LFU，以及过期 dict ttl 内存管理策略 生效。处理流程为，当 Redis 内存占用超过阀值后，按策略从主 dict 或者带过期时间的 expire dict 中随机选择 N 个 key，N 默认是 5，计算每个 key 的 idle 值，按 idle 值从小到大的顺序插入 evictionPool 中，然后选择 idle 最大的那个 key，进行淘汰。选择淘汰策略时，可以通过配置 Redis 的 maxmemory 设置最大内存，并通 maxmemory_policy 设置超过最大内存后的处理策略。如果 maxmemory 设为 0，则表明对内存使用没有任何限制，可以持续存放数据，适合作为存储，来存放数据量较小的业务。如果数据量较大，就需要估算热数据容量，设置一个适当的值，将 Redis 作为一个缓存而非存储来使用。Redis 提供了 8 种 maxmemory_policy 淘汰策略来应对内存超过阀值的情况： noeviction，它是 Redis 的默认策略。在内存超过阀值后，Redis 不做任何清理工作，然后对所有写操作返回错误，但对读请求正常处理。noeviction 适合数据量不大的业务场景，将关键数据存入 Redis 中，将 Redis 当作 DB 来使用； volatile-lru，它对带过期时间的 key 采用最近最少访问算法来淘汰。使用这种策略，Redis 会从 redisDb 的 expire dict 过期字典中，首先随机选择 N 个 key，计算 key 的空闲时间，然后插入 evictionPool 中，最后选择空闲时间最久的 key 进行淘汰。这种策略适合的业务场景是，需要淘汰的key带有过期时间，且有冷热区分，从而可以淘汰最久没有访问的key； volatile-lfu，它对带过期时间的 key 采用最近最不经常使用的算法来淘汰。使用这种策略时，Redis 会从 redisDb 中的 expire dict 过期字典中，首先随机选择 N 个 key，然后根据其 value 的 lru 值，计算 key 在一段时间内的使用频率相对值。对于 lfu，要选择使用频率最小的 key，为了沿用 evictionPool 的 idle 概念，Redis 在计算 lfu 的 Idle 时，采用 255 减去使用频率相对值，从而确保 Idle 最大的 key 是使用次数最小的 key，计算 N 个 key 的 Idle 值后，插入 evictionPool，最后选择 Idle 最大，即使用频率最小的 key，进行淘汰。这种策略也适合大多数 key 带过期时间且有冷热区分的业务场景； volatile-ttl，它是对带过期时间的 key 中选择最早要过期的 key 进行淘汰。使用这种策略时，Redis 也会从 redisDb 的 expire dict 过期字典中，首先随机选择 N 个 key，然后用最大无符号 long 值减去 key 的过期时间来作为 Idle 值，计算 N 个 key 的 Idle 值后，插入evictionPool，最后选择 Idle 最大，即最快就要过期的 key，进行淘汰。这种策略适合，需要淘汰的key带过期时间，且有按时间冷热区分的业务场景； volatile-random，它是对带过期时间的 key 中随机选择 key 进行淘汰。使用这种策略时，Redis 从 redisDb 的 expire dict 过期字典中，随机选择一个 key，然后进行淘汰。如果需要淘汰的key有过期时间，没有明显热点，主要被随机访问，那就适合选择这种淘汰策略； allkey-lru，它是对所有 key，而非仅仅带过期时间的 key，采用最近最久没有使用的算法来淘汰。这种策略与 volatile-lru 类似，都是从随机选择的 key 中，选择最长时间没有被访问的 key 进行淘汰。区别在于，volatile-lru 是从 redisDb 中的 expire dict 过期字典中选择 key，而 allkey-lru 是从所有的 key 中选择 key。这种策略适合，需要对所有 key 进行淘汰，且数据有冷热读写区分的业务场景； allkeys-lfu，它也是针对所有 key 采用最近最不经常使用的算法来淘汰。这种策略与 volatile-lfu 类似，都是在随机选择的 key 中，选择访问频率最小的 key 进行淘汰。区别在于，volatile-flu从expire dict 过期字典中选择 key，而 allkeys-lfu 是从主 dict 中选择 key。这种策略适合的场景是，需要从所有的 key 中进行淘汰，但数据有冷热区分，且越热的数据访问频率越高； allkeys-random，它是针对所有 key 进行随机算法进行淘汰。它也是从主 dict 中随机选择 key，然后进行删除回收。如果需要从所有的 key 中进行淘汰，并且 key 的访问没有明显热点，被随机访问，即可采用这种策略。 " }, { "title": "Redis 内部数据结构", "url": "/posts/cache-redis-07/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2020-06-25 11:33:22 +0000", "snippet": "RdeisDbRedis 中所有数据都保存在 DB 中，一个 Redis 默认最多支持 16 个 DB。Redis 中的每个 DB 都对应一个 redisDb 结构，即每个 Redis 实例，默认有 16 个 redisDb。用户访问时，默认使用的是 0 号 DB，可以通过 select $dbID 在不同 DB 之间切换。redisDb 主要包括： 2 个核心 dict 字典 一个 dict 主字典。用来存储当前 DB 中的所有数据，它将 key 和各种数据类型的 value 关联起来，该 dict 也称 key space。 一个 expires 过期字典。用来存储过期时间 key，存的是 key 与过期时间的映射。 日常的数据存储和访问基本都会访问到 redisDb 中的这两个 dict。 3 个非核心 dict 字典 一个字段名叫 blocking_keys 的阻塞 dict。在执行 Redis 中 list 的阻塞命令 blpop、brpop 或者 brpoplpush 时，如果对应的 list 列表为空，Redis 就会将对应的 client 设为阻塞状态，同时将该 client 添加到 DB 中 blocking_keys 这个阻塞 dict。所以该 dict 存储的是处于阻塞状态的 key 及 client 列表。 一个字段名叫 ready_keys 的解除阻塞 dict。当有其他调用方在向某个 key 对应的 list 中增加元素时，Redis 会检测是否有 client 阻塞在这个 key 上，即检查 blocking_keys 中是否包含这个 key，如果有则会将这个 key 加入 read_keys 这个 dict 中。同时也会将这个 key 保存到 server 中的一个名叫 read_keys 的列表中。这样可以高效、不重复的插入及轮询。 一个是字段名叫 watched_keys 的 watch 监控 dict。当 client 使用 watch 指令来监控 key 时，这个 key 和 client 就会被保存到 watched_keys 这个 dict 中。redisDb 中可以保存所有的数据类型，而 Redis 中所有数据类型都是存放在一个叫 redisObject 的结构中。 dbID 其他辅助属性。 redisObjectredisObject 由 5 个字段组成： type：即 Redis 对象的数据类型，目前支持 7 种 type 类型，分别为： OBJ_STRING OBJ_LIST OBJ_SET OBJ_ZSET OBJ_HASH OBJ_MODULE OBJ_STREAM encoding：Redis 对象的内部编码方式，即内部数据结构类型，目前支持 10 种编码方式包括： OBJ_ENCODING_RAW OBJ_ENCODING_INT OBJ_ENCODING_HT OBJ_ENCODING_ZIPLIST 等。 LRU：存储的是淘汰数据用的 LRU 时间或 LFU 频率及时间的数据。 refcount：记录 Redis 对象的引用计数，用来表示对象被共享的次数，共享使用时加 1，不再使用时减 1，当计数为 0 时表明该对象没有被使用，就会被释放，回收内存； ptr：它指向对象的内部数据结构。比如一个代表 string 的对象，它的 ptr 可能指向一个 sds 或者一个 long 型整数。dictRedis 中的数据实际是存在 DB 中的 2 个核心 dict 字典中的。实际上 dict 也是 Redis 的一种使用广泛的内部数据结构。Redis 中的 dict，类似于 Memcached 中 hashtable。都可以用于 key 或元素的快速插入、更新和定位。dict 字典中，有一个长度为 2 的哈希表数组，日常访问用 0 号哈希表，如果 0 号哈希表元素过多，则分配一个 2 倍 0 号哈希表大小的空间给 1 号哈希表，然后进行逐步迁移，rehashidx 这个字段就是专门用来做标志迁移位置的。在哈希表操作中，采用单向链表来解决 hash 冲突问题。dict 中还有一个重要字段是 type，它用于保存 hash 函数及 key/value 赋值、比较函数。dictht 中的 table 是一个 hash 表数组，每个桶指向一个 dictEntry 结构。dictht 采用 dictEntry 的单向链表来解决 hash 冲突问题。dictht 是以 dictEntry 来存 key-value 映射的。其中 key 是 sds 字符串，value 为存储各种数据类型的 redisObject 结构。dict 可以被 redisDb 用来存储数据 key-value 及命令操作的辅助信息。还可以用来作为一些 Redis 数据类型的内部数据结构。dict 可以作为 set 集合的内部数据结构。在哈希的元素数超过 512 个，或者哈希中 value 大于 64 字节，dict 还被用作为哈希类型的内部数据结构。sds字符串是 Redis 中最常见的数据类型，其底层实现是简单动态字符串即 sds。简单动态字符串本质是一个 char*，内部通过 sdshdr 进行管理。sdshdr 有 4 个字段。len 为字符串实际长度，alloc 当前字节数组总共分配的内存大小。flags 记录当前字节数组的属性；buf 是存储字符串真正的值及末尾一个 \\0。sds 的存储 buf 可以动态扩展或收缩，字符串长度不用遍历，可直接获得，修改和访问都很方便。由于 sds 中字符串存在 buf 数组中，长度由 len 定义，而不像传统字符串遇 0 停止，所以 sds 是二进制安全的，可以存放任何二进制的数据。简单动态字符串 sds 的获取字符串长度很方便，通过 len 可以直接得到，而传统字符串需要对字符串进行遍历，时间复杂度为 O(n)。sds 相比传统字符串多了一个 sdshdr，对于大量很短的字符串，这个 sdshdr 还是一个不小的开销。在 3.2 版本后，sds 会根据字符串实际的长度，选择不同的数据结构，以更好的提升内存效率。当前 sdshdr 结构分为 5 种子类型，分别为 sdshdr5、sdshdr8、sdshdr16、sdshdr32、sdshdr64。其中 sdshdr5 只有 flags 和 buf 字段，其他几种类型的 len 和 alloc 采用从 uint8_t 到 uint64_t 的不同类型，以节省内存空间。sds 可以作为字符串的内部数据结构，同时 sds 也是 hyperloglog、bitmap 类型的内部数据结构。ziplist为了节约内存，并减少内存碎片，Redis 设计了 ziplist 压缩列表内部数据结构。压缩列表是一块连续的内存空间，可以连续存储多个元素，没有冗余空间，是一种连续内存数据块组成的顺序型内存结构。ziplist 的结构如图所示，主要包括 5 个部分： zlbytes 是压缩列表所占用的总内存字节数； Zltail 尾节点到起始位置的字节数； Zllen 总共包含的节点/内存块数； Entry 是 ziplist 保存的各个数据节点，这些数据点长度随意； Zlend 是一个魔数 255，用来标记压缩列表的结束。如图所示，一个包含 4 个元素的 ziplist，总占用字节是 100bytes，该 ziplist 的起始元素的指针是 p，zltail 是 80，则第 4 个元素的指针是 P+80。压缩列表 ziplist 的存储节点 entry 的结构如图，主要有 6 个字段： prevRawLen 是前置节点的长度； preRawLenSize 编码 preRawLen 需要的字节数； len 当前节点的长度； lensize 编码 len 所需要的字节数； encoding 当前节点所用的编码类型； entryData 当前节点数据。 由于 ziplist 是连续紧凑存储，没有冗余空间，所以插入新的元素需要 realloc 扩展内存，所以如果 ziplist 占用空间太大，realloc 重新分配内存和拷贝的开销就会很大，所以 ziplist 不适合存储过多元素，也不适合存储过大的字符串。因此只有在元素数和 value 数都不大的时候，ziplist 才作为 hash 和 zset 的内部数据结构。其中 hash 使用 ziplist 作为内部数据结构的限制时，元素数默认不超过 512 个，value 值默认不超过 64 字节。可以通过修改配置来调整 hash_max_ziplist_entries 、hash_max_ziplist_value 这两个阀值的大小。zset 有序集合，使用 ziplist 作为内部数据结构的限制元素数默认不超过 128 个，value 值默认不超过 64 字节。可以通过修改配置来调整 zset_max_ziplist_entries 和 zset_max_ziplist_value 这两个阀值的大小。quicklistRedis 在 3.2 版本之后引入 quicklist，用以替换 linkedlist。因为 linkedlist 每个节点有前后指针，要占用 16 字节，而且每个节点独立分配内存，很容易加剧内存的碎片化。而 ziplist 由于紧凑型存储，增加元素需要 realloc，删除元素需要内存拷贝，天然不适合元素太多、value 太大的存储。而 quicklist 快速列表应运而生，它是一个基于 ziplist 的双向链表。将数据分段存储到 ziplist，然后将这些 ziplist 用双向指针连接。快速列表的结构如图所示： head、tail 是两个指向第一个和最后一个 ziplist 节点的指针； count 是 quicklist 中所有的元素个数； len 是 ziplist 节点的个数； compress 是 LZF 算法的压缩深度。快速列表中，管理 ziplist 的是 quicklistNode 结构。quicklistNode 主要包含一个 prev/next 双向指针，以及一个 ziplist 节点。单个 ziplist 节点可以存放多个元素。快速列表从头尾读写数据很快，时间复杂度为 O(1)。也支持从中间任意位置插入或读写元素，但速度较慢，时间复杂度为 O(n)。快速列表当前主要作为 list 列表的内部数据结构。zskiplist跳跃表 zskiplist 是一种有序数据结构，它通过在每个节点维持多个指向其他节点的指针，从而可以加速访问。跳跃表支持平均 O(logN) 和最差 O(n) 复杂度的节点查找。在大部分场景，跳跃表的效率和平衡树接近，但跳跃表的实现比平衡树要简单，所以不少程序都用跳跃表来替换平衡树。如果 sorted set 类型的元素数比较多或者元素比较大，Redis 就会选择跳跃表来作为 sorted set有序集合的内部数据结构。跳跃表主要由 zskipList 和节点 zskiplistNode 构成。zskiplist 结构如图，header 指向跳跃表的表头节点。tail 指向跳跃表的表尾节点。length 表示跳跃表的长度，它是跳跃表中不包含表头节点的节点数量。level 是目前跳跃表内，除表头节点外的所有节点中，层数最大的那个节点的层数。跳跃表的节点 zskiplistNode 的结构如图所示。ele 是节点对应的 sds 值，在 zset 有序集合中就是集合中的 field 元素。score 是节点的分数，通过 score，跳跃表中的节点自小到大依次排列。backward 是指向当前节点的前一个节点的指针。level 是节点中的层，每个节点一般有多个层。每个 level 层都带有两个属性，一个是 forwad 前进指针，它用于指向表尾方向的节点；另外一个是 span 跨度，它是指 forward 指向的节点到当前节点的距离。如图所示是一个跳跃表，它有 3 个节点。对应的元素值分别是 S1、S2 和 S3，分数值依次为 1.0、3.0 和 5.0。其中 S3 节点的 level 最大是 5，跳跃表的 level 是 5。header 指向表头节点，tail 指向表尾节点。在查到元素时，累加路径上的跨度即得到元素位置。在跳跃表中，元素必须是唯一的，但 score 可以相同。相同 score 的不同元素，按照字典序进行排序。在 sorted set 数据类型中，如果元素数较多或元素长度较大，则使用跳跃表作为内部数据结构。默认元素数超过 128 或者最大元素的长度超过 64，此时有序集合就采用 zskiplist 进行存储。由于 geo 也采用有序集合类型来存储地理位置名称和位置 hash 值，所以在超过相同阀值后，也采用跳跃表进行存储。整体看一下，8 种数据类型，具体采用数据结构是： 对于 string 字符串，Redis 主要采用 sds 来进行存储； 对于 list 列表，Redis 采用 quicklist 进行存储； 对于 set 集合类型，Redis 采用 dict 来进行存储； 对于 sorted set 有序集合类型，如果元素数小于 128 且元素长度小于 64，则使用 ziplist 存储，否则使用 zskiplist 存储。 对于哈希类型，如果元素数小于 512，并且元素长度小于 64，则用 ziplist 存储，否则使用 dict 字典存储。 对于 hyperloglog，采用 sds 简单动态字符串存储。 对于 geo，如果位置数小于 128，则使用 ziplist 存储，否则使用 zskiplist 存储； 对于 bitmap，采用 sds 简单动态字符串存储。除了这些主要的内部数据结构，还有在特殊场景下也会采用一些其他内部结构存储，比如： 如果操作的字符串都是整数，同时指令是 incr、decr 等，会对字符串采用 long 型整数存储。" }, { "title": "Spring Security 对系统的安全性进行测试", "url": "/posts/test-safe/", "categories": "Java, Spring", "tags": "Spring Security", "date": "2020-06-10 15:33:00 +0000", "snippet": "Spring Security 是一款安全性开发框架，提供的是内嵌到业务系统中的基础设施类功能。涉及大量组件之间的依赖关系，这是在测试安全性功能中面临的最大挑战，需要采用特定的测试方法。​因此，在使用具体的测试用例之前，有必要先梳理下安全性测试方法，以及 Spring Security 中提供的测试解决方案。​安全性测试与 Mock 机制验证安全性功能正确性的难点在于组件与组件之间的依赖关系，为了弄清楚这个关系，就需要了解测试领域非常重要的一个概念： Mock（模拟）。​针对测试组件涉及的外部依赖，关注点在于： 组件之间的调用关系， 返回的结果或发生的异常 不是组件内部的执行过程​因此常见的技巧就是：使用 Mock 对象来替代真实的依赖对象，模拟真实的调用场景。​接着，以最常见的三层 Web 服务架构为例来进一步解释 Mock 的实施方法。​在常见的三层模型中： Controller 层会访问 Service 层； Service 层又会访问 Repository 层； 当对 Controller 层的端点进行验证时，需要模拟 Service 层组件的功能。同样，对 Service 层组件进行测试时，需要假定 Repository 层组件的结果是可以获取的，如下所示：对于 Spring Security 而言，上图所展示的原理同样适用，例如：可以通过模拟用户的方式来测试用户认证和授权功能的正确性。​Spring Security 中的测试解决方案​开展单元测试、集成测试以及基于 Mock 的测试，需要有一套完整的技术体系。​和 Spring Boot 1.x 版本一样，Spring Boot 2.x 同样提供了针对测试的 spring-boot-starter-test 组件。​在 Spring Boot 中集成该组件的方法就是在 pom 文件中添加如下依赖：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-test&amp;lt;/artifactId&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;通过这个依赖，一系列组件被自动引入到了代码工程的构建路径中，包括： JUnit JSON Path AssertJ Mockito Hamcrest …and so on​上面的测试组件都非常有用。​因为 Spring Boot 程序的入口是 Bootstrap 类，因此提供了一个 @SpringBootTest 注解来测试 Bootstrap 类。所有配置都会通过 Bootstrap 类去加载，而该注解可以引用 Bootstrap 类的配置。​此外，Spring Security 也提供了专门用于测试安全性功能的 spring-security-test 组件，如下所示：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.security&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-security-test&amp;lt;/artifactId&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;该组件提供了通过相关的注解来模拟用户登录信息或者调用用户登录的方法。​测试 Spring Security 功能测试用户​在使用 Spring Security 时，首先测试的无疑是合法的用户。​假设实现了如下所示的一个简单 Controller：@RestControllerpublic class HelloController { @GetMapping(&quot;/hello&quot;) public String hello() { return &quot;Hello&quot;; } }一旦启用 Spring Security 认证功能，那么对上述/hello端点就可以执行两种测试，分别面向认证和非认证用户。​非认证用户的测试方法@SpringBootTest@AutoConfigureMockMvcpublic class HelloControllerTests {     @Autowired    private MockMvc mvc;     @Test    public void testUnauthenticatedUser() throws Exception {        mvc.perform(get(&quot;/hello&quot;))                .andExpect(status().isUnauthorized()); }}上面的示例中，加入了 @AutoConfigureMockMvc 注解，该注解一般会和 @SpringBootTest 注解与 结合使用，因为它会通过 @SpringBootTest 加载的 Spring 上下文环境中会自动装配 MockMvc 这个测试工具类。​MockMvc 是用来对 Web MVC 的执行过程进行模拟。MockMvc 类提供的基础方法如下所示： perform：执行一个 RequestBuilder 请求，会自动执行 SpringMVC 流程，并映射到相应的 Controller 中进行处理； get/post/put/delete：声明发送一个 HTTP 请求的方式，根据 URI 模板和 URI 变量值得到一个 HTTP 请求，支持 GET、POST、PUT、DELETE 等 HTTP 方法。 param：添加请求参数，发送 JSON 数据时不能使用这种方式，而应该采用 @ResponseBody 注解。 andExpect：添加 ResultMatcher 验证规则，通过对返回的数据进行判断来验证 Controller 执行结果是否正确。 andDo：添加 ResultHandler 结果处理器，比如调试时打印结果到控制台。 andReturn：最后返回相应的 MvcResult，然后执行自定义验证或做异步处理。在上述代码示例中，通过 perform、accept 和 andExpect 方法最终模拟 HTTP 请求的整个过程并验证请求的返回状态是否为非认证。​认证用户的测试方法​测试用例如下所示：@Test@WithMockUserpublic void testAuthenticatedUser() throws Exception {    mvc.perform(get(&quot;/hello&quot;))        .andExpect(content().string(&quot;Hello&quot;))        .andExpect(status().isOk());}这里有用了一个新的注解： @WithMockUser 注解，这个注解是 Spring Security 所提供的，专门用来模拟认证用户。​现在，既然已经有了认证用户，就可以验证响应的返回值以及状态，正如上述代码所示。​通过 @WithMockUser注解，还可以指定用户的详细信息，例如下面代码模拟了一个用户名为“admin”、角色为“USER”和“ADMIN”的认证用户：@WithMockUser(username=&quot;admin&quot;,roles={&quot;USER&quot;,&quot;ADMIN&quot;})​进一步，还可以通过模拟 UserDetailsService 来提供自定义的 UserDetails 用户信息。为此，Spring Security 中专门提供了一个 @WithUserDetails注解，示例代码如下所示：@Test@WithUserDetails(&quot;jianxiang&quot;)public void testAuthenticatedUser() throws Exception {    mvc.perform(get(&quot;/hello&quot;))        .andExpect(content().string(&quot;Hello&quot;))        .andExpect(status().isOk());}测试认证测试完用户，接着来测试针对用户的认证过程。​为了对整个认证过程有更多的定制化实现，定义了一个 AuthenticationProvider 接口的实现类 MyAuthenticationProvider，如下所示：@Componentpublic class MyAuthenticationProvider implements AuthenticationProvider {    @Override    public Authentication authenticate(Authentication authentication) throws AuthenticationException {        String username = authentication.getName();        String password = String.valueOf(authentication.getCredentials());        if (&quot;jianxiang&quot;.equals(username) &amp;amp;&amp;amp; &quot;123456&quot;.equals(password)) {            return new UsernamePasswordAuthenticationToken(username, password, Arrays.asList());        } else {            throw new AuthenticationCredentialsNotFoundException(&quot;Error!&quot;);        }    }     @Override    public boolean supports(Class&amp;lt;?&amp;gt; authenticationType) {        return UsernamePasswordAuthenticationToken.class.isAssignableFrom(authenticationType);    } }现在，基于 HTTP 基础认证机制来编写测试用例，如下所示：@SpringBootTest@AutoConfigureMockMvcpublic class AuthenticationTests {     @Autowired    private MockMvc mvc;     @Test    public void testAuthenticatingWithValidUser() throws Exception {        mvc.perform(get(&quot;/hello&quot;)                .with(httpBasic(&quot;jianxiang&quot;,&quot;123456&quot;)))                .andExpect(status().isOk());    }    @Test    public void testAuthenticatingWithInvalidUser() throws Exception {        mvc.perform(get(&quot;/hello&quot;)                .with(httpBasic(&quot;noexiseduser&quot;,&quot;123456&quot;)))                .andExpect(status().isUnauthorized());    }}这里使用了前面介绍的 @AutoConfigureMockMvc 注解和 MockMvc 工具类，然后通过 httpBasic() 方法来实现 HTTP 基础认证，紧接着分别针对正确和错误的用户名/密码组合来执行 HTTP 请求并根据返回状态对认证结果进行校验。测试方法安全前面的内容都是面向 Web 应用，也就是测试的对象都是 HTTP 端点。​那么，如何针对方法级别的安全性进行测试呢？针对全局方法安全机制，前面介绍的 @WithMockUser 注解 @WithUserDetails 注解实际上也都是可以正常使用的。但因为已经脱离了 Web 环境，所以 MockMvc 工具类显然是无效的。此时，要做的事情就是在测试用例中直接注入目标方法即可，如下面的代码，假设一个非 Web 类的应用程序中存在如下一个 Service 类：@Servicepublic class HelloService {    @PreAuthorize(&quot;hasAuthority(&#39;write&#39;)&quot;)    public String hello() {        return &quot;Hello&quot;;    }}可这里使用了 @PreAuthorize 注解限制了只有具备“write”权限的用户才能访问这个方法。​现在编写针对方法访问安全的第一个测试用例，如下所示：@Autowiredprivate HelloService helloService;@Testvoid testMethodWithNoUser() {      assertThrows(AuthenticationException.class,           () -&amp;gt; helloService.hello());}在没有认证的情况下访问 helloService 的 hello() 方法，抛出一个** AuthenticationException 异常**，上述测试用例验证了这一点。​如果使用一个具备不同权限的认证用户去访问这个方法时，对应测试用例如下所示：@Test@WithMockUser(authorities = &quot;read&quot;)void testMethodWithUserButWrongAuthority() {     assertThrows(AccessDeniedException.class,           () -&amp;gt; helloService.hello());}这里使用了 @WithMockUser 模拟了一个具有“read”权限的认证用户，又因为 @PreAuthorize 注解中指定只有“write”权限的用户才能访问这个方法，所以会抛出一个 AccessDeniedException 异常。​最后，测试正常流程下的结果，测试用例如下所示：@Test@WithMockUser(authorities = &quot;write&quot;)void testMethodWithUserButCorrectAuthority() {     Stringresult = helloService.hello();     assertEquals(&quot;Hello&quot;, result);}测试 CSRF 和 CORS 配置基于在[](攻击应对：如何实现 CSRF 保护和 CORS？》]()中的总结，对于 POST、**PUT **和 **DELETE **等 HTTP 请求，需要添加针对 CSRF 的安全保护。​为了测试 CSRF 配置的正确性，假设存在这样一个 HTTP 端点，请注意它的 HTTP 方法是 POST：@RestControllerpublic class HelloController {    @PostMapping(&quot;/hello&quot;)    public String postHello() {        return &quot;Post Hello!&quot;;    }}现在，通过 MockMvc 工具类发起 post 请求，测试用例如下所示：@Testpublic void testHelloUsingPOST() throws Exception {     mvc.perform(post(&quot;/hello&quot;))            .andExpect(status().isForbidden());}此时，这个 post 请求没有携带 CSRF Token，所以响应的状态是 HTTP 403 Forbidden。​接下来，让重构上述测试用例，如下所示：@Testpublic void testHelloUsingPOSTWithCSRF() throws Exception {     mvc.perform(post(&quot;/hello&quot;).with(csrf()))            .andExpect(status().isOk());}​上述 csrf() 方法的作用是：在请求中添加 CSRF Token，显然，这时候的响应结果是正确的。​讨论完 CSRF，再来看 CORS。在《09 |攻击应对：如何实现 CSRF 保护和 CORS？》中，已经通过 CorsConfiguration 设置了 HTTP 响应消息头，如下所示：@Overrideprotected void configure(HttpSecurity http) throws Exception {         http.cors(c -&amp;gt; {            CorsConfigurationSource source = request -&amp;gt; {                CorsConfiguration config = new CorsConfiguration();                config.setAllowedOrigins(Arrays.asList(&quot;*&quot;));                config.setAllowedMethods(Arrays.asList(&quot;*&quot;));                return config;            };            c.configurationSource(source);        });        …}对上述配置进行测试的方法很简单，通过 MockMvc 发起请求，然后对响应的消息头进行验证即可，测试用例如下所示：@SpringBootTest@AutoConfigureMockMvcpublic class MainTests {    @Autowired    private MockMvc mvc;    @Test    public void testCORSForTestEndpoint() throws Exception {        mvc.perform(options(&quot;/hello&quot;)                .header(&quot;Access-Control-Request-Method&quot;, &quot;POST&quot;)                .header(&quot;Origin&quot;, &quot;http://www.test.com&quot;))        .andExpect(header().exists(&quot;Access-Control-Allow-Origin&quot;))        .andExpect(header().string(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;))        .andExpect(header().exists(&quot;Access-Control-Allow-Methods&quot;))        .andExpect(header().string(&quot;Access-Control-Allow-Methods&quot;, &quot;POST&quot;))        .andExpect(status().isOk());    }}可以看到，针对 CORS 配置，分别获取了响应结果的 “Access-Control-Allow-Origin” 和 “Access-Control-Allow-Methods” 消息头并进行了验证。​总结对于一个应用程序而言，无论其表现形式是否是一个 Web 服务，都需要对其进行安全性测试，Spring Security 提供的测试解决方案，很大程度上依赖于对 Mock 机制的合理应用。在使用 Spring Security 测试用户和认证过程中，如果对用户进行有效的 Mock？" }, { "title": "Spring Security 响应式编程", "url": "/posts/react/", "categories": "Java, Spring", "tags": "Spring Security, React", "date": "2020-06-08 15:33:00 +0000", "snippet": "对于大多数日常业务场景而言，软件系统在任何时候都需要确保具备即时响应性。​响应式编程（Reactive Programming）就是用来构建具有即时响应性的是一种新的编程技术。​随着 Spring 5 的发布，迎来了响应式编程的全新发展时期。而 Spring Security 作为 Spring 家族的一员，同样实现了一系列的响应式组件。​什么是响应式编程？​响应式编程的基本概念 在响应式系统中，任何操作都可以被看作是一种事件； 事件构成了数据流，数据流对于技术栈而言是一个全流程的概念； 无论是从底层数据库，向上到达服务层，最后到 Web 层，亦或是在这个流程中所包含的任意中间层组件，整个数据传递链路都应该是采用事件驱动的方式来进行运作；响应是编程的核心特点： 可以不采用传统的同步调用方式来处理数据 由位于数据库上游的各层组件自动来执行事件针对数据流的具体操作方法都定义在响应式流（Reactive Stream）规范中。在 Java 的世界中，关于响应式流规范的实现也有一些主流的开源框架，包括 RxJava、Vert.x 以及 Project Reactor。​Project Reactor Spring 5 选择 Project Reactor 作为它的内置响应式编程框架， 该框架提供了两种数据流的表示方式 包含 0 到 n 个元素异步序列的 Flux 组件， 包含 0 个或 1 个元素的 Mono 组件 下面通过一个简单的代码示例来创建一个 Flux 对象，如下所示：private Flux&amp;lt;Order&amp;gt; getAccounts() { List&amp;lt;Account&amp;gt; accountList = new ArrayList&amp;lt;&amp;gt;(); Account account = new Account(); account.setId(1L); account.setAccountCode(&quot;DemoCode&quot;); account.setAccountName(&quot;DemoName&quot;); accountList.add(account);       return Flux.fromIterable(accountList); // 构建了 Flux 对象并进行返回，Flux.fromIterable() 是构建 Flux 的一种常用方法}同时， Mono 组件也提供了一组有用的方法来创建 Mono 数据流，例如：private Mono&amp;lt;Account&amp;gt; getAccountById(Long id) {           Account account = new Account(); // 首先构建一个 Account 对象        account.setId(id);        account.setAccountCode(&quot;DemoCode&quot;);        account.setAccountName(&quot;DemoName&quot;);        accountList.add(account); return Mono.just(account); // 通过 Mono.just() 方法返回一个 Mono 对象。}Spring WebFlux针对一个完整的应用程序开发过程，Spring 5 提供了： 针对 Web 层的 WebFlux 框架 针对数据访问层的 Spring Data Reactive 框架等​由于 Spring Security 主要用于 Web 应用程序，所以这里对 WebFlux 做一些展开。想要在 Spring Boot 中使用 WebFlux，需要引入如下依赖：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-webflux&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;  ​注意这里的 spring-boot-starter-webflux 是构成响应式 Web 应用程序开发的基础。​基于 WebFlux 构建响应式 Web 服务的编程模型，有两种选择： 第一种是使用基于 Java 注解的方式， 第二种则是使用函数式编程模型​其中，基于 Java 注解的方式与使用 Spring MVC 完全一致，如下：@RestControllerpublic class HelloController {    @GetMapping(&quot;/&quot;)    public Mono&amp;lt;String&amp;gt; hello() {        return Mono.just(&quot;Hello!&quot;);    }}以上代码只有一个地方值得注意： 即 hello() 方法的返回值从普通的 String 对象转化为了一个 Mono 对象。这点是完全可以预见的​使用 Spring WebFlux 与 Spring MVC 的不同之处在： Spring WebFlux 使用的类型都是 Reactor 中提供的 Flux 和 Mono 对象，而不是普通的 POJO 传统的 Spring MVC 构建在 Java EE 的 Servlet 标准之上，该标准本身就是阻塞式和同步的。 Spring WebFlux 构建在响应式流以及它的实现框架 Project Reactor 之上的一个开发框架，因此可以基于 HTTP 协议来构建异步非阻塞的 Web 服务 底部的容器支持。当使用 Spring WebFlux 时，会注意到它默认采用了 Netty 作为运行时容器。这是因为 Spring MVC 是运行在传统的 Servlet 容器之上，而 Spring WebFlux 则需要支持异步的运行环境，比如 Netty、Undertow 以及 Servlet 3.1 （在 Servlet 3.1 中引入了异步 I/O 支持）之上的 Tomcat 和 Jetty引入响应式 Spring Security对于 Spring Security 而言，引入响应式编程技术同样会对传统实现方法带来一些变化​比方前面总结了 UserDetailsService 的作用，是用来获取用户信息，可以把它理解为是一种数据源，这样针对数据源的数据访问过程同样需要支持响应式编程，下面这些变化。​响应式用户认证在响应式 Spring Security 中，提供了一个响应式版本的 UserDetailsService，即 ReactiveUserDetailsService，定义如下：public interface ReactiveUserDetailsService {    Mono&amp;lt;UserDetails&amp;gt; findByUsername(String username);}请注，这里的 findByUsername() 方法返回的是一个 Mono 对象。​ReactiveUserDetailsService 接口有一个实现类 MapReactiveUserDetailsService，提供了基于内存的用户信息存储方案，实现过程如下所示：​public class MapReactiveUserDetailsService implements ReactiveUserDetailsService, ReactiveUserDetailsPasswordService {    private final Map&amp;lt;String, UserDetails&amp;gt; users;    public MapReactiveUserDetailsService(Map&amp;lt;String, UserDetails&amp;gt; users) {        this.users = users;    }     public MapReactiveUserDetailsService(UserDetails... users) {        this(Arrays.asList(users));    }    public MapReactiveUserDetailsService(Collection&amp;lt;UserDetails&amp;gt; users) {        Assert.notEmpty(users, &quot;users cannot be null or empty&quot;);        this.users = new ConcurrentHashMap&amp;lt;&amp;gt;();        for (UserDetails user : users) {             this.users.put(getKey(user.getUsername()), user);        }    }     @Override    public Mono&amp;lt;UserDetails&amp;gt; findByUsername(String username) {        String key = getKey(username);        UserDetails result = users.get(key);        return result == null ? Mono.empty() : Mono.just(User.withUserDetails(result).build());    }    @Override    public Mono&amp;lt;UserDetails&amp;gt; updatePassword(UserDetails user, String newPassword) {        return Mono.just(user)                 .map(u -&amp;gt;                     User.withUserDetails(u)                         .password(newPassword)                         .build()                 )                 .doOnNext(u -&amp;gt; {                     String key = getKey(user.getUsername());                     this.users.put(key, u);                 });    }    private String getKey(String username) {        return username.toLowerCase();    }}从上面的代码中可以看到： 首先，使用了一个 Map ，保存用户信息 然后，在 findByUsername() 方法中，通过 Mono.just() 方法，返回一个 Mono 对象。 最后，注意到在 updatePassword() 方法中，用到的 map() 方法，实际上是 Project Reactor 所提供的一个操作符，用于实现对一个对象执行映射操作。基于 MapReactiveUserDetailsService，可以在业务系统中，通过以下方式构建一个 ReactiveUserDetailsService:@Beanpublic ReactiveUserDetailsService userDetailsService() {        UserDetails u = User.withUsername(&quot;john&quot;)                .password(&quot;12345&quot;)                .authorities(&quot;read&quot;)                .build();         ReactiveUserDetailsService uds = new MapReactiveUserDetailsService(u);        return uds;}当然，针对用户认证，响应式 Spring Security ，也提供了响应式版本的 ReactiveAuthenticationManager 来执行具体的认证流程。​响应式授权机制介绍完认证，接着来看授权，假设系统中存在这样一个简单的 HTTP 端点：​这里使用了 Spring Webflux 构建了一个响应式端点，注意到 hello() 的返回值是一个 Mono 对象。同时，输入的也是一个 Mono 对象，因此，访问这个端点显然是需要认证的。​通过覆写 WebSecurityConfigurerAdapter 中的 configure(HttpSecurity http) 方法来设置访问权限。这种配置方法，在响应式编程体系中，无法再使用了，取而代之的是，使用一个叫 SecurityWebFilterChain 的配置接口来完成配置，该接口定义如下：public interface SecurityWebFilterChain {    //评估交互上下文 ServerWebExchange 是否匹配    Mono&amp;lt;Boolean&amp;gt; matches(ServerWebExchange exchange);    //一组过滤器    Flux&amp;lt;WebFilter&amp;gt; getWebFilters();}从命名上看，不难理解 SecurityWebFilterChain 代表一个过滤器链，而 ServerWebExchange 则是包含请求和响应的一种交互上下文，这在响应式环境中是一种固定属性，因为整个交互过程不是在单纯的发送请求和接受响应，而是在交换（Exchange）数据。如果想要使用 SecurityWebFilterChain，可以采用类似如下所示的代码示例：@Beanpublic SecurityWebFilterChain securityWebFilterChain(ServerHttpSecurity http) {        return http.authorizeExchange()                .pathMatchers(HttpMethod.GET, &quot;/hello&quot;).authenticated()                .anyExchange().permitAll()                .and().httpBasic() .and().build();}这里的 ServerHttpSecurity 可以用来构建 SecurityWebFilterChain 的实例，它的作用类似于非响应式系统中所使用的 HttpSecurity。同时，ServerHttpSecurity 也提供了一组熟悉的配置方法来设置各种认证和授权机制。​需要注意的是，在响应式系统中，因为处理的对象是 ServerWebExchange，而不是传统的 ServerRequest，所以在涉及与请求相关的方法命名时都统一做了调整。例如： 使用了 authorizeExchange() 方法来取代 authorizeRequests() 使用 anyExchange() 取代了 anyRequest() 这里的 pathMatchers() 也可以等同于以前介绍的 mvcMatchers() ​响应式方法级别访问控制​Spring Security 所提供的一个非常强大的功能，即全局安全方法机制。​通过这种机制，无论是 Web 服务还是普通应用，都可以基于方法的执行过程来应用授权规则。​在响应式编程中，称这种方法级别的授权机制为响应式方法安全（Reactive Method Security）机制，以便与传统的全局方法安全机制进行区分。​想要在应用程序中使用响应式方法安全机制，需要专门引入一个新的注解，即 @EnableReactiveMethodSecurity。这个注解与 @EnableGlobalMethodSecurity 注解类似，用来启用响应式安全方法机制：@Configuration@EnableGlobalMethodSecuritypublic class SecurityConfig 使用响应式方法安全机制的代码示例：@RestControllerpublic class HelloController {    @GetMapping(&quot;/hello&quot;)    @PreAuthorize(&quot;hasRole(&#39;ADMIN&#39;)&quot;)    public Mono&amp;lt;String&amp;gt; hello() {        return Mono.just(&quot;Hello!&quot;);    }}可以看到，这里使用了 @PreAuthorize 注解，并通过 “hasRole(‘ADMIN’)” 这一 SpEL 表达式来实现基于角色的授权机制。​就这个注解的使用方式而言，可以发现，它与传统应用程序中使用方式是一致的。​但不幸的是，就目前而言，响应式方法安全机制还不是很成熟，只提供了: @PreAuthorize @PostAuthorize​而 @PreFilter 和 @PostFilter 注解还没有实现。​总结响应式编程是技术发展趋势，为构建高弹性的应用程序提供了一种新的编程模式 在实现授权机制时，响应式编程模式与传统编程模式有什么区别？" }, { "title": "OAuth2", "url": "/posts/OAuth2/", "categories": "Java, Spring", "tags": "Spring Security", "date": "2020-06-06 10:33:00 +0000", "snippet": "对于微服务架构而言，安全性设计的最核心考虑点还是认证和授权。由于一个微服务系统中各服务之间，存在相互调用的关系，因此针对每一个服务，需要考虑下面两点： 自客户端的请求， 来自另一个服务的请求​因此，安全访问控制也面临以下两点： 从客户端请求到服务 从服务到服务等多种授权场景​为此，需要引入专门用于处理分布式环境下的授权体系，OAuth2 协议就是应对这种应用场景的有效解决方案。OAuth2 协议详解 Open Authorization 的简称 该协议解决的是授权问题，而不是认证问题 目前普遍被采用的是 OAuth 2.0 版协议。 OAuth2 是一个相对复杂的协议，对涉及的角色和授权模式给出了明确定义 OAuth2 协议的应用场景 在常见的电商系统中，存在类似工单处理的系统，工单的生成，在使用用户基本信息的同时，势必也依赖于用户的订单记录等数据。​为降低开发成本，假设整个商品订单模块不是自己研发的，而是集成了外部的订单管理平台，此时为生成工单记录，就必须让工单系统读取用户在订单管理平台上的订单记录。​在这个场景中，难点在于只有得到用户的授权，才能同意工单系统读取用户在订单管理平台上的订单记录。​此时问题就来了，工单系统如何获得用户的授权呢？一般能够想到的方法是： 用户将自己在订单管理平台上的用户名和密码告诉工单系统， 工单系统通过用户名和密码登录到订单管理平台并读取用户的订单记录整个过程如下图所示：上图中的方案虽然可行，但显然存在几个严重的缺点：​ 工单系统为了开展后续的服务，会保存用户在订单管理平台上的密码，这样很不安全；如果用户密码不小心被泄露了，就会导致订单管理平台上的用户数据发生泄露； 工单系统拥有了获取用户存储在订单管理平台上所有资料的权限，用户无法限制工单系统获得授权的范围和有效期； 如果用户修改了订单管理平台的密码，那么工单系统就无法正常访问订单管理平台了，这会导致业务中断，但又不能限制用户修改密码。 既然这个方案存在如此多的问题，那么有没有更好的办法呢？答案是肯定的，OAuth2 协议的诞生就是为了解决这些问题。​首先，针对密码的安全性，在 OAuth2 协议中： 密码还是由用户自己保管，避免了敏感信息的泄露 OAuth2 协议中提供的授权，具有明确的应用范围和有效期，用户可以根据需要限制工单系统所获取授权信息的作用效果 如果用户对自己的密码等身份凭证信息进行了修改，只需通过 OAuth2 协议重新进行一次授权即可，不会影响到相关联的其他第三方应用程序。​OAuth2 协议的角色OAuth2 协议能够具备这些优势，主要的原因在于把整个系统涉及的各个角色及其职责做了很好地划分。​OAuth2 协议中定义了四个核心的角色： 资源 客户端 授权服务器 资源服务器​​可以把 OAuth2 中的角色与现实中的应用场景对应起来。 OAuth2 协议中把需要访问的接口或服务统称为资源（Resource），每个资源都有一个拥有者（Resource Owner），也就是案例中的用户。 案例的工单系统代表的是一种第三方应用程序（Third-party Application），通常被称为客户端（Client）。 与客户端相对应的，OAuth2 协议中还存在一个服务提供商，案例中的订单管理平台就扮演了这个角色。服务提供商拥有一个资源服务器（Resource Server）和一个授权服务器（Authorization Server），其中资源服务器存放着用户资源，案例中的订单记录就是一种用户资源；而授权服务器的作用就是完成针对用户的授权流程，并最终颁发一个令牌，也就是我们所说的 Token。OAuth2 协议的 Token令牌是 OAuth2 协议中非常重要的一个概念，本质上是一种代表用户身份的授权凭证，​但与普通的用户名和密码信息不同，令牌具有针对资源的访问权限范围和有效期。​如下所示就是一种常见的令牌信息：{ &quot;access_token&quot;: &quot;0efa61be-32ab-4351-9dga-8ab668ababae&quot;, &quot;token_type&quot;: &quot;bearer&quot;, &quot;refresh_token&quot;: &quot;738c42f6-79a6-457d-8d5a-f9eab0c7cc5e&quot;, &quot;expires_in&quot;: 43199, &quot;scope&quot;: &quot;webclient&quot;}上述令牌信息中的各个字段都很重要，如下表：| access_token | 代表 OAuth2 的令牌 | 当访问受保护的资源时，用户需要携带这个令牌以便进行验证 || — | — | — || token_type | 代表令牌类型 | OAuth2 协议中有多种可选的令牌类型，包括 ： Bearer 类型（最常见的） MAC 类型等 || expires_in | 用于指定 access_token 的有效时间 | 超过这个有效时间，access_token 将会自动失效 || refresh_token | 下一个 access_token | 当 access_token 过期后，重新下发一个新的 access_token || scope | 可访问的权限范围 | 指定的是访问 Web 资源的“webclient” |令牌完成基于 OAuth2 协议的授权工作流程。整个流程如下图所示：​​上述流程如下： 客户端向用户请求授权，请求中一般包含资源的访问路径、对资源的操作类型等信息； 如果用户同意授权，就会将这个授权返回给客户端； 客户端获取到用户的授权信息，向授权服务器请求访问令牌； 授权服务器向客户端发放访问令牌 客户端携带访问令牌访问资源服务器上的资源 资源服务器获取访问令牌后会验证令牌的有效性和过期时间，并向客户端开放其需要访问的资源。 OAuth2 协议的授权模式 在整个工作流程中，最为关键的是第二步，即获取用户的有效授权。在 OAuth 2.0 中，定义了四种授权方式，即： 授权码模式（Authorization Code） 简化模式（Implicit） 密码模式（Password Credentials） 客户端模式（Client Credentials）。​授权码模式（Authorization Code）​当用户同意授权后，授权服务器返回的只是一个授权码，不是最终的访问令牌。​在这种授权模式下，需要客户端携带授权码去换令牌，这就需要客户端自身具备与授权服务器进行直接交互的后台服务。​授权码模式的执行流程： 用户在访问客户端时，被客户端导向授权服务器 用户可以选择是否给予客户端授权 当用户同意授权，授权服务器通过调用客户端后台服务提供的一个回调地址，将一个授权码返回给客户端 客户端收到授权码后向授权服务器申请令牌 授权服务器核对授权码并向客户端发送访问令牌。这里要注意的是，第 4 步是系统自动完成的，不需要用户的参与，用户需要做的就是在流程启动阶段同意授权。​密码模式（Password Credentials）密码模式的授权流程如下图所示：​可以看到，密码模式比较简单，也更加容易理解，步骤如下： 用户要提供用户名和密码访问客户端， 客户端会基于用户名和密码向授权服务器请求令牌 授权服务器成功执行用户认证操作后将会发放令牌OAuth2 中的客户端模式和简化模式因为在日常开发过程中应用得不是很多，就步总结了。​此时注意到了，虽然 OAuth2 协议解决的是授权问题，但它也应用到了认证的概念，这是因为只有验证了用户的身份凭证，我们才能完成对他的授权。所以说。OAuth2 是一款技术体系比较复杂的协议，综合应用了信息摘要、签名认证等安全性手段，并需要提供令牌以及背后的公私钥管理等功能。​OAuth2 协议与微服务架构对应到微服务系统中，服务提供者充当的角色就是资源服务器，而服务消费者就是客户端。所以每个服务本身既可以是客户端，也可以作为资源服务器，或者两者兼之。当客户端拿到 Token 之后，该 Token 就能在各个服务之间进行传递。如下图所示：​​总结在整个 OAuth2 协议中，最关键的问题就是如何获取客户端授权。​就目前主流的微服架构来说，当发起 HTTP 请求时，关注的是如何通过 HTTP 协议透明而高效地传递令牌，此时授权码模式下通过回调地址进行授权管理的方式就不是很实用，密码模式反而更加简洁高效。​ OAuth2 协议中所具备的四大角色以及四种授权模式吗​​" }, { "title": "Redis 协议的请求和响应的“套路”", "url": "/posts/cache-redis-03/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2020-06-05 13:33:22 +0000", "snippet": "Redis 协议为了方便以一种统一风格和原则来设计和使用指令，Redis 设计了 RESP，即 Redis Serialization Protocol，中文意思是 Redis 序列化协议。RESP 是二进制安全协议，可以供 Redis 或其他任何 Client-Server 使用。在 Redis 内部，还会基于 RESP 进一步扩展细节。设计原则Redis 序列化协议设计原则有三个： 实现简单； 可快速解析； 便于阅读。 Redis 协议的请求响应模型有三种，除了 2 种特殊模式，其他基本都是 ping-pong 模式（乒乓缓存机制），即 client 发送一个请求，server 回复一个响应，一问一答的访问模式。2 种特殊模式： pipeline 模式，即 client 一次连续发送多个请求，然后等待 server 响应，server 处理完请求后，把响应返回给 client； pub/sub 模式。即发布订阅模式，client 通过 subscribe 订阅一个 channel，然后 client 进入订阅状态，静静等待。当有消息产生时，server 会持续自动推送消息给 client，不需要 client 的额外请求。而且客户端在进入订阅状态后，只可接受订阅相关的命令如 SUBSCRIBE、PSUBSCRIBE、UNSUBSCRIBE 和 PUNSUBSCRIBE，除了这些命令，其他命令一律失效。请求和响应套路Redis 协议请求和响应有固定套路。请求格式有 2 种，分别是： ==inline cmd 内联命令格式。==当没有 redis-client，但还希望用 telnet，直接与 Redis 交互时（Redis 协议虽然简单易于阅读，但在交互式会话中使用，并不容易拼写），用此格式。 使用 inline cmd 内联格式，只需要用空格分隔请求指令及参数，简单快速，如 mget key1 key2\\r\\n。 ==Array 数组格式类型。==请求指令用的数组类型，与 Redis 响应的数组类型相同。响应格式有 5 种，分别是： simple strings 简单字符串类型，以 + 开头，后面跟字符串，以 CRLF（即 \\r\\n）结尾。 这种类型不是二进制安全类型，字符串中不能包含 \\r 或者 \\n。比如许多响应回复以 OK 作为操作成功的标志，协议内容就是 +OK\\r\\n ； Redis 协议将错误作为一种专门的类型，格式同简单字符串类型，唯一不同的是以 -（减号）开头。 Redis 内部实现对 Redis 协议做了进一步规范，减号后面一般先跟 ERR 或者 WRONGTYPE，然后再跟其他简单字符串，最后以 CRLF（回车换行）结束。这里给了两个示例，client 在解析响应时，一旦发现 - 开头，就知道收到 Error 响应； Integer 整数类型。整数类型以 :开头，后面跟字符串表示的数字，最后以回车换行结尾。 Redis 中许多命令都返回整数，但整数的含义要由具体命令来确定。比如，对于 incr 指令，：后的整数表示变更后的数值；对于 llen 表示 list 列表的长度，对于 exists 指令，1 表示 key 存在，0 表示 key 不存在。这里给个例子，：后面跟了个 1000，然后回车换行结束； bulk strings 字符串块类型。字符串块分头部和真正字符串内容两部分。 字符串块分头部和真正字符串内容两部分。字符串块类型的头部， 以 $ 开头，随后跟真正字符串内容的字节长度，然后以 CRLF 结尾。字符串块的头部之后，跟随真正的字符串内容，最后以 CRLF 结束字符串块。字符串块用于表示二进制安全的字符串，最大长度可以支持 512MB。 一个常规的例子：&quot;$6\\r\\nfoobar\\r\\n&quot;; 对于空字串：可以表示为 &quot;$0\\r\\n\\r\\n”; NULL字串： “$-1\\r\\n”。 Arrays 数组类型，如果一个命令需要返回多条数据就需要用数组格式类型，另外，前面提到 client 的请求命令也是主要采用这种格式。 以 * 开头，随后跟一个数组长度 N，然后以回车换行结尾；然后后面跟随 N 个数组元素，每个数组元素的类型，可以是 Redis 协议中除内联格式外的任何一种类型。比如： 一个字符串块的数组实例，*2\\r\\n$3\\r\\nget\\r\\n$3\\r\\nkey\\r\\n； 整数数组实例：”*3\\r\\n:1\\r\\n:2\\r\\n:3\\r\\n&quot;； *混合数组实例：&quot;*3\\r\\n :1\\r\\n-Bar\\r\\n$6\\r\\n foobar\\r\\n”; 空数组：”*0\\r\\n” *NULL数组：”*-1\\r\\n”。 协议分类Redis 协议主要分为 16 种，其中 8 种协议对应基本的 8 种数据类型，选择了使用什么数据类型，就使用对应的响应操作指令即可。剩下 8 种协议如下所示： pub-sub 发布订阅协议，client 可以订阅 channel，持续等待 server 推送消息； 事务协议，事务协议可以用 multi 和 exec 封装一些列指令，来一次性执行； 脚本协议，关键指令是 eval、evalsha 和 script 等； 连接协议，主要包括权限控制，切换 DB，关闭连接等； 复制协议，包括 slaveof、role、psync 等； 配置协议，config set/get 等，可以在线修改/获取配置； 调试统计协议，如 slowlog，monitor，info 等； 其他内部命令，如 migrate，dump，restore 等。Redis client 的使用及改进Redis 使用广泛，几乎所有主流语言都有对 Redis 开发了对应的 client。以 Java 语言为例，广泛使用的有 Jedis、Redisson 等。对于 Jedis client ： 轻量，简洁，便于集成和改造； 支持连接池，提供指令维度的操作，几乎支持 Redis 的所有指令； 不支持读写分离。对于 Redisson： Redisson 基于 Netty 实现，非阻塞 IO，性能较高； 支持异步请求和连接池； 支持读写分离、读负载均衡； 内建 tomcat Session； 支持 spring session 集成； 实现相对复杂。在新项目启动时，如果只是简单的 Redis 访问业务场景，可以直接用 Jedis，甚至可以简单封装 Jedis，实现 master-slave 的读写分离方案。如果想直接使用读写分离，想集成 spring session 等这些高级特性，也可以采用 redisson。Redis client 在使用中，需要根据业务及运维的需要，进行相关改进。在 client 访问异常时，可以增加重试策略，在访问某个 slave 异常时，需要重试其他 slave 节点。需要增加对 Redis 主从切换、slave 扩展的支持，比如采用守护线程定期扫描 master、slave 域名，发现 IP 变更，及时切换连接。对于多个 slave 的访问，还需要增加负载均衡策略。最后，Redis client 还可以与配置中心、Redis 集群管理平台整合，从而实时感知及协调 Redis 服务的访问。" }, { "title": "Spring Security 案例设计和初始化", "url": "/posts/protect-web-application/", "categories": "Java, Spring", "tags": "Spring Security", "date": "2020-06-04 13:33:00 +0000", "snippet": "本文，通过构建一个简单但完整的小型 Web 应用程序，完成基于 Spring Security 的认证和授权功能保护 Web 应用程序。​主要是：当合法用户成功登录系统之后，浏览器会跳转到一个系统主页，并展示一些个人健康档案（HealthRecord）数据。​​案例设计本文案例，有两条独立的代码流程： 完成系统业务逻辑处理的代码流程。采用经典的三层架构，即Web 层、服务层和数据访问层，因此会存在 HealthRecordController、HealthRecordService 以及 HealthRecordRepository 实现核心功能，自定义的用户认证的代码流程。构建独立的 UserDetailsService、 AuthenticationProvider 以及 User 以及 UserRepository 等组件以上两条代码流程整合在一起，得到案例的整体设计蓝图，如下图所示：​系统初始化要想实现上图中的效果，需要先对系统进行初始化。这部分工作涉及领域对象的定义、数据库初始化脚本的整理以及相关依赖组件的引入。​针对领域对象，重点是如下所示的 User 类定义：@Entitypublic class User {    @Id    @GeneratedValue(strategy = GenerationType.IDENTITY)    private Integer id;    private String username;    private String password;    @Enumerated(EnumType.STRING)    private PasswordEncoderType passwordEncoderType;    @OneToMany(mappedBy = &quot;user&quot;, fetch = FetchType.EAGER) private List&amp;lt;Authority&amp;gt; authorities; …}上面的 User 类中，指定了 主键 id 用户名 username 密码 password 加密算法枚举值 EncryptionAlgorithm，本案例使用了 BCryptPasswordEncoder 和 SCryptPasswordEncoder 两种可用的密码解密器，可以通过该枚举值进行设置 Authority 列表，用来指定该 User 所具备的权限信息，Authority 类的定义如下所示：```java@Entitypublic class Authority {    @Id    @GeneratedValue(strategy = GenerationType.IDENTITY)    private Integer id;    private String name;    @JoinColumn(name = “user”)    @ManyToOne private User user;…}```不难看出 User 和 Authority 之间是一对多的关系，这点和 Spring Security 内置的用户权限模型是一致的，这里使用了一系列来自 JPA（Java Persistence API，Java 持久化 API）规范的注解来定义领域对象之间的关联关系。" }, { "title": "Redis 的数据类型", "url": "/posts/cache-redis-02/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2020-06-03 16:33:22 +0000", "snippet": "Redis 有 8 种核心数据类型，分别是 ： string 字符串类型； list 列表类型； set 集合类型； sorted set 有序集合类型； hash 类型； bitmap 位图类型； geo 地理位置类型； HyperLogLog 基数统计类型。string 字符串 最基本的数据类型； 二进制安全，可以包含任何数据； 普通 string 采用 ==raw encoding（原始编码方式）==，该编码方式会动态扩容，并通过提前预分配冗余空间，来减少内存频繁分配的开销。 在字符串长度小于 1MB 时，按所需长度的 2 倍来分配； 超过 1MB，按照每次额外增加 1MB 的容量来预分配。 数字也存为 string 类型，但编码方式跟普通 string 不同，数字采用==整型编码==，字符串内容直接设为整数值的二进制字节序列；在存储普通字符串，序列化对象，以及计数器等场景时，都可以使用 Redis 的字符串类型，字符串数据类型对应使用的指令包括 set、get、mset、incr、decr 等。list 列表 一个快速双向链表，存储了一系列的 string 类型的字串值; 按照插入顺序排列； 插入元素的方式： 通过 lpush 将一个或多个元素插入到列表的头部； 通过 rpush 将一个或多个元素插入到队列尾部； 通过 lset、linsert 将元素插入到指定位置或指定元素的前后。 list 列表的获取： 通过 lpop、rpop 从对头或队尾弹出元素，如果队列为空，则返回 nil； 通过 Blpop、Brpop 从队头/队尾阻塞式弹出元素，如果 list 列表为空，没有元素可供弹出，则持续阻塞，直到有其他 client 插入新的元素。这里阻塞弹出元素，可以设置过期时间，避免无限期等待； 通过 LrangeR 获取队列内指定范围内的所有元素。 Redis 中，list 列表的偏移位置都是基于 0 的下标，即列表第一个元素的下标是 0，第二个是 1。偏移量也可以是负数，倒数第一个是 -1，倒数第二个是 -2，依次类推； 对于常规的 pop、push 元素，性能很高，时间复杂度为 O(1)，因为是列表直接追加或弹出； 对于通过随机插入、随机删除，以及随机范围获取，需要轮询列表确定位置，性能低下； feed timeline 存储时，由于 feed id 一般是递增的，可以直接存为 list，用户发表新 feed，就直接追加到队尾。另外消息队列、热门 feed 等业务场景，都可以使用 list 数据结构； 操作 list 列表时： 可以用 lpush、lpop、rpush、rpop、lrange 来进行常规的队列进出及范围获取操作； 在某些特殊场景下，可以用 lset、linsert 进行随机插入操作，用 lrem 进行指定元素删除操作； 在消息列表消费时，可以用 Blpop、Brpop 进行阻塞式获取，从而在列表暂时没有元素时，可以安静的等待新元素的插入，而不需要额外持续的查询。 set 集合类型 string 类型的无序集合，set 中的元素是唯一的，即 set 中不会出现重复的元素。 Redis 中的集合一般是通过 dict 哈希表实现的，所以插入、删除，以及查询元素，可以根据元素 hash 值直接定位，时间复杂度为 O(1)。对 set 类型数据的操作，除了常规的添加、删除、查找元素外，还可以用以下指令对 set 进行操作： sismember 指令判断该 key 对应的 set 数据结构中，是否存在某个元素，如果存在返回 1，否则返回 0； sdiff 指令来对多个 set 集合执行差集； sinter 指令对多个集合执行交集； sunion 指令对多个集合执行并集； spop 指令弹出一个随机元素； srandmember 指令返回一个或多个随机元素。set 集合的特点是查找、插入、删除特别高效，时间复杂度为 O(1)。所以在社交系统中，可以用于存储关注的好友列表，用来判断是否关注，还可以用来做好友推荐使用。另外，还可以利用 set 的唯一性，来对服务的来源业务、来源 IP 进行精确统计。sorted set 有序集合类型 也称为 zset，有序集合同 set 集合类似，也是 string 类型元素的集合，且所有元素不允许重复。但有序集合中，每个元素都会关联一个 double 类型的 score 分数值。有序集合通过这个 score 值进行由小到大的排序。有序集合中，元素不允许重复，但 score 分数值却允许重复。有序集合除了常规的添加、删除、查找元素外，还可以通过以下指令对 sorted set 进行操作： zscan 指令：按顺序获取有序集合中的元素； zscore 指令：获取元素的 score 值； zrange指令：通过指定 score 返回指定 score 范围内的元素； 在某个元素的 score 值发生变更时，还可以通过 zincrby 指令对该元素的 score 值进行加减； 通过 zinterstore、zunionstore 指令对多个有序集合进行取交集和并集，然后将新的有序集合存到一个新的 key 中，如果有重复元素，重复元素的 score 进行相加，然后作为新集合中该元素的 score 值。sorted set 有序集合的特点是： 所有元素按 score 排序，而且不重复； 查找、插入、删除非常高效，时间复杂度为 O(1)。因此，可以用有序集合来统计排行榜，实时刷新榜单，还可以用来记录学生成绩，从而轻松获取某个成绩范围内的学生名单，还可以用来对系统统计增加权重值，从而在 dashboard 实时展示。hash 类型Redis 中的哈希实际是 field 和 value 的一个映射表。hash 数据结构的特点是在单个 key 对应的哈希结构内部，可以记录多个键值对，即 field 和 value 对，value 可以是任何字符串。而且这些键值对查询和修改很高效。所以可以用 hash 来存储具有多个元素的复杂对象，然后分别修改或获取这些元素。hash 结构中的一些重要指令，包括：hmset、hmget、hexists、hgetall、hincrby 等。 hmset 指令批量插入多个 field、value 映射； hmget 指令获取多个 field 对应的 value 值； hexists 指令判断某个 field 是否存在； 如果 field 对应的 value 是整数，还可以用 hincrby 来对该 value 进行修改。bitmap 位图类型 Redis 中的 bitmap 位图是一串连续的二进制数字，底层实际是基于 string 进行封装存储的，按 bit 位进行指令操作的； bitmap 中每一 bit 位所在的位置就是 offset 偏移，可以用 setbit、bitfield 对 bitmap 中每个 bit 进行置 0 或置 1 操作，也可以用 bitcount 来统计 bitmap 中的被置 1 的 bit 数，还可以用 bitop 来对多个 bitmap 进行求与、或、异或等操作。bitmap 位图的特点是： 按位设置、求与、求或等操作很高效，存储成本非常低，用来存对象标签属性的话，一个 bit 即可存一个标签。可以用 bitmap，存用户最近 N 天的登录情况，每天用 1 bit，登录则置 1。个性推荐在社交应用中非常重要，可以对新闻、feed 设置一系列标签，如军事、娱乐、视频、图片、文字等，用 bitmap 来存储这些标签，在对应标签 bit 位上置 1。对用户，也可以采用类似方式，记录用户的多种属性，并可以很方便的根据标签来进行多维度统计。bitmap 位图的重要指令包括： setbit getbit bitcount bitfield bitop bitpos 等。geo 地理位置类型在移动社交时代，LBS 应用越来越多，比如微信、陌陌中附近的人，美团、大众点评中附近的美食、电影院，滴滴、优步中附近的专车等。要实现这些功能，就得使用地理位置信息进行搜索。地球的地理位置是使用二维的经纬度进行表示的，我们只要确定一个点的经纬度，就可以确认它在地球的位置。Redis 在 3.2 版本之后增加了对 GEO 地理位置的处理功能。Redis 的 GEO 地理位置本质上是基于 sorted set 封装实现的。在存储分类 key 下的地理位置信息时，需要对该分类 key 构建一个 sorted set 作为内部存储结构，用于存储一系列位置点。在存储某个位置点时，首先利用 Geohash 算法，将该位置二维的经纬度，映射编码成一维的 52 位整数值，将位置名称、经纬度编码 score 作为键值对，存储到分类 key 对应的 sorted set 中。需要计算某个位置点 A 附近的人时，首先以指定位置 A 为中心点，以距离作为半径，算出 GEO 哈希 8 个方位的范围， 然后依次轮询方位范围内的所有位置点，只要这些位置点到中心位置 A 的距离在要求距离范围内，就是目标位置点。轮询完所有范围内的位置点后，重新排序即得到位置点 A 附近的所有目标： 使用 geoadd，将位置名称（如人、车辆、店名）与对应的地理位置信息添加到指定的位置分类 key 中； 使用 geopos 方便地查询某个名称所在的位置信息； 使用 georadius 获取指定位置附近，不超过指定距离的所有元素； 使用 geodist 来获取指定的两个位置之间的距离。这样，是不是就可以实现，找到附近的餐厅，算出当前位置到对应餐厅的距离，这样的功能了？Redis GEO 地理位置，利用 Geohash 将大量的二维经纬度转一维的整数值，这样可以方便的对地理位置进行查询、距离测量、范围搜索。但由于地理位置点非常多，一个地理分类 key 下可能会有大量元素，在 GEO 设计时，需要提前进行规划，避免单 key 过度膨胀。Redis 的 GEO 地理位置数据结构，应用场景很多，比如查询某个地方的具体位置，查当前位置到目的地的距离，查附近的人、餐厅、电影院等。GEO 地理位置数据结构中，重要指令包括 geoadd、geopos、geodist、georadius、georadiusbymember 等。HyperLogLog 基数统计类型 用来做基数统计的数据类型，当输入巨大数量的元素做统计时，只需要很小的内存即可完成。HyperLogLog 不保存元数据，只记录待统计元素的估算数量，这个估算数量是一个带有 0.81% 标准差的近似值，在大多数业务场景，对海量数据，不足 1% 的误差是可以接受的； 如果计数数量不大，采用稀疏矩阵存储，随着计数的增加，稀疏矩阵占用的空间也会逐渐增加，当超过阀值后，则改为稠密矩阵，稠密矩阵占用的空间是固定的，约为12KB字节； 通过 hyperLoglog 数据类型，利用 pfadd 向基数统计中增加新的元素，可以用 pfcount 获得 hyperLogLog 结构中存储的近似基数数量，还可以用 hypermerge 将多个 hyperLogLog 合并为一个 hyperLogLog 结构，从而可以方便的获取合并后的基数数量； hyperLogLog 的特点是统计过程不记录独立元素，占用内存非常少，非常适合统计海量数据。在大中型系统中，统计每日、每月的 UV 即独立访客数，或者统计海量用户搜索的独立词条数，都可以用 hyperLogLog 数据类型来进行处理。" }, { "title": "Redis", "url": "/posts/cache-redis-01/", "categories": "Database, Cache", "tags": "Cache, Redis", "date": "2020-06-03 16:33:22 +0000", "snippet": "简介 一款基于 ANSI C 语言编写的，BSD 许可的，日志型 key-value 存储组件； 所有数据结构都存在内存中，可以用作缓存、数据库和消息中间件； Remote dictionary server （远程字典服务）的缩写； 一个 Redis 实例有多个存储数据的字典，客户端通过 select 来选择字典（即 DB 进行数据存储）。特性数据结构丰富同为 key-value 存储组件，Memcached 只能支持二进制字节块一种数据类型；Redis 的数据类型却丰富的多，有 8 种核心数据类型，每种数据类型都有一些列指令对应： string list set sorted set hash bitmap geo hyperloglogRedis 的所有内存数据结构都存在全局的 dict（类似 Memcached 的 hashtable） 字典中：Redis 的 dict 有 2 个哈希表： 插入新 key 时，一般用 0 号哈希表，随着 key 的插入或删除，当 0 号哈希表的 keys 数大于哈希表桶数，或 kyes 数小于哈希桶的 1/10 时，就对 hash 表进行扩缩。 dict 中，哈希表解决冲突的方式，与 Memcached 相同，也是使用桶内单链表，来指向多个 hash 相同的 key/value 数据。高性能Redis 是单进程/单线程组件，Redis 网络 IO 和命令处理，都在核心进程中，由单线程处理。Redis 基于 Epoll 事件模型开发，可以进行非阻塞网络 IO，同时由于单线程命令处理，整个处理过程不存在竞争，不需要加锁，没有上下文切换开销，所有数据操作都是在内存中操作。因此 Redis 的性能很高，单个实例（单线程压测）可达到 10 ~ 11w 级的 QPS。核心线程除了负责网络 IO 及命令处理外，还负责写数据到缓冲，以方便将最新写操作同步到 ==AOF==、==slave==。除主进程，Redis 会 fork 一个子进程，来进行重负荷任务的处理。Redis fork 子进程主要有 3 种场景： 收到 bgrewriteaof 命令时，Redis 调用 fork，构建一个子进程，子进程往临时 AOF 文件中，写入重建数据库状态的所有命令，当写入完毕，子进程则通知父进程，父进程把新增的写操作也追加到临时 AOF 文件，然后将临时文件替换老的 AOF 文件，并重命名。 收到 bgsave 命令时，Redis 构建子进程，子进程将内存中的所有数据通过快照做一次持久化落地，写入到 RDB 中。 当需要进行全量复制时，master 也会启动一个子进程，子进程将数据库快照保存到 RDB 文件，在写完 RDB 快照文件后，master 就会把 RDB 发给 slave，同时将后续新的写指令都同步给 slave。主进程中，除了主线程处理网络 IO 和命令操作外，还有 3 个辅助 BIO 线程。这 3 个 BIO 线程分别负责处理，文件关闭、AOF 缓冲数据刷新到磁盘，以及清理对象这三个任务队列。Redis 在启动时，会同时启动这三个 BIO 线程，然后 BIO 线程休眠等待任务。当需要执行相关类型的后台任务时，就会构建一个 bio_job 结构，记录任务参数，然后将 bio_job 追加到任务队列尾部。然后唤醒 BIO 线程，即可进行任务执行。存储Redis 所有数据读写操作，不仅内存中进行，还可以将所有数据进行落盘做持久化。Redis 提供了两种持久化方式： 快照方式，将某时刻所有数据都写入硬盘的 RDB 文件； RDB，只记录某个时间点的快照，可以通过设置指定时间内修改 keys 数的阀值，超过则自动构建 RDB 内容快照。线上运维，一般会选择在业务低峰期定期进行。RDB 存储的是构建时刻的数据快照，内存数据一旦落地，不会理会后续的变更。 追加文件方式，即将所有写命令都以追加的方式写入硬盘的 AOF 文件中； 记录是构建整个数据库内容的命令，它会随着新的写操作不断进行追加操作。由于不断追加，AOF 会记录数据大量的中间状态，AOF 文件会变得非常大，此时，可以通过 bgrewriteaof 指令，对 AOF 进行重写，只保留数据的最后内容，来大大缩减 AOF 的内容。 一般会同时使用两种方式，通过开启 appendonly 和关联配置项，将写命令及时追加到 AOF 文件，同时在每日流量低峰时，通过 bgsave 保存当时所有内存数据快照。热备对于互联网系统的线上流量，读操作远远大于写操作。大量读请求，通常会远超 Redis 的可承载范围。此时，使用 Redis 的复制特性，让一个 Redis 实例作为 master，然后通过复制挂载多个不断同步更新的副本，即多个 slave。通过读写分离，把所有写操作落在 Redis 的 master，所有读操作随机落在 Redis 的多个 slave 中，从而大幅提升 Redis 的读写能力。脚本Lua 是一个高效、简洁、易扩展的脚本语言，可以方便的嵌入其他语言中使用。Redis 自 2.6 版本开始支持 Lua。通过支持 client 端自定义的 Lua 脚本，Redis 可以减少网络开销，提升处理性能，还可以把脚本中的多个操作作为一个整体来操作，实现原子性更新。事务Redis 还支持事务，在 multi 指令后，指定多个操作，然后通过 exec 指令一次性执行，如果中途出现异常，则不执行所有命令操作，否则，按顺序一次性执行所有操作，执行过程中不会执行任何其他指令；集群Redis 还支持 Cluster 特性，可以通过自动或手动方式，将所有 key 按哈希分散到不同节点。在容量不足时，通过 Redis 的迁移指令，把其中一部分 key 迁移到其他节点。Redis 集群管理有 3 种方式： client 分片访问，client 对 key 做 hash，然后按取模或一致性 hash，把 key 的读写分散到不同的 Redis 实例上； 在 Redis 前加一个 proxy，把路由策略、后端 Redis 状态维护的工作都放到 proxy 中进行，client 直接访问 proxy，后端 Redis 变更，只需修改 proxy 配置即可； 直接使用 Redis cluster。Redis 创建之初，使用方直接给 Redis 的节点分配 slot，后续访问时，对 key 做 hash 找到对应的 slot，然后访问 slot 所在的 Redis 实例。在需要扩容缩容时，可以在线通过 cluster setslot 指令，以及 migrate 指令，将 slot 下所有 key 迁移到目标节点，即可实现扩缩容的目的。" }, { "title": "缓存的七个坑", "url": "/posts/cache-02/", "categories": "Database, Cache", "tags": "Cache", "date": "2020-06-03 16:33:22 +0000", "snippet": "在缓存系统设计架构中，有很多坑，如果设计不当会导致很多严重的后果。设计不当，轻则请求变慢、性能降低，重则会数据不一致、系统可用性降低，甚至会导致缓存雪崩，整个系统无法对外提供服务。缓存设计中的七个大坑，如下： 缓存失效 缓存穿透 缓存雪崩 数据不一致 数据并发竞争 Hot key Big key缓存失效现象描述由于服务系统查数据，首先会查缓存。如果缓存数据不存在，就进一步查 DB，最后查到数据后回种到缓存并返回！缓存性能比 DB 高 50~100 倍以上，因此希望数据查询尽可能命中缓存，这样系统负荷最小，性能最佳。缓存里存储的数据，基本上都是以 key 为索引进行存储和获取。业务访问时，如果大量的 key 同时过期，很多缓存数据访问都会 找不到，进而穿透到 DB，DB 的压力就会明显上升，由于 DB 的性能较差，只在缓存的 1%~2% 以下，这样请求的慢查率会明显上升。原因导致缓存失效，特别是很多 key 一起失效的原因，和日常写缓存的过期时间息息相关。写缓存时，根据业务访问特点，给每种业务数据预置一个过期时间，写入缓存时把这个过期时间带上，让缓存数据在这个固定过期时间后被淘汰。一般情况下，缓存数据是逐步写入的，所以也是逐步过期被淘汰的。某些场景，一大批数据会被系统主动或被动从 DB 批量加载，然后写入缓存。这些数据写入缓存时，由于使用相同的过期时间，在经历这个过期时间之后，这批数据就会一起到期，从而被缓存淘汰。此时，对这批数据的所有请求，都会出现缓存失效，从而都穿透到 DB，DB 由于查询量太大，就很容易压力大增，请求变慢。场景很多业务场景，稍不注意，就出现大量缓存失效，从而导致系统 DB 压力大、请求变慢的情况。类似的场景有： 同一批火车票、飞机票，当可以售卖时，系统会一次性加载到缓存，如果缓存写入时，过期时间按照预先设置的过期值，过期时间到期后，系统就会因缓存失效出现变慢的问题； 微博业务，会有后台离线系统，持续计算热门微博，每当计算结束，会将这批热门微博批量写入对应的缓存； 很多业务，在部署新 IDC 或新业务上线时，会进行缓存预热，也会一次性加载大批热数据。方案设计缓存的过期时间时，使用公式：==过期时间 = baes 时间 + 随机时间。==即相同业务数据写缓存时，在基础过期时间之上，再加一个随机的过期时间，让数据在未来一段时间内慢慢过期，避免瞬时全部过期，对 DB 造成过大压力。缓存穿透现象描述缓存穿透是一个很有意思的问题。因为缓存穿透发生的概率很低，所以一般很难被发现。一旦发现，且量还不小，可能立即就会经历一个忙碌的夜晚。对于正常访问，访问的数据即便不在缓存，也可以通过 DB 加载回种到缓存。而缓存穿透，意味着有特殊访客在查询一个不存在的 key，导致每次查询都会穿透到 DB，如果这个特殊访客再控制一批肉鸡机器，持续访问系统里不存在的 key，就会对 DB 产生很大的压力，从而影响正常服务。原因在系统设计时，更多考虑的是正常访问路径，对特殊访问路径、异常访问路径考虑相对欠缺。缓存访问设计的正常路径，是先访问 cache，cache miss 后查 DB，DB 查询到结果后，回种缓存返回。对于正常 key 访问是没有问题，但是如果用户访问的是一个不存在的 key，查 DB 返回空（即一个 NULL），就不会把这个空写回 cache，不管查询多少次这个不存在的 key，都会 cache miss，都会查询 DB。整个系统就会退化成一个 “前端 + DB“ 的系统，由于 DB 的吞吐只在 cache 的 1%~2% 以下，如果有特殊访客，大量访问这些不存在的 key，就会导致系统的性能严重退化，影响正常用户的访问。场景 通过不存在的 UID 访问用户； 通过不存在的车次 ID 查看购票信息 用户输入错误偶尔几个这种请求问题不大，如果是大量这种请求，就会对系统影响非常大。解决方案方案一查询不存在的数据时，第一次查 DB，即使没查到结果返回 NULL，仍然记录这个 key 到缓存，只是这个 key 对应的 value 是一个特殊设置的值。此方案需要注意的坑：如果特殊访客持续访问大量的不存在的 key，这些 key 即便只存一个简单的默认值，也会占用大量的缓存空间，导致正常 key 的命中率下降。进一步的改进措施是： 对不存在的 key 只存较短的时间，让它们尽快过期； 将这些不存在的 key 存在一个独立的公共缓存，从缓存查找时，先查正常的缓存组件，如果 miss，则查一下公共的非法 key 的缓存，如果后者命中，直接返回，否则穿透 DB，如果查出来是空，则回种到非法 key 缓存，否则回种到正常缓存。方案二构建一个 BloomFilter 缓存过滤器，记录全量数据，访问数据时直接通过 BloomFilter 判断这个 key 是否存在，如果不存在直接返回，无需查缓存和 DB。此方案需要注意的坑：BloomFilter 要缓存全量的 key，要求全量的 key 数量不大，10亿 条数据以内最佳，因为 10亿 条数据大概要占用 1.2GB 的内存。也可以用 BloomFilter 缓存非法 key，每次发现一个 key 是不存在的非法 key，就记录到 BloomFilter 中，这种记录方案，会导致 BloomFilter 存储的 key 持续高速增长，为了避免记录 key 太多而导致误判率增大，需要定期清零处理。缓存雪崩现象描述指部分缓存节点不可用，导致整个缓存体系甚至服务系统不可用的情况。缓存雪崩按照缓存是否 rehash（是否漂移）分两种情况： 缓存不支持 rehash 导致的系统雪崩不可用 缓存支持 rehash 导致的缓存雪崩不可用原因支持 rehash缓存支持 rehash 时产生的雪崩，一般是由于较多缓存节点不可用，请求穿透导致 DB 也过载不可用，最终整个系统雪崩不可用的。缓存支持 rehash 时产生的雪崩，大多跟流量洪峰有关，流量洪峰到达，引发部分缓存节点过载 Crash，然后因 rehash 扩散到其他缓存节点，最终整个缓存体系异常不支持 rehash缓存节点不支持 rehash，较多缓存节点不可用时，大量 Cache 访问会失败。根据缓存读写模型，这些请求会进一步访问 DB，而且 DB 可承载的访问量要远比缓存小的多，请求量过大，就很容易造成 DB 过载，大量慢查询，最终阻塞甚至 Crash，从而导致服务异常。这是因为缓存分布设计时，会选择一致性 Hash 分布方式，同时在部分节点异常时，采用 rehash 策略，即把异常节点请求平均分散到其他缓存节点。在一般情况下，一致性 Hash 分布 + rehash 策略可以很好得运行，但在较大的流量洪峰到临之时，如果大流量 key 比较集中，正好在某 1～2 个缓存节点，很容易将这些缓存节点的内存、网卡过载，缓存节点异常 Crash，然后这些异常节点下线，这些大流量 key 请求又被 rehash 到其他缓存节点，进而导致其他缓存节点也被过载 Crash，缓存异常持续扩散，最终导致整个缓存体系异常，无法对外提供服务。场景微博、Twitter 等系统在运行的最初若干年都遇到过很多次。比如： 微博最初很多业务缓存采用一致性 Hash + rehash 策略，在突发洪水流量来临时，部分缓存节点过载 Crash 甚至宕机，然后这些异常节点的请求转到其他缓存节点，又导致其他缓存节点过载异常，最终整个缓存池过载； 机架断电，导致业务缓存多个节点宕机，大量请求直接打到 DB，也导致 DB 过载而阻塞，整个系统异常。最后缓存机器复电后，DB 重启，数据逐步加热后，系统才逐步恢复正常。解决方案方案一对业务 DB 的访问增加读写开关，当发现 DB 请求变慢、阻塞，慢请求超过阀值时，就会关闭读开关，部分或所有读 DB 的请求进行 failfast 立即返回，待 DB 恢复后再打开读开关。方案二对缓存增加多个副本，缓存异常或请求 miss 后，再读取其他缓存副本，而且多个缓存副本尽量部署在不同机架，从而确保在任何情况下，缓存系统都会正常对外提供服务。方案三 对缓存体系进行实时监控，当请求访问的慢速比超过阀值时，及时报警，通过机器替换、服务替换进行及时恢复； 通过各种自动故障转移策略，自动关闭异常接口、停止边缘服务、停止部分非核心功能措施，确保在极端场景下，核心功能的正常运行。这三种方案可以三管齐下，避免缓存雪崩的发生。数据不一致现象描述 同一份数据，同时存在 DB 和缓存之中，DB 和缓存的数据不一致； 缓存有多个副本，多个缓存副本里的数据也可能会发生不一致现象。 原因不一致的问题大多跟缓存更新异常有关。比如： 更新 DB 后，写缓存失败，从而导致缓存中存的是老数据； 如果系统采用一致性 Hash 分布，同时采用 rehash 自动漂移策略，在节点多次上下线之后，会产生脏数据； 缓存有多个副本时，更新某个副本失败，也会导致这个副本的数据是老数据。场景导致数据不一致的场景也不少： 在缓存机器的带宽被打满，或者机房网络出现波动时，缓存更新失败，新数据没有写入缓存，就会导致缓存和 DB 的数据不一致。 缓存 rehash 时，某个缓存机器反复异常，多次上下线，更新请求多次 rehash。这样，一份数据存在多个节点，且每次 rehash 只更新某个节点，导致一些缓存节点产生脏数据。解决方案第一个方案cache 更新失败后，先进行重试，如果重试失败，则将失败的 key 写入队列机服务，待缓存访问恢复后，将这些 key 从缓存删除。这些 key 在再次被查询时，重新从 DB 加载，从而保证数据的一致性。第二个方案缓存时间适当调短，让缓存数据及早过期后，然后从 DB 重新加载，确保数据的最终一致性。第三个方案不采用 rehash 漂移策略，而采用缓存分层策略，尽量避免脏数据产生。以上三个，需要根据实际情况进行选择。数据并发竞争现象描述互联网系统，线上流量较大，缓存访问中很容易出现数据并发竞争的现象。数据并发竞争，是指在高并发访问场景，一旦缓存访问没有找到数据，大量请求就会并发查询 DB，导致 DB 压力大增的现象。原因主要是由于多个进程/线程中，有大量并发请求获取相同的数据，而这个数据 key 因为正好过期、被剔除等各种原因在缓存中不存在，这些进程/线程之间没有任何协调，然后一起并发查询 DB，请求那个相同的 key，最终导致 DB 压力大增。场景在大流量系统比较常见。比如：车票系统。如果某个火车车次缓存信息过期，仍然有大量用户在查询该车次信息。解决方案方案一使用全局锁。当缓存请求 miss 后，先尝试加全局锁，只有加全局锁成功的线程，才可以到 DB 去加载数据。其他进程/线程在读取缓存数据 miss 时，如果发现这个 key 有全局锁，就进行等待，待之前的线程将数据从 DB 回种到缓存后，再从缓存获取。方案二对缓存数据保持多个备份，即便其中一个备份中的数据过期或被剔除了，还可以访问其他备份，从而减少数据并发竞争的情况。Hot key现象描述对于大多数互联网系统，数据是分冷热的。比如最近的新闻、新发表的微博被访问的频率最高，而比较久远的之前的新闻、微博被访问的频率就会小很多。在突发事件发生时，大量用户同时去访问这个突发热点信息，访问这个 Hot key，这个突发热点信息所在的缓存节点就很容易出现过载和卡顿现象，甚至会被 Crash。原因Hot key 引发缓存系统异常，主要是因为突发热门事件发生时，超大量的请求访问热点事件对应的 key，比如微博中数十万、数百万的用户同时去吃一个新瓜。数十万的访问请求同一个 key，流量集中打在一个缓存节点机器，这个缓存机器很容易被打到物理网卡、带宽、CPU 的极限，从而导致缓存访问变慢、卡顿。场景引发 Hot key 的业务场景很多，比如： 明星结婚、离婚、出轨这种特殊突发事件； 奥运、春节这些重大活动或节日， 秒杀、双12、618 等线上促销活动；解决方案要解决这种极热 key 的问题，首先要找出这些 Hot key 来。对于重要节假日、线上促销活动、集中推送这些提前已知的事情，可以提前评估出可能的热 key 来。而对于突发事件，无法提前评估，可以通过 Spark，对应流任务进行实时分析，及时发现新发布的热点 key。而对于之前已发出的事情，逐步发酵成为热 key 的，则可以通过 Hadoop 对批处理任务离线计算，找出最近历史数据中的高频热 key找到热 key 后，就有很多解决办法了。首先可以将这些热 key 进行分散处理，比如一个热 key 名字叫 hotkey，可以被分散为 hotkey#1、hotkey#2、hotkey#3，……hotkey#n，这 n 个 key 分散存在多个缓存节点，然后 client 端请求时，随机访问其中某个后缀的 hotkey，这样就可以把热 key 的请求打散，避免一个缓存节点过载，其次，也可以 key 的名字不变，对缓存提前进行多副本+多级结合的缓存架构设计。再次，如果热 key 较多，还可以通过监控体系对缓存的 SLA 实时监控，通过快速扩容来减少热 key 的冲击。最后，业务端还可以使用本地缓存，将这些热 key 记录在本地缓存，来减少对远程缓存的冲击。Big key现象描述大 Key 的问题。指在缓存访问时，部分 Key 的 Value 过大，读写、加载易超时的现象。原因 大 key 占总体数据的比例很小，存 Mc，对应的 slab 较少，导致很容易被频繁剔除，DB 反复加载，从而导致查询较慢； 大 key 很多，并被大量访问，缓存组件的网卡、带宽很容易被打满，也会导致较多的大 key 慢查询； 如果大 key 缓存的字段较多，每个字段的变更都会引发对这个缓存数据的变更，同时这些 key 也会被频繁地读取，读写相互影响，也会导致慢查现象； 大 key 一旦被缓存淘汰，DB 加载可能需要花费很多时间，也会导致大 key 查询慢问题。场景比较常见。比如互联网系统中需要保存用户最新 1万 个粉丝的业务，比如一个用户个人信息缓存，包括基本资料、关系图谱计数、发 feed 统计等。微博的 feed 内容缓存也很容易出现，一般用户微博在 140 字以内，但很多用户也会发表 1千 字甚至更长的微博内容，这些长微博也就成了大 key，。解决方案第一种方案如果数据存在 Mc 中，设计一个缓存阀值，当 value 的长度超过阀值，则对内容启用压缩，让 KV 尽量保持小的 size，其次评估大 key 所占的比例，在 Mc 启动之初，就立即预写足够数据的大 key，让 Mc 预先分配足够多的 trunk size 较大的 slab。确保后面系统运行时，大 key 有足够的空间来进行缓存。第二种方案如果数据存在 Redis 中，比如业务数据存 set 格式，大 key 对应的 set 结构有几千几万个元素，这种写入 Redis 时会消耗很长的时间，导致 Redis 卡顿。此时，可以扩展新的数据结构，同时让 client 在这些大 key 写缓存之前，进行序列化构建，然后通过 restore 一次性写入。第三种方案将大 key 分拆为多个 key，尽量减少大 key 的存在。同时由于大 key 一旦穿透到 DB，加载耗时很大，所以可以对这些大 key 进行特殊照顾，比如设置较长的过期时间，比如缓存内部在淘汰 key 时，同等条件下，尽量不淘汰这些大 key。BloomFilterBloomFilter 是一个非常有意思的数据结构: 可以挡住非法 key 攻击； 可以低成本、高性能地对海量数据进行判断。比如一个系统有数亿用户和百亿级新闻 feed，可以用 BloomFilter 来判断某个用户是否阅读某条新闻 feed。BloomFilter 的目的==检测一个元素是否存在于一个集合内==。BloomFilter 的原理用 bit 数据组来表示一个集合，对一个 key 进行多次不同的 Hash 检测，如果所有 Hash 对应的 bit 位都是 1，则表明 key 非常大概率存在，平均单记录占用 1.2 字节即可达到 99%，只要有一次 Hash 对应的 bit 位是 0，就说明这个 key 肯定不存在于这个集合内。BloomFilter 的算法首先分配一块内存空间做 bit 数组，数组的 bit 位初始值全部设为 0，加入元素时，采用 k 个相互独立的 Hash 函数计算，然后将元素 Hash 映射的 K 个位置全部设置为 1。检测 key 时，仍然用这 k 个 Hash 函数计算出 k 个位置，如果位置全部为 1，则表明 key 存在，否则不存在。BloomFilter 的优势内存操作，性能很高。另外空间效率非常高，要达到 1% 的误判率，平均单条记录占用 1.2 字节即可。而且，平均单条记录每增加 0.6 字节，还可让误判率继续变为之前的 1/10，即平均单条记录占用 1.8 字节，误判率可以达到 1/1000；平均单条记录占用 2.4 字节，误判率可以到 1/10000。以此类推。这里的误判率是指，BloomFilter 判断某个 key 存在，但它实际不存在的概率，因为它存的是 key 的 Hash 值，而非 key 的值，所以有概率存在这样的 key，它们内容不同，但多次 Hash 后的 Hash 值都相同。对于 BloomFilter 判断不存在的 key ，则是 100% 不存在的，反证法，如果这个 key 存在，那它每次 Hash 后对应的 Hash 值位置肯定是 1，而不会是 0。" }, { "title": "缓存", "url": "/posts/cache-01/", "categories": "Database, Cache", "tags": "Cache", "date": "2020-06-03 16:33:22 +0000", "snippet": "定义 侠义缓存的定义（缓存最初的含义），用于==加速 CPU 数据交换的 存储器 RAM（随机存取存储器==），通常这种存储器使用昂贵但快速的静态 RAM（SRAM）技术，用以对 DRAM进 行加速。这是一个狭义缓存的定义。 广义缓存的定义，更宽泛，任何可以==用于数据高速交换的存储介质都是缓存，可以是硬件也可以是软件。==意义通过开辟一个新的数据交换缓冲区，来解决原始数据获取代价太大的问题，让数据得到更快的访问！基本思想利用时间局限性原理，通过空间换时间来达到加速数据获取的目的。由于缓存空间的成本较高，在实际设计架构中要考虑访问延迟和成本的权衡问题。三个关键点： 时间局限性原理。即被获取过一次的数据在未来会被多次引用，比如一条微博被一个人感兴趣并阅读后，它大概率还会被更多人阅读，当然如果变成热门微博后，会被数以百万/千万计算的更多用户查看。 空间换时间。因为原始数据获取太慢，所以开辟一块高速独立空间，提供高效访问，来达到数据获取加速的目的。 性能成本权衡。构建系统时，希望系统的访问性能越高越好，访问延迟越低越好。但是维持相同数据规模的存储以及访问，性能越高延迟越小，成本也会越高，因此在系统架构设计时，需要在系统性能和开发运行成本做取舍。优点 提升访问性能。缓存存储原始数据，可以大幅提升访问性能。 减少网络流量，降低网络拥堵。在实际业务场景中，缓存中存储的往往是需要频繁访问的中间数据甚至最终结果，这些数据相比 DB 中的原始数据小很多，这样就可以减少网络流量，降低网络拥堵。 减轻服务负载。由于减少了解析和计算，调用方和存储服务的负载也可以大幅降低 增强可扩展性。缓存读写性能很高，预热快，在数据访问存在性能瓶颈或遇到突发流量，系统读写压力大增时，可快速部署上线，同时在流量稳定后，也可以随时下线，从而使系统的可扩展性大大增强。缺点（代价）任何事情都有两面性，缓存也不例外，在享受缓存带来好处的同时，也注定需要付出一定的代价。 服务系统中引入缓存，会增加系统的复杂度； 由于缓存相比原始 DB 存储的成本更高，所以系统部署及运行的费用也会更高。 由于一份数据同时存在缓存和 DB 中，甚至缓存内部也会有多个数据副本，多份数据就会存在==一致性问题==，同时缓存体系本身也会存在可用性问题和分区的问题。这就需要加强对缓存原理、缓存组件以及优秀缓存体系实践的理解，从系统架构之初就对缓存进行良好设计，降低缓存引入的副作用，让缓存体系成为服务系统高效稳定运行的强力基石。读写模式业务系统读写缓存有 3 中模式： Cache Aside（旁路缓存） Read/Write Through（读写穿透） Write Behind Caching（异步缓存写入）Cache Aside（旁路缓存）套路对于写请求： 先更新 DB 后，然后将 key 从 cache 中删除，最后由 DB 驱动缓存数据的更新；对于读请求： 先读 cache，如果 cache 没有，则读 DB，同时将从 DB 中读取的数据回写到 cache。特点 业务端处理所有数据访问细节，同时利用 Lazy 计算的思想，更新 DB 后，直接删除 cache 并通过 DB 更新，确保数据以 DB 结果为准，则可以大幅降低 cache 和 DB 中数据不一致的概率。适合场景 对于没有专门的存储服务，同时是对数据一致性要求比较高的业务，或者是缓存数据更新比较复杂的业务，这些情况都比较适合使用 Cache Aside 模式。 如微博发展初期，不少业务采用这种模式，这些缓存数据需要通过多个原始数据进行计算后设置。在部分数据变更后，直接删除缓存。同时，使用一个 Trigger 组件，实时读取 DB 的变更日志，然后重新计算并更新缓存。如果读缓存的时候，Trigger 还没写入 cache，则由调用方自行到 DB 加载计算并写入 cache。 Read/Write Through（读写穿透）对于 Cache Aside 模式，业务应用需要同时维护 cache 和 DB 两个数据存储方，过于繁琐，于是就有了 Read/Write Through 模式。在这种模式下，业务应用只关注一个存储服务即可，业务方的读写 cache 和 DB 的操作，都由存储服务代理。套路对于写请求： 首先查 cache，如果数据在 cache 中不存在，则只更新 DB，如果数据在 cache 中存在，则先更新 cache，然后更新 DB。对于读请求： 如果命中 cache 直接返回，否则先从 DB 加载，回种到 cache 后返回响应。特点存储服务封装了所有的数据处理细节，业务应用端代码只用关注业务逻辑本身，系统的隔离性更佳。另外，进行写操作时，如果 cache 中没有数据则不更新，有缓存数据才更新，内存效率更高。适合场景数据有冷热区分。微博 Feed 的 Outbox Vector（即用户最新微博列表）就采用这种模式。一些粉丝较少且不活跃的用户发表微博后，Vector 服务会首先查询 Vector Cache，如果 cache 中没有该用户的 Outbox 记录，则不写该用户的 cache 数据，直接更新 DB 后就返回，只有 cache 中存在才会通过 CAS 指令进行更新。Write Behind Caching（异步缓存写入）与 Read/Write Through 模式类似，也由数据存储服务来管理 cache 和 DB 的读写。不同点是，数据更新时，Read/write Through 是同步更新 cache 和 DB，而 Write Behind Caching 则是只更新缓存，不直接更新 DB，而是采用异步批量的方式来更新 DB。套路对于写请求： 只更新缓存，缓存服务异步更新 DB； miss 后由缓存服务加载，并写入 cache。特点数据存储的写性能最高，定期异步刷新，存在数据丢失概率。适合场景非常适合一些变更特别频繁的业务，特别是可以合并写请求的业务，比如对一些计数业务，一条 Feed 被点赞 1万 次，如果更新 1万 次 DB 代价很大，而合并成一次请求直接加 1万，则是一个非常轻量的操作。这种模型有个显著的缺点，即数据的一致性变差，甚至在一些极端场景下可能会丢失数据。比如系统 Crash、机器宕机时，如果有数据还没保存到 DB，则会存在丢失的风险。所以这种读写模式适合变更频率特别高，但对一致性要求不太高的业务，这样写操作可以异步批量写入 DB，减小 DB 压力。三种模式各有优劣，不存在最佳模式。实际上，也不可能设计出一个最佳的完美模式出来，和空间换时间、访问延迟换低、成本高一样，高性能和强一致性从来都是有冲突的，系统设计从来就是取舍权衡。重点是如何根据业务场景，更好的做权衡取舍，从而设计出更好的服务系统。缓存分类按宿主层次分类分为本地 Cache、进程间 Cache 和远程 Cache。 本地 Cache 业务进程内的缓存； 由于在业务系统进程内，所以读写性能超高，且无任何网络开销； 不足是会随着业务系统重启而丢失。 进程间 Cache 本机独立运行的缓存； 缓存读写性能较高，不会随着业务系统重启丢数据，并且可以大幅减少网络开销； 不足是业务系统和缓存都在相同宿主机，运维复杂，且存在资源竞争。 远程 Cache 跨机器部署的缓存； 这类缓存因为独立设备部署，容量大且易扩展，在互联网企业使用最广泛。 不足是远程缓存需要跨机访问，在高读写压力下，带宽容易成为瓶颈。 常用缓存 本地 Cache 的缓存组件有 Ehcache、Guava Cache 等，也可以用 Map、Set 等轻松构建一个自己专用的本地 Cache； 进程间 Cache 和远程 Cache 的缓存组件相同，只是部署位置的差异罢了，这类缓存组件有 Memcached、Redis、Pika 等。按存储介质分类分为内存型缓存和持久化型缓存。 内存型缓存 将数据存储在内存，读写性能很高； 不足是缓存系统重启或崩溃后，内存数据会丢失。 持久化缓存 将数据存储到 SSD/Fusion-IO 硬盘中； 相同成本下，这种缓存的容量会比内存型缓存大 1 个数量级以上，而且数据会持久化落地，重启不丢失， 但读写性能相对低 1～2 个数量级。 常用缓存Memcached 是典型的内存型缓存，而 Pika 以及其他基于 RocksDB 开发的缓存组件等则属于持久化型缓存。缓存组件选择在设计架构缓存时，首先要选定缓存组件。用 Local-Cache，还是 Redis、Memcached、Pika 等开源缓存组件。如果业务缓存需求比较特殊，可以考虑是直接定制开发一个新的缓存组件，还是对开源缓存进行二次开发，来满足业务需要。缓存数据结构设计确定好缓存组件后，要根据业务访问的特点，进行缓存数据结构的设计。 对于直接简单 KV 读写的业务，这些业务数据封装为 String、Json、Protocol Buffer 等格式，序列化成字节序列，然后直接写入缓存中。读取时，先从缓存组件获取到数据的字节序列，再进行反序列化操作即可； 对于只需要存取部分字段或需要在缓存端进行计算的业务，可以把数据设计为 Hash、Set、List、Geo 等结构，存储到支持复杂集合数据类型的缓存中，如 Redis、Pika 等。缓存分布设计确定缓存组件，设计好缓存数据结构，接着就是设计缓存的分布。从 3 个维度来进行缓存分布设计： 选择分布式算法，是采用==取模==还是==一致性 Hash== 进行分布。 取模分布的方案简单，每个 key 只会存在确定的缓存节； 一致性 Hash 分布的方案相对复杂，一个 key 对应的缓存节点不确定。但一致性 Hash 分布，可以在部分缓存节点异常时，将失效节点的数据访问均衡分散到其他正常存活的节点，保证缓存系统的稳定性。 分布读写访问如何进行实施，是由==缓存 Client 直接进行 Hash 分布定位读写==，还是==通过 Proxy 代理==来进行读写路由 Client 直接读写，读写性能最佳，但需要 Client 感知分布策略。在缓存部署发生在线变化时，需要及时通知所有缓存 Client，避免读写异常，Client 实现也较复杂； 通过 Proxy 路由，Client 只需直接访问 Proxy，分布逻辑及部署变更都由 Proxy 来处理，对业务应用开发最友好，但业务访问多一跳，访问性能会有一定的损失； 缓存系统运行过程中，如果待缓存的数据量增长过快，会导致大量缓存数据被剔除，缓存命中率会下降，数据访问性能会随之降低，这样就需要将数据从缓存节点进行动态拆分，把部分数据水平迁移到其他缓存节点。 这个迁移过程需要考虑，是由==Proxy 进行迁移==还是==缓存 Server 自身进行迁移==，甚至根本就==不支持迁移==。 对于 Memcached，一般不支持迁移； 对 Redis，社区版本是依靠缓存 Server 进行迁移； 对 Codis 则是通过 Admin、Proxy 配合后端缓存组件进行迁移。 缓存架构部署及运维管理设计完毕缓存的分布策略后，接下来就要考虑缓存的架构部署及运维管理。架构部署主要考虑如何对缓存进行分池、分层、分 IDC（互联网数据中心，Internet Data Center），以及是否需要进行异构处理。 核心的、高并发访问的不同数据，需要分别分拆到独立的缓存池中，进行分别访问，避免相互影响；访问量较小、非核心的业务数据，则可以混存； 对海量数据、访问超过 10～100万 级的业务数据，要考虑分层访问，并且要分摊访问量，避免缓存过载； 如果业务系统需要多 IDC 部署甚至异地多活，则需要对缓存体系也进行多 IDC 部署，要考虑如何跨 IDC 对缓存数据进行更新： 采用直接跨 IDC 读写， 采用 DataBus 配合队列机进行不同 IDC 的消息同步，然后由消息处理机进行缓存更新， 还可以由各个 IDC 的 DB Trigger 进行缓存更新； 某些极端场景下，需要把多种缓存组件进行组合使用，通过缓存异构达到最佳读写性能。 站在系统层面，要想更好得管理缓存，要考虑缓存的服务化，考虑缓存体系如何更好得进行集群管理、监控运维等。考虑点 考虑点     读写方式 全部整体读写部分读写及变更是否需要内部计算 用户粉丝数，很多普通用户的粉丝有几千到几万，而大 V 的粉丝更是高达几千万甚至过亿，因此，获取粉丝列表肯定不能采用整体读写的方式，只能部分获取。判断某用户是否关注了另外一个用户时，也不需要拉取该用户的全部关注列表，直接在关注列表上进行检查判断，然后返回 True/False 或 0/1 的方式更为高效。 KV size 不同业务数据缓存 KV 的 size 如果单个业务的 KV size 过大，需要分拆成多个 KV 来缓存。如果不同缓存数据的 KV size 差异过大，也不能缓存在一起，避免缓存效率的低下和相互影响。 key 的数量 数据量中/小数据量大/海量 如果 key 数量不大，可以在缓存中存下全量数据，把缓存当 DB 存储来用，如果缓存读取 miss，则表明数据不存在，根本不需要再去 DB 查询。如果数据量巨大，则在缓存中尽可能只保留频繁访问的热数据，对于冷数据直接访问 DB。 读写峰值 峰值 &amp;lt;= 10 w 10 w &amp;lt; 峰值 &amp;lt; 100 w 如果小于 10万 级别，简单分拆到独立 Cache 池即可一旦数据的读写峰值超过 10万 甚至到达 100万 级的 QPS，需要对 Cache 进行分层处理，可以同时使用 Local-Cache 配合远程 cache，甚至远程缓存内部继续分层叠加分池进行处理。微博业务中，大多数核心业务的 Memcached 访问都采用的这种处理方式。 命中率 核心高并发访问持续监控 缓存的命中率对整个服务体系的性能影响甚大。对于核心高并发访问的业务，需要预留足够的容量，确保核心业务缓存维持较高的命中率。比如微博中的 Feed Vector Cache，常年的命中率高达 99.5% 以上。为了持续保持缓存的命中率，缓存体系需要持续监控，及时进行故障处理或故障转移。同时在部分缓存节点异常、命中率下降时，故障转移方案，需要考虑是采用一致性 Hash 分布的访问漂移策略，还是采用数据多层备份策略。 过期策略 设置时间自动过期key 带过期时间戳 可以设置较短的过期时间，让冷 key 自动过期；也可以让 key 带上时间戳，同时设置较长的过期时间，比如很多业务系统内部有这样一些 key : key_20190801。 平均缓存穿透加载时间 配置更大容量确保命中率 平均缓存穿透加载时间在某些业务场景下也很重要，对于一些缓存穿透后，加载时间特别长或者需要复杂计算的数据，而且访问量还比较大的业务数据，要配置更多容量，维持更高的命中率，从而减少穿透到 DB 的概率，来确保整个系统的访问性能。 缓存可运维性 集群管理一键扩容监控报警运维工具集成 需要考虑缓存体系的集群管理：如何进行一键扩缩容，如何进行缓存组件的升级和变更，如何快速发现并定位问题，如何持续监控报警，最好有一个完善的运维平台，将各种运维工具进行集成。 缓存安全性 限制来源 IP 内网访问关键指令，增加访问权限 对于缓存的安全性考虑，一方面可以限制来源 IP，只允许内网访问；对于一些关键性指令，需要增加访问权限，避免被攻击或误操作时，导致重大后果。 " }, { "title": "Spring Security 中的权限和角色", "url": "/posts/spring-security-permissions-and-roles/", "categories": "Java, Spring", "tags": "Spring Security", "date": "2020-06-02 12:33:00 +0000", "snippet": "认证是实现授权的前提和基础。在执行授权操作时需要明确目标用户，只有明确目标用户才能明确它所具备的角色和权限。用户、角色和权限也是 Spring Security 中所采用的授权模型。实现访问授权的基本手段是使用配置方法，配置方法处理过程位于 WebSecurityConfigurerAdapter 类中，但使用的是另一个 configure(HttpSecurity http) 方法，示例代码如下所示：protected void configure(HttpSecurity http) throws Exception { http .authorizeRequests().anyRequest().authenticated() .and().formLogin() .and().httpBasic();}基于权限进行访问控制在 Spring Security 的用户对象以及它们之间的关联关系，如下图：上图中的： GrantedAuthority 对象，代表的就是一种权限对象 UserDetails 对象具备一个或多个 GrantedAuthority 对象通过这种关联关系，就可以对用户的权限做一些限制，如下所示：如果用代码来表示这种关联关系，可以采用如下所示的实现方法：UserDetails user = User.withUsername(&quot;jianxiang&quot;)     .password(&quot;123456&quot;)     .authorities(&quot;create&quot;, &quot;delete&quot;)     .build();可以看到，这里创建了一个名为“jianxiang”的用户，该用户具有“create”和“delete”这两个权限。Spring Security 提供了一组针对 GrantedAuthority 的配置方法。例如： hasAuthority(String)，允许具有特定权限的用户进行访问； hasAnyAuthority(String)，允许具有任一权限的用户进行访问。可以使用上述两个方法来判断用户是否具备对应的访问权限。在 WebSecurityConfigurerAdapter 的 configure 方法中添加如下代码：@Overrideprotected void configure(HttpSecurity http) throws Exception {    http.httpBasic(); http.authorizeRequests().anyRequest().hasAuthority(&quot;CREATE&quot;);        }这段代码的作用是对于任何请求，只有权限为“CREATE”才能采用访问。如果将代码成下面的样子：http.authorizeRequests().anyRequest().hasAnyAuthority(&quot;CREATE&quot;, &quot;DELETE&quot;);此时，只要具备“CREATE”和“DELETE”中任意一种权限的用户都能进行访问。这两个方法实现起来都比较简单，但局限性也很大： 无法基于一些来自环境和业务的参数灵活控制访问规则针对着局限性，Spring Security 提供了一个 access() 方法，该方法允许传入一个表达式进行更加细粒度的权限控制。这里，使用 SpEL，它是 Spring Expression Language 的简称，是 Spring 框架提供的一种动态表达式语言。基于 SpEL，只要该表达式的返回值是 true，access() 方法就会允许用户访问。如下示例：http.authorizeRequests().anyRequest().access(&quot;hasAuthority(&#39;CREATE&#39;)&quot;);上面代码与使用 hasAuthority() 方法的效果是完全一致的，但如果是更为复杂的场景，access() 方法的优势就很明显了。可以灵活创建一个表达式，然后通过 access() 方法确定最后的结果，示例代码如下所示：String expression = &quot;hasAuthority(&#39;CREATE&#39;) and !hasAuthority(&#39;Retrieve&#39;)&quot;; http.authorizeRequests().anyRequest().access(expression);基于角色进行访问控制角色可以看成是拥有多个权限的一种数据载体。如下图所示，这里分别定义了两个不同的角色“User”和“Admin”，它们拥有不同的权限：此时，有可能会认为 Spring Security 应该提供一个独立的数据结构来承载角色的含义。但事实上，Spring Security ，没有定义类似“GrantedRole”这种专门用来定义用户角色的对象，而是复用了 GrantedAuthority 对象。其中，以“ROLE_”为前缀的 GrantedAuthority 就代表了一种角色，因此可以使用如下方式初始化用户的角色：UserDetails user = User.withUsername(&quot;jianxiang&quot;)      .password(&quot;123456&quot;)      .authorities(&quot;ROLE_ADMIN&quot;)      .build();上述代码相当于为用户“jianxiang”指定了“ADMIN”这个角色。为了给开发人员提供更好的开发体验，Spring Security 还提供了另一种简化的方法来指定用户的角色，如下所示：UserDetails user = User.withUsername(&quot;jianxiang&quot;)      .password(&quot;123456&quot;)      .roles(&quot;ADMIN&quot;)      .build();和权限配置一样，Spring Security 也通过使用对应的 hasRole() 和 hasAnyRole() 方法来判断用户是否具有某个角色或某些角色，使用方法如下所示：http.authorizeRequests().anyRequest().hasRole(&quot;ADMIN&quot;);当然，针对角色，我们也可以使用 access() 方法完成更为复杂的访问控制。而 Spring Security 还提供了其他很多有用的控制方法供开发人员进行灵活使用。作为总结，下表展示了常见的配置方法及其作用： 配置方法 作用 anonymous() 允许匿名访问 authenticated() 允许认证用户访问 denyAll() 无条件禁止一切访问 hasAnyAuthority(String) 允许具有任一权限的用户进行访问 hasAnyRole(String) 允许具有任一角色的用户进行访问 hasAuthority(String) 允许具有特定权限的用户进行访问 hasIpAddress(String) 允许来自特定 IP 地址的用户进行访问 hasRole(String) 允许具有特定角色的用户进行访问 permitAll() 无条件允许一切访问 使用配置方法控制访问权限Spring Security 提供了三种强大的匹配器（Matcher）来实现HTTP 请求与权限控制过程关联起来，分别是： MVC 匹配器 Ant 匹配器 正则表达式匹配器。假如如下所示的一个 Controller：@RestControllerpublic class TestController {     @GetMapping(&quot;/hello_user&quot;)    public String helloUser() {        return &quot;Hello User!&quot;; } @GetMapping(&quot;/hello_admin&quot;)    public String helloAdmin() {        return &quot;Hello Admin!&quot;;    }     @GetMapping(&quot;/other&quot;)    public String other() {        return &quot;Other!&quot;;    }}同时，创建两个具有不同角色的用户，如下所示：UserDetails user1 = User.withUsername(&quot;jianxiang1&quot;) .password(&quot;12345&quot;) .roles(&quot;USER&quot;) .build(); UserDetails user2 = User.withUsername(&quot;jianxiang2&quot;) .password(&quot;12345&quot;) .roles(&quot;ADMIN&quot;) .build();然后，基于这个 Controller 中暴露的各个 HTTP 端点，对三种不同的匹配器总结。MVC 匹配器MVC 匹配器的使用方法比较简单，就是基于 HTTP 端点的访问路径进行匹配，如下所示：http.authorizeRequests() .mvcMatchers(&quot;/hello_user&quot;).hasRole(&quot;USER&quot;) .mvcMatchers(&quot;/hello_admin&quot;).hasRole(&quot;ADMIN&quot;);现在，如果使用角色为“USER”的用户“jianxiang1”来访问“/hello_admin”端点，那么将会得到如下所示的响应：{ &quot;status&quot;:403, &quot;error&quot;:&quot;Forbidden&quot;, &quot;message&quot;:&quot;Forbidden&quot;, &quot;path&quot;:&quot;/hello_admin&quot; }显然，MVC 匹配器已经生效了，因为“/hello_admin”端点只有角色为“ADMIN”的用户才能访问。如果使用拥有“ADMIN”角色的“jianxiang2”来访问这个端点就可以得到正确的响应结果。没有被 MVC 匹配器所匹配的端点，其访问不受任何的限制，效果相当于如下所示的配置：http.authorizeRequests() .mvcMatchers(&quot;/hello_user&quot;).hasRole(&quot;USER&quot;) .mvcMatchers(&quot;/hello_admin&quot;).hasRole(&quot;ADMIN&quot;); .anyRequest().permitAll();讲到这里，又出现了一个新问题：如果一个 Controller 中存在两个路径完全一样的 HTTP 端点呢？这种情况是存在的，因为对于 HTTP 端点而言，就算路径一样，只要所使用的 HTTP 方法不同，那就是不同的两个端点。针对这种场景，MVC 匹配器还提供了重载的 mvcMatchers 方法，如下所示：mvcMatchers(HttpMethod method, String... patterns)这样，就可以把 HTTP 方法作为一个访问的维度进行控制，示例代码如下所示：http.authorizeRequests() .mvcMatchers(HttpMethod.POST, &quot;/hello&quot;).authenticated() .mvcMatchers(HttpMethod.GET, &quot;/hello&quot;).permitAll() .anyRequest().denyAll();在上面这段配置代码中，如果一个 HTTP 请求使用了 POST 方法来访问“/hello”端点，那么就需要进行认证。而对于使用 GET 方法来访问“/hello”端点的请求则全面允许访问。最后，其余访问任意路径的所有请求都会被拒绝。同时，如果想要对某个路径下的所有子路径都指定同样的访问控制，那么只需要在该路径后面添加“*”号即可，示例代码如下所示：http.authorizeRequests() .mvcMatchers(HttpMethod.GET, &quot;/user/*&quot;).authenticated() 通过上述配置方法，如果访问“/user/jianxiang”“/user/jianxiang/status”等路径时，都会匹配到这条规则。Ant 匹配器Ant 匹配器的表现形式和使用方法与前面介绍的 MVC 匹配器非常相似，它也提供了如下所示的三个方法来完成请求与 HTTP 端点地址之间的匹配关系： antMatchers(String patterns) antMatchers(HttpMethod method) antMatchers(HttpMethod method, String patterns)从方法定义上不难明白，可以组合指定请求的 HTTP 方法以及匹配的模式，例如：http.authorizeRequests() .antMatchers( &quot;/hello&quot;).authenticated();虽然，从使用方式上看，Ant 匹配器和 MVC 匹配器并没有什么区别，但在日常开发过程中，我想推荐你使用 MVC 匹配器而不是 Ant 匹配器，原因就在于 Ant 匹配器在匹配路径上有一些风险，主要体现在对于”/”的处理上。为了更好地说明，我举一个简单的例子。基于上面的这行配置，如果你发送一个这样的 HTTP 请求：http://localhost:8080/hello肯定认为 Ant 匹配器是能够匹配到这个端点的，但结果却是：{ &quot;status&quot;:401, &quot;error&quot;:&quot;Unauthorized&quot;, &quot;message&quot;:&quot;Unauthorized&quot;, &quot;path&quot;:&quot;/hello&quot; }现在，如果把 HTTP 请求调整为这样，请注意，此时在请求地址最后添加了一个”/”符号，那么就会得到正确的访问结果：http://localhost:8080/hello/显然，Ant 匹配器处理请求地址的方式有点让人感到困惑，而 MVC 匹配器则没有这个问题，无论在请求地址最后是否存在“/”符号，它都能完成正确的匹配。正则表达式匹配器最后是正则表达式匹配器，同样，它也提供了如下所示的两个配置方法：、 regexMatchers(HttpMethod method, String regex) regexMatchers(String regex)使用这一匹配器的主要优势在于它能够基于复杂的正则表达式对请求地址进行匹配，这是 MVC 匹配器和 Ant 匹配器无法实现的，可以看一下如下所示的这段配置代码：http.authorizeRequests()   .mvcMatchers(&quot;/email/{email:.*(.+@.+\\\\.com)}&quot;)   .permitAll()   .anyRequest()   .denyAll();可以看到，这段代码就对常见的邮箱地址进行了匹配，只有输入的请求是一个合法的邮箱地址才能允许访问。" }, { "title": "PasswordEncoder", "url": "/posts/encryption-and-decryption-technology-in-spring-security/", "categories": "Java, Spring", "tags": "Spring Security", "date": "2020-06-01 13:33:00 +0000", "snippet": "用户认证的过程会涉及到密码的校验，密码的安全性也是需要考虑的核心问题。Spring Security 作为一个功能完备的安全性框架，除了了提供了完成认证操作的 PasswordEndocer 组件，还包一个独立完整的加密模块。​整个用户认证流程：在 AuthenticationProvider 中，使用 PasswordEncoder 组件验证密码的正确性，如下图说是：自定义 PasswordEncoder尽管 Spring Security 已经提供了丰富的 PasswordEncoder，可以通过实现这个接口来设计满足自身需求的任意一种密码编解码和验证机制。​例如，如下所示的一个 PlainTextPasswordEncoder：public class PlainTextPasswordEncoder implements PasswordEncoder {  @Override  public String encode(CharSequence rawPassword) { return rawPassword.toString();  }  @Override  public boolean matches( CharSequence rawPassword, String encodedPassword) {        return rawPassword.equals(encodedPassword);  }}PlainTextPasswordEncoder 的功能与 NoOpPasswordEncoder 类似，没有对明文进行任何处理。如果想使用某种算法集成 PasswordEncoder，就可以实现类似如下所示的 Sha512PasswordEncoder，这里使用了 SHA-512 作为加解密算法：public class Sha512PasswordEncoder implements PasswordEncoder {  @Override  public String encode(CharSequence rawPassword) {     return hashWithSHA512(rawPassword.toString());  }  @Override  public boolean matches(CharSequence rawPassword, String encodedPassword) {     String hashedPassword = encode(rawPassword); return encodedPassword.equals(hashedPassword);  } private String hashWithSHA512(String input) {   StringBuilder result = new StringBuilder();  try {  MessageDigest md = MessageDigest.getInstance(&quot;SHA-512&quot;);  byte [] digested = md.digest(input.getBytes());  for (int i = 0; i &amp;lt; digested.length; i++) { result.append(Integer.toHexString(0xFF &amp;amp; digested[i]));  } catch (NoSuchAlgorithmException e) { throw new RuntimeException(&quot;Bad algorithm&quot;);  }   return result.toString(); } }上述代码中，hashWithSHA512() 方法就使用了前面提到的单向散列加密算法来生成消息摘要（Message Digest），其主要特点在于单向不可逆和密文长度固定。同时也具备“碰撞”少的优点，即明文的微小差异就会导致所生成密文完全不同。​SHA（Secure Hash Algorithm）以及MD5（Message Digest 5）都是常见的单向散列加密算法。​在 JDK 自带的 MessageDigest 类中已经包含了默认实现，我们直接调用方法即可。代理式 DelegatingPasswordEncoder在对密码进行加解密过程中，只会使用到一个 PasswordEncoder，如果这个 PasswordEncoder 不满足需求，那么就需要替换成另一个 PasswordEncoder。​这就引出了一个问题，Spring Security 如何优雅地应对这种变化呢?​虽然 DelegatingPasswordEncoder 也实现了 PasswordEncoder 接口，但事实上，它更多扮演了一种代理组件的角色，这点从命名上也可以看出来。 ​DelegatingPasswordEncoder 将具体编码的实现根据要求代理给不同的算法，以此实现不同编码算法之间的兼容并协调变化，如下图所示：​Spring Security 加密模块​使用 Spring Security 时，涉及用户认证的部分会用到加解密技术。就应用场景而言，加解密技术是一种通用的基础设施类技术，不仅可以用于用户认证，也可以用于其他任何涉及敏感数据处理的场景。​因此，Spring Security 也充分考虑到了这种需求，专门提供了一个加密模式（Spring Security Crypto Module，SSCM）。尽管 PasswordEncoder 也属于这个模块的一部分，但这个模块本身是高度独立的，可以脱离于用户认证流程来使用这个模块。​​Spring Security 加密模块的核心功能有两部分。首先就是加解密器（Encryptors），典型的使用方式如下：BytesEncryptor e = Encryptors.standard(password, salt);上述方法使用了标准的 256 位 AES 算法对输入的 password 字段进行加密，返回的是一个 BytesEncryptor。​同时，也看到这里需要输入一个代表盐值的 salt 字段，而这个 salt 值的获取就可以用到 Spring Security 加密模块的另一个功能——键生成器（Key Generators），使用方式如下所示：String salt = KeyGenerators.string().generateKey();将加解密器和键生成器结合起来，就可以实现通用的加解密机制，如下所示：String salt = KeyGenerators.string().generateKey(); String password = &quot;secret&quot;; String valueToEncrypt = &quot;HELLO&quot;; BytesEncryptor e = Encryptors.standard(password, salt); byte [] encrypted = e.encrypt(valueToEncrypt.getBytes()); byte [] decrypted = e.decrypt(encrypted);在日常开发过程中，可以根据需要调整上述代码并嵌入到自己的系统中。​​对于一个 Web 应用程序来说，一旦需要实现用户认证，势必涉及用户密码等敏感信息的加密。为此 Spring Security 提供 PasswordEncoder 组件（内置一批即插即用的 PasswordEncoder）对密码进行加解密，并通过代理机制完成了各个组件的版本兼容和统一管理。这种思想值得学习和借鉴。" }, { "title": "Spring Security 中的用户和认证", "url": "/posts/understanding-spring-security-user-authentication/", "categories": "Java, Spring", "tags": "Spring Security", "date": "2020-05-30 13:33:00 +0000", "snippet": "Spring Security 中认证过程由一组核心对象组成，大致可以分成两大类: 用户对象 认证对象1.1 Spring Security 中的用户对象Spring Security 中的用户对象用来描述用户并完成对用户信息的管理，涉及四个核心对象： UserDetails，描述 Spring Security 中的用户 GrantedAuthority，定义用户的操作权限 UserDetailsService，定义了对 UserDetails 的查询操作 UserDetailsManage，扩展 UserDetailsService，添加了创建用户、修改用户密码等功能。这四个对象之间的关联关系如下图所示，​显然，对于由 UserDetails 对象所描述的一个用户而言，它应该具有 1 个或多个能够执行的 GrantedAuthority：​结合上图，我们先来看承载用户详细信息的 UserDetails 接口，如下所示：public interface UserDetails extends Serializable {    //获取该用户的权限信息    Collection&amp;lt;? extends GrantedAuthority&amp;gt; getAuthorities();    // 获取密码 String getPassword();   // 获取用户名    String getUsername(); // 判断该账户是否已失效    boolean isAccountNonExpired();     // 判断该账户是否已被锁定    boolean isAccountNonLocked();     // 判断该账户的凭证信息是否已失效    boolean isCredentialsNonExpired();    // 判断该用户是否可用    boolean isEnabled(); }通过 UserDetails，可以获取用户相关的基础信息，并判断其当前状态。​同时，UserDetails 中保存着一组 GrantedAuthority 对象。而 GrantedAuthority 指定了一个用来获取权限信息的方法，如下所示：public interface GrantedAuthority extends Serializable {    //获取权限信息    String getAuthority();}UserDetails 存在一个子接口 MutableUserDetails，从命名上不难看出后者是一个可变的 UserDetails，可变的内容就是密码。​MutableUserDetails 接口的定义如下所示：interface MutableUserDetails extends UserDetails {    //设置密码    void setPassword(String password);}如果在应用程序中创建一个 UserDetails 对象，可以使用如下所示链式语法：UserDetails user = User.withUsername(&quot;jianxiang&quot;) .password(&quot;123456&quot;) .authorities(&quot;read&quot;, &quot;write&quot;) .accountExpired(false) .disabled(true) .build();Spring Security 还专门提供了一个 UserBuilder 对象来辅助构建 UserDetails，使用方式也类似：User.UserBuilder builder = User.withUsername(&quot;jianxiang&quot;); UserDetails user = builder .password(&quot;12345&quot;) .authorities(&quot;read&quot;, &quot;write&quot;) .accountExpired(false) .disabled(true) .build();针对 UserDetails 专门提供了一个 UserDetailsService，该接口用来管理 UserDetails，定义如下：public interface UserDetailsService {     //根据用户名获取用户信息    UserDetails loadUserByUsername(String username) throws UsernameNotFoundException;}而 UserDetailsManager 继承了UserDetailsService，并提供了一批针对 UserDetails 的操作接口，如下所示：public interface UserDetailsManager extends UserDetailsService {    //创建用户    void createUser(UserDetails user);    //更新用户    void updateUser(UserDetails user);     //删除用户    void deleteUser(String username);    //修改密码    void changePassword(String oldPassword, String newPassword);    //判断指定用户名的用户是否存在    boolean userExists(String username);}几个核心用户对象之间的关联关系就很清楚了，接下进一步明确具体的实现过程。​来看 UserDetailsManager 的两个实现类: ​基于内存存储的 InMemoryUserDetailsManager， 基于关系型数据库存储的 JdbcUserDetailsManager。这里，以 JdbcMemoryUserDetailsManager 为例展开分析，它的 createUser 方法如下所示：public void createUser(final UserDetails user) {        validateUserDetails(user);        getJdbcTemplate().update(createUserSql, ps -&amp;gt; {             ps.setString(1, user.getUsername());             ps.setString(2, user.getPassword());             ps.setBoolean(3, user.isEnabled());             int paramCount = ps.getParameterMetaData().getParameterCount();             if (paramCount &amp;gt; 3) {                 ps.setBoolean(4, !user.isAccountNonLocked());                 ps.setBoolean(5, !user.isAccountNonExpired());                 ps.setBoolean(6, !user.isCredentialsNonExpired());             }        });         if (getEnableAuthorities()) {             insertUserAuthorities(user);        }}可以看到，这里直接使用了 Spring 框架中的JdbcTemplate 模板工具类实现了数据的插入，同时完成了 GrantedAuthority 的存储。​UserDetailsManager 是一条相对独立的代码线，为了完成用户信息的配置，还存在另一条代码支线，即 UserDetailsManagerConfigurer。该类维护了一个 UserDetails 列表，并提供了一组 withUser 方法完成用户信息的初始化，如下所示：private final List&amp;lt;UserDetails&amp;gt; users = new ArrayList&amp;lt;&amp;gt;(); public final C withUser(UserDetails userDetails) {        this.users.add(userDetails);        return (C) this;}而 withUser 方法返回的是一个 UserDetailsBuilder 对象，该对象内部使用了前面介绍的 UserBuilder 对象，因此可以实现类似 .withUser(&quot;spring_user&quot;).password(&quot;password1&quot;).roles(&quot;USER&quot;) 这样的链式语法，完成用户信息的设置。​` Spring Security` 中与用户对象相关的一大批实现类，它们之间的关系如下图所示：1.2 Spring Security 中的认证对象有了用户对象，就可以讨论具体的认证过程了，首先来看认证对象 Authentication，如下所示：public interface Authentication extends Principal, Serializable {    //安全主体具有的权限    Collection&amp;lt;? extends GrantedAuthority&amp;gt; getAuthorities(); //证明主体有效性的凭证    Object getCredentials();    //认证请求的明细信息    Object getDetails();    //主体的标识信息    Object getPrincipal();     //认证是否通过    boolean isAuthenticated();    //设置认证结果    void setAuthenticated(boolean isAuthenticated) throws IllegalArgumentException;}认证对象代表认证请求本身，保存该请求访问应用程序过程中涉及的各个实体的详细信息。​在安全领域，请求访问该应用程序的用户通常被称为主体（Principal），​在 JDK 中存在一个同名的接口，而 Authentication 扩展了这个接口。​显然，Authentication 只代表了认证请求本身，而具体执行认证的过程和逻辑需要由专门的组件来负责，这个组件就是 AuthenticationProvider，定义如下：​public interface AuthenticationProvider {    //执行认证，返回认证结果    Authentication authenticate(Authentication authentication)             throws AuthenticationException;    //判断是否支持当前的认证对象    boolean supports(Class&amp;lt;?&amp;gt; authentication);}这里，可能会认为 Spring Security 是直接使用 AuthenticationProvider 接口完成用户认证的，其实不然。翻阅 Spring Security 的源码，会发现它使用了 ****AuthenticationManager** 接口来代理 **AuthenticationProvider** 提供的认证功能**。这里，以 InMemoryUserDetailsManager 中的 changePassword 为例，分析用户认证的执行过程（为了展示简洁，部分代码做了裁剪）：public void changePassword(String oldPassword, String newPassword) {        Authentication currentUser = SecurityContextHolder.getContext()                 .getAuthentication();         if (currentUser == null) {             throw new AccessDeniedException(                     &quot;Can&#39;t change password as no Authentication object found in context &quot;                             + &quot;for current user.&quot;);        }         String username = currentUser.getName();        if (authenticationManager != null) {             authenticationManager.authenticate(new UsernamePasswordAuthenticationToken(                     username, oldPassword));        } else {             …        }        MutableUserDetails user = users.get(username);        if (user == null) {             throw new IllegalStateException(&quot;Current user doesn&#39;t exist in database.&quot;);        }        user.setPassword(newPassword);}可以看到，这里使用了 AuthenticationManager 而不是 AuthenticationProvider 中的 authenticate() 方法来执行认证。同时，也注意到这里出现了 UsernamePasswordAuthenticationToken 类，这就是 Authentication 接口的一个具体实现类，用来存储用户认证所需的用户名和密码信息2 实现定制化用户认证方案通过前面的分析，明确了用户信息存储的实现过程实际上是可以定制化的。​Spring Security 所做的工作只是把常见的、符合一般业务场景的实现方式嵌入到了框架中。如果有特殊的场景，完全可以实现自定义的用户信息存储方案。​现在，已经知道 UserDetails 接口代表着用户详细信息，而负责对 UserDetails 进行各种操作的则是 UserDetailsService 接口。​因此，实现定制化用户认证方案主要就是实现 ****UserDetails** 和 **UserDetailsService** 这两个接口**。2.1 扩展 UserDetails扩展 UserDetails 的方法就是直接实现该接口。​例如可以构建如下所示的 SpringUser 类：public class SpringUser implements UserDetails {    private static final long serialVersionUID = 1L;    private Long id;      private final String username;    private final String password;    private final String phoneNumber;    //省略 getter/setter    @Override    public String getUsername() {        return username;    }        @Override    public String getPassword() {        return password;    }    @Override    public Collection&amp;lt;? extends GrantedAuthority&amp;gt; getAuthorities() {        return Arrays.asList(new SimpleGrantedAuthority(&quot;ROLE_USER&quot;));    }      @Override  public boolean isAccountNonExpired() {      return true;  }   @Override  public boolean isAccountNonLocked() {      return true;  }   @Override  public boolean isCredentialsNonExpired() {      return true;  }  @Override  public boolean isEnabled() {      return true;  } }显然，这里使用了一种最简单的方法来满足 UserDetails 中各个接口的实现需求。一旦构建了这样一个 SpringUser 类，就可以创建对应的表结构存储类中定义的字段。同时，也可以基于 Spring Data JPA 来创建一个自定义的 Repository，如下所示：public interface SpringUserRepository extends CrudRepository&amp;lt;SpringUser, Long&amp;gt; {    SpringUser findByUsername(String username);  }SpringUserRepository 扩展了 Spring Data 中的 CrudRepository 接口，并提供了一个方法名衍生查询 findByUsername。2.2 扩展 UserDetailsService接着，实现 UserDetailsService 接口，如下所示：@Servicepublic class SpringUserDetailsService         implements UserDetailsService { @Autowired  private SpringUserRepository repository;   @Override  public UserDetails loadUserByUsername(String username)      throws UsernameNotFoundException {     SpringUser user = repository.findByUsername(username);    if (user != null) {      return user;    }    throw new UsernameNotFoundException(                    &quot;SpringUser &#39;&quot; + username + &quot;&#39; not found&quot;);  }}UserDetailsService 接口只有一个loadUserByUsername 方法需要实现。因此，基于 SpringUserRepository 的 findByUsername 方法，根据用户名从数据库中查询数据。​2.3 扩展 AuthenticationProvider扩展 AuthenticationProvider 的过程就是提供一个自定义的 AuthenticationProvider 实现类。这里以最常见的用户名密码认证为例，梳理自定义认证过程所需要实现的步骤，如下所示：​上图中的流程并不复杂， 通过 UserDetailsService 获取一个 UserDetails 对象 根据该对象中的密码与认证请求中的密码进行匹配 如果一致则认证成功，反之抛出一个 BadCredentialsException 异常示例代码如下所示：@Componentpublic class SpringAuthenticationProvider implements AuthenticationProvider {    @Autowired    private UserDetailsService userDetailsService;    @Autowired    private PasswordEncoder passwordEncoder;    @Override    public Authentication authenticate(Authentication authentication) {        String username = authentication.getName();        String password = authentication.getCredentials().toString();         UserDetails user = userDetailsService.loadUserByUsername(username);        if (passwordEncoder.matches(password, user.getPassword())) {            return new UsernamePasswordAuthenticationToken(username, password, u.getAuthorities());        } else {            throw new BadCredentialsException(&quot;The username or password is wrong!&quot;);        }    }     @Override    public boolean supports(Class&amp;lt;?&amp;gt; authenticationType) {        return authenticationType.equals(UsernamePasswordAuthenticationToken.class);    }}这里同样使用了UsernamePasswordAuthenticationToken 来传递用户名和密码，并使用一个 PasswordEncoder 对象校验密码。2.2 整合定制化配置最后，创建一个 SpringSecurityConfig 类，该类继承了 WebSecurityConfigurerAdapter 配置类。这次，我们将使用自定义的 SpringUserDetailsService 来完成用户信息的存储和查询，需要对原有配置策略做一些调整。调整之后的完整 SpringSecurityConfig 类如下所示：@Configurationpublic class SpringSecurityConfig extends WebSecurityConfigurerAdapter {     @Autowired    private UserDetailsService springUserDetailsService;    @Autowired    private AuthenticationProvider springAuthenticationProvider;     @Override    protected void configure(AuthenticationManagerBuilder auth) throws Exception {    auth.userDetailsService(springUserDetailsService) .authenticationProvider(springAuthenticationProvider); }}这里注入了 SpringUserDetailsService 和 SpringAuthenticationProvider，并将其添加到 AuthenticationManagerBuilder 中，这样 AuthenticationManagerBuilder 将基于自定义的 SpringUserDetailsService 完成 UserDetails 的创建和管理，并基于自定义的 SpringAuthenticationProvider完成用户认证。" }, { "title": "Spring Security 的配置体系", "url": "/posts/use-spring-security-to-build-a-user-authentication-system/", "categories": "Java, Spring", "tags": "Spring Security", "date": "2020-05-28 14:33:00 +0000", "snippet": "用户认证主要涉及用户账户体系构建（实现授权管理的前提）。在 Spring Security 中，实现用户认证的方式有很多，本文主要结合框架提供的配置体系进行梳理。​在 Spring Security中，认证和授权等功能不止一种实现方法，因此 ss 提供了一套完整的配置体系来对这些功能进行灵活设置。使用认证和授权等功能时就依赖于如何合理利用和扩展这套配置体系。​例如，针对用户账户存储这个切入点，就可以设计出多种不同的策略 将用户名和密码保存在内存中，作为一种轻量级的实现方式， 更常见的，将这些认证信息存储在关系型数据库中 如果使用了 LDAP 协议，文件系统也是一种不错的存储媒介。 LDAP 协议，全称是 Lightweight Directory Access Protocol，「轻量目录访问协议」 ​针对上面可选择的实现方式，需要提供一种机制，以便根据自身需求进行灵活设置，这就是配置体系的作用。​在上文的示例中，没有进行任何配置也能让 Spring Security 发挥作用，这就说明框架内部功能采用了特定的默认配置。​就用户认证这一场景而言，Spring Security内部初始化一个默认用户名“user”，在应用程序启动时自动生产一个密码，通过这种方式自动生产的密码，在每次启动应用时都会发生变化，不适合面向正式的应用。​翻阅框架源代码来进一步理解 Spring Security 的默认配置。​在 Spring Security 中，初始化用户信息依赖的配置类是 WebSecurityConfigurer 接口，该接口是一个空接口，继承了更为基础的 SecurityConfigurer 接口。​在日常开发中，不需要自己实现这个接口，使用 WebSecurityConfigurerAdapte 类来简化该配置类的使用。​在 WebSecurityConfigurerAdapter 中发现了如下所示的 configure 方法：protected void configure(HttpSecurity http) throws Exception { http .authorizeRequests() .anyRequest().authenticated() .and().formLogin().and().httpBasic();}上述代码是 Spring Security 中用户认证和访问授权的默认实现，这里用到了多个常见的配置方法。​一旦在代码类路径中引入 Spring Security 框架之后，访问任何端点时就会弹出一个登录界面用来完成用户认证。认证是授权的前置流程，认证结束之后就可以进入到授权环节。​结合这些配置方法，简单分析一下默认效果的实现： 通过 HttpSecurity 类的 authorizeRequests()方法对所有访问 HTTP 端点的 HttpServletReques 进行限制； anyRequest().authenticated() 语句指定了对所有请求都需要执行认证（没有通过认证的用户无法访问任何端点）； formLogin() 语句用于指定使用表单登录作为认证方式（会弹出一个登录界面） httpBasic() 语句表示可以使用 HTTP 基础认证（Basic Authentication）方法来完成认证在日常开发过程中，通过继承 WebSecurityConfigurerAdapter 类并且覆写上述的 configure() 方法来完成配置工作。​在 Spring Security 中，存在一批类似于 WebSecurityConfigurerAdapter 的配置类。实现 HTTP 基础认证和表单登录认证httpBasic() 和 formLogin() 是控制用户认证的实现手段，分别代表了： HTTP 基础认证 表单登录认证。在构建 Web 应用程序时，在 Spring Security 提供的认证机制的基础上进行扩展，以满足日常开发需求。HTTP 基础认证HTTP 基础认证原理较为简单，只需要通过 ****HTTP**协议消息头携带的用户名和密码**进行登录验证。通过浏览器简单验证用户登录操作，现在，引入 Postman 对登录的请求和响应进一步分析。​在 Postman 中，之间访问 http://localhost:8080/hello 端点，得到如下所示的响应：{ &quot;timestamp&quot;: &quot;2021-02-08T03:45:21.512+00:00&quot;, &quot;status&quot;: 401, &quot;error&quot;: &quot;Unauthorized&quot;, &quot;message&quot;: &quot;&quot;, &quot;path&quot;: &quot;/hello&quot;}显然，状态码 401 说明没有访问该端点的权限。同时，在响应中出现了 WWW-Authenticate消息头，其值是Basic realm=&quot;Realm&quot;，这里的 Realm表示 Web 服务器中受保护资源的安全域。​接下来，执行 HTTP基础认证，通过设置认证类型为 Basic Auth 并输入对应用户名和密码完成HTTP 端点的访问。​再次查看 HTTP 请求，可以看到 Request Header 中添加了 Authorization 标头，格式为：Authorization: &amp;lt;type&amp;gt; &amp;lt;credentials&amp;gt;，这里的 type 就是“Basic”，而 credentials 则是这样一个字符串：dXNlcjo5YjE5MWMwNC1lNWMzLTQ0YzctOGE3ZS0yNWNkMjY3MmVmMzk=这个字符串就是将用户名和密码组合在一起，再经过 Base64 编码得到的结果（Base64 是一种编码方式，并没有集成加密机制，本质上传输的还是明文形式）​在应用程序中启用 HTTP 基础认证是比较简单的，只需要在 WebSecurityConfigurerAdapter 的 configure 方法中添加如下配置即可：protected void configure(HttpSecurity http) throws Exception { http.httpBasic();}HTTP 基础认证比较简单，没有定制的登录页面，所以单独使用场景有限。​在使用 Spring Security 时，一般会把 HTTP 基础认证和表单登录认证结合起来一起使用。​表单登录认证在 WebSecurityConfigurerAdapter 的configure() 方法中，一旦配置了 HttpSecurity 的 formLogin() 方法，就启动了表单登录认证，如下所示：protected void configure(HttpSecurity http) throws Exception { http.formLogin();}formLogin() 方法的执行效果是提供了一个默认的登录界面。​对于登录操作而言，登录界面通常是定制化的，同时，也需要对登录的过程和结果进行细化控制。此时，可以通过如下所示的配置内容来修改系统的默认配置：@Overrideprotected void configure(HttpSecurity http) throws Exception {    http        .formLogin()                .loginPage(&quot;/login.html&quot;) //自定义登录页面        .loginProcessingUrl(&quot;/action&quot;) //登录表单提交时的处理地址        .defaultSuccessUrl(&quot;/index&quot;); //登录认证成功后的跳转页面        }配置 Spring Security 用户认证体系​因为 Spring Security 默认提供的用户名是固定的，而密码会随着每次应用程序的启动而变化，所以很不灵活。​在 Spring Boot 中，可以通过在 application.yml 配置文件中添加如下所示的配置项来改变这种默认行为：spring:security:user:name: spring      password: spring_password重启应用，就可以使用上述用户名和密码完成登录。​基于配置文件的用户信息存储方案简单直接，但也缺乏灵活性，因为无法在系统运行时动态加载对应的用户名和密码。​因此，在现实中，主要还是通过使用 WebSecurityConfigurerAdapter 配置类来改变默认的配置行为。​通过前面的内容中，可以通过 WebSecurityConfigurerAdapter 类的 configure(HttpSecurity http) 方法来完成认证。​认证过程涉及 Spring Security 中用户信息的交互，通过继承 WebSecurityConfigurerAdapter 类并且覆写 configure(AuthenticationManagerBuilder auth) 方法来完成对用户信息的配置工作。​请注意,这是两个不同的** configure()**** 方法**。​针对 WebSecurityConfigurer 配置类，首先明确配置的内容。实际上，初始化用户信息非常简单，只需要指定用户名（Username）、密码（Password）和角色（Role）这三项数据即可。​在 Spring Security 中，AuthenticationManagerBuilder 工具类提供了基于内存、JDBC、LDAP 等多种验证方案。​3.1 使用基于内存的用户信息存储方案基于内存的用户信息存储方案，实现方法就是调用 AuthenticationManagerBuilder 的 inMemoryAuthentication 方法，示例代码如下：@Overrideprotected void configure(AuthenticationManagerBuilder builder) throws Exception {    builder.inMemoryAuthentication()        .withUser(&quot;spring_user&quot;) .password(&quot;password1&quot;) .roles(&quot;USER&quot;)        .and()        .withUser(&quot;spring_admin&quot;) .password(&quot;password2&quot;) .roles(&quot;USER&quot;, &quot;ADMIN&quot;);}可以看到，基于内存的用户信息存储方案实现也比较简单，但同样缺乏灵活性，因为用户信息是写死在代码里的。基于数据库的用户信息存储方案既然是将用户信息存储在数据库中，势必需要创建表结构。​可以在 Spring Security 的源文件中找到对应 `org/springframework/security/core/userdetails/jdbc/users.ddl 找到 SQL 语句，如下所示：create table users ( username varchar_ignorecase(50) not null primary key, password varchar_ignorecase(500) not null, enabled boolean not null);create table authorities ( username varchar_ignorecase(50) not null, authority varchar_ignorecase(50) not null, constraint fk_authorities_users foreign key(username) references users(username));create unique index ix_auth_username on authorities (username,authority);然后通过注入一个 DataSource 对象进行用户数据的查询，如下所示：@AutowiredDataSource dataSource;@Overrideprotected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.jdbcAuthentication().dataSource(dataSource) .usersByUsernameQuery(&quot;select username, password, enabled from Users &quot; + &quot;where username=?&quot;) .authoritiesByUsernameQuery(&quot;select username, authority from UserAuthorities &quot; + &quot;where username=?&quot;) .passwordEncoder(new BCryptPasswordEncoder());}这里使用了 AuthenticationManagerBuilder 的 jdbcAuthentication 方法来配置数据库认证方式。​内部则使用了 JdbcUserDetailsManager 这个工具类。在该类中，定义了各种用于数据库查询的 SQL 语句，以及使用 JdbcTemplate 完成数据库访问的具体实现方法。请注意，这里用到一个 **passwordEncoder()**** 方法，这是 Spring Security 中提供的一个密码加解密器**，" }, { "title": "初识 Spring Security", "url": "/posts/what-kind-of-framework-is-Spring-Security/", "categories": "Java, Spring", "tags": "Spring Security", "date": "2020-05-26 13:13:11 +0000", "snippet": "Spring Security 是 Spring 家族中历史悠久的框架，具备完整强大的功能体系。本文围绕它的功能体系进行总结梳理。其实，在 Sping Boot 出现之前，Spring Security 就已经诞生多年了。但 Spring Security 发展一直不顺利。主要问题在于： 继承、配置 Spring Secutiy 框架过程比较复杂。但是随着 Spring Boot 的兴起，基于 Spring Boot 所提供的针对 Spring Security 的自动配置方案，可以零配置使用 Spring Security 。​如果想要在 Spring Boot 中使用 Spring Security ，只需要在 Maven 工程的 pom 文件中添加如下依赖：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-security&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;下面，通过构建一个简单的 HTTP 端点，来演示下 Spring Security 的初步使用。@RestControllerpublic class DemoController { @GetMapping(&quot;/hello&quot;) public String hello() { return &quot;Hello World!&quot;; }}启动这个 Spring Boot 应用程序，然后通过浏览器访问 “/hello” 端点。可能希望得到的是 “Hello World!” 这个输出结果，但事实上，浏览器跳转到一个如下所示的登录页面：​会弹出这个登录页面原因在于添加了 Spring Security 依赖之后，Spring Security 为应用程序自动嵌入了用户认证机制。接下来，就围绕着这个登录场景，分析下如何获取登录所需的用户名和密码。注意到 Spring Boot 的控制台启动日志中，出现了如图下所示的一行日志：这行日志就是 Spring Security 生产的一个密码，而用户名则是系统默认的 “user”。通过输入正确的用户名和密码，浏览器就会输出 “Hello World！”这个响应结果。上面就是 Spring Security 提供的认证功能，也是 Spring Security 众多功能中的一项基础功能。接下来就深挖一下 Spring Security 中的完整功能体系。2. Spring Security 功能体系Spring Secturity 提供的是一整套完整的安全性解决方案。面向不同的业务需求和应用场景，Spring Security 分别提供了对应的安全性能。下面分别从单体应用、微服务架构以及响应式系统这三个维度对这些功能展开探讨和总结。2.1 Spring Security 与单体应用在软件系统中，把需要访问的内容称为**资源（Resource）, **而安全性设计的目的是对这些资源进行保护，确保它们的访问是安全性的。例如：在一个 Web 应用程序中，对外暴露的 HTTP 端点就可以被理解为是资源。​对于资源的安全性访问，业界存在一些常见的技术体系，这些技术体系中都存在既容易理解又容易混淆的两个概念： 认证（Authentication） 授权（Authorization）​认证和授权结合起来，**构成了对系统中的资源进行安全性管理的最常见的解决方案 **： 判断资源访问者的有效身份 确定其是否有对这个资源进行访问的合法权限这个流程如下图所示：​上图代表一种通用方案，该方案面对不同应用场景和技术体系可以衍生出很多具体的实现策略。​Web 应用系统中的认证和授权模型与上图相似。但在具体设计和实现过程中也有其特殊性。​ 针对认证。这部分的需求相对比较明显，有以下两个要点 构建一套完整的存储体系，保存和维护用户信息， 确保在处理请求过程中合理利用这些用户信息。 针对授权。情况相对认证较复杂。对于某一特定 Web 应用程序而言，面临两个主要问题： 判断一个 HTTP 请求是否具备访问权限 对访问进行精细化管理（虽然一个请求具备访问该应用程序的权限，但是不意味着能够访问所有 HTTP 端点，某些核心功能需要具备较高的权限才能访问，而有些则不需要） 如下图所示：​在上图中，假设该请求具备对应用程序中端点2、3、4 的访问权限，但是不具备访问端点 1 的权限。想要达到这个效果，一般的做法引入角色体系 针对不同的用户设置不同等级的角色 角色等级对应的访问权限等级 每一个请求绑定到某一个角色，也就具备了某一个权限的访问接下来，把认证和授权结合起来，梳理出 Web 应用程序访问场景下的安全性实现方案，如下图所示：结合示意图可以看到： 通过请求传递用户凭证完成用户认证 根据该用户信息中具备的角色信息获取访问权限， 最终完成对 HTTP 端点的访问授权围绕认证和授权，还需要一系列的额外功能确保整个流程得以实现。这些功能包括： 用于密码保护的加解密机制 用于实现方法级的安全访问 支持跨域等这些功能在以后再总结。2.2 Spring Security 与微服务架构​微服务架构的情况要比单体应用复杂的很多，因为涉及了服务与服务之间的调用关系。​上述中资源，对应到微服务架构中： 服务提供者充当的角色是资源服务器 服务消费者充当的角色是客户端因此，服务既可以是客户端，也可以是资源服务器。​同样将认证和授权结合起来，微服务访问场景下的安全实现方案，如下图所示：微服务架构下的认证和授权整合示意图可以看到，与单体应用相比，微服务架构把认证和授权的过程进行集中化管理，所以在上图中出现了一个授权中心 授权中心获取客户端请求中带有的身份凭证信息 基于凭证信息生成一个 Token（包含了权限范围和有效期） 客户端获取 Token 后，基于该 Token 发起对微服务的访问。 资源服务器对客户端获取的 Token 进行认证 资源服务器根据 Token 的权限范围和有效期从授权中心获取该请求能够访问的特定资源在微服务系统中，对外的资源表现形式同样可以理解为一个个 HTTP 端点。​上图中关键点就在于构建用于生成和验证 Token 的授权中心，为此需要引入OAuth2 协议。​OAuth2 协议为客户端程序和资源服务器之间设置了一个授权层，确保 Token 能够在各个微服务中进行有效传递，如下图所示：OAuth2 是一个相对复杂的协议。 综合应用摘要认证、签名认证、HTTPS 等安全性手段，提 供 Token 生成和校验以及公私钥管理等功能 权限粒度控制​应当避免自行实现这类复杂协议，倾向于借助于特定工具，以免重复造轮子，Spring Security 就提供了实现这一协议的完整解决方案，可以使用该框架完成适用于微服务系统中的认证授权机制。​ PS ：单体架构只需要用户登陆以及对用户调用某个接口时进行鉴权即可。微服务的话，多了其它服务(应用而非用户)来调用接口，需要判断这个上游服务有没有权限来调用这个接口2.2 Spring Security 与响应式系统随着 Spring 5 的发布，迎来了响应式编程（Reactive Programming）的全新发展时期。响应式编程是 Spring 5 最核心的新功能，也是 Spring 家族目前重点推广的技术体系。Spring 5 的响应式编程模型以 Project Reactor 库为基础，后者则实现了响应式流规范。事实上，Spring Boot 从 2.x 版本开始也全面依赖 Spring 5。同样，在 Spring Security 中，用户账户体系的建立、用户认证和授权、方法级别的安全访问、OAuth2 协议等传统开发模式下具备的安全性功能都具备对应的响应式版本。" }, { "title": "ArrayList", "url": "/posts/ArrayList/", "categories": "Java, Collection", "tags": "List, ArrayList", "date": "2020-05-02 15:33:00 +0000", "snippet": "基本用法ArrayList 的主要方法有： public boolean add(E e) // 添加元素到末尾public void add(int index, E element) // 在指定位置添加元素，index 为 0 表示添加到最前面，index 为 ArrayList 的长度表示添加到最后面public boolean isEmpty() // 判断是否为空public int size() // 获取长度public E get(int index) // 访问指定位置的元素public int indexof(Object o) // 查找元素，如果找到，返回索引位置，否则返回 -1public int lastIndexOf(Object o) // 从后往前找public boolean contains(Object o) // 是否包含指定元素，依据的是 equals() 方法的返回值public E remove(int index) // 删除指定位置的元素，返回值为被删对象public boolean remove(Object o) // 删除指定对象，只删除第一个相同的对象，返回值表示是否删除了元素，如果 o 为 null，则删除值为 null 的元素public void clear() // 删除所有元素public E set(int index, E element) // 修改指定位置的元素内容ArrayList 是一个泛型容器，新建 ArrayList 需要实例化泛型参数，比如：void basicUsage() { ArrayList&amp;lt;Integer&amp;gt; intList = new ArrayList&amp;lt;Integer&amp;gt;(); intList.add(8848); intList.add(6379); for (int i = 0; i &amp;lt; intList.size(); i++) { System.out.println(intList.get(i)); } ArrayList&amp;lt;String&amp;gt; strList = new ArrayList&amp;lt;String&amp;gt;(); strList.add(&quot;清锋&quot;); strList.add(&quot;码呀&quot;); strList.add(3,&quot;快乐码呀&quot;); for (int i = 0; i &amp;lt; strList.size(); i++) { System.out.println(strList.get(i)); }}基本原理ArrayList 内部有一个数组 elementData，一般会有一些预留的空间；有一个整数 size 记录实际的元素个数（基于 Java 8），如下所示：/** * The array buffer into which the elements of the ArrayList are stored. * The capacity of the ArrayList is the length of this array buffer. Any * empty ArrayList with elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA * will be expanded to DEFAULT_CAPACITY when the first element is added. */transient Object[] elementData; // non-private to simplify nested class access(非私有以简化嵌套类访问)/** * The size of the ArrayList (the number of elements it contains). * * @serial */private int size;ArrayList 中各种 public 方法内部操作的基本都是这个数组和这个整数，elementData 会随着实际元素个数的增多而重新分配，而 size 则始终记录实际的元素个数。比如 add 和 remove 方法的实现。add() 方法的主要代码为：/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &amp;lt;tt&amp;gt;true&amp;lt;/tt&amp;gt; (as specified by {@link Collection#add}) */public boolean add(E e) { ensureCapacityInternal(size + 1); elementData[size++] = e; return true;}可以看到，add() 代码的执行步骤是： 首先调用 ensureCapacityInternal() 方法来确保数组容量是够的，ensureCapacityInternal() 的代码如下： private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity)); } private static int calculateCapacity(Object[] elementData, int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { return Math.max(DEFAULT_CAPACITY, minCapacity); } return minCapacity; } private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code(溢出意识代码) if (minCapacity - elementData.length &amp;gt; 0) grow(minCapacity); } 该方法，先判断数组是不是空的，如果是空的，则首次只少要分配的大小为 DEFAULT_CAPACITY（此字段的值为 10）； 接下来调用 ensureExplicitCapacity() 方法，其中的 modCount 字段表示内部的修改次数，自然modCount++操作表示增加修改次数 如果需要的长度大于当前数组的长度，则调用 grow 方法，主要代码为：```java /** Increases the capacity to ensure that it can hold at least the number of elements specified by the minimum capacity argument.* @param minCapacity the desired minimum capacity*/ private void grow(int minCapacity) { // overflow-conscious code(溢出意识代码) int oldCapacity = elementData.length; // 右移一位相当于除以 2，所以，newCapacity 相当于 oldCapacity 的 1.5 倍 int newCapacity = oldCapacity + (oldCapacity » 1); // 如果扩展 1.5 倍还是小于 minCapacity ，就扩展为 minCapacity if (newCapacity - minCapacity &amp;lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &amp;gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);``` remove() 方法的代码，如下所示： /** * Removes the element at the specified position in this list. * Shifts any subsequent elements to the left (subtracts one from their * indices). * * @param index the index of the element to be removed * @return the element that was removed from the list * @throws IndexOutOfBoundsException {@inheritDoc} */ public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; // 计算要移动的元素个数 if (numMoved &amp;gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work（将 size 减 1，同时释放引用以便原对象被垃圾回收） return oldValue; }由此可见，remove() 方法也增加了 modCount，而后计算要移动的元素个数，从 index 往后的元素都要往前移动一位，实际调用 System.arraycopy() 方法移动元素；elementData[--size] = null，这行代码将 size 减 1，同时将最后一个位置设为 null，设为 null 后不再引用原来对象，如果原来对象也不再被其他对象引用，就可以被垃圾回收。迭代迭代（遍历）是 ArrayList 的常见的操作，迭代(遍历)可以循环打印出 ArrayList 中的每一个元素，有两种方式，代码如下所示： void iterate() { ArrayList&amp;lt;Integer&amp;gt; intList = new ArrayList&amp;lt;Integer&amp;gt;(); intList.add(123); intList.add(456); intList.add(789); intList.add(321); intList.add(654); intList.add(987); // 方式一 for (Integer a: intList) { System.out.println(a); } // 方式二 for (int i = 0; i &amp;lt; intList.size(); i++) { System.out.println(intList.get(i)); } }通过上面的代码示例，很明显 foreach 看上去更为简洁，而且它适用于各种容器、更为通用。编译器会将 foreach 语法转换为类似如下代码：Iterator var2 = intList.iterator();while(var2.hasNext()) { Integer a = (Integer)var2.next(); System.out.println(a);}1. 迭代器接口2. ListIterator除了 iterator(), ArrayList 还提供了两个返回 Iterator 接口的方法3. 迭代的陷阱关于迭代器，有一种常见的误用：在迭代中间调用容器的删除方法。比如，要删除一个整数 ArrayList 中所有小于 100 的数，在直觉是，代码可以这样写： public void remove(ArrayList&amp;lt;Integer&amp;gt; list) { for (Integer a: list) { if (a &amp;lt;= 100) { list.remove(a); } } }在运行时会抛出异常：java.util.ConcurrentModificationException ，并发修改异常！发生这样的情景是因为：迭代器内部会维护一些索引位置相关的数据，要求在迭代过程中，容器不能发生结构性变化（指的是添加、插入和删除元素，只是修改元素内容不算结构性变化），否则这些索引位置就失效了。想要避免这样的异常，使用迭代器的 remove 方法就可以了，如下所示： public static void remove(ArrayList&amp;lt;Integer&amp;gt; list) {// for (Integer a: list) {// if (a &amp;lt;= 100) {// list.remove(a);// }// } Iterator&amp;lt;Integer&amp;gt; it = list.iterator(); while (it.hasNext()) { if (it.next() &amp;lt;= 100) { it.remove(); } } }4. 迭代器的实现原理5. 迭代器的好处 foreach 语法更为简洁，更为通用，适用于各种容器类 迭代器表示的是一种关注点分离的思想，将数据的实际组织方式与数据的迭代遍历相分离，是一种常见的设计模式。需要访问容器元素的代码只需要一个 Iterator 接口的引用，不需要关注数据的实际组织方式，可以使用一致和统一的方式进行访问 提供 Iterator 接口的代码了解数据的组织方式，提高高效的实现 在 ArrayList 中，size/get(index) 语法与迭代器性能是差不多，但在其他的容器中，则不一定，比如 LinkedList，迭代器性能就要很多 从封装的思路来说，迭代器封装了各种数据组织方式的迭代操作，提供了简单和一致的接口实现的接口其他方法特点分析" }, { "title": "DevOps - 软件质量", "url": "/posts/devops-20/", "categories": "DevOps", "tags": "DevOps", "date": "2020-04-14 15:33:00 +0000", "snippet": "上一课时介绍通过提高工程效率来提高价值交付效率，从而提高企业对市场的响应速度。在提高响应速度的同时，也不能降低软件的质量，这就是所谓的“保质保量”。具备高质量软件，高效率的企业走得更快更远。相反，低劣的软件质量，高效率则会让企业死得更快。高质量的软件是企业一直在追求的目标，那么又有哪些指标可以帮助我们识别软件存在的问题呢？这就是今天就介绍一些有关这方面的内容。什么是软件质量？如今，任何一个企业都是数字化企业，任何一家数字化企业都是以软件为业务核心。因此，软件的质量是企业生死存亡的关键因素，务必要引起重视。既然软件质量如此重要，领导者需要了解当前软件质量是多少，存在什么问题，软件质量的发展趋势是什么，这些就是软件质量的度量。软件质量也包含两部分：内部质量和外部质量。 内部质量：是指被开发人员感知的质量，比如，代码的缺陷、坏味道、不合理的架构设计等。内部质量是造成外部质量的源头，在开发过程中要尽早发现、尽早修复内部质量问题，提高发布到生产环境中产品的外部质量。 外部质量：是指能够被用户感知到的质量。比如，用户在使用产品的过程中出现异常，服务不可用，响应迟钝等现象，影响用户体验。外部质量是决定产品是否成功的关键，提高外部质量是团队成员的最终目标。 下面分别从内部质量和外部质量两个方面介绍软件的质量。 内部质量 企业在实施 DevOps 的实践中，也一直在尝试将代码质量的检查集成到流程中，比如持续交付流水线中集成静态代码检查，单元测试覆盖率检查等环节。针对代码质量检查的工具也有很多，常用的有 SonarQube、PMD、FindBugs 等。下面这张图是 SonarQube 代码质量检查的概览页面。 代码质量检查代码质量度量是针对代码本身的度量，根据开发人员的主动和被动，以及对软件造成的影响大小，可以分为Bug 和漏洞以及技术债务。Bug 和漏洞Bug 和漏洞是开发人员在开发业务功能时，在无意识行为下产生的代码问题，即并不是开发人员故意为之。这类问题一般不易被发现，一旦被发现需要及时修复，因为会对软件造成严重影响。Bug 是指代码中的错误，可能会阻止程序按预期运行，影响的是程序的可靠性。漏洞是指代码中的问题，心怀不轨的人会利用这些问题破坏程序的安全性。比如：Java 语言中，字符串和装箱类型的比较使用 equals() 进行比较。下面这段代码就会检查出 Bug。String firstName = getFirstName(); String lastName = getLastName();if (firstName == lastName) { ... };这是因为使用==或!=比较运算符，比较的是内存地址而不是具体的值。在某些情况下，即便 firstName 和 lastName 具体的值相等，但也返回 false。缺陷和漏洞的度量，一般采用数量和级别，级别分为BLOCKER（阻断）、CRITICAL（严重）、MAJOR（主要）、MINOR（次要）、INFO（提示）。技术债务技术债务是指开发人员在开发和设计的时候，为了能满足短期的效益而采取的权宜之计。比如：缺乏自动化测试的代码，包含坏味道的代码。坏味道是指不会阻止程序的正常运行，但可能会对代码的可维护性产生影响。如上图中技术债务需要1天偿还，包含坏味道 86 个，这些就是对技术债务的度量。如下面就是一个坏味道的例子，当数组或集合返回 null 时，调用方需要做 null 判断，否则就会抛出空指针异常。public static List&amp;lt;Result&amp;gt; getResults() { return null; // Noncompliant}public static Result[] getResults() { return null; // Noncompliant}public static void main(String[] args) { Result[] results = getResults(); if (results != null) { // Nullity test required to prevent NPE   for (Result result: results) {     /* ... */   } }}除此之外，还包含圈复杂度、函数代码行、文件代码行、重复代码率、重复文件数等度量。测试质量检查测试阶段又称为质量保证（QA）阶段，是软件开发过程中确保软件功能性和非功能性需求满足用户要求的阶段。为了提高测试效率，很多企业逐渐减少人工测试的比率，提高自动化测试的比率。测试阶段的质量度量可以使用测试覆盖率和测试缺陷数量来表示。测试覆盖率测试覆盖率是衡量代码质量的一个方法，是指自动化测试中代码的覆盖程度，包含单元测试、集成测试、回归测试的测试覆盖率。上图中 54.6% 是测试覆盖率的度量。测试覆盖率越高，发现问题的概率越大，在测试阶段发现的问题越多，软件发布到生产环境后问题就会越少。但是关于测试覆盖率“多少算是合适？”这一问题，很多人是存在分歧的。业界普遍认为测试覆盖率达到 80% 就足够了。这里强调的是，测试一定是有效测试，无效的测试即便 100% 覆盖也没有任何意义。测试缺陷数量测试缺陷数量是指在测试阶段发现的代码问题的数量。如下图所示。每一个缺陷又可以按缺陷类型、严重程度、发现阶段进行标记。 缺陷类型：用户体验问题、性能问题、接口问题、界面问题、环境问题等。 严重程度：致命缺陷、严重缺陷、一般缺陷、轻微缺陷和建议等。 发现阶段：功能测试、单元测试、集成测试、用户验收测试等。测试阶段的目的就是发现问题，所以我们不能惧怕发现问题。在实际开发过程中，测试人员给开发人员提 Bug，开发人员会很抵触，好像是污蔑自己的编码智商，使得开发和测试也会处于对立局面。另外，测试人员要分清哪些是 Bug，哪些是需求改进，不要将需要优化的需求也作为 Bug 提给开发人员。虽然会度量测试阶段的缺陷数量，但不要作为衡量团队成员能力的依据，也不会作为绩效考核的标准。还是前面提到的，要以结果性、全局性的指标为最终指标。外部质量上面介绍了内部质量，以及通过代码检查和自动化测试来保证内部质量，在开发流程中也集成了工具和制度。虽然我们做了大量的质量保证活动，就一定能交付高质量的产品吗？答案是“不一定”。内部质量并不能说明用户对产品是满意的还是抱怨的，也不能说明用户使用过后，是想继续使用还是想舍弃。由于缺少这些相关的度量信息，以至于无法判断产品的质量状态。因此，要从用户满意度、产品非功能性等方面评估产品的外部质量。用户满意度用户满意度是从最终用户的角度对产品的评判。企业在调查用户满意度方面已经很成熟了，有多种方式可以收集用户对产品或服务的评价信息。拨打过 10086 的同学都知道，客服在结束时都会说“请您稍后对我的服务做出评价，满意请按 1，不满意请按 2”，这就是收集用户满意信息的一种方式，其他的还有： 调查问卷； 互联网产品卸载时的弹窗； 投诉与建议。这几种方式，都可以了解用户对产品的哪些功能不满意，为后期进行产品功能优化时提供依据。那么，用什么方式度量用户满意度比较合适呢？业界认为“净推荐值(NPS)”是衡量用户满意度的黄金标准。这是计算某个客户会向其他人推荐某个企业或服务可能性的指数，采用 0-10 分进行打分，分数越高说明你越愿意推荐这个企业或服务。产品非功能性除了用户本身对产品或服务的直观感受外，用户在使用产品过程中感知的产品非功能性问题也是衡量产品质量一个因素。比如产品的可靠性、性能等。 可靠性：是指用户在使用产品的过程中出现服务不可用的概率。这里既可以指具体的人使用产品功能，也可以指系统间的调用或通信。总之，给用户带来的影响是不能正常的使用产品。 性能：是指用户在使用产品时的流畅性，未出现卡顿、延迟等现象。比如，打开一个页面需要 10s 以上，虽然还能够使用产品，但用户体验不好。产品的非功能性问题会最终影响用户满意度，一般通过用户反馈的缺陷和问题数量及严重程度来度量产品的可靠性，通过应用程序性能监控系统（APM） 度量产品的性能。随着DevOps实践的不断深入，通过蓝绿部署、金丝雀发布等方法，先在一小部分用户使用新版本，以便提前发现软件存在的问题，从而避免让更多用户受到影响。以及使用混沌工程，提前发现问题，减少产品不可用的概率。这些方法都是针对产品的非功能性采取的防控措施。总结软件的质量分为内部质量和外部质量，二者相辅相成，互相影响。内部质量是源头，外部质量是结果。提高内部质量会进一步提升外部质量，外部质量也会反过来促进内部质量的提升。DevOps 的目标是在提高研发效率的同时，也要提高软件产品的质量。如今市场竞争越发激烈，用户在第一次使用后，认为产品或服务没有达到满意，是不会再有第二次机会的。因此，软件的质量是企业研发的重中之重，也是企业实施 DevOps 的目标之一。" }, { "title": "DevOps - 响应速度", "url": "/posts/devops-19/", "categories": "DevOps", "tags": "DevOps", "date": "2020-04-10 15:33:00 +0000", "snippet": "事实上，团队的能力在一定程度上反映了软件的交付能力；而响应速度则是企业能否快速占领市场的重要因素。试想一下，有两个企业都发现了市场上的机会，这个时候谁能够将机会变成产品发布到市场上，谁就赢得了先机。今天要介绍的主要内容就是响应速度，一起我们来学习，哪些因素会影响响应速度？如何度量团队的响应速度？什么是响应速度俗话说“天下武功，唯快不破”，《孙子兵法》中也提到“兵之情主速，乘人之不及”。这都是说在战争中，速度是取胜的关键。商场如战场，在如今数字化企业时代，软件的发布速度，问题的修复速度是企业制胜的法宝。在软件开发过程中，响应速度包含多个方面。 软件发布：是指从软件立项到发布到生产环境，交付给用户使用的时间，是端到端发布的时间。代表了团队对市场机会的响应速度。 特性发布：是指从特性评审完成到集成测试、验收测试，再到发布到生产环境的时间。代表了团队对用户需求的响应速度。 缺陷发布：是指从发现缺陷/故障到定位，再到修复的时间。代表了团队对测试、运维的响应速度。可以看出，提高响应速度的根本是缩短软件的交付周期。但作为软件开发人员，我们都知道通过缩短软件的交付周期来提高响应速度并不是一件容易的事。下面介绍下影响响应速度的因素有哪些。影响响应速度的因素软件开发是一项复杂的、交付最终价值的工作。复杂是指特性从需求开始需要经过多个阶段才能完成，特性的难易、大小也不一样。交付最终价值是指软件开发过程中的产出都是未产生价值的，要从全局层面进行度量和优化，而不是只局限在单个阶段的优化，比如测试阶段的效率提升未必缩短软件端到端的交付周期。那么，影响软件交付周期的因素有哪些？资源利用率资源利用率是指影响软件交付的相关资源在软件交付活动中所占时间的比例，比如团队成员、服务器设备、时间等。很多人认为，如果把所有人员、所有时间都充分利用起来，就能够缩短软件交付的周期。事实并非如此，原因是： 软件开发是一个顺序执行的操作，需要按照需求分析、设计、开发、测试、部署的阶段执行。每个阶段都依赖前面的阶段，这意味着即使投入再多的资源，交付周期也不可能无限制的缩短。 资源利用率越高，工作任务在队列中等待的时间就会越长，交付周期就会增长。就像高速公路一样，如果高速公路的路面都被利用上了，汽车就跑不起来了，高速公路就变成了停车场。如果一个同时处理多个任务，任务之间上下文的切换也会占用额外的时间。关键资源、关键人物被积压着长长的任务队列，慢慢就会成为整个交付的瓶颈。 任务的大小 任务的大小也是影响软件交付的因素，当任务的大小不一致，就会导致处理任务的时间不一样，就会产生等待。比如开发人员正在开发一个大的特性或是解决一个疑难杂症，在测试环节处理完任务后，没有任务具备测试条件，就不得不进入等待。在精益思想里提到，等待是一种浪费，会延长软件的交付周期。因此，在拆分需求的时候，尽可能地将任务拆分的均衡，减少等待的时间。 在制品数量 在前面的课时讲过通过看板方法可视化进行中的工作，以此控制在制品的数量。在制品数量越多，团队成员就会不断地进行任务切换，导致其他任务的等待和团队成员的利用率太高，造成瓶颈。可以通过限制在制品数量，一个完成再做下一个，可以实现频繁交付，缩短每个任务的交付周期。 自动化程度 自动化是影响响应速度的重要因素，减少手工操作，尽可能将一切自动化。包含编译构建、自动化扫描、自动化测试、自动化部署等。之前介绍的部署流水线就是将整个过程自动化的实践。在软件开发过程中，测试阶段是占用时间较长的阶段，应该尽可能地进行自动化测试，不仅包括集成测试、回归测试、功能测试和性能测试，甚至 UI 测试、UAT 测试都自动化实现。 度量响应速度的指标 前面提到，响应速度就是软件交付周期，提高响应速度，就是要缩短软件的交付周期。跟软件交付效率的度量有两种：价值交付效率和工程效率。下面分别介绍一下这两种度量。 价值交付效率 价值交付效率是以交付用户价值的度量。如用户的需求多久能发布，也就是上一课时提到的“前置时间”。在敏捷开发方法中，采用 Sprint（迭代）小批量的方式进行开发，每个迭代完成后，交付软件的增量。这里介绍一下“累积流图”。 累积流图累积流图是看板系统中的重要度量，是用来展示开发过程中各个阶段的在制品数量。能够根据不同的值分析当前存在的问题，还可以计算出平均前置时间，如下图所示。 在制品数量从进入“开发线”到“完成线”之间的高度，代表了当前时间在制品的数量。随着时间地推移，这个高度的变化反映了在制品数量的变化。如果在某个时间点，这个高度突然增大，说明在该时间点出现了瓶颈，可能是工作项难度太大，出现了积压，或者新增了工作项。 前置时间从进入“开发线”到“完成线”之间的长度，代表了从开发启动到完成的周期，也反映了团队的交付能力。如果某个时间点，这个长度较长，说明在该时间点出现了瓶颈，可能是工作项出现了积压，或者工作项遇到困难。工程效率工程效率主要是指 DevOps 平台本身的能力。影响软件交付效率的因素有：编译效率、测试效率、部署效率等。下面依次介绍一下。 编译效率代码需要编译打包之后才能部署到测试环境和生产环境中，在没有打包之前，测试就只能等待打包完成。因此，测试阶段的执行是强依赖前面的编译打包过程，因此要提高编译效率。影响编译效率的因素有： 编译机的资源：代码编译需要 CPU 资源，每个编译机能够同时处理的编译任务是有限的，当有多个编译任务需要执行时，就会有任务处于等待队列中。为了最大化利用编译机资源，可以采用容器构建的方式。当有编译任务需要处理时，就启动一个 Docker 容器，编译完成后就销毁，将资源释放出来。 代码库的大小：代码库的大小和编译时长成正比。在之前的企业里，有一个代码库里面包含了 50 多个子模块，每构建一次都是一场噩梦，哪怕只修改了一行代码，也需要下载、编译整个代码库，整个过程就需要1个多小时，这样的编译时长如何能加快整体的交付速度。在微服务时代，每个服务都是一个单独的代码库，代码库的大小减少了，编译时长也自然会缩短，编译效率也就提高了。每次变更代码，也只需要回归这个服务就可以，不需要整体回归。 编译工具的设置：代码是通过编译工具来编译的，比如 Java 语言中的 Maven，Gradle。编译工具本身为了加快编译速度也会进行优化，因此要使用尽可能新的版本。另外，为了加快编译速度，可以开启依赖缓存，不用每次都从中央仓库下载依赖包。DevOps 平台中应该记录每次编译的信息，包括代码库、开始编译时间、结束编译时间、编译结果、编译人等信息，在数据统计时，可以统计每天的编译次数、编译时长的中位数，编译成功率，针对编译时长较长的代码库进行优化。 测试效率在瀑布时代，测试周期少则数周，多则数月。在这么长的测试周期下，交付周期也不会太短。测试阶段又分为多个不同的阶段，如回归测试、功能测试和非功能测试等。每一种类型的测试用于满足不同的测试需求，测试所需要的时间也不一样。 回归测试：一般用于完成一轮产品回归所需时间的度量。当代码修改后，验证该变更有没有引入新的错误，或者有没有对既有功能产生影响所进行测试的时间。回归测试应该尽可能地自动化，这样才能满足日益增加地频繁发布。 功能测试：一般用于完成所有功能验证所需时间的度量。全量功能验证一般在发布之前执行一次，以验证所有功能都是正常的。功能测试也应该是自动化的，缩短发布前的测试时长。 非功能测试：一般用于完成非功能性测试所需时间的度量。根据产品要求，有些版本需要进行性能测试、压力测试、安全测试等，这些测试需要用专业的工具完成，通常模拟大数据量，高并发量的测试。非功能测试基本都是通过工具自动化完成。DevOps 平台中应该记录每次测试的信息，包含测试的总体时长，测试的用例数，测试的成功率等信息，测试的总体时长是测试效率的直接度量，影响因素有执行的用例数和测试的成功率。用例数越多，执行的时间就会越长；成功率越低，需要返工的工作项越多，最终测试的时间就越长。部署效率部署效率是指从开始部署到部署完成需要的时间。部署频率是一个全局指标，部署效率决定了部署频率，如果部署效率很高，部署频率自然也会高。这里要注意，部署效率不应该以牺牲准确性为代价。如果部署失败率高而不下，反而会影响整体的部署频率。在软件交付效率中，价值交付效率是最终目标，但价值交付效率的提升是以工程效率的提升为基础。俗话说“工欲善其事必先利其器”，就跟我们现在的交通工具与古代的交通工具相比，货物运输的效率是不是提高了呢。总结在当前市场竞争日益激烈的时代，速度是企业制胜的法宝。我们首先分析了影响响应速度的几个因素，比如：资源利用率、任务的大小、在制品数量和自动化程度。了解了问题所在，通过优化这些因素来提升响应速度。接下来介绍流量度量响应速度的几个指标，从价值交付效率和工程效率两个方面分别介绍了几个关键的指标。效率的提升不会一蹴而就，是通过不断优化，持续改进来实现的，这也是使用度量指标来指导改进的目的。" }, { "title": "DevOps - 团队能力=交付能力", "url": "/posts/devops-18/", "categories": "DevOps", "tags": "DevOps", "date": "2020-04-06 15:33:00 +0000", "snippet": "众所周知，软件开发是团队成员完成的，团队成员的能力在一定程度上代表了软件的交付能力。相信在你的周围也有过这样的团队：只要有一位技术大牛，交付软件的质量和效率就非常高。技术大牛以一顶十的技术水平带动了软件业的不断发展。我们熟悉的 Linux 和 Git 的创始人，大神 Linus 就是很好的例子。本文总结如何度量团队能力？团队的 T 型人才观韩愈的《师说》里讲道：“闻道有先后，术业有专攻”。“术业有专攻”的意思是每个人在技能和学术上都有各自的研究方向。参加工作后，大多也都是职能型组织，每个部门都有各个领域方向的专业人员，比如 DBA、网络管理员、存储管理员、前端工程师、大数据工程师等。然而，当人员过于专业化时，就会形成筒仓。进行软件研发活动时，需要在不同部门之间进行多次沟通和交接，当没有空闲时还需要排队，这就导致了交付时间的推迟。为了解决这个问题，一种常见的对策就是让每一位团队成员都成为通才，也就是现在经常提到的全栈工程师。如下表所示。全栈工程师比专业人才可以做更多的工作，同时因为减少了等待时间，从而提高了整体的交付效率。在企业里可以通过交叉培训的方式，为工程师提供学习的机会，并定期让他们在不同的职位间进行轮岗，来提高相关的技能。跨团队协作的问题相信很多人都遇到过，特别是像 DevOps 这种跨团队协作的岗位。团队的技能图谱在上面提到团队需要的是 T 型人才。想要 T 型人才团队就需要掌握团队成员在技能上的熟练程度，并对薄弱环节的人员进行有针对性的培训。采用技能图谱的方式呈现团队当前在各个技术方面的能力状态。如下表所示。在图谱中，每个团队成员在每项技能上都有一个分值，它代表了该成员对该项技能的熟练程度。最高分 10 分，最低分 0 分，分值越高表示越精通。根据团队的技能图谱，可以了解团队成员的优势在哪里，弱势在哪里。团队成员由此可以根据自己的兴趣和项目的需要，制定自己在图谱上的提升计划。上图中的数据真实反映目前 DevOps 团队的技能水平。由于都不是做云出身，底层网络、交换机以及 OpenStack 是我们团队的薄弱环节。因此我们可以有针对性的培训和学习这几个技能，成员不需要达到精通的程度，当在部署整个私有云的时候，遇到问题能够排查，满足部署流水线的需求就可以了。技能图谱制定完成后，团队需要对其进行定期更新，比如，每隔 2-3 个月团队成员要更新自己的技能评估，并制定下一步的提升计划。如何度量团队的交付能力在实际开发过程中，领导层想知道团队一周能开发多少个需求？如果有一个比较紧急的项目，应该交给哪个团队比较合适？这些都需要通过具体的指标来度量团队的能力。但是，度量团队的研发效能并不是件容易的事情，有以下几个原因。 研发过程可视化差：软件研发过程并不像制造业生产过程，是具体的、可见的。我们很难准确的定义一个需求开发完成具体需要多长时间，因为这和具体执行者的技能水平有很大关系。也很难定义每个团队每个迭代的具体产出，这跟需求的大小以及需求的难易有关系。研发过程可视化在之前的看板方法中有所涉及，就是要将研发过程中的事项以可视化的方式展示出来，便于掌握团队的工作情况。 工作切分的随意性：软件研发的工作事项没有统一的标准。需求可大可小，有的需要一周，有的需要一天，有的甚至一小时即可完成。有的已经知道具体的解决方案，有的可能是一个新的课题，需要时间调研、实验才能知道解决方案。这些现实的问题增加了度量团队研发效能的难度，使得度量的指标不准确，不具有指导意义。度量团队效能的误区在实际工作中，使用指标度量团队的效能存在几个误区。这样的度量有两个不足： 侧重产出而不是结果。 侧重个人或局部的度量，而不是团队或全局的度量。其中常见的有“代码行数”、“故事点数”这些经常被误使用的指标。1. 代码行数。使用代码行数来度量团队的交付效能这种方式在很多企业中依然存在。在一些公司甚至要求统计开发人员每天、每周的代码提交量，以此作为团队工作饱和度的评判依据。这样会使得开发人员为了增加代码行数，在完成代码逻辑时尽可能地多写代码，导致软件变得越来越臃肿，使得代码维护成本更高。同样，也不能用最小代码行来作为衡量标准，这样会导致代码足够精简而增加了理解的难度。因此，代码行数不应该作为度量效能的指标，最有效的代码应该是能够解决业务问题的。2. 故事点数在敏捷软件开发方法中，将需求拆分成故事，用故事点数来估计每个故事的大小，以此表示预期完成这些故事需要的工作量。 在有些团队中，也会根据故事点数来衡量团队能够交付的工作量大小，或者推断团队完成一个需求需要多长时间。使用故事点来作为评判团队交付效能会使得团队成员只关注那些故事点数多的任务，或者过多的评估故事点数。因此，故事点数也不应该作为度量效能的指标，只是计划产能的度量，不是真实生产效率的度量。度量团队效能的指标那么什么样的指标是度量团队效能的有效指标呢，这类指标应该具备两个关键特征： 应该专注全局结果，以确保团队之间不会相互竞争。比如：奖励开发人员的高吞吐量以及运维团队的系统稳定性。这是开发和运维之间有一道墙的关键因素，即开发为了提高吞吐量将质量差的代码提交给运维进行部署，而运维为了保证系统的稳定性通过复杂烦琐的变更流程来抑制产品发布。 应该关注结果而不是产出。在企业里，领导经常会说“我只看结果不看过程”，也是这个道理。很多人以“没有功劳也有苦劳”作为跟领导辩驳的理由或许收效甚微。没有为企业创造价值的付出都是徒劳的。下面介绍一下有哪些可以度量团队效能的指标，这里参考权威的全球 DevOps 现状报告中的内容。《2018 全球 DevOps 现状报告》中引入了“软件交付效能”这个概念，并将不同的团队划分为精英、高效能、中等效能和低效能四种级别。为了度量团队的交付效能，采用了 5 个软件交付效能的度量指标，用来展示软件交付的 3 个方面。如下图所示： 部署频率：是指团队将应用程序部署到生产环境频率的度量。根据报告指出精英组织会按需部署，并且每天都会部署多次。相比之下，低效能组织一周或一个月部署一次。 前置时间：是指从代码提交，到代码成功运行，到生产环境的时间。精英组织的变更前置时间一般不到一个小时，而低效能组织则需要 1 到 6 个月的前置时间。 恢复时间：是指服务故障到服务恢复所需要的时间，也就是经常说的 MTTR。精英组织服务恢复时间一般在 1 小时以内，而低效能组织则需要 1 周到 1 个月之间。 变更失败率：是指应用程序部署到生产环境后需要回滚的百分比。精英组织的变更失败率一般在 0% 到 15% 之间，而低效能组织则在 46% 到 60% 之间。 可用性：是指团队确保软件产品或服务可用的能力，可以通过 SLA 度量。精英组织和高效能具有卓越的可用性，一般是低效能组织的 3.5 倍。从以上可以看出，最优秀的精英组织总是能在吞吐量和稳定性上同时达到高绩效水平，而不是在两者之间取舍。在精英组织中，通过将任务尽可能地自动化，来将团队成员从低价值的任务中解放出来，从而从事那些可增加真正价值的创新型工作。在任何行业里，团队的效能都是驱动组织业务发展的关键因素，最终实现组织的商业目标。" }, { "title": "DevOps - 度量指标", "url": "/posts/devops-17/", "categories": "DevOps", "tags": "DevOps", "date": "2020-04-05 15:33:00 +0000", "snippet": "DevOps 除了要关注技术方面的问题，还需要关注度量方面事实上很多团队和组织在实施 DevOps 时都专注于技术，而忽略了度量方面。度量是实施 DevOps 的关键要素，如果把 DevOps 比作一辆车，那么之前的造工具、搭平台就是这辆车的车身，度量就是车的仪表盘。DevOps 的度量也需要一些指标来指导 DevOps 的持续改进。什么样的指标是好指标？如何找到好指标？为什么要度量指标？度量在很多企业里落地的效果并不好，主要有以下两个方面： 一是因为度量的前提是要有一套打通端到端的 DevOps 平台，否则再优秀的度量也只是局部度量。目前国内很多企业还都处于建设 DevOps 平台的基础阶段，因此落实下来也并不容易； 二是，度量本身投入产出比并不像 CICD 效果明显。很多工程只是为了“给上面看看”而完成的任务，并没有从度量的本质上去考虑。因为这两点原因，度量指标在企业内的落地还存在问题。我认为“有哪些度量指标”“指标如何获取”这些问题是从一开始就要考虑的，原因有以下几点： 精益思想的核心理念是持续改进，只有清晰明确的度量指标作为指引，才能达到持续改进的目标。在持续改进这条路上，没有终点，永远在路上。 度量能够提供信息来帮助我们知道现在在哪里，距离目标还有多远，我们是在沿着目标前进，还是在倒退，程度如何。 度量指标是需要从 DevOps 平台获取的，一开始要考虑有哪些度量指标，如何获取，对 DevOps 平台的设计有指导意义。 这样要强调的是，度量指标不是目的，而是手段；不是控制，而是改进“目的”容易给人以到达终点的错觉，“手段”是为了发现潜在的问题。“控制”容易给人以一种静态目标的心理暗示，“改进”则是以动态目标植入人心。这有助于不断地发现问题，改正问题。什么样的指标是好指标？关于寻找度量指标这块，有一个误区，就是要“度量所有内容”。不能一拍脑袋要度量几百个指标，以期望能从这么多的指标中找到一些重要信息。这种方式是不正确的，有以下两个缺点： 更多的指标需要投入更多的资源来关注软件研发的各个方面，最终会导致每个指标的效果并不好； 以 KPI 的形式完成指标，最终完成的只是数量，不是质量。保证度量指标的质量，找到好指标，可以遵循以下五个点： 可度量的：指标必须是可衡量的，即是一个定量的指标，而不是“非常好”，“非常快”这种定性的指标。 相关联的：指标必须能够度量对业务有重要影响的因素。 不可更改的：团队成员不能影响度量指标的结果。 可实施的：指标是能够通过技术的手段获取并且数值是真实可靠的。 可追溯的：指标必须是能够直接反映软件研发过程中存在的问题。因此，我们不可能度量所有的指标，要选出哪些满足这些要求的指标，指标不在多，而在精。在找出要跟踪的 DevOps 指标之前，需要确定组织面临的挑战以及要解决的问题。好的指标是用来解决实际业务问题的。因此，应该避免那些不符合 DevOps 时代、对用户没有价值的指标，比如以下几点。 传统的工程指标：比如 MTBF（平均故障间隔时间）在 DevOps 时代意义就不大。系统的长期稳定性并不是首要目标，因为 DevOps 时代是通过快速部署来保证系统的稳定性的。基于虚拟化和基础设施即代码的工程实践，可以通过频繁的部署来进行线上测试，这些测试可能会经常失败，但有利于制定更好的方案。这种情况下，MTBF 对业务需求来说并不是好指标。 基于竞争的指标：切勿基于团队成员或团队之间的竞争来建立指标。比如按团队成员完成的需求数量进行排名、按开发人员出现的 Bug 数进行排名等。度量指标的目的是用来解决业务问题，不是用来晾晒团队成员技术水平的手段。 虚荣性指标：比如每周代码行的统计。不应该以代码行数这样无意义的指标评判开发人员工作量的指标。最终交付功能的及时性和质量才是最重要的。在度量指标的时候，不要根据获取指标的难易来取舍指标。在一项重要的指标上哪怕花费更多的成本都是值得的，在一项无用的指标上投入再少的时间也是在浪费。如何选择指标？在上面也提到了，好的指标是用来解决问题的。当我们在选择指标时的依据也是要解决的问题。在软件开发过程中，需要解决的问题很多。代码质量、团队成员、发布效率的等都有可能成为问题的来源。这些指标中，有些是给上层领导做决策用的，有些是为了提升团队技能水平的，有些则是为了提升软件质量。不管用途是什么，衡量的标准就是解决或改善现有的问题。我举了下面几个例子。 缩短产品上市时间：用于衡量从用户需求被提出到最终交付给用户之间的时间，可以使用“前置时间”这个指标。因为更短的上市时间代表了企业在市场竞争中的反应速度越快。 提高软件开发的效率：可以使用“流动效率”这个指标，以查看瓶颈点，并将工作重心放在如何改善流动瓶颈的地方。等待的时间越少，软件开发的效率就越高。 解决团队正在处理的事项和计划外事项的冲突：可以使用“在制品数量”这个指标，以暴露工作内容过载的团队或团队成员，使得每个团队成员的工作更加均衡。 解决未完成的重要工作不被遗忘的问题：可以使用“停留时间”或“过期时间”等指标，来度量未完成的工作在系统里停留了多长时间，如果超过设置的阈值则进行预警以暴露风险。 减少生产环境中用户发现的问题数量：可以使用“缺陷逃逸率”这个指标，争取尽可能多的 Bug 是在测试环境或预生产环境中发现，以最大程度建设用户发现的缺陷数量。 如何使用指标？ 当根据上面的标准选择好指标后，应该如何使用这些指标？反馈循环是有效改进的基础，通过度量指标的反馈，有助于更加精准的调整团队的行动，改善整个组织的沟通。下图是度量指标的反馈循环，需要有以下几个步骤： STEP 1：收集数据。收集关于软件研发过程中的数据，作为后续分析的原材料。在大多数企业，度量面对的问题不是数据准不准确，而是有没有数据的问题。如果要有效地收集数据，需要从两个方面入手。 平台方面：平台本身需要具备收集数据的能力。在设计平台时，要有针对度量指标方面的设计。比如每个任务都要有开始时间和结束时间，每个事件都应该有发生、处理、解决的时间记录，事物之间的关联（如代码提交与任务或缺陷的关联，代码库与产品线的关联，流水线构建与代码库的关联等）。平台具备收集这些数据的能力外，还可以提高统计报表，用更直观的方式进行展示。 人的方面：团队成员的有效参与能够充分发挥平台的能力。DevOps 平台中，虽然将研发流程中的操作尽可能自动化了，但有些内容还是需要人工配合。比如：在提交代码时按照规范提交，将需要关联的需求 ID 和缺陷 ID 添加到 message 里，从而建立提交的代码与需求和缺陷的关联。需求的拆解，任务的启动、过程跟踪以及完成后的关闭操作，都需要人工配合，才能使数据更加准确。 STEP 2 ：分析数据。 基于收集的数据进行分析，以便能发现当前存在的问题。举个例子，通过数据收集系统发现：需求完成的数量在减少，代码行数在增加，同时缺陷的数量在增长。下面通过这些数据进行分析： 需求完成的数量减少，说明团队花在需求上的时间减少了，是什么原因导致的呢？继续往下分析。 代码行数在增加，说明团队成员花费大量的时间在修改代码上。既然完成的需求在减少，可以断定代码不是为开发需求而写的。 缺陷的数量在增加，可以说明当前在测试阶段，并且测试出了很多问题。通过分析得出结论：说明软件进入到测试阶段后，问题很多，导致团队成员需要花费大量的时间修复缺陷，从而影响了正常的需求开发。STEP 3：调整流程。根据上面的分析判断，开发人员在开发阶段对软件质量的控制效果并不好，可能的原因有： 开发人员没有进行有效自测； 开发人员没有编写单元测试或者覆盖率较低。因此，可以采取一些措施改善流程，尽早发现软件中的问题。比如在持续集成流水线中集成单元测试，通过设置门限阈值来控制单元测试的有效性和覆盖率；通过自动化的API接口测试，验证服务以及服务之间调用的正确性等。STEP 4：重复执行。重复上面的步骤，再次收集指标，观察指标的变化，并根据指标的值调整流程，直到满足要求。总结DevOps 以精益思想为基础，精益思想的基础是持续改进，持续改进的基础就是清晰明了的度量指标。" }, { "title": "DevOps - 百度效率云 DevOps 解决方案学习", "url": "/posts/devops-16/", "categories": "DevOps", "tags": "DevOps", "date": "2020-03-30 15:33:00 +0000", "snippet": "本文通过百度效率云这一 DevOps 平台，学习其如何做代码审查的。效率云简介效率云是百度自主研发的整套 DevOps 解决方案，涵盖了创新管理、产品管理、项目管理、代码托管、持续交付、微服务治理、线上反馈等软件开发全生命周期。核心功能有以下几点： 项目管理 iCafe：主要包含用户故事地图、迭代管理、需求定制化查询、文档协同、定制化报表等功能。基于这些功能团队可以进行产品的整体规划、需求的拆解和录入，可视化看板的定制等 代码托管 iCode：除了包含代码托管功能之外、还提供了一系列可配置规则。iCode是基于开源代码评审工具 Gerrit 的代码托管平台，也是采用的 Change Request 机制，当开发人员提交代码后，就会触发 iCode 内置的代码检查和流水线检查，最大程度的保证代码入库的质量 持续交付 iPipe：主要包含持续集成、持续交付等功能，内置了 maven、gradle、npm、pip 等多种语言的依赖管理和制品管理功能。支持基于主干、Branch 和 Change 的构建流水线，支持串行、并行两种任务执行策略以及手动、自动、定时多种触发策略。除了上面的核心组件，还有代码扫描 iScan、制品管理 iRepo、接口测试组件 ITP、*压力测试组件 dumeter *等。效率云分为百度内部版本、百度云上 SaaS 版本，以及独立部署的版本。代码托管 iCode代码审查这个实践主要跟效率云中的代码托管平台 iCode 相关，下面的内容主要介绍在 iCode 中是如何进行代码审查。与当前比较流行的 GitHub 和 GitLab 一样，iCode 也是基于分布式版本控制系统 Git 搭建的代码托管平台。二者区别在于 iCode 的权限管理、代码评审模型是基于 Google 的 Gerrit 实现的，而 GitHub 及 GitLab 则是原生设计的。开发协作模型DevOps 中关于代码质量检查的原则是尽可能前置，代码问题发现的越早，修复的成本就越低。在代码托管平台中，不同的开发协作模型决定了代码检查的时机。这四种代码托管平台的开发协作模型如下： Pull Request 模型——GitHub； Merge Request 模型——GitLab； Change Request 模型——Gerrit，iCode。第一，Pull Request 模型。Pull Request 模型是 GitHub 设计的开发协作模型，是为了满足全球各地的开发者共同参与开源项目开发的需求。GitHub Flow如下图所示。主要包含几个步骤： 开发者如果想修复一个开源项目的 Bug，首先将开源项目的远程代码库fork到个人代码库中； 在个人代码库中修复该开源项目的 Bug； 将修改的代码提交到个人代码库中； 在 GitHub 中远程仓库发起一个 PR； 远程仓库的 Owner 同意后，代码即可合入远程仓库。Pull Request 模型的代码评审阶段一般发生在开发者向开源项目提交 PR的时候。第二，Merge Request 模型。Merge Request 模型是 GitLab设计的开发协作模型，主要是满足企业内部团队之间的协作开发，是一种分支开发、主干发布的设计思想。GitLab Flow 如下图所示。主要包含几个步骤： 当团队成员开发一个新功能时，先从主干分支拉取新的特性分支； 在本地开发新功能； 将代码提交到该特性分支； 在 GitLab 中发起 MR，请求将该特性分支的代码合并到主干分支； 负责人对 MR 进行 Code Review，同意后点击 Merge，即可合入主干分支。Merge Request 模型的代码评审阶段一般发生在开发者将特性分支合并到主干分支的时刻。第三，Change Request 模型。上面的PR和MR模型提供了基本管理方式，但在使用的过程中也存在一定的问题，比如： 特性分支需要在合并的时候才能进行 CodeReview，此时需要评审的代码量很多，无法保证 Code Review 的效果。 因为只有在分支合并的时候才能进行 CodeReview，每次需要建立新的分支和分支的合并，对于小型团队来说，增加了流程的复杂性。Change Request 模型是 Gerrit 设计的开发协作模型，也是 iCode 目前采用的模型，能够将代码检查前置到代码提交阶段。Change Request 的开发协作流程如下图所示。主要包含几个步骤： 执行 git pull， 将远程分支的代码更新到本地工作空间； 在本地工作空间开发新功能； 执行 git commit 将代码提交到本地仓库； 执行 git push HEAD:refs/for/branch 发起一个 Change Request； 经过流水线检查和 Code Review后，才能将代码合入代码库。Change Request 模型的代码评审阶段是在代码提交阶段，而不是分支合并的时候。与 Pull Request 模型和 Merge Request 模型相比，Change Request 模型代码评审的时机更靠前。提交规则设置上面介绍了 iCode 中采用的是 Change Request 的开发协作模型。在新建代码库后，默认并未开启该协作模型，可以在“提交规则”里进行相关设置，这样可以满足不同的团队对代码评审的要求。如下图所示。提交规则包含四个方面：提交、机器评审、人工评审和合入策略。下面是我工作中代码评审相关的几个规则：1. 提交。提交代码必须经过评审（使用 git push origin HEAD:refs/for/[分支名]），选中该配置项即可开启 Change Request 的代码评审。当团队对提交代码的质量有严格要求时，需通过开启代码评审，进行代码检查，来控制提交代码的质量。2. 机器评审。开启 iPipe 流水线检查，该配置项是用于当提交代码评审后，会自动触发 Change 流水线，对代码执行编译、单元测试等流水线检查。流水线检查通过后代码评审会+1。3. 人工评审。必须有评审人+2，该配置项是用于保证人工评审的效果。这是 Gerrit 的规则，在代码评审中，任何人都可以查看任何人提交的代码，并给予不具有约束力的+1。只有一小部分人才能批准提交的代码并将其合并到代码库中，也就是必须要有一个评审人给予+2。通过上面三个配置项，可以根据团队的需求灵活的设置代码评审的开启和关闭，以及代码合入的准则。评审人设置评审人设置功能是为了在每次发起评审时，系统会自动将这里设置的评审人加入该评审单的评审人，不需要开发者手工添加。除了这里的评审人，其他人员还是可以评审的。如下图所示：该设置能够细化到按分支和目录设置评审人，如果一个代码库包含多个不同的功能模块，每个功能模块在不同的目录下时会非常有用。默认评审人的设置分为以下两种情况。 所有分支：如果“分支”列是所有分支，也就是选择*时，“文件”列需要留空，表示所有分支的所有文件。 特定分支：如果“分支”列选择某个特定分支，如 master，“文件”列可以输入特定路径，需要以^开头，以/.*结尾。 代码评审示例 下面，我通过一个示例演示代码评审的整个过程。该演示过程是在百度效率云的 SaaS 版本中进行的。分为代码提交、代码评审和代码合入几个步骤。 代码提交 假设本次提交的内容是“修改版本号”，在 build.gradle 文件中将 version = ‘0.0.1-SNAPSHOT’ 修改为 version = ‘1.0.1-SNAPSHOT’。提交代码命令如下。```bashD:\\code\\devops&amp;gt;git add .warning: LF will be replaced by CRLF in build.gradle.The file will have its original line endings in your working directory D:\\code\\devops&amp;gt;git commit -m “update version”[master 4bef3f8] update version 1 file changed, 1 insertion(+), 1 deletion(-)D:\\code\\devops&amp;gt;git push origin HEAD:refs/for/masterPassword for ‘https://xinglong_devopser@xly.bce.baidu.com’:Enumerating objects: 5, done.Counting objects: 100% (5/5), done.Delta compression using up to 4 threadsCompressing objects: 100% (3/3), done.Writing objects: 100% (3/3), 325 bytes | 325.00 KiB/s, done.Total 3 (delta 2), reused 0 (delta 0), pack-reused 0remote: Resolving deltas: 100% (2/2)remote:remote: Processing changes: new: 1, refs: 1, doneremote:remote: New Changes:remote:   http://xly.bce.baidu.com/icode/myreview/changes/71999 update versionremote:To https://xly.bce.baidu.com/code/devopser/demo-project/devops [new branch]     HEAD -&amp;gt; refs/for/masterD:\\code\\github\\devops&amp;gt;```注意，在提交到远程仓库时使用的是 git push origin HEAD:refs/for/master 命令。从提交日志中可以看到两个重要信息：生成了一个新的Change，路径http://xly.bce.baidu.com/icode/myreview/changes/71999，该URL为此次评审详情页面的地址；此次提交是 push 到了 refs/for/master 这个临时分支，并不是直接 push 到 master 分支。代码评审打开上面的地址，可以看到代码评审的详情页面，评审人就是在该评审页面中进行 Code Review。如下图所示：从这个评审页面中，可以看到几个关键的信息，用于辅助评审人进行 Code Review。 基本信息：代码提交的时间、提交人，message，CommitId 以及要合入的分支。 代码变更：本次变更的文件以及每个文件变更的内容，可以在变更对比视图中清晰地展示出来，本次变更只修改了版本号。评审人可以在变更内容的位置添加行间评论，发表对此次变更的意见。 代码评审：本地提交需要人工评审 +2 后才能合入。目前已经有评审人 +2，并且状态显示为自动化任务检查通过，人工评审通过，可以合入。代码合入此次变更已经通过负责人的 Code Review，便可以点击“合入”按钮合入目标分支中。合入后，评审的状态显示为“已合入”。在 master 分支的提交历史中也会显示此次变更。这是百度效率云基于 Change Request 模型的代码评审过程。该模型可以有效管控提交代码的质量，因为每次提交都会生成一个 Code Review，如果代码变更存在问题，那么此次变更就不会合入代码库中，根本不会影响到代码库中的代码。总结百度效率云的 iCode 采用 Change Request 开发协作模型，能够在 git push 的时候进行代码评审，从而控制了提交入库的代码的质量。基于 Change Request 模型，iCode 平台提高了相应的提交规则设置，团队可以根据自己的需求开启和关闭代码评审。最后，通过一个实际操作演示了从代码提交、代码评审到代码合入的整个过程。百度效率云的 SaaS 服务已经部署在百度云上，感兴趣的同学可以试用一下。" }, { "title": "DevOps - 混沌工程", "url": "/posts/devops-15/", "categories": "DevOps", "tags": "DevOps", "date": "2020-03-25 15:33:00 +0000", "snippet": "随着软件工程不断发展，近些年，出现了一种新的实践 —— 混沌工程。它通过在生产环境中对系统进行破坏，来不断增强软件的健壮性。什么是混沌工程？《混沌工程原理》中这样定义：“混沌工程（Chaos Engineering）是在分布式系统上进行实验的学科, 目的是建立对系统抵御生产环境中失控条件的能力以及信心。”简而言之，混沌工程就是“故意破坏事物”的特殊方法，通过在生产环境中捣乱。比如随机重启生产环境中的服务器等，以发现生产环境中可能出现的隐藏问题；通过不断修复系统的缺陷，从而使系统更健壮、更具容错能力。这里强调的是混沌工程并不仅仅是“搞破坏”，因为搞破坏非常容易，但在搞完破坏后，能不能有效控制破坏的爆炸半径，能不能有效控制对用户造成的影响，以及判断该问题是否需要修复并寻找修复方法……这些才是混沌工程中最关键的。混沌工程和传统测试有很多重叠的部分。混沌工程应该是（成为）传统测试的补充，是经过传统测试后系统已经足够稳定，可以在生产环境中被任意“破坏”，来进一步增强系统的稳定性的工程。由于需要生产环境中的真实场景，这类测试是不能通过单元测试和集成测试来模拟的。混沌工程的核心思想是以可控的方式主动注入故障，以验证系统的行为是否符合我们的预期，并在不正常的情况下进行修复，以此提高系统的稳定性。实施混沌工程的必要性创建可靠软件是企业获取用户，赢得市场竞争的基础。特别是当我们的系统迁移到分布式架构，一些不可预知的问题时常发生。传统的测试只能保证软件的应用层的质量，无法保证应用程序以及各种服务或整个系统在任何情况下都能正常使用，不管是“正常情况”还是极端负载或异常情况。应用程序的任何异常都会影响用户体验。混沌工程可以主动测试生产环境中各种压力下的行为。通过比较假设行为和实际行为，我们可以在系统出现故障之前发现问题并修复问题。混沌工程可以做以下几件事情： 对软件和基础设施进行比传统形式更广泛的测试和验证； 发现传统测试无法发现的问题； 帮助团队了解系统在真实生产环境中的行为，服务如何被中断以及都有哪些Bug？因此，混沌工程可以帮助我们增强系统的稳定性和可靠性，带来更好的用户体验。如何实施混沌工程？混沌工程也是近几年出现的一个新的工程实践，目前只是在少数大公司里实施，如 Google、Facebook、阿里巴巴等。那么，如何在企业里实施混沌工程？可以通过下面几个步骤来实施混沌工程。建立基线指标在进行混沌工程实验之前，要先收集一组基线指标数据。这些指标包含基础设施的监控指标、告警指标、严重级别指标、应用程序指标等，具体如下： 基础设施的监控指标：包含服务器的CPU 峰值、IO峰值、磁盘使用率、内存使用率，网络的延迟、数据丢包率、DNS 等指标。 告警指标：可以按服务统计每周的告警数量，处理告警的时间，以及每种服务每周最频繁的告警类型。 严重级别指标：可以按服务统计每周不同严重级别的事件数量，以及按服务统计每种严重级别的 MTTD（平均检测时间）、MTTR（平均故障恢复时间）和MTBF（平均故障间隔时间）。 应用指标：应用程序的可观察性指标，事件数量，请求的响应时间，数据库连接数，QPS（每秒查询数量），TPS（每秒事务数量）。模拟真实事件在生产系统中模拟真实事件来进行实验，有两种方式：攻击和场景 攻击：将故障注入系统中，如消耗计算资源、关闭系统、丢弃网络包等方法，攻击是就是单个的故障注入方式 场景：将一组攻击保存的集合。场景中的攻击按顺序执行，可以更好地控制攻击的执行方式，并可以模拟较为复杂的故障。保存下来的场景可以被重复执行，并能够观察系统随着时间的行为变化。不管使用哪种方式，在执行完成后，需要记录上述指标的观察结果并与基线进行比较。分析结果基于从实验中获得的结果数据与假设进行比较，并得出结论。这里有几个问题需要给出答案： 系统行为是否符合预期？ 如果系统有监控告警等系统，是否按预期运行？ 本次实验发现了哪些新问题？ 告警系统多长时间检测到问题并发出通知？该时间是否可以接收？ 实验结束后，系统是否自动恢复到正常状态？还是需要人工干预？重复实验修复问题后，重复执行该实验以确保问题得到彻底解决。如果系统成功抵御了攻击，说明该问题已经被修复。此时，应该考虑增加攻击的程度，爆炸半径或者一次性攻击目标的系统数量。这对于测试集群系统、自动扩展系统或负载均衡系统比较有用。自动化实验一旦系统能够抵御该攻击，就可以按照常规测试惯例定期执行攻击。可以将该实验的执行嵌入到 CI/CD 流水线中，这样有利于新的变更不会引起新的可靠性问题。下图显示了可以在软件生命周期中执行不同类型的混沌实验的各个阶段。只要有设计良好的混沌实验，就可以在每次执行流水线时都会执行这些混沌实验。这一步的目的是通过在生产之前或者在生产中引起问题之前发现实际问题。混沌工程案例将 Chaos Monkey 集成到 Spring Boot 应用程序中。SpringBoot 集成 ChaosMonkeyNetflix 不仅制定了《混沌工程原理》，还提供了一个将理论付出实际的强大工具：ChaosMonkey。ChaosMonkey 是一种工具，该工具会随机终止生产环境中运行的虚拟机实例和容器，使工程师能够构建更加弹性的服务。Spring Boot 是目前构建 Java 后台应用程序最受欢迎的框架。Spring Boot Chaos Monkey 是一个依赖库，可以将混沌工程的实践集成到 Spring Boot 的应用中。只需要下面两步就可以将 Chaos Monkey 添加的应用程序中。STEP 1：在应用程序中添加 ChaosMonkey 的依赖包。&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;de.codecentric&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;chaos-monkey-spring-boot&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.2.0&amp;lt;/version&amp;gt;&amp;lt;/dependency&amp;gt;STEP 2：在启动应用程序的时候，需要激活 chaos-monkey的profile 来初始化 ChaosMonkey。java -jar chaosmonkeyforspringboot.jar --spring.profiles.active=chaos-monkey启动后，就会在控制台中打印出 Chaos Moneky 的字样。ChaosMonkey 配置Chaos Monkey 在引入后并未开启，需要通过 chaos.monkey.enabled 配置项来开启。Chaos Monkey 提供了四种不同的攻击方式： 延迟攻击； 异常攻击； 杀掉应用程序攻击； 内存攻击。这种攻击的开启和关闭可以通过下面四个配置项决定，并且每种攻击方式也有相应的配置参数。比如，延迟攻击是在每个请求处理时添加随机的延迟时间，该值由 chaos.monkey.assaults.latency-range-start 和chaos.monkey.assaults.latency-range-end 两个参数的区间值来设置。chaos.monkey.assaults.latency-active=truechaos.monkey.assaults.exceptions-active=truechaos.monkey.assaults.memory-active=truechaos.monkey.assaults.kill-application-active=trueChaosMonkey 的配置项清单可以通过 Spring Boot Actuator 的访问端口查看，首先需要通过下面两个配置项开启并将 chaosmonkey 添加到暴露的端口列表中。management.endpoint.chaosmonkey.enabled=truemanagement.endpoints.web.exposure.include=health,info,chaosmonkey在地址栏里输入 http://localhost:8080/actuator/chaosmonkey 可以看到如下配置项清单：{ &quot;chaosMonkeyProperties&quot;: { &quot;enabled&quot;: true }, &quot;assaultProperties&quot;: { &quot;level&quot;: 5, &quot;latencyRangeStart&quot;: 1000, &quot;latencyRangeEnd&quot;: 2000, &quot;latencyActive&quot;: true, &quot;exceptionsActive&quot;: true, &quot;exception&quot;: { &quot;type&quot;: null, &quot;arguments&quot;: null }, &quot;killApplicationActive&quot;: true, &quot;memoryActive&quot;: true, &quot;memoryMillisecondsHoldFilledMemory&quot;: 90000, &quot;memoryMillisecondsWaitNextIncrease&quot;: 1000, &quot;memoryFillIncrementFraction&quot;: 0.15, &quot;memoryFillTargetFraction&quot;: 0.25, &quot;runtimeAssaultCronExpression&quot;: &quot;OFF&quot;, &quot;watchedCustomServices&quot;: null},&quot;watcherProperties&quot;: { &quot;controller&quot;: false, &quot;restController&quot;: false, &quot;service&quot;: true, &quot;repository&quot;: false, &quot;component&quot;: false }}测试示例项目在 Chaos Monkey 的设置里开启 chaos.monkey.assaults.exceptions-active=true ，添加一个测试的 Controller 类，如下：@RestController@RequestMapping(&quot;/v1/test/chaosmonkey&quot;)public class OrderController { @Autowired private OrderMapper orderMapper; @GetMapping(&quot;/orders&quot;) public List&amp;lt;Order&amp;gt; getOrders() { try { return orderMapper.selectAll(); } catch (Exception e) { e.printStackTrace(); return null; } } }当调用该接口时，会随机产生异常。下图是使用 postman 批量调用该接口产生的结果，可以看出该接口执行了 10 次，其中成功 8 次，失败 2 次。这 2 次失败就是因为 Chaos Monkey 导致的。在服务的后台日志中也打印出来异常信息，如下图所示。从日志可以看出，该 RuntimeException 是由 Chaos Monkey 抛出的。混沌工程的落地离不开工具或平台，Spring Boot Chaos Monkey 是一个不错的开源项目，可以应用在企业内部的故障演练中，暴露服务本身以及服务与服务之间的调用问题，提升系统的健壮性。总结混沌实验可以在软件开发生命周期的多个阶段进行开展，尽可能在部署到生产环境之前做尽可能多的测试，减少部署到生产环境中出现问题的风险。当有些测试场景无法在测试环境中模拟时，需要在生产环境中进行实验，此时对应用程序来说也是最大的挑战。在生产环境中进行混沌实验时，务必要进行充分的设计和回滚方案的制定，以及对故障产生的影响范围的把控，以为真的对业务系统造成破坏。" }, { "title": "DevOps - 部署流水线(14)", "url": "/posts/devops-14/", "categories": "DevOps", "tags": "DevOps", "date": "2020-03-23 15:33:00 +0000", "snippet": "部署流水线，是搭建一套从开发到测试，再到运维的流水线，能够实现一键式将软件部署到生产环境。什么是部署流水线？软件开发过程是一个将客户或用户的想法变成一个真实可用的特性的过程。部署流水线是这个过程中的一部分，是指软件从版本控制库到用户手中的自动化展现形式。这一过程包括具体包括： 编译构建 代码检查 上传制品库 测试 部署等 上述这些阶段的执行都应该是自动执行的。自动化带来的好处是使这个流程变得更快、可重复且更加可靠。部署流水线主要包含下面几个阶段： 提交阶段，代码提交后会运行代码编译、自动化单元测试以及对代码进行动静态扫描 自动化测试阶段，主要是从功能性和非功能性方面验证系统是否可用，并且满足用户需求 手工测试阶段，主要是用户验收测试，用于验证系统是否为用户提供了价值 发布阶段，将软件交付给用户，交付方式有多种，可以是独立部署的方式，也可以是 SaaS 服务的方式部署流水线落地笔记为了更好地发挥部署流水线的作用，在实施部署流水线的时候要遵循以下几个准则：一包到底一包到底是将软件从源代码编译构建出的部署包，在后续的流程中都统一使用该部署包。这样做的好处有以下两点： 减少了编译时间：每次编译都需要花费时间，并且占用编译机的资源，如果代码库比较大，重复编译将是一场灾难。 保证部署包的一致性：由于各阶段进行测试的部署包都是同一个，这样可以保证部署到生产环境中的部署包与前面测试阶段验证过的部署包是完全一样的。保证每次部署的包都是同一个的做法是：之前的流水线中，每次生成部署包的同时也会生成一个 md5 值，后续每次部署时利用这个 md5 值对部署包进行验证（这目前大多数企业中部署时采用的一种方法）另外，对于每次构建出的部署包还需要存放到专门的制品库中，如 Jfrog Artifactory。这一步骤可以集成到持续集成流水线中，上传制品库时同时也会携带该部署包的属性，如：代码库、版本号、CommitId。Jfrog Artifactory 支持对制品打标签，可以在后续的测试阶段将测试结果以标签的形式打到制品上，作为是否进阶到下一个阶段的判断条件。相同的部署方式使用相同流水线、相同部署方式部署任意一套环境，包括生产环境！ 这样既能对构建和部署流程进行有效测试，提高部署流水线的稳定性和健壮性。又能保证不同环境的部署过程是一致的。当部署的服务出现问题时，可以排除部署脚本导致的因素。实际情况中，每套环境有很多不同之处，比如机器的 IP 会不同，操作系统和中间件的配置不同等。不同的环境信息并不意味着就要为每套环境都准备一套部署脚本，我们可以采用将部署脚本与配置信息分离的方法。在“环境管理”课时中提到，将部署脚本作为模板存储在 Git 代码库中，将每个环境不同的配置信息存储在 CMDB 中，就能实现通过一套部署脚本部署所有环境。采用相同的部署方式是降低发布风险的方法之一。因为不管是测试环境、还是生产环境都是相同的部署方式，在向生产环境部署之前，已经在测试环境部署了 n 次了，部署脚本已经非常健壮，能够大大降低向生产环境部署的风险。对部署冒烟测试在应用程序部署完成后，要有相应的脚本对应用程序进行冒烟测试，以确保应用程序启动并运行了。这个测试可以很简单，比如： 调用接口检查是否能正常返回 检查依赖的服务，比如数据库和缓存服务等，可以调用从依赖服务获取数据的接口，结果可以是空，只要能确保连接是正常的即可冒烟测试又称为部署测试，它是环境部署完成并交付使用的有效验证方法。如果服务不可用，也能知道是什么原因导致的不可用：是服务本身还是依赖的服务？这对排除应用程序无法正常运行问题也很有帮助。实现部署流水线的步骤下图是部署流水线的结构图，反映了真实的软件交付过程。起点是开发人员将代码提交到版本控制系统中，终点是将软件部署到生产环境交付给用户。上面介绍的部署流水线的相关实践，在该图中都有所体现。 版本控制系统和制品库是源代码和制品的单一可信数据源。制品库中的部署包只能通过版本控制系统中的源代码编译构建产出，并上传到制品库。后面测试环境和生产环境需要的部署包都来自同一个制品库，保证了部署包的唯一性。 测试环境和生产环境都是通过部署平台统一部署，测试环境可以由测试人员自服务部署，生产环境可以由运维人员一键部署。 不管是测试环境还是生产环境，当软件部署完成后，通过冒烟测试验证服务是否正常启动。 如下图所示，自动化部署平台提供一键部署的功能。用户只需要选择在哪个环境、使用哪个软件版本，采用什么样的部署策略即可。自动化部署平台自动化部署平台是部署流水线中的重要组件，通过封装统一的部署流程，提供易用的用户界面，对外提供统一的软件部署的能力。部署平台提供一套部署脚本，屏蔽测试环境和生产环境的差异。在进行自动化部署时，只需要提供部署的软件版本以及要部署的目标环境，采用什么样的部署策略（只对生产环境有效），点击“开始”按钮，所有的部署过程都是自动化的。关于“部署软件”和“发布软件”，很多人认为它们是一样的，它们的区别在于 “发布”是一个业务行为。部署软件是将软件部署到生产环境中，但部署的服务是否发布给用户是由业务决定的。目前很多企业中，部署和发布是相等的，软件部署到生产环境就直接发布给用户使用了。但有些功能这样做会有问题，特别是跟时间、位置相关的功能。比如促销活动，只能在固定的时间范围内有效。只不过目前的发布并不是由业务人员手动点击发布按钮触发，而是由程序自动化的触发。部署策略部署策略是针对生产环境的，因为生产环境是用户使用的真实环境，部署失败可能会对用户造成严重影响。因此，在部署生产环境时要采取低风险、零停机的部署方式。目前常用的方式有蓝绿部署、金丝雀发布和特性开关等方式。 蓝绿部署：蓝绿部署是指有两套相同的生产环境，一套叫蓝环境，一套叫绿环境。如上图中两种不同的颜色。假如当前用户正在使用作为生产环境的蓝环境。如果要发布一个新版本，先把该版本发布到绿环境中，并在绿环境中进行冒烟测试来检查服务是否可以正常工作。当一切准备就绪后，将用户引导到绿环境即可完成新版本的升级。如果此时出现问题，再将用户切换回之前的蓝环境即可完成回滚。这样不会对用户造成太大的影响。 金丝雀发布：金丝雀发布是指将新版本的服务部署到生产环境的一部分服务器中，如上图所示。通过一小撮用户试用的方式，可以快速得到反馈，及时的发现新版本中存在的问题，而不会影响大部分用户。如果新版本出现问题，只要不把流量引导到有问题的新版本上就行。另外，金丝雀发布的方式可以用来做 A/B 测试，将一部分用户引导至新版本和旧版本上，分别分析不同版本对用户、收入等指标的差异。 特性开关：特性开关是一种轻松开启和关闭功能的方式。当软件部署到生产环境中，此时该功能并未对用户开放，只有通过特性开关启用该功能时，用户才能使用该功能。如果此时发现问题，只需要将开关关闭即可。特性开关实现也比较简单，可以在代码中通过 if-else 的方式控制代码执行的路径。总结本文总结了使用部署流水线实现一键部署软件到测试环境及生产环境，通过自动化的方式完成软件部署的最后一公里。部署流水线并不是指用于部署，而是从代码提交到代码库到最终部署到生产环境的整个过程，包含代码扫描、自动化测试和环境管理等阶段。为了更好地实施部署流水线，通过自动化部署平台完成部署这个阶段的操作。最后，零停机发布到生产环境的几种部署策略，实现低风险发布。部署流水线是 CICD 的重要组成部分，也是实现持续部署的重要环节。这一部分在工作中属于运维侧平台，随着 DevOps 的普及，开发和运维的界限越来越模糊，开发人员可以利用运维开发的部署平台自服务，根据业务需求随时发布软件到生产环境" }, { "title": "DevOps - 自动化测试 (13)", "url": "/posts/devops-13/", "categories": "DevOps", "tags": "DevOps", "date": "2020-03-20 15:33:00 +0000", "snippet": "持续集成保证软件处于可工作状态的实践，而实施持续集成有一个必不可少的步骤——测试。 只有尽可能全面的测试覆盖，才能降低软件出错的概率。基于人工来完成的测试环节，测试周期长，覆盖率低，使得持续集成的效果并不明显。而自动化测试，是将人工完成的测试尽可能地自动化，从而达到减少测试时间，提高测试覆盖率，提高软件质量的目的。什么是自动化测试？自动化测试不是指自动化生成测试代码，而是自动化地执行由开发人员或测试人员编写的测试代码。正如谚语：绝不要手工去做任何可以被自动化处理的事情。——Curt Hibbs像之前是人工点击页面上的按钮进行功能测试，人工设置不同的性能参数进行性能测试，都可以通过脚本或测试平台自动化执行。这类测试包含： 单元测试 组件测试 集成测试 验收测试 非功能性测试，如安全测试和性能测试。但有些测试还是需要人工介入，比如用户体验测试、探索性测试等。自动化测试优点？ 节约时间和降低执行成本：在软件开发全生命周期中，测试是一个非常频繁且重复的活动。每次提交代码之后，都需要进行测试以确保新的代码变动不会受到影响。在每次软件发版之前，也需要进行系统的回归测试。一旦自动化测试建设完成，就可以做到无人值守运行，甚至可以在多台机器上并行执行。自动化测试大大缩短了测试的时间。 减少出错概率，提高准确性：自动化测试每次执行时都会执行相同的步骤，并且每次都会生成详细的测试报告。这些测试报告不受“人”的因素影响。手工测试容易受个人经验和情绪的影响，容易出错，人员的流动又使得测试知识无法沉淀。因此，自动化测试可以减少出错率，提高准确性。 提升测试覆盖度：自动化测试可以增加测试的深度和范围，从而提高软件质量。比如，由于自动化测试的速度很快，可以在很短的时间里执行数千个测试用例，从而提高测试的覆盖度。 加快反馈效率：自动化测试在每次提交代码之后自动触发，并将测试结果通知到团队中的开发人员，大大缩短了开发人员获得反馈的时间。 模拟手工无法测试的场景：自动化测试可以模拟成千上万用户并发访问的场景，这样的测试场景是手工测试无法模拟的。总之，自动化测试通过快速的批量执行测试用例，减少测试的时间，加速反馈回路，提升软件的质量。另外，使用自动化测试执行那些重复性较强的工作，可以让团队成员有更多的时间研究更有挑战性和更有价值的活动，提高团队工作效率。自动化测试的实现通过与持续集成流水线进行集成，能够实现代码提交后触发自动化测试，实时反馈测试结果。对于一个企业来说，实施自动化测试的步骤如下：STEP1：定义自动化测试的范围。在实施自动化测试之前，先确定哪些类型的测试可以被自动化。根据 Brian Marick 提出的敏捷测试四象限，不同类型的测试可以分为下图几种。敏捷测试象限表明不同类型的测试有不同的目的，主要维度包括： 面向技术 面向业务 面向支持团队 面向评价产品然后根据这几个维度按不同的组合分为四个象限，敏捷测试中涉及的所有测试类型都可以归类到这四个象限中。第 1 象限：面向业务、评价产品的测试 参与人员：业务人员和测试人员执行的测试 测试类型：探索性测试、情景测试、可用性测试和用户验收测试等 测试目标：验证产品功能是否满足业务和终端用户的需求 以手工测试为主第 2 象限：面向业务、支持团队的测试 参与人员：测试人员执行的测试 测试类型：功能测试、用户故事测试、原型测试、模拟测试等 测试目标：验证一个功能或者用户故事是否按验收标准正确实施 以自动化测试为主，手工测试为辅第 3 象限：面向技术、支持团队的测试 参与人员：开发人员执行的测试 测试类型：单元测试、模块测试、集成测试等 测试目标：验证单元模块被正确地实施 以自动化测试为主。第 4 象限：面向技术、评价产品的测试 参与人员：测试人员执行，项目团队配合的测试 测试类型：性能测试、负载测试、安全性测试和其他非功能性测试 测试目标：验证产品是否符合非功能性要求 以自动化测试和手工结合的方式。由此得出，自动化测试的范围应该在 2，3，4 象限。敏捷测试四象限虽然可以看到相对全面的测试总览，但对于这些测试如何落地还是不够明确，比如哪种类型的测试要多做，要尽早做。STEP2：定义自动化测试的层次。敏捷专家 Mike Cohn 在 2003 年提出的测试分层金字塔。该测试金字塔分为三层： 底层是单元测试， 中间层是服务测试 上层是 UI 测试底层的单元测试需要做最多的测试工作，越往上的单元，测试工作越少。根据《谷歌软件测试之道》的经验，这三个层次对于精力投入的比例是。 70% 的精力放在单元测试，20% 放在服务测试，而剩下 10% 放在 UI 测试 {: .prompt-warning }但从整个测试的过程来看，自动化测试不仅仅是跟代码相关的测试，不仅仅是测试执行过程的自动化，还应该包含测试数据和测试环境的自动化，也称为基础设施的测试。根据经验来看，测试数据和测试环境的准备时间占据了整个测试过程的将近一半的时间。所以可以通过自动化的方式提高基础设施准备的效率。根据上面的测试分层，从自动化的角度实施不同层次的测试。每一层测试的内容和测试的工具各不相同，比如：基础设施层 准备用于自动化测试的数据和环境 使用自动化或者基于容器的方式进行构建 常用的工具有Ansible、Chef、Puppt、Jenkins 等单元测试层 针对代码的方法、类和包进行测试 这些测试属于代码级的测试，与企业内部的持续集成流水线集成 常用的工具有 xUnit 系列工具。服务测试层 针对服务之间的接口进行测试 这些测试一般是服务接口之间的交互测试 常用的工具有 Postman、SoapUI 等UI 测试层 该层主要针对界面上的功能测试 这些测试一般是在一个或多个应用里进行端到端的流程测试，且应关注重点功能 常用的工具有 Selenium、Appium 等STEP3：与持续集成流水线集成。前面两个步骤确定了自动化测试的范围、分层以及需要使用的工具。第三步就是要搭建自动化测试平台并与持续集成流水线进行集成。下图是持续集成流水线及自动化测试相关的流程图：上图涉及的测试流程是： 开发人员提交代码到 Git 仓库或进行分支合并操作。 持续集成服务器接收到合并事件后，触发编译构建、单元测试等检查，并将测试结果通知给开发人员。 上述检查通过后，部署到 SIT 测试环境中。该环境为集成环境，部署了该服务所依赖的其他组件。当服务部署时即可将 API 接口注册到 API 管理平台，并执行服务之间的 API 接口测试，验证服务集成是否有问题，随后将测试结果通知给开发人员。 SIT 测试环境完成测试并达到进阶要求时，即可进入 UAT 测试环境进行用户验收测试。该环境测试主要通过自动化测试平台完成该服务的功能测试。自动化测试平台包含了测试用例管理和测试数据管理。该步骤可以进行精细化地测试策略管理，可以根据代码关联的需求完成该需求相关的测试用例的测试，可以每天晚上执行全量的回归测试。 SIT 环境和 UAT 环境等基础设施的管理可以通过环境部署平台完成。环境部署平台可以根据自动化测试的要求，进行定制化的部署和优化设置，并对基础环境进行先决条件检查，确保自动化测试执行之前满足环境的要求。 冒烟测试主要是测试软件的基本功能，这个概念是来自电路板测试，当电路板做好后，首先会加电测试，如果没有冒烟就进行其他测试，如果冒烟了，就需要重新制作。软件的冒烟测试一般是在开发人员开发完成之后，测试人员进行测试之前，如果冒烟测试没通过，就说明软件的基本功能没通过，就不需要进行后续的测试了。" }, { "title": "DevOps - API 管理(12)", "url": "/posts/devops-12/", "categories": "DevOps", "tags": "DevOps", "date": "2020-03-16 15:33:00 +0000", "snippet": "RESTful API目前最常用的 API 架构风格是 REST API。它没有严格的标准，但提供了一些要遵循的指导原则和约束条件。Roy Fielding（罗伊·菲尔丁）在它的博士论文中描述了这些限制，并创造了Representational State Transfer（代表性状态转移） 这个名字。REST 依赖于无状态、可缓存和客户端和服务器端通信协议（如 HTTP）。通过遵循 REST 的原则并将其应用于 HTTP 等无状态协议，可以构建出从任何设备和操作系统调用的 API 接口。API 接口是客户端与服务器端通信的桥梁，设计良好的 RESTful API 是微服务时代企业应用程序成功的必备条件。REST的原则下面是构建 RESTful 架构的几个限制条件。 统一接口描述： 用于定义客户端和服务器端之间通信的契约 该契约规定了交互的机制和格式以及客户端如何访问服务器端资源 只要遵循该契约，客户端和服务器端的应用程序就可以独立开发，有利于构建前后端分离的架构 客户端-服务器端： 客户端和服务器端这种调用约束构建了松耦合和可扩展的 Web 体系结构 只要客户端和服务器端都遵循统一的接口，就可以独立开发，不管使用的是何种开发语言。 无状态： 无状态是 RESTful 服务的重要原则 规定 Web 服务器不需要记住客户端的状态 状态信息可以作为变量包含在 URL 中，也可以作为查询参数、Header 参数或者请求体 服务器处理后，通过 Header 和响应体返回给客户端 这样做有助于减少服务器端维护和传输客户端状态的负担，从而提高服务器端的可伸缩性。 可缓存性： 缓存是 RESTful 服务的另一个原则 提高了服务器端应用程序的可伸缩性和整体性能 一般情况下，服务器端可以指定缓存响应的有效期，根据有效期，客户端决定是使用缓存响应还是单独请求获取实时数据 缓存响应数据可以提高客户端的响应速度。 分层系统： 是指在客户端应用与服务器端之间增加一个中间层，一般为代理或网关 用于实现安全性、缓存、流量限制、负载均衡等。 在中间层实现缓存和负载均衡可以提高系统的可伸缩性。 构建 RESTful API 的最佳实践API 的设计应该简单而直观的，这样才能更容易的使用。下面总结了一些设计 RESTful API 的简易方法。 使用 RESTful URL 设计 API 根据资源的逻辑分组设计 API API URL 应指向资源/子资源的集合或者集合中的单个实体 例如 /customers 指向了客户集合，而 /customers/{customerId} 指向了客户集合中的单个客户实体 使用 HTTP 动词对资源执行 CRUD 操作 使用 POST 用于创建新资源 使用 GET 用于读取资源 PUT 用于更新资源 DELETE 用于删除资源 另外，可以考虑使用 PATCH 对部分资源更新 当 HTTP 动词无法映射到操作时，使用 URL 中的操作 有时候，对资源的操作无法映射到 HTTP 动词 比如：register、activate 等操作不能直接映射到 HTTP 动词，这些操作可能适用于单个资源或者一组资源集合 此时，可以将该操作视为 URL 中的一个子资源或者一个资源的一个动作 例如：/customers/123/activate 或者 /customers/123/actions:activate API 版本 版本控制提供了平滑升级和迭代 API 的方法，可以同时支持多个版本的 API，这样为客户端升级到新版本和注销旧版本提供了时间 最常用的版本控制的方法是在 URL 路径中包含版本信息 API 管理平台架构图如今，用户会通过多种类型的数字终端来访问我们的数据和服务。为了满足用户的需求，需要以敏捷、灵活、安全和可扩展的方式提供数据和服务。API 是对外提供数据和服务的通道 它允许应用程序可以使用像 HTTP 这样轻量级的协议实现通信 随着 API 的不断增加，对 API 的管理就变得格外重要。API 管理包含的功能，如下：下面是API 管理平台的架构图。结构图所示包含两个部分：API 管理控制台和API Gateway。 API 管理控制台：包含 API 声明周期的管理、API 开发者门户等功能。API 创建完成后通过自动化的方式注册到 API Gateway 中并在管理控制台中以待测试的状态显示。测试完成的 API 以待发布的状态显示，发布完成的 API 以已发布的状态显示。在管理控制台中能清晰的展示 API 的提供者、API 的状态、API 的测试报告等信息。API 管理控制台的用户为 API 的使用者、提供者和管理者。 API Gateway：负责转发其他服务的 API 请求并对请求做拦截处理。API 部署到环境中时会注册到 API Gateway 中，其他客户端可以通过 API Gateway 调用后台的服务，完成 API 接口的自动化测试和性能测试。API Gateway 的调用者为 App、Web 或小程序等。API 生命周期管理下面是 API 生命周期管理。该功能提供了 API 从创建、测试到发布的全过程以及后期迭代的版本管理。API 管理员可以管理 API 的使用策略、调用者、调用者权限等，比如只有已发布的 API 才能被其他调用者使用。为了防止 API 调用的混乱，只有具备权限的调用者才能调用等。API 创建开发人员能够设计 REST API 接口，有两种方式可以设计并创建 API。 基于 API 设计器生成 API。该方法主要在项目初期使用。API 的设计可以帮助梳理项目的功能和实现逻辑，并基于设计的 API 自动化生成代码。目前比较成熟的工具是 Swagger Editor。Swagger Editor 可以在浏览器中使用 YAML 编写服务 OpenAPI 规范的 API 文档，并能够实时预览文档以及自动化生成代码。Swagger Editor 是一个开源工具，可以集成到 API 管理平台。 从代码扫描生成。该方法主要在项目中后期使用。在项目迭代过程中，会不断更新现有的 API 接口和增加新 API 接口。此时最好的更新 API 的方法是自动化扫描，这样就能保证生成的 API 接口与代码中是一致的。目前常用的工具也是 Swagger，可以在项目中添加依赖和配置即可生成。API 测试开发完 API 后，在持续集成阶段会将 API 接口同步到 API 管理平台，当前状态为新增或待测试。API 的测试包含几个部分： API 的接口逻辑 API 文档 API 的性能测试API 接口逻辑API 接口定义了客户端和服务器端通信的契约。包含所需的输入参数和 API 的预期响应。在进行 API 接口测试时要测试不同参数的组合对结果的影响，包含正确和错误的情况。以下面查询产品信息的接口为例：https://api.devops.com/v1/products 该接口会接收多个可选参数作为输入，包含 category、name 等，比如以下。https://api.devops.com/v1/products?category=computer在进行该接口测试时的测试用例有： 当不传任何参数时，接口的默认行为是什么？ 当传入正确的参数、正确的值时，接口的行为是什么？ 当传入的参数名不正确时，接口的行为是什么？ 当传入的参数不包含值时，接口的行为是什么？ 当传入的参数值不正确时，接口的行为是什么？…API 文档的测试API 测试时需要验证 API 接口文档是否是正确且最新的。当发布新版本的 API 时，应更新 API 文档到当前版本。API 文档应该和 API 一样是有版本控制的，并且 API 文档应该是自动化生成，这样既能保证 API 文档与代码的一致性，也能保证 API 文档的正确性。目前常用的 API 文档生成工具，还是 Swagger。通过在当前项目中引入下面依赖。&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;io.springfox&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;springfox-boot-starter&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.0.0&amp;lt;/version&amp;gt;&amp;lt;/dependency&amp;gt;并添加如下 SwaggerConfig 类即可生成 API 文档。@Configurationpublic class SwaggerConfig { @Bean Docket docket() { return new Docket(DocumentationType.OAS_30) .select() //配置需要扫描的controller位置 .apis(RequestHandlerSelectors.any()) //配置路径 .paths(PathSelectors.any()) //构建 .build() //文档信息 .apiInfo(apiInfo()); } private ApiInfo apiInfo() { return new ApiInfoBuilder().title(&quot;DevOps OpenAPI&quot;).version(&quot;1.0.0&quot;) .description(&quot;&amp;lt;p&amp;gt;DevOps OpenAPI 文档描述，此处省略500字&amp;lt;p&amp;gt;&quot;) .termsOfServiceUrl(&quot;http://www.devops.com&quot;) .license(&quot;http://unlicense.org&quot;) .contact(new Contact(&quot;&quot;,&quot;&quot;,&quot;admin@devops.com&quot;)) .build(); }}如果要想检查每次提交有没有 API 接口的变更，可以参考微软 Azure 的一个开源项目 openapi-diff。API 性能测试API 不仅是对外提供获取数据的服务，并且能够在负载下很好的工作。大多数情况下，应用程序的整体性能取决于为应用程序提供基础的 API 性能。因此，API 的性能和负载测试就显得非常重要。API 性能测试的步骤如下。第一步：收集有关 API 的性能要求数据主要有： 每个 API 每秒请求数的平均吞吐量 在给定的峰值处理请求的最大数量 客户端应用程序预期的并发用户数，能够预测 API 在负载下预期处理的并发连接总数在决定 API 测试的性能要求后，在实际测试时就要以该要求为标准进行验证。如果未达到该要求，就要调整和优化 API 平台的参数，从而获得更好的吞吐量。如果通过优化仍然未达到要求，应该考虑更高配置的硬件设施。第二步：选择性能测试的方法 基线测试：该测试的目的是找出系统在正常预期负载下的表现，测试结果用于分析 API 响应时间的平均值和峰值。 负载测试：在负载测试期间，增加负载，以研究在不断增长的流量下 API 的性能。可以查看的指标有响应时间、吞吐量等。该测试的目的是了解预期的系统行为和处理预期峰值负载的能力。 压力测试：该测试的目的是找到平台的临界点，确定系统可以处理的最大吞吐量。当随着流量逐渐增加，直到性能开始下降或 API 调用的错误开始增加时便到达临界点。 浸泡测试：该测试的目的是指当系统长时间测试时是否有任何系统不稳定的问题。第三步：选择合适的工具或平台进行测试。目前有很多开源的工具可以执行 API 性能测试，其中比较常用的有 JMeter 和 LoadUI 等。API 发布API 经过测试达到可发布的要求后即可发布，发布后的 API 可以被其他客户端使用。版本管理当 API 发布后，开发人员就会根据 API 定义的契约在客户端应用程序中使用它。客户端应用程序是强依赖该 API 的，如果 API 发生变化，就会导致客户端应用程序不可用。随着时间的推移，API 的变化是不可避免的，比如有新需求可能会导致 API 的变化。因此，需要通过版本管理的方式降低 API 的变化对客户端的影响。API 版本的原则有： API 版本不应该破坏任何现有客户端； 尽可能少频率的变更 API 的主版本号； 进行向后兼容的变更，避免生成新的 API 版本； API 版本不应该与软件版本直接挂钩。API 版本的管理方法。 使用 URL 管理：API 通常由 URL 标识，所以在URL中加入版本控制是有意义的，也是目前最常用的方法。比如 https://api.devops.com/v1/products https://api.devops.com/2018/products 等； 使用 HTTP Header 管理：在 Header 中指定 API 的版本，比如使用 X-API-Version 的 header 标识。 使用查询参数管理：在查询条件中指定版本，比如 https://api.devops.com/products?version=2这几种方法每种方法都能实现 API 版本控制的目的，采用任何一种方法都需要与客户端进行协商，客户端也需要采用该方法调用 API，这个机制是要统一的。 DevOps的目标是提高软件的效率和质量。在微服务时代，系统之间的交互都是通过 API 实现，因此 API 的质量、版本控制和测试的效率 对软件的整体交付影响是很大的。" }, { "title": "DevOps - 持续集成(11)", "url": "/posts/devops-11/", "categories": "DevOps", "tags": "DevOps", "date": "2020-03-12 15:33:00 +0000", "snippet": "什么是持续集成？持续的意思并不是“始终，一直”，它的意思是“随时”。比较恰当的频率是：每当有人提交代码，同时集成一次。“持续集成”实践最早来自 1996 年 Kent Beck（肯特·贝克）提出的极限编程方法（XP）。他在《解析极限编程——拥抱变化》一书中给出了一个简短的说明：“持续集成是每天多次集成和生成系统，每次都完成一个构建任务”。在 2006 年 5 月 1 日，Martin Fowler（马丁·福勒）在其个人官网给出了相对正式的定义： 持续集成是一种软件开发实践，团队成员频繁地将他们的工作成果集成在一起。通常是每人每天至少提交一次，这样每天就有多次集成。每次提交后，自动触发一次包含自动化测试的构建任务，以便能尽早发现集成问题。通过这种方式，许多团队大大减少了集成阶段的问题。由此可以看出，持续集成是一种质量反馈的机制，能够尽早地发现代码中的问题，并提前解决问题。为什么要做持续集成？众所周知，软件开发有一个特点，就是在没有开发完成之前，很长一段时间内是无法运行应用程序的。一般都是前期大家开发各自的功能模块，最后在集成阶段将功能集成在一起，进行验收测试。这样做就会导致几个问题： 由于长期在各自的分支上开发，导致在集成阶段合并分支时产生大量冲突，无法合并； 由于之前并未进行过任何集成，导致在集成阶段耗时太长，或者根本无法集成； 由于之前并未进行过任何测试，导致系统集成后发现并不满足要求。解决上面问题的最佳方法就是持续集成。目前在谈到集成时，主要有以下两种集成方式： 即时集成：是指团队成员每次提交之后就进行集成，并执行编译、构建、自动化测试等任务来检查个人提交的代码是否可用 这种集成方式要求执行的时间要短，要快速反馈结果，因此只能执行一些简单地测试。 定时集成：类似每日构建（Daily Build），是指每天定时（一般为晚上）自动执行一次集成过程，第二天将执行结果发送给关系人 -这种集成方式注重的是检测的全面性和彻底性，对执行时长要求不高。持续集成一般是指即时集成，但定时集成在某些场景下也是非常有用的。比如，比较耗时的集成过程。持续集成的工作流程持续集成服务器可以采用目前最受欢迎的 Jenkins。每次提交后具体的执行步骤是： 开发人员在本地工作空间提交代码到代码仓库； 版本控制系统通过 WebHook 等机制实时通知持续集成服务器； 持续集成服务器克隆最新的代码和构建脚本到服务器本地，或者专用的服务器； 在持续集成服务器或专用服务器上执行构建脚本，对最新的代码进行检查，包括编译构建、代码动静态扫描、单元测试以及部署到测试环境运行功能测试等； 运行结束后，自动生成执行结果报告； 将执行结果通过邮件等方式通知给开发人员。为了保证每次集成的效率，以上步骤都是通过自动化方式执行的。开发人员只需要专注于代码开发，提交后只需等待几分钟就能够收到执行结果。这里需要强调的是，虽然开发人员不需要关注除代码开发之外的事情，但对每次提交有要求，比如：要保证每次提交的是一个完整的功能项。代码只写了一半就提交，功能测试肯定是通不过的。如何实现持续集成实现持续集成需要一定的先决条件，也就是上图中的几个组成部分，包含版本控制系统、持续集成系统和自动化测试。版本控制系统版本控制系统用于存储和软件相关的所有内容。比如应用程序的源代码、数据库脚本、构建脚本和部署脚本等。目前采用的都是基于 Git 的分布式版本控制系统，如 GitLab、GitHub、Gitea 等。不管是自研的还是开源的，对于应用持续集成实践，版本控制系统提供了基本的版本控制能力，能够记录哪个版本的代码是通过测试的，哪个版本还存在问题，并对有问题的版本进行回滚。为了提高持续集成的效率，当代码提交后，都会采用实时通知机制。目前的版本控制系统都提供了基于 WebHook 或基于消息的通知机制，当不同类型的事件发生后，就会通知注册方。持续集成系统虽然持续集成实践中并未要求一定要有个持续集成的工具。但工欲善其事必先利其器，使用持续集成工具可以达到事半功倍的效果。目前常用的持续集成工具有开源的 Jenkins，功能比较完备，基于插件体系可以与构建和部署领域的很多工具进行集成。持续集成系统与版本控制系统集成时，一个关键点就是如何触发构建。Jenkins 提供了多种触发策略来满足不同的需求，重点如下： 定时构建：提供了类似 cron 的功能来定期执行。一般主要用于像每日构建/每周构建这样定期执行的构建。 Build when a change is pushed to GitLab：当 GitLab 发生变更后触发构建，该选项需要添加 GitLab 的 WebHook，当事件发生时才能进行通知。 轮询 SCM：通过轮询的方式检查版本控制系统是否发生变更。该选项是一个成本很高的操作，因为每次轮询都需要 Jenkins 扫描整个工作空间并与服务器进行验证。自动化测试持续集成的目的是提前发现代码中存在的问题，保证软件是可工作的。如果没有全面的自动化测试，构建成功只能意味着应用程序能够编译通过，并不能保证软件的功能是正常的。因此，在持续集成流程中，自动化测试是必须的。有三类测试需要加入持续集成的流程中，分别是单元测试、集成测试和验收测试。 单元测试：用于单独测试应用程序中某些小单元的行为，比如一个方法、一个函数。通常不需要启动整个应用程序就可以执行，而且也不需要连接数据库、文件系统和网络。 集成测试：用于测试应用程序中几个组件的行为。与单元测试一样，通常也需要启动整个应用程序，但有可能需要连接数据库、访问文件系统或其他外部系统或接口。 验收测试：用于验证应用程序是否满足业务需求所定义的验收条件，包含应用程序提供的功能，以及其他非功能性需求，如容量、安全性等。验收测试通常要将整个应用程序运行于测试环境之中。 开发人员的要求 为了做好持续集成，除了上面提到的一些工具作为支撑，对每个开发人员也提出了一些要求，主要体现在以下三个方面。频繁提交正如在开头提到，持续的含义是“随时”。我也给出了最恰当的频率：只要有人提交就需要进行集成。对于持续集成来说，开发人员需要做的就是频繁提交代码到版本控制库中一，每天至少提交一次。这样做有几个好处： 提交得越频繁，越早收到集成的反馈； 每次提交的变更很小，就很少会导致构建失败； 即便是导致构建失败，也很容易知道是哪里的问题，很容易修复或者回滚。在持续集成的实践中，建议采用“主干开发”的工作模式，因为这种模式才能真正做到持续集成。但在很多企业里都是采用 GitFlow 等基于分支的工作流模式来进行协同开发。因为每个开发人员都是在自己的分支上开发，并未与其他分支的代码实现实时集成。这里开发人员需要注意的是，不仅要频繁将代码提交到各自的分支，同时每天要至少一次将各自的分支合并到发布分支，进行全流程测试。提交一个完整的任务不要只为了频繁提交而随意提交代码，频繁提交不等于随意提交。保证每次提交的内容都是有意义的，都是一个完整的任务。比如要添加一个查询家庭地址的接口，要把该功能开发完成后一次性提交，而不是接口写了一半就提交了。这个要求和之前谈到的限制在制品数量是有关系，如果同时并行的任务数太多，就会出现在一次提交里同时包含多个任务的代码。这种情况提交的代码会严重影响测试效果。这种情况非常普遍，我正在处理一个 Bug 或者开发一个功能，已经改了很多文件。但这时需要处理其他更紧急的工作，如果在当前状态下修改就会出现同一次提交包含多个任务代码的问题。这里介绍一个方法来解决这个问题：可以使用 git stash 命令，保存当前对代码库做的更改，并恢复到上次提交的状态。当提交完穿插的任务后，再恢复到上一个更改的状态，继续完成该任务。这样就能保证每次提交都是一个完整的任务。下面是几个常见的命令：#保存未提交的变更$ git stash#查看已保存的清单$ git stash liststash@{0}: WIP on master: 8a15b20 New Message#恢复最后一次保存的记录$ git stash apply #恢复指定的保存记录$ git stash apply stash@{0}构建失败后立即修复当某个团队成员提交代码后导致构建失败，说明本次提交影响了软件的整体质量。这个时候，整个团队都不能再继续提交代码了，而是先处理构建失败的问题。这个实践也是应用了丰田生产管理系统（TPS）中的“立即停止原则”。其含义是：当生产线上已经发现了问题，就应该立即停止来解决问题。如果问题没有得到有效解决还仍然是生产线保持运行，则会生产出更多的残次品。同样的道理，对于软件团队来说，如果提交构建失败后没有立即修复，其他人再次提交新的代码，就一定会引起提交构建失败。那么，这次失败是上次失败导致的，还是新的代码导致的，就变得很复杂了。总结使用持续集成来解决软件在开发后期进行集成出现的各种问题。实践的思想就是快速反馈，通过尽早地集成并进行自动化的测试来发现代码中存在的问题，尽早修复。持续集成并不是一个工具，而是一个实践。通过工具、平台加快持续集成落地的效率，通过让团队成员对持续集成的准则达成共识，来提升持续集成的效果。持续集成在很多企业里都是最先被落地的，这里介绍了只是实现持续集成的思路和方法，欢迎你在评论区分享你的实践心得。 主干开发是指开发人员总是将代码提交到主干，而不是分支。这种情况下就能及时获得其他人的修改，确保所有的代码都被持续集成，Google就是采用的这种模式。 cron 是Linux下执行定时任务的程序。 “立即停止”的具体操作：对于软件开发来说，可以采取禁止他人提交的策略，只能由导致失败的人进行修复，修复后再对其他人放开。" }, { "title": "DevOps - 环境管理(10)", "url": "/posts/devops-10/", "categories": "DevOps", "tags": "DevOps", "date": "2020-03-10 15:33:00 +0000", "snippet": "由于业务需要，对环境的数量、环境部署的速度有很高的要求。因此，部署一套快速的交付可用的环境就变得格外重要。什么是环境管理？这里的环境指的是应用程序运行所需要的所有资源和配置信息。比如下面的配置信息： 服务器的硬件信息，比如 CPU 的类型和数量、内存大小、硬盘和网卡信息，以及服务器之间互联使用的交换机信息。 操作系统的信息，操作系统的类型和版本，操作系统的优化配置，如文件描述符的数量。 应用程序运行所需要的中间件信息，如消息中间件 Kafka 的版本及配置信息。 应用程序本身的版本、数据及配置信息。环境管理就是准备部署环境的过程以及部署之后对环境的管控。既能保证准备环境的快速和一致性，又使得部署后的环境能够有效利用。进行环境管理必要性？做这件事情的意义。在之前的开发过程中，环境部署基本都是研发人员手动完成的。手动部署环境会导致诸多问题，比如： 手动配置环境很容易出错。由于环境的配置项较多，哪怕是改错一个配置项，都会让整个应用无法启动，或者影响服务的性能。 手动配置环境非常低效。通常以周为单位，拖慢了开发、测试的进度。 手动配置环境不方便测试验证。如果出现问题，很难复现，因此给测试验证带来了很大的困难。因此，需要一种能够完全自动化的创建环境的过程，它带来的好处是： 降低了环境管理的成本和风险， 快速交付环境 自动化的过程使环境的部署是可重复的、时间是可预测的。环境管理的实现为了解决上面提到的问题，每个团队的实施方案并不完全一样。这就要根据落地 DevOps 的过程，采用因地制宜的方法。有的团队环境的数量、用途是固定的。SIT 测试环境就是用来做集成测试的；性能测试的环境就是用来做性能测试的。一般情况下，环境的物理服务器、操作系统是不会经常发生变化的，变化的只是上层应用程序以及依赖的中间件服务。这时可以完全采用基于 Git 的配置管理方式来快速创建环境。包含硬件物理服务器、操作系统、上层的中间件、应用程序在内的所有元素的管理。可以认为，这是从裸金属服务器如何搭建成一套可用的环境。这个场景一般是在基础设施团队或者云基础设施的场景中用的比较多。下图是该方案的实施示意图。这个方案主要包含下面三个内容： Git 仓库：用于存储创建环境的部署脚本，并能够实现版本控制。部署脚本中尽可能将变化的信息抽取成变量，在部署的时候从 CMDB 中获取。比如，在哪些服务器上部署，部署哪些服务，用哪个操作系统镜像等。因此，这里的部署脚本可以理解为模板。 CMDB：用于存储每个环境相关的配置信息。包含硬件信息，如物理服务器的信息，CPU型号和数量，内存大小，以及硬盘、网卡等信息；软件信息，如部署哪些应用服务以及服务本身的启动参数和运行参数（数据库连接池大小等）。为了实现模块化部署，每个服务可以设置是否安装等参数。 部署平台：用于从 Git 仓库克隆部署脚本模板，从 CMDB 获取指定环境的硬件配置信息和软件配置信息，用 CMDB 中的数据填充部署脚本模板中的变量。然后执行具体的部署任务，创建出符合要求的环境。这种基于 Git 版本控制系统存储脚本模板，利用 CMDB 存储个性化的配置项，最终通过部署平台进行组装的方式，可以快速地、可重复地创建一致性的环境。即通过这种方式，每次创建的环境 A 都是一样的。如果这个环境的配置信息发生改变，只需要在 CMDB 中进行变更即可，再次部署时就会应用到新的环境中。Git 仓库采用 Git 仓库存储创建环境的部署脚本的实践，和基础设施即代码的实践是一样的。现在有个通用的术语来表示环境——基础设施。基础设施即代码，就是基于软件开发实践的基础设施自动化的方法。通过对代码进行更改，实现构建和变更基础设施的一致性和可重复性。并通过自动化测试等实践验证基础设施的可用性。基础设施即代码是目前快速、可靠的构建高质量基础设施的重要方法。通过代码的方式来存储部署脚本的好处有以下三点。 可重用性：如果将一个环境定义为代码，可以用一份代码创建一个环境的多个实例。当发现问题时，也可以快速修复代码并重建环境。 一致性：通过代码创建的环境，每次都以相同的方式构建，保证了每次创建环境的一致性。因为都是相同的构建代码和构建方式，每次创建环境所需的时间是可预测的。 透明性：创建环境的代码每个人都可以查看，了解代码的构建逻辑，并提出改进建议。使得每个人都能学习、掌握创建环境中用到的技能，同时也提高了代码的健壮性。配置管理数据库配置管理数据库（CMDB）是用于存储跟硬件和软件相关的配置信息。从上面也能看出，环境中涉及的元素很多，配置项也很多，并且也会经常新增模型和配置项。因此，如果要让 CMDB 解决环境部署的问题，就需要考虑下面这几个问题： 如何建模才能包含环境中涉及的元素，以及元素与元素之间的关联关系？ 数据如何录入？能不能自动化？能不能自定义查询接口？ 如何与部署脚本集成，获取哪些数据？1. 模型架构。CMDB 中模型的设计采取了平铺的方式。不管是硬件模型，如机房、机柜、交换机、服务器等等，还是软件模型，如业务相关的具体服务、进程等，都统一采用模型进行定义。每个模型可以包含多个实例，模型与模型之间以及实例与实例之间都是可以建立关联的。在模型和实例之上是业务和环境。一个业务包含多个环境，模型属于业务，实例属于环境。如下图所示：2. 数据准备。CMDB 中数据的准备工作量非常大，完全依靠人工是不现实的。在数据准备这部分，采取了自动扫描和人工录入两种方式。 自动扫描：基于 ironic-inspector 收集裸机的信息，能够自动扫描服务器的基本信息，如 CPU 型号和数量、内存大小，硬盘和网卡的信息，以及服务器的网卡上联的交换机以及网卡信息。基于扫描的数据就可以形成该环境的完整物理拓扑。 人工录入：最初的初始化数据需要由人工整理导入到 CMDB 中，比如服务器所在的机房、机柜、IPMI 的 IP、IPMI 的账号和密码等，这些数据是自动扫描的基础数据。另外，和当前环境相关的业务数据会由人工录入或者基于当前值进行人工确认。比如，哪些服务属于哪个服务器组，服务器组包含哪些服务器等等。总之，数据准备的原则是减少人工操作、尽量自动化，提高数据准备的效率和准确率。该阶段完成后，创建环境需要的几个关键信息就比较清楚了： 该环境中的物理服务器信息； 该环境需要部署的服务信息； 服务部署在哪些服务器上的信息。3. 查询接口。CMDB 通过查询接口对外提供数据查询能力。在设计时 CMDB 很关键的一点是灵活性。主要体现在以下几点上。 模型：支持添加任意模型。 配置项：支持添加任意类型的配置项。 查询接口：支持查询任意模型下、任意实例的配置项及关联关系。CMDB 是配置管理数据库，对配置项的维护和查询需求是多种多样的。不用修改代码即可满足这种灵活性，这是一个难点，也是一个优势。目前我们基本上可以满足上面的灵活性，添加查询接口也可以在线添加。部署平台经过上面的步骤，创建环境所使用的部署脚本模板和相对应的配置项都已经准备好了，下面就通过部署平台将其进行组装，并执行具体的创建环境的任务。分为几个步骤：STEP 1：克隆代部署脚本模块$git clone https://xxxx/deploy.gitSTEP 2：调用CMDB接口并封装成部署所需要的文件对于搭建一套环境，部署脚本并不能直接使用的 CMDB 中数据，需要调用 CMDB 接口封装成部署所需要的文件，并将其放置于指定的位置。比如：1. 生成 ansible 的 hosts 文件，内容示例为如下[service-a]hostname-1 ansible_ssh_host=192.168.1.20[service-a:vars]ansible_ssh_user=rootansible_ssh_pass=&quot;123456&quot;deploy_type=&quot;docker&quot;2. 生成 /etc/hosts 文件，内容示例如下192.168.1.101 hostname-1192.168.1.102 hostname-2192.168.1.103 hostname-3192.168.1.104 hostname-4192.168.1.105 hostname-5STEP 3：执行部署脚本$sh deploy.sh通过上面三个步骤就能够创建一套环境。如果要变更服务的部署方式，比如：将 deploy_type 改为 K8s，只需要在 CMDB 中将该配置项改为 K8s，然后从 STEP 2 再次执行即可。这是因为，在部署脚本的模板里已经有了 K8s 部署的脚本，在 K8s 基础组件已经具备的前提下，不需要做其他任何改动，该服务就会以 K8s 的方式进行部署。最后，我建议一开始就考虑自动化，通过部署平台将整个环境部署的流程以自动化的方式实现，因为： 环境的变更比你想象得要频繁得多； 自动化执行环境配置可以保证从一开始就是一致的； 自动化帮助快速可靠的实施变更，速度造就了质量，质量保证了速度 应用程序的多版本，在基础环境一样的情况下，主要还是体现在应用程序本身的功能、配置、数据库、测试数据以及服务依赖的不同上面 将配置和制品分离，通过配置来构建不同版本的运行时环境。可以是Git或者CMDB的方式。比如，V1和V2,对于制品是两个不同的版本，对于配置也是两套不同的配置。" }, { "title": "DevOps - 配置管理(09)", "url": "/posts/devops-9/", "categories": "DevOps", "tags": "DevOps", "date": "2020-03-08 15:33:00 +0000", "snippet": "在软件开发过程中，当开发完成后，就会将软件进行编译打包发布到不同的环境。一般情况下，企业里的环境会分为 开发环境 测试环境 SIT 测试环境 UAT 测试环境 性能测试环境 生产环境每一套环境都有相对应的配置信息。最简单的办法是把配置信息打包到每个环境的部署包里，但这样每次都需要构建多个环境的部署包，导致编译打包时间较长且部署包体积较大，并且不能保证每个环境测试的软件的唯一性。在每个环境使用相同的部署包，通过配置项来管理每个环境的差异，既不会降低编译和传输速度，也能确保最终发布的软件就是经过测试过的软件。什么是配置管理配置管理最初是指版本控制，发展到现在已经超出了版本控制的范畴。在《持续交付—发布可靠软件的系统方法》一书是这样对配置管理进行定义的： 配置管理是指一个过程，通过该过程，所有与项目相关的产物，以及它们之间的关系都被唯一定义、修改、存储和检索。我们都知道，一个应用程序是由软件代码、运行数据以及配置信息共同组成。 在 《12 因素应用》（The Twelve-Factor App）中提到，应用程序的配置在不同部署环境（开发环境、测试环境、生产环境等）之间会有很大的差异。比如：数据库连接地址、缓存连接地址、第三方证书等。因此，需要将代码和配置分离，通过配置文件屏蔽各个部署环境的差异。《12 因素应用》中也推荐将应用的配置存储于环境变量中。环境变量可以非常方便地在不同的部署间做修改，却不用动一行代码。这里的配置主要是指应用程序的配置，配置管理主要是指如何存储不同环境的应用程序配置，以保证各个环境使用的都是同一份代码。配置信息的描述通常情况下，配置项以键值对的形式来表示，比如 spring.application.name=service-a，这代表一个配置项。应用程序使用配置文件来存储多个配置项，通过层级来组织配置项，特别是当以 yaml 格式展现时，层级会更加清晰，这也是目前采用最多的展现形式。除此之外还有 properties、xml 等形式。配置信息的存储配置信息比较常见的存储形式有数据库、版本控制库、文件目录、环境变量等数据库优点: 充分利用数据库的检索功能 按不同的条件查询配置项 可以维护多个环境的配置缺点： 需要对配置项进行建模，开发单独的配置管理系统用于管理数据库中的配置项 配置项的版本、版本的回退都需要单独维护。版本控制库优点： 充分利用版本控制库本身的特性 对配置项的变更进行版本控制和变更追溯 非常容易地获取任意时刻的版本以及版本的回退 不需要开发额外的系统 能管理多个环境的配置缺点： 不支持单个配置项的查询功能 每次更新都需要全量更新。文件目录优点： 直接从本地获取 不依赖于其他系统缺点： 不能有效地进行版本控制 每个环境都是单独的文件目录 不能有效管理和控制多环境下的配置信息。环境变量优点： 方便设置与读取 能够更好地与脚本集成，与每个环境绑定缺点： 不能有效地进行版本控制。以上几种存储形式中： 如果在脚本中需要一些全局配置可以使用环境变量 如果需要对配置项进行复杂的检索和版本控制，需要使用数据库和版本控制库存储配置管理的时机在应用程序生命周期中，不少阶段可以对应用程序进行配置，比如构建、部署、启动、运行和发布阶段。下面介绍下这几个阶段的是如何进行配置的。 构建阶段：在构建时，可以将配置文件直接添加到生成的二进制文件中。这种方式由于二进制文件与配置文件捆绑在一起，每个环境都需要生成一个单独的二进制文件，违反了12 因素应用中的原则，所以不推荐使用该方法。 部署阶段：在安装应用程序时，部署脚本或安装程序获取必需的配置信息。这种方式可以保证在二进制文件是同一个，遵循了 12 因素应用中将代码和配置分离的原则。 启动阶段：在应用程序启动时，将配置文件加载进来。该方式需要保证影响应用程序启动的配置信息能够获取，否则程序无法启动。 运行时阶段：在应用程序运行时，动态的变更配置文件。该方式主要用于在不需要停止服务的情况下变更配置信息。 发布阶段：是指在应用程序真正发布上线的时候，将配置文件改为生产环境的版本。该方式只是用于生产环境发布，由于此时发布的软件和之前测试的是同一个，因此测试通过是具备发布上线条件的。从上面可以看出，如果想实现一包到底的目标，是可以在部署阶段和启动阶段来获得环境专属的配置信息。配置管理的实现方式配置管理目前常见的实现方式： 有 Spring Boot 的 Profile 形式； 基于 Git 的配置管理； 配置管理系统如携程的 Apollo； 配置管理数据库 CMDB。这里主要介绍一下 Spring Boot 的 Profile 和基于 Git 的版本控制形式。这两种实现方式都是基于现有的框架和工具，实现和维护都比较简单。根据命名规范，后续代码中统一采用：dev、test、prod代表开发环境、测试环境和生产环境。Spring Boot 的配置管理前面提到，当将应用程序部署到不同的环境时，通常每个环境的配置是不一样的。Spring 框架自 3.1 版本就引入了对 profile 的支持。Profiles 是一种基于条件的配置，哪个 profile 被激活就使用哪个配置。在 Spring 中有两种方式表示配置信息。配置类：使用 @Configuration 注解，该类会在运行时生成一个或多个能够被 Spring 容器处理的 @Bean 对象。可以在配置类中添加 @Profile 注解标识属于哪个环境的配置，如下面代码所示。开发环境如下，@Profile(&quot;dev&quot;)@Configuration@EnableWebSecuritypublic class SecurityConfigDev extends WebSecurityConfigurerAdapter { ... }测试环境：@Profile(&quot;test&quot;)@Configuration@EnableWebSecuritypublic class SecurityConfigDev extends WebSecurityConfigurerAdapter { ...}生产环境如下：@Profile(&quot;prod&quot;@Configuration@EnableWebSecuritypublic class SecurityConfigPro extends WebSecurityConfigurerAdapter { ...}配置文件：配置文件是以 properties、yaml 为结尾的文件，一般会包含多个 application-${profile}.properties 的格式文件。比如以下代码：#开发环境application-dev.properties#测试环境application-test.properties#生产环境application-prod.properties当某个 profile 被激活时，相对应的配置就会生效，否则会被忽略。激活 profile 的方式有以下几种：1. 配置文件激活，是指直接在 application.yaml 配置文件中激活特定的 profile。spring.profiles.active=prod2. 构建时激活，是指在构建软件时指定要激活的 profile。以Maven构建为例，首先在 pom.xml 中定义 profile 配置：&amp;lt;profiles&amp;gt; &amp;lt;profile&amp;gt; &amp;lt;id&amp;gt;dev&amp;lt;/id&amp;gt; &amp;lt;properties&amp;gt; &amp;lt;activatedProperties&amp;gt;dev&amp;lt;/activatedProperties&amp;gt; &amp;lt;/properties&amp;gt; &amp;lt;activation&amp;gt; &amp;lt;activeByDefault&amp;gt;true&amp;lt;/activeByDefault&amp;gt; &amp;lt;/activation&amp;gt; &amp;lt;/profile&amp;gt; &amp;lt;profile&amp;gt; &amp;lt;id&amp;gt;test&amp;lt;/id&amp;gt; &amp;lt;properties&amp;gt; &amp;lt;activatedProperties&amp;gt;test&amp;lt;/activatedProperties&amp;gt; &amp;lt;/properties&amp;gt; &amp;lt;/profile&amp;gt; &amp;lt;profile&amp;gt; &amp;lt;id&amp;gt;prod&amp;lt;/id&amp;gt; &amp;lt;properties&amp;gt; &amp;lt;activatedProperties&amp;gt;prod&amp;lt;/activatedProperties&amp;gt; &amp;lt;/properties&amp;gt; &amp;lt;/profile&amp;gt;&amp;lt;/profiles&amp;gt;请注意 true 标签，该标签意味着在构建时如果未指定要激活哪个 profile，将使用 dev 作为默认配置文件。将 Maven 与 Spring Boot 的配置文件结合使用，可以在构建时指定某个 profile 的配置文件。在 application.yaml 里添加一个参数 spring.profiles.active=@activatedProperties@，该参数可以告诉 Spring 到底使用哪个 profile。在使用 Maven 构建时通过 -P 参数指定使用哪个 profile。如下构建命令会用 prod 值替换上面指定的参数，从而实现激活 prod 配置文件的目的。$ mvn -Pprod clean package3. 运行时激活，是指运行应用程序时通过传参的方式指定需要激活的 profile。参考如下命令，其中 -D 指定的参数放置于 jar 之前，– 指定的参数放置于 jar 之后。$ java –jar -Dspring.profiles.active=prod app.jar或者$ java –jar app.jar --spring.profiles.active=prod上面三种激活 profile 的方式，后面的可以覆盖前面的。相比较而言，第三种方式更加灵活，能实现同一份代码在不同环境中使用的目的。目前在实际开发过程中，也多采用的第三种方式。基于 Git 的配置管理基于 Git 的配置管理是指使用 Git 作为后端服务来存储配置信息的方式。对于每个部署环境，如开发环境（dev）、测试环境（test）、生产环境（prod）等，都有一个对应的 Git 分支。每个服务都有自己独立的配置仓库，环境与分支之间的对应关系是一对一。这样在获取该服务在特定环境中的配置时，只需要克隆该代码库，并切换到对应的分支即可。如果想对获得的配置信息进行更复杂的逻辑处理，比如以 json 格式输出或者输出单个配置项的值，可以在 Git 库与环境之间增加单独的服务——配置管理服务，来满足个性化要求。注意：该服务不是必须得。通常情况下，Git 库中的分支不会合并。这样就可以更改分支中的配置信息，并且应用到特定的环境中。同时，基于 Git 本身的版本控制特性，可以非常简单的实现变更追溯：谁？在什么时间？进行了哪些变更？基于 Git 的配置服务体系结构示意图如下：Step1：新建配置库并存储配置文件。假设有一个名称为“服务A”的应用程序，需要运行在开发、测试和生产三个环境中。因此，需要在 Git 仓库中新建一个名为config-data-service-a的配置库，并且新建dev、test和prod三个分支，分别存储三个环境的配置。下面以 application.properties 配置文件为例，在 dev 分支中，该配置文件的内容是：spring.application.name=service-aspring.datasource.name=devspring.datasource.url=jdbc:mysql://mysql-dev.devops.com:3306/devops?useSSL=false&amp;amp;useUnicode=true&amp;amp;characterEncoding=utf8 spring.datasource.username=devopsspring.datasource.password=123456spring.datasource.type=com.zaxxer.hikari.HikariDataSource在 test 分支中，该配置文件的内容如下所示：spring.application.name=service-aspring.datasource.name=testspring.datasource.url=jdbc:mysql://mysql-test.devops.com:3306/devops?useSSL=false&amp;amp;useUnicode=true&amp;amp;characterEncoding=utf8 spring.datasource.username=devopsspring.datasource.password=123456spring.datasource.type=com.zaxxer.hikari.HikariDataSource在 prod 分支中，该配置文件的内容是：spring.application.name=service-aspring.datasource.name=prodspring.datasource.url=jdbc:mysql://mysql-prod.devops.com:3306/devops?useSSL=false&amp;amp;useUnicode=true&amp;amp;characterEncoding=utf8 spring.datasource.username=devopsspring.datasource.password=123456spring.datasource.type=com.zaxxer.hikari.HikariDataSource这三个分支中不同的内容为 spring.datasource.name 和 spring.datasource.url 的值。Step2：获取配置文件并启动服务。在配置管理服务中，可以按服务名称、环境获取配置文件。比如下面的接口。用于获取特定服务、特定环境下的配置文件，默认是 application.properties。$curl http://config-service/api/v1/services/{serviceName}/envs/{dev}还是以“服务A”为例，serviceName为service-a。获取 dev 环境的配置接口是：$curl http://config-service/api/v1/services/service-a/envs/dev获取 test 环境的配置接口如下所示：$curl http://config-service/api/v1/services/service-a/envs/test提供这样的接口之后，在部署脚本里只需要通过不同的参数来识别不同的环境即可，在与CICD集成时也会更加方便。增加配置管理服务的好处是将屏蔽了处理配置文件的复杂性，比如克隆代码库，解析文件，转换内容等。部署脚本与配置管理服务之间只需通过接口交互。下面是一段shell脚本的示例:#!/bin/bash#通过参数获取服务名和环境信息SERVICE_NAME=$1ENV=$2#调用接口获取配置文件并写到特定路径下echo $(http://config-service/api/v1/services/${SERVICE_NAME}/envs/${ENV})&amp;gt;/usr/local/${SERVICE_NAME}/application.properties#切换到服务目录下cd /usr/local/${SERVICE_NAME}#启动服务sh start.sh当配置文件需要变更时，就像提交代码一样，将变更后的配置文件提交到代码库中。再通过接口获取文件时，配置管理服务会检测配置文件是否发生变更？这时，当检测到发生变更后，会重新克隆最新的 Git 仓库，返回给调用方。基于 Git 存储配置信息的实践也有很多，它也是目前 GitOps 理念的基本做法。其他的配置管理的工具还有携程的 Apollo。它是由携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端。并且，它具备规范的权限、流程治理等特性，适用于微服务配置管理的场景。目前很多企业都在使用。这个工具目前已经开源，地址是https://github.com/ctripcorp/apollo，上面有比较详细的介绍文档，可以自行查阅。" }, { "title": "DevOps - 技术债务(08)", "url": "/posts/devops-8/", "categories": "DevOps", "tags": "DevOps", "date": "2020-03-04 14:33:00 +0000", "snippet": "在软件开发的过程中，技术债务是不可避免的，有技术债务也是正常的。那么，如何合理、有效的管理技术债务，让软件实现健康地发展呢？什么是技术债务在阅读或修改过他人写的代码，特别是遗留下来的老代码。如果要修改其中的一个 Bug，或者在这些系统之上增加新的功能，那将非常痛苦。我听到很多次：“改他人的代码，还不如我自己写一个。”有时候并不是说大家喜欢重复造轮子，而是因为目前这个“轮子”年久失修，已经不具备重复使用的可能性。由于交付期限的压力，都存在这样的问题： 档没有写或者根本与当前版本不同步； 架构设计也只是满足当时的需求 代码中留下了很多待优化的代码片段；遗留代码缺乏文档和单元测试，无人能改，无人敢改。上面提到的这些问题你应该都能理解，这些问题大多是技术债务没有有效处理导致的结果。技术债务的概念是由敏捷先驱 Ward Cunningham（沃德·柯宁汉）在 1992 年提出的，是指那些现在选择不做，但如果一直不做，就会影响未来软件发展的事情。如下面的两个例子: 过时的架构设计：支撑 1 万人的架构和支撑 1 千万人的架构肯定是不一样的。在最初用户量不大时，采用单体架构满足业务需求是没问题的，但随着用户量的增长，系统架构不做升级，就会影响到业务的发展。 需要重构的代码：在单体架构时，为了加快数据的加载速度，会在内存中将预先加载的数据缓存起来。但当采用分布式系统架构后，这段代码逻辑就要进行相应的重构，采用像 Redis 这种分布式缓存方案，否则就会出现数据不一致的情况。 要注意的是，未实现的功能需求不属于技术债务因此，技术债务是在短期内不会产生影响，但从长期来看，经常背负技术债务的团队会面临潜在的严重问题，甚至会危及软件的可持续性。每个技术债务，无论多么小，偿还的成本都会随着时间的推移而不断增加，还会导致额外的技术债务。技术债务是如何产生的生命周期技术债务的生命周期，掌握技术债务从产生到消亡的整个过程。如下图所示。技术债务经历了四个阶段： 产生阶段。由于各种原因产生了技术债务，此时并未意识到技术债务的存在 意识阶段。开发人员意识到技术债务的存在，但并未影响到软件的正常发展，在此之前一直享受技术债务带来的红利 临界点阶段。技术债务已经阻碍软件的正常发展了，出现负债的情况 补救阶段。采取措施，偿还技术债务，恢复到正常开发流程产生的原因从上图可以看出，技术债务的产生是源头。要想有效管理技术债务，首先就要了解技术债务产生的原因。下面总结了技术债务产生的常见原因，以便能够清楚地认识技术债务，从而选择正确的规避方法。这些原因主要与业务、上下文改变、开发流程和团队有关，如下图所示。业务业务目标、需求、具备的资源以及能够承担的风险都会影响软件产品。业务问题导致技术问题，进而产生技术债务。时间和成本压力开发团队通常因为项目交付的时间、项目预算问题，而优先处理用户需要的功能。业务人员和客户也只关心如何更快的向用户交付新功能，抢占市场先机。因此，开发团队没有太多时间设计具有低耦合、高内聚、可扩展的服务层，而需要继续在现有的系统中添加这些功能。虽然短期内能暂时满足项目需求，但此时技术债务已经产生。业务目标不匹配开发团体的设计方案和技术选型不是为了实现业务目标，而是其他方面的原因。比如，快速开发一个 App。但技术人员考虑到性能因素，采用了原生的 Android 和 iOS 语言开发。由于对语言的掌握有限，导致系统设计差，用户体验不好，最终导致大量返工。需求不足当需求不明确，就会产生技术债务，特别是对软件的非功能性需求，没有明确的要求（如安全性、性能、可扩展性等）情况下。上下文改变技术债务是一个与时间有关的概念。即便是在当初做决策时未产生任何技术债务的设计，随着时间的推移也会过时，需要重新设计。这种重新设计是由业务、技术的变化，或者自然进化造成的技术债务的结果。业务上下文变化系统的所有决策在当初都是合理的，可突如其来的外部事件改变了现有的业务目标。比如企业被收购，收购后系统需要进行融合等。技术变化软件、硬件和中间件技术的更新迭代，导致现有系统债务重重。比如：十年前使用 JDK5 开发的应用系统，在今天的技术形态下只能推翻重来了。自然进化系统在演化的过程中会发生变化，如修复问题、增加新功能。这种变化本身也会削弱一个系统，这是自然进化的结果。就跟人类身体的零部件随时年龄慢慢老化一样，如果做过手术，更会元气大伤。开发流程通常将没有按照软件工程实践和开发规范完成的事项也归类为技术债务。这类技术债务要分析哪些是没有遵循流程规范，哪些是流程规范本身不合理导致的。再根据分析结果进行相应的处理。无效的文档文档，一直都是软件开发过程中的痛。开发人员对编写文档的抵触一直都是存在的。事实上，关键性文档的缺失或过时增加了系统产生技术债务的风险，比如架构设计文档、测试用例文档等。最初的设计人员在交付压力下，并未有效纪录架构设计的细节、约束和遗留的问题，后续的开发人员会犹豫是否要更改他们不确定的代码。测试自动化不足随着系统功能不断增加，开发人员会着重关注当前开发的新功能是否可用，而对已有功能的关注就相应减少了。这时就需要有一套自动化测试工具，能够对已有的功能进行测试。这样就可以校验当前的变更是否对已有功能产生影响。流程不匹配为了保障研发过程顺利开展，所有的软件研发团队都有自己的研发流程。团队成员如果偏离该流程，就有可能导致技术债务，比如代码评审，分支策略，导致版本分支混乱。团队影响系统发展的一个关键的、经常被忽略的因素是人。需求是人提出的，架构是人设计的，代码是人编写的，系统是人使用的。有很多现实的例子表明，团队成员的业务和技术水平对于技术债务的产生至关重要。经验不足一般情况下，团队中经验较少的队员在编码时更容易产生技术债务。比如缺乏模块化设计、代码复杂度的理解。由于企业内部对员工级别的限制和团队梯队的合理性，每个团队中都会有一些初级开发人员，因此，对初级开发人员的工作结果审核和技能培训是至关重要的。分布式团队沟通问题可能会导致对设计决策理解不一致。分布式团队经常会面临着沟通协调的问题。比如广州的团队进行架构设计，北京的团队负责代码实现，在进行任务沟通时，将会是一个很大的挑战。非专属团队团队成员横跨多个项目，特别是经验丰富的开发人员。多任务并行不仅需要在多个任务间进行切换，而且往往团队和个人会将注意力集中在最紧迫的项目上，而忽略对其他项目的关注。不能高效、专注地完成任务，不可避免的会产生技术债务。管理技术债务的原则和实践介绍完技术债务产生的原因，下面就要介绍如何有效地管理技术债务。管理技术债务不是一项一次性活动，它是软件开发过程中的一部分。下面按照避免产生、提前发现和尽快修复的顺序和原则介绍如何有效管理技术债务。避免产生技术债务团队成员的综合能力和丰富经验可以识别技术债务以及避免产生技术债务。因此，从团队成员来说，避免产生技术债务的方法是：让具有丰富经验和能力的人参加到项目中；让具有丰富经验和能力的人把控项目的进度和质量；建立系统化的业务知识和技能的培训，提升团队的整体实力。提前发现技术债务大多数情况下，技术债务都是无意产生的。如果要想尽早发现这些技术债务并不是一件容易的事。目前有大量的免费、开源的工具可以帮助识别代码中的技术债务。单元测试框架单元测试是开发人员自己编写测试，以验证代码中单个代码单元的正确性。如果单元测试失败，团队成员能够立即发现并修复导致失败的代码。单元测试还可以检验当前变更是否对已有功能有影响。目前，每一种编程语言都有对应的单元测试框架 xUnit。比如 Java 语言的 JUnit，Python 语言的 PyUnit 和 C++ 语言的 CppUnit 等等。静态代码分析静态代码分析工具可以识别代码的编码规范、漏洞和缺陷，以及重复代码等问题。这些工具可以方便地与流水线进行集成，自动化的执行扫描和生成检查报告。*持续集成传统软件开发中，一般要到每个功能完成后才会进行集成，此时会有各种各样的问题，如果该问题影响范围较大，甚至将设计方案推翻重来，那付出的代价将会是巨大的。持续集成是一种不同于传统开发的方法，团队每天至少集成一次或者每次提交到代码库时，自动执行集成和验证的过程。通过频繁的集成尽早发现集成中遇到的问题，降低最终集成时的风险。尽快偿还技术债务当我们通过代码扫描工具或者在开发过程中发现技术债务后，就要考虑尽快偿还技术债务。根据技术债务的难易、大小、缓急采取不同的偿还策略。立即偿还技术债务对于能当时处理的就立即偿还掉。这种方法可能是最有效的方法，但也是实施起来难度最大的方法。团队习惯于对当前正在处理的功能进行编码，而是把技术债务留到以后去改进，即便是“以后”永远也不会到来……无论如何，尽快偿还是解决技术债务和使软件更具有可维护性的最有效方法。标记技术债务当发现技术债务的代码时，如果不能立即处理，就先记录一下，然后在后面的某个时间点进行处理。之前的做法是添加 TODO 或 FIXME 这样的注释作为临时占位符，标记这里是存在技术债务的。但随着时间的推移，这些注释不但效果不好，也不会阻止有问题的代码提交到代码库。更有效的做法是在发现技术债务的代码中添加运行时异常，这样就可以使代码运行时处于中断的状态，阻止有问题的代码提交到代码库。可以看下面的例子，在代码中灵活配置门限的阈值。代码片段如下：try { File thresholdConfig = new File(&quot;c:/threshold.config&quot;); FileInputStream fis = new FileInputStream(thresholdConfig); Properties infoProperties = new Properties(); infoProperties.load(fis); String thresholdVal = infoProperties.getProperty(&quot;threshold&quot;); System.out.println(thresholdVal);} catch (IOException e) { e.printStackTrace();}这段代码的问题是以硬编码的方式从本地加载配置文件。开发人员也了解这样写是不对的，因为他只是想调试一下代码逻辑的正确性，并且不希望分散目前的注意力。那么可以将运行时异常添加到验证成功代码的后面，这样既能验证代码逻辑的正确性又不会遗忘此处的技术债务。如下所示：try { File thresholdConfig = new File(&quot;c:/threshold.config&quot;); FileInputStream fis = new FileInputStream(thresholdConfig); Properties infoProperties = new Properties(); infoProperties.load(fis); String thresholdVal = infoProperties.getProperty(&quot;threshold&quot;); System.out.println(thresholdVal); throw new RuntimeException(&quot;不要以硬编码的方式加载配置文件&quot;);} catch (IOException e) { e.printStackTrace();}对技术债务进行排期当发现技术债务较大，并不能在短时间内解决时，就需要将技术债务添加到产品列表（Product Backlog）中，产品经理将技术债务和需求一起排列优先级，共享团队的资源和时间，并和需求一样进行跟踪和验收，保证技术债务确实被偿还。" }, { "title": "DevOps - 有效管理三方组件(07)", "url": "/posts/devops-7/", "categories": "DevOps", "tags": "DevOps", "date": "2020-02-29 02:33:00 +0000", "snippet": "依赖组件也是一个非常烦人的问题比如，依赖组件的某个版本存在缺陷或安全漏洞，不再继续使用。不管是第三方的依赖组件，还是企业内部开发的依赖组件，这类问题都是非常普遍的。当发现组件有问题时，要能知道该组件的基本信息和影响范围，以及采取什么样的措施。使得在节省时间和精力的同时，让系统更可控。这也是我在之前的企业中落地的一个实践。做第三方组件管理的原因优点： 可复用的三方组件让软件开发变得越来越简单 第三方组件不是自己写的，可以直接拿来使用的代码，避免重复造轮子缺点： 存在缺陷或安全漏洞 版本会过时 额外的维护成本随着系统的架构越来越复杂，需要依赖的组件也会越来越多，风险和成本也就越来越大。使用第三方组件本质上是安全风险问题和增加维护成本与减少开发时间的回报之间的平衡问题。通过第三方组件的有效管理，可以有效控制第三方组件产生的风险，这是要做第三方组件管理的主要原因。第三方组件的管控方式第三方组件是一个统称，范围很广，涉及任何开发语言。本文的第三方组件主要指 Java 语言中，通过 Maven、Gradle 等构建管理工具添加到应用程序中的依赖组件。为了后面更好的理解第三方组件的管理，这里先叙述第三方组件的表示方法和添加方式，进而引出最合适的管控时机。表示方法在 Maven、Gradle 等构建管理工具中，第三方组件的命名一般包含三部分。groupId 所有项目中唯一标识 遵循 Java 的包名命名规则，即以反向域名开头 如 org.springframework.bootartifactId 不包含版本的模块的名称 如 spring-boot-starter-testversion 当前模块的版本 一般为数字和点的形式，如 2.4.0 也会跟组件发布状态有关，如 SNAPSHOT 表示快照版，RELEASE 表示发布版本 添加方式以 spring-boot-starter-test 依赖组件为例，介绍第三方组件的添加方式：在 Maven 中， 表示一个第三方组件，将组件的坐标按如下格式添加到项目的 pom.xml 文件 中，即可完成添加。&amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;spring-boot-starter-test&amp;lt;/artifactId&amp;gt;    &amp;lt;version&amp;gt;2.4.0&amp;lt;/version&amp;gt;    &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;在 Gradle 中，可将如下格式的依赖声明添加到 build.gradle 文件中的 dependencies 中，即可完成添加。testCompile group: &#39;org.springframework.boot&#39;, name: &#39;spring-boot-starter-test&#39;, version: &#39;2.4.0&#39;管控时机构建管理工具在构建软件时会经过一系列的阶段，比如：编译、测试、打包，安装、部署等环节。其中编译，是一个关键环节，是对第三方组件进行数据收集和管控的最好时机。将第三方组件的坐标描述添加到文件中，并不会真正地将组件添加到项目中。只有当触发编译时，构建管理工具才会根据仓库地址的设置进行读取。一般情况下，为了加快读取的速度，会先在本地仓库中查找，如果没有再从远程仓库下载到本地）第三方组件管理的需求引入第三方组件存在风险和问题，也是影响软件交付质量的重要因素。关于对第三方组件的管理，在现有的 DevOps 平台中增加了依赖管理的功能。主要的功能项目如下表：依赖扫描 扫描系统的依赖数据并上传到服务器依赖查询 查询依赖组件的坐标信息，并按系统名、版本进行检索依赖管理 维护依赖组件的坐标信息，黑白名单，控制策略和影响范围依赖控制 在编译阶段对依赖组件进行检查，当检测到依赖组件中包含了黑名单中的组件，按照设置好的控制策略对该组件进行处理，比如编译失败，邮件通知负责人等依赖同步 从远程仓库同步依赖组件的版本信息，判断该组件版本是否过时反向依赖 按依赖组件查询应用系统。当发现组件有问题时，判断该组件的影响范围时非常有用 黑名单指，不允许使用的组件； 白名单是，可以使用的组件； 控制策略，指的是当发现组件有问题时采取的措施，如阻断，警告； 影响范围，指的该控制策略的应用范围，如当前系统，所有系统等。 第三方组件管理的设计实现上面的功能需求，下面从架构设计、流程图设计、类图设计和接口设计四个方面对第三方组件管理服务进行说明。通过这四个方面的介绍，基本上就可以按照这个设计编码实现第三方组件管理功能了。下面的图形主要展示的是设计的关键部分，在具体实现的过程中，可以根据实际情况进行扩展。架构设计第三方组件管理这个功能涉及的核心组件有两个：依赖管理服务和持续集成平台。 依赖管理服务。 用于维护依赖组件的信息，包含存储和查询依赖组件 添加、修改和删除依赖组件的控制策略 同步依赖组件的最新版本 为前端提供数据查询接口等 持续集成平台。 用于在编译阶段对依赖组件进行扫描和扫描数据上报 当发现有黑名单中的依赖组件时，根据依赖组件的控制策略进行处理 流程图设计根据上面架构图的设计，第三方组件管理的业务流程图，如下图所示，包含2个主要的流程：数据维护流程和依赖扫描流程。 数据维护流程。该流程比较简单，就是准备数据。比如：安全部门发现fastjson&amp;lt;=1.2.62存在远程代码执行的漏洞，公司里任何系统都不允许依赖该范围的版本。这时就可以将该依赖组件添加到黑名单，groupId 为com.alibaba，artifactId 为fastjson，version 为1.2.62，逻辑运算符为&amp;lt;=，控制策略为阻断，应用范围为所有。 依赖扫描流程。这一部分是该功能的核心流程。由持续集成平台发起，分为以下几个步骤： 克隆代码库，编译源代码； 利用 maven 插件，执行依赖扫描任务，获取依赖组件信息； 调用依赖管理服务的接口获取黑名单列表； 判断扫描的依赖组件中是否包含黑名单中的组件； 如果有，则判断控制策略是否是阻断； 如果是阻断，则直接阻断当前任务，编译失败； 如果是警告，则输出警告信息，上传编译数据，编译通过； 如果不包含，则上传依赖数据，编译通过。 类图设计该功能涉及的几个关键类，如下图所示，主要有：DmService（服务类）、DmScanRecord（扫描记录类）、DmScanDependency（扫描依赖类）、DmDependency（依赖类）和 DmBlackList（黑名单类）。这几个类的用途是： DmService（服务类）：该类用于存储业务系统与代码库的关联关系。当进行反向依赖查询时，能知道该依赖组件有哪些系统在使用。 DmScanRecord（扫描记录类）：该类用于存储每次的依赖扫描记录。为了能知道每次代码提交的依赖组件信息，一般要跟代码库、分支、CommitId 进行关联。 DmScanDependency（扫描依赖类）：该类用于存储每次扫描的依赖组件信息，包含层次关系。当发现该组件有问题时，对该组件进行标记。为了减少存储的数据量，这里只存储依赖组件的ID信息，具体详细信息存储在 DmDependency 类中。 DmDependency（依赖类）：该类用于存储具体的依赖组件信息，通过 groupId、artifactId、version、classifier 来标记唯一组件。 DmBlackList（黑名单类）：该类用于存储依赖组件的黑名单信息。接口设计经过上面几个设计图，第三方组件管理功能基本上已经很清晰了，知道了有哪些组件以及组件的调用关系、数据维护和依赖扫描的流程、以及需要哪些类来存储这些元数据。那么，接口设计这部分，就是要基于这些元数据，定义对外暴露的接口，将上面的流程串联起来，最终实现组件间的相互调用。下面是按照 OpenApi 3.0 的规范编写的接口文档。数据维护相关的有：服务管理和黑名单管理接口。依赖扫描相关的有：黑名单查询和依赖上报接口。查询相关的有：依赖查询和反向依赖查询等这些就是第三方组件管理功能主要的设计内容，功能不是很复杂，但能达到有效管理和控制第三方组件的效果。通过嵌入到持续集成流程里，可以作为代码预检查的检查项，尽早发现有问题的组件。" }, { "title": "DevOps - 代码预检查(06)", "url": "/posts/devops-6/", "categories": "DevOps", "tags": "DevOps, 代码预检查", "date": "2020-02-27 02:33:00 +0000", "snippet": "什么是代码预检查代码预检查是指在代码提交到代码库之前对代码进行检查，包括静态检查、Code Review、测试、编译等多种方式，目的是保证提交到代码库的代码的质量版本控制系统版本控制系统有两个用途： 维护软件每次修改的完整历史， 团队高效协作的工具。让团队一起开发软件的不同部分然而一旦低质量的代码提交到代码库，就会更新到团队每个成员的本地仓库，低质量的代码不仅会导致后续维护困难，也为软件的健壮性埋下了祸根。因此，将低质量代码直接拦在门外，然后通过代码检查等方式指导开发人员哪里有问题，如何修改，开发人员按照提示进行修改，直到满足要求。做代码预检查的原因可读性：代码不只是给机器理解的，也是给人看的。要能够使团队中的每个人都能容易的阅读和理解代码。可维护性：当维护和测试高质量的代码时更加容易、安全和省时，且不易出问题。减低技术债务：高质量的代码设计良好，技术债少，开发人员无须花费大量的时间修复代码的问题和重构，因此，可以加快软件开发的进度。新团队成员容易理解代码，更容易加入项目。高质量的代码是高质量软件的基础，不管是什么业务，高质量的软件都可以使你免去生产问题带来的烦恼。代码预检查实践有哪些 质量不是一种行为，它是一种习惯——亚里士多德虽然代码质量对于现在的软件项目非常重要，但绝不要搞“代码质量月，利用一个月的时间，将代码质量提高到百分之百，消灭所有的 Bug”这样的活动。因该以循环的方式不断改进，将提高代码质量的活动融入每个迭代中，形成一种习惯。本地检查开发者本地执行的检查，比如 IDE 或命令行的方式。优点 按照检查前置的原则，本地检查的时机是最合适的，发现问题和修复问题的成本是最低的 本地执行，执行检查的效率最快缺点 依靠开发者具有很高的自觉性工具与实践 IDE 插件：SonarLint、FindBugs、CheckStyle、PMD、阿里规范插件等，可以检查代码的编码风格，坏味道，漏洞等 本地构建：本地构建：maven 编译、gradle 编译，可以检查代码语法问题，是否能够编译过 本地测试：单元测试，可以检查代码的逻辑问题本地提交检查本地执行git commit的时候进行的检查优点 时机适中 效率较高 Git 的 Hook 机制，在每次提交时运行 Hook，自动识别代码中的简单问题，代码审阅者只专注于代码逻辑和系统结构，避免因为琐碎的样式问题而浪费时间缺点 本地提交对提交速度要求很高，只能进行省时的静态检查，如代码风格等 客户端的 Hook，因为是在开发人员本地，并未实现完全“强制”检查 同样依赖开发人员的自觉性工具与实践 使用 Git 的 pre-commit 钩子在提交时检查代码问题 但随着创建的项目越来越多，使用的编程语言也各不相同，在不同的代码库之间共享 pre-commit 的钩子脚本是一件非常痛苦的事情。需要在每个项目之间拷贝脚本，还需要手动更改脚本才能适用于不同的项目结构 这里介绍一个 pre-commit 的多语言包管理工具——pre-commit。在每个代码库中，只需要指定所需要的钩子列表，在每次提交之前，pre-commit 就会安装并执行任何语言的钩子 pre-commit 安装教程可以参考官网远程提交检查在本地执行git push提交到远程仓库时执行的检查，远程提交检查同样有优点和缺点。优点 能够进行较为深层次的动态检查，比如漏洞、Bug，检查的效果较好 能够做到强制检查，保证任何人提交的代码都是高质量的 代码提交后并未直接提交到代码库中，可以加入人工评审环节，可以检查代码的业务逻辑和架构设计等较复杂问题 可以控制每次提交的代码质量缺点 检查的时机靠后，反馈周期较长； 需要搭建代码检查服务器； 维护成本较高； 每次提交都检查，会拖慢团队的开发节奏工具与实践 代码检查。需要搭建代码检查服务器，比如 SonarQube。当代码提交后，自动触发代码检查并根据检查结果决定是否合入代码库。为了避免历史债务对问题修复的影响，可以考虑使用增量检查，只检查本次提交修改的文件，这样就不会因为问题太多打消提交者解决问题的积极性。在之前的实践中，我们每次提交时进行增量检查，控制新增代码的质量。在发版时，对全量代码进行检查，作为系统发版的质量关卡。质量分值每月递增，从而达到持续提升代码质量的目的。 人工评审。代码提交后，由团队中高级别人员对代码进行评审。检查本次变更是否符合编码规范以及是否采用了最佳的技术解决方案，并在评审页面以行间评论的方式发表评审意见，提交人根据评审意见进行修改，直到满足评审人的要求。目前常用的代码评审工具如 Gerrit，通过与Jenkins 集成可以实现机器检查和人工评审的同时审核 自动化测试。上面两个检查具有一定的局限性。代码检查能够发现代码问题，但不能发现功能性问题。人工检查对评审人的要求较高。比较有效的做法是使用 JaCoCo 检测单元测试覆盖率，或者执行一定范围的集成测试。建议选择系统的核心场景、10 分钟以内能够执行完成的测试用例分支合并检查在远程仓库里执行分支合并的时候进行的检查优点 每次在分支合入时进行检查，既能控制合入其他分支的代码质量，又不会对开发节奏造成影响； 分支合并时一般表示一个功能和问题开发完成，可以有相对充足的时间执行较为深层次的动态检查，比如漏洞，Bug 等； 可以设置强制检查，保证合入分支的代码是高质量的； 可以做 Code Review，评审人一次性评审完整的功能代码（不推荐长期分支，大功能开发）; 代码托管平台支持，如 GitHub、GitLab。缺点 检查的时机靠后，反馈周期较长工具与实践 GitHub 的 PR：当进行分支合并时，提交人申请 Pull Request，目标分支通过 Pull 的方式从提交分支上更新代码。在这个 PR 申请中，评审人进行 Code Review，同时触发 GitHub Action 执行自动化构建、测试、代码扫描等 GitLab 的 MR：当进行分支合并时，提交人申请 Merge Request，目标分支通过 Merge 的方式从提交分支上更新代码。在这个 MR 申请中，评审人进行 Code Review，同时触发 GitLab CI/CD pipeline 进行自动化构建，测试，环境部署等· 总结代码质量是一种习惯，是要融入每个开发人员的日常工作中，在开发的过程中就关注代码的质量。因此，对于软件从业人员来说，特别推荐使用 “本地检查” 的方法随时随地的提示、指导我们开发高质量的代码。" }, { "title": "Devops 5", "url": "/posts/devops-5/", "categories": "", "tags": "", "date": "2020-02-24 00:00:00 +0000", "snippet": "" }, { "title": "DevOps - 看板方法(04)", "url": "/posts/devops-4/", "categories": "DevOps", "tags": "DevOps, 看板方法", "date": "2020-02-21 02:33:00 +0000", "snippet": "当我们的团队都理解了用户需求之后，就进入到后续的产品设计、代码开发、功能测试、指导生产部署等环节。作为一个软件从业人员晓得，后续步骤不可能一帆风顺。总有这样或那样的问题影响软件的整体交付进度。比如： 当前迭代中被插入其他工作事项 需要解决的问题太多导致没有时间开发新功能 团队处于不断救火的状态这种过载现象会影响团队成员的身心健康，同时交付的软件也是问题不断，运维老哥苦不堪言，这是一种不可持续的开发模式。这种现象也有解决之道，就是看板方法。看板方法。是一个能将工作流程可视化的管理方法。它通过限制每个周期中卡片的数量来限制进行中的工作项的数量，只有团队具备了处理能力，才能拉入新工作项。这种拉动的工作模式，只要设置合适的能力阈值，团队就不会出现过载的现象。什么是看板方法看板（kan-ban） 是一个日本词语，来源于日本丰田。看板本意为“信号卡”，在生产制造环境中，这种卡片用作一种信号，通知生产过程中的上游工序继续生产。在这种生产过程中，除非从下游工序获得该信号，否则每个工序的工人不准进行任何额外生产。这和我们生活中，地铁通过限流来防止客流量超过系统承载量，是类似的——都是通过信号控制生产系统过载。在软件开发中，卡片的数量代表团队工作能力能够处理的上限。这里的卡片代表工作任务，除了包含之前提到的用户故事，也包含问题修复、培训等其他任务，因为这些都是需要占用团队时间和精力完成的。在这些任务没有完成之前，新到达的任务只能在队列中等待，直到有任务完成才能开始。除非是用高优先级的任务替换当前进行中的低优先任务。为了解决团队过载问题，需要将团队成员的工作可视化，了解每个人的工作内容及工作量、任务延期的卡点，从而改进团队在研发过程中存在的问题。因此，看板方法是一个包含可视化工作流程，定义流程运作规则，限制在制品数量的拉动系统，是一种在团队成员工作不过载的前提下，实现按时交付的管理方法。看板方法是一种从现状出发，以渐进式、增量式为特色的改进方法。团队在当前的工作流程中应用看板方法，可以发现系统中存在的工作不均、团队过载、工作延期等问题，从而采取改进措施，解决系统中的瓶颈。看板方法实施起来相对比较简单。下面我介绍一下如何使用看板方法。如何使用看板方法STEP 1：价值流映射第一步，要完成价值流映射。价值流是指从响应客户请求创造价值开始，到完成请求交付用户价值为止，所需要实施的一系列的相关行动。而将价值流可视化的工作，称为“价值流映射”。为什么要做价值流映射？软件开发的最终目标是交付用户，整个流程中会涉及到产品、开发、测试、运维等多个环节。因此，如果想要更快交付用户，产生价值，需要了解整个流程的运行情况。价值流映射能够帮助我们了解整个流程。下图是某企业某产品线的价值流映射图，展示了从需求提出到用户上线的整个周期的价值流动过程。每一个环节下面都标明了完成该环节的相关数据，含义： LT:Lead Time，前置时间或交付时间，指一个工作项从开始到结束所消耗的时间，包含了等待时间； PT:Process Time，处理时间，是指处理该工作项的时间； %C/A: The Percent Complete and Accurate 完整度与准确度百分比，该值表示返工的比率，一般为估计值。由此可以看出，一个需求从提出到上线验证完成总共需要 69 天。而其中只有 36 天的时间是用来从事价值产出的活动，其他 33 天或被用来等待下一步工作，或修复发现的缺陷。这 33 天并没有产生价值。从价值流映射中，我们可以发现影响价值交付的因素和瓶颈，并对这些因素和瓶颈进行优化。STEP 2：定义工作项类型上面第一步完成了价值流映射，第二步就要识别价值流中的工作项类型，一般包含以下几种： 以需求为中心的工作项，包括需求，功能，用户故事等； 以开发为中心的工作项，包括重构，缺陷，优化等； 以运维为中心的工作项，包括扩容，备份，故障等； 其他事务型的工作项，包括培训，会议等。这里涉及到的工作项类型要体现在下面看板中。在整个软件开发流程中，不仅要关注能够增值的工作项（如需求），也要关注非增值工作项（如缺陷，重构，备份等）。因为这些非增值工作项可能会成为阻碍项，务必关注！！！STEP 3：看板可视化设计将价值流映射作为输入，将其中的步骤抽象为分析、设计、开发、测试、发布几个关键环节，就可以设计出工作项需要流经的看板的列。如下图所示。需要注意的是，每一列分为“进行中”和“完成”两个子列。“完成”列是下游输入的“等待”列，这里，“完成”列是给下游的一个信号，说明上游工作已经完成，可以拉取并开始下游的工作。从上面看板中，可以非常清晰的看到以下几个关键点： 团队成员的工作项可视化。团队成员的主要工作项要在看板中全部体现出来。每一个黑色的卡片代表一个工作事项。主要是一个可验证、可交付的用户需求。 端到端的交付流程可视化。从需求提出到交付给用户的整个过程。一般由需求分析、设计、开发、测试、发布等多个环境构成。其中”进行中”一列代表工作环节，“完成”一列代表等待环节。 系统瓶颈和问题可视化。发现当前工作流程中的问题和瓶颈。设计和开发阶段是当前系统的瓶颈，设计人员人手不足，导致分析完成的需求不能及时被处理，越积越多。开发人员当前任务饱和且功能相对较大。与此同时，测试人员处于无功能可测的状态，属于严重的资源浪费。STEP 4：显示化定义规则如果想要让设计出来的看板能够运作起来，就需要定义一些规则。比如： 任务卡片的书写规则是什么？ 需求具备分析的条件有什么？ 需求进入开发之前，需要满足的条件是什么？ 需求在进入测试之前，需要满足的条件是什么？ 每个任务完成的条件是什么？显示化定义这些规则，目的是让团队成员对流程规则有一致的理解和承诺，为团队提供明确的决策依据。其中，状态流转规则是可以放到看板中的，如下图所示。除此之外，还有一些与团队运作相关的规则，比如站会。站会的时间、地点、形式和内容也需要大家遵守。STEP 5：限制在制品数量在制品是指进行中或等待中未完成的工作项。俗话说：“贪多嚼不烂”。同时处理多件任务，需要切换多个任务的上下文，而且也不能保证每个任务的交付质量。看板方法可以限制每个环节中在制品的数量，当环节中的在制品数量小于这个限制时，就可以从上一个环节已完成的部分拉入新的工作项，否则不允许拉入新的工作项。如下图所示。除“发布”列之外，每列都设置了在制品的限制，明确了该环节所允许的最多可并行处理的工作项个数。在设置该值的时候，可以按当前实际的在制品数量作为初始值，当运行一段时候后，再回头审视是否需要调整。这样也符合看板方法中“从当前工作状态开始”的原则。另外，还可以根据职能团队的人的数量来设置。比如，开发人员处理“开发”列的工作，测试人员处理“测试”列的工作。那么，这两列的在制品的限制就是对应人员的数量。比如，开发人员有 5 个人，那么可将“开发”列的在制品限制为“5”。STEP 6：跟踪工作项看板方法是为了顺畅的交付高质量的用户价值。只有每个工作项都能够顺利的，按部就班地完成，才能最终交付用户价值。因此，对工作项的跟踪，了解团队运作和工作项流动情况，就显得格外重要。目前采取的实践是：站会。站会一般是由站会主持人带领团队成员，在每个工作日，同一时间，同一地点，对着看板来检视工作项完成情况。主持人，一般由团队负责人或者每个团队成员轮流担任。在站会时，主持人带领大家从右到左审查看板。从右到左的顺序，体现了看板方法价值拉动的方向，只有下游的任务处理完成才能从上游拉取任务。开站会的目的在于跟踪工作项，促进价值顺畅流动。因此，我们不需要检视每一个任务项，而是关注影响价值流动的工作项。如下图所示。一般需要重点关注的方面有： 积压的需求。表明该环节已经过载，影响到整体价值流的顺畅流动，如图中的开发环节； 中断的需求。表明某个环节供给不足，价值流出现中断，如图中的测试环节； 阻碍的需求。表明该需求由于某种原因无法正常进展，该类需求应推动问题的解决，尽快恢复流动，如图中的设计和开发环节中有红色标记的需求 即将或已经到期的需求。表明在规定完成的时间内未完成的需求； 长期停滞的需求。表明长时间未更新状态的需求 STEP 7：持续改进 持续改进是敏捷软件开发方法中非常重要的一个原则。通过小批量、短周期迭代，团队能尽快收集用户关于软件产品的反馈，从而在下一个迭代中进行改进。与敏捷软件开发方法 “增量式的迭代开发方式”不同，看板方法在开发上并未切分成小循环的过程，它强调的是“渐进式的变革”，是一个不断做调整的流程控制方法。这一点，可以采用戴明提出的 **PDCA **持续改善模式。PDCA 是 Plan-Do-Check-Action 的缩写，是一个四步循环，包括工作计划的内容，产生的预期结果，以及计划完成后的验收标准； Plan（规划），指在一个周期里制定一个明确的工作计划，包括工作计划的内容，产生的预期结果，以及计划完成后的验收标准； Do (执行)，执行上一步制定的工作计划，并收集执行过程中的信息和数据； Check（检查），核对上一步收集的信息和数据，和预期结果进行对比，并对哪些做对了，哪些做错了进行总结。 Action (处理)，对上一步总结的结果进行处理，好的经验继续保持，对于需要改进的方面提出改进方案，并交给下一个 PDCA 循环去解决。以上面看板为例，最初制定规划，以当前现状为基础实施看板方法。在执行中，通过的跟踪工作项步骤，发现了其中积压的需求、中断的需求和阻碍的需求等问题，并记录了积压需求的数量，需求任务的工时，团队成员的数量和能力等数据。结合这些数据分析造成这些问题的原因，比如需求量多，人手不足，需求难度大等。那么在下一个规划中就要采取改进措施，有针对性的解决这些问题。不管是做企业还是做产品，不可能一蹴而就，都需要在不断试错、改进中渐渐完善。看板方法具有很强的问题暴露和提示改进机会的能力，能有效提高组织在完整价值流上的关注程度和绩效表现。" }, { "title": "DevOps - 用户故事(03)", "url": "/posts/devops-3/", "categories": "DevOps", "tags": "DevOps, 用户地图", "date": "2020-02-12 02:33:00 +0000", "snippet": "在工作中，经常会遇到下面的场景。 对于一个用户需求，产品、开发和测试对这个需求的理解完全不一样，最终交付的产品根本不是用户想要的上面所述的情况在实际开发中非常普遍，本文就此问题写下若干心得体会。用户故事，是一种将需求可视化的工具，它通过将需求拆分成一个又一个的用户故事，来组织软件开发。每一个用户故事都是软件开发过程中相关角色沟通的媒介，也是一种通过合作沟通的方式来理解用户需求的工具。软件开发是一项团体项目众所周知，软件是由诸多功能组成的，而每个功能都经历从需求分析、代码开发、软件测试、部署的整个开发流程。整个软件开发流程里，功能从需求开始流经整个软件开发的过程，其中涉及到多种角色： 产品经理对功能的需求分析， 开发人员对功能的代码实现， 测试人员对功能的功能测试， 最后运维人员将功能部署到生产环境交付给用户每个领域的人员都有一个共同的目标：将功能交付给用户。团队协作是个老生常谈的话题，但真正做到高效协作并非易事。软件开发流程中涉及的各个人员，都有自己的工作规范，语言和节奏也各不相同。同时，软件开发中的功能与篮球不同，看不见摸不着，当功能在每个角色之间流转时，是将自己领域的产出交付下一个角色，产品经理交付开发功能需求，开发交付测试代码，测试交付运维是的制品。由于交付物不同，增加了彼此之间沟通协调的难度。因此，需要一种能够让大家达成共识的方法。这就需要让软件开发过程中的每个角色都能理解用户的需求是什么，工作内容是什么。比如：产品经理提出“下订单时，送货地址从地址簿里选择”的需求，产品经理、开发人员、测试人员三者对这个需求的理解是不一样的。地址簿是下拉列表显示还是弹窗？这个隐含的需求决定了开发人员如何编写代码，测试人员如何编写测试用例。所以只有达成共识了，才能保证大家的工作方向是一致的。共享文档并不代表达成共识写文档，是软件开发过程中的通用动作，但是写文档存在的问题。在传统开发模式中，产品经理通过先写一份用户需求说明书，然后传给开发、测试等相关人员，并期望他们能够按照这个文档去开发、测试，最终实现用户需求，但最终交付的软件常常并不能达到用户需求。事实上，我们会习惯性地根据经验在文档范围内去思考，难免对内容理解不到位，造成误解。比如在上面的例子，产品经理想说的是“送货地址从地址簿里选择”，而开发就认为下拉列表的形式也是满足要求的，但用户可能需要一个弹窗的形式。很明显，文档并不能让人们彼此之间达成共识。看文档，就像看旅游时拍的照片一样。当我们在看文档时，重要的并不是文档里记录的内容，而是当我们阅读文档时，脑子里能够回忆起什么。所以只有面对基于讨论写出的文档，参与讨论的各方才能对文档有一致的理解。没有参与讨论的人，是无法通过仅阅读文档来了解这些细节的。因此，达成共识的唯一方法是各相关方坐到一起，用有效的方式进行讨论，最终寻求一致的理解。这种有效的方式就是讲用户故事。用户故事之所以叫“故事”，是因为它是用来“讲”，而不是用来“写”的。它从用户的角度来讲述如何使用软件的功能，带来怎样的收益。整个软件团队通过讲用户故事，让大家获得对软件产品的一致理解，然后共同创造出更符合用户需求的产品。如何“讲”好用户故事用户故事的概念比较抽象。比如，用户说：“当我在输入‘送货地址’的时候，能立即显示出目前存在的地址。这样就可以从列表中选择一个，而不是每次都需要重新输入。”这是一个典型的用户使用场景，是一个用户故事。就像上面提到的，用户故事是用来“讲”的，而不只是“写”的。上面的例子中，如果这个不讨论用户故事，你能知道【立即显示】是输入 1 个字、 2 个字，还是有光标就能显示呢？因此，用户故事的关键就是“讲”。那么，怎样才能“讲”好用户故事呢？聚焦全景软件开发最终交付给用户的是一个可以工作的软件，这就需要先实现一个一个单独的功能。因此，我们需要将一个大需求进行合理拆分，拆分后的需求就称为“用户故事”。通过拆分用户故事的方式来组织开发产品，每个用户故事完成后，都需要与整个软件进行集成。不过值得注意的是，当我们在将大需求拆解为用户故事，或者在讨论单个用户故事时，都需要从整体的视角出发，和团队一起理解正在开发的产品，共同讨论每个故事在整个产品中的定位、优先级、共同权衡和取舍。杰夫·巴顿（Jeff Patton）将这种做事的方法称为“用户故事地图”。马丁·福勒（Martin Fowler）在给杰夫·巴顿（Jeff Patton）的《用户故事地图》一书作序时说：“故事地图是一门在需求拆分过程中保持全景图的技术。”全景图为一致的用户体验提供了基准，可以帮助团队之间，以及团队与用户之间进行有效的沟通。下图是新开普云产品中用户故事地图功能的示例图。在该用户故事地图的顶部是主干。主干通常分两级： 一级是组成该产品的基本骨架，如图中的用户场景层级，包含账号管理、产品首页、商品列表页等； 另一级是每个用户场景下包含的用户任务层级，如产品首页场景，包含首页轮播图、产品橱窗、产品搜索等。从左到右看，整个用户故事地图的主干是一个所有用户和他们使用该软件产品的完整故事。从左到右的顺序，也是我们在讲故事时候的叙事主线，更容易被接受和理解。再往下是具体的产品发布路线图，是团队根据市场需求和用户要求综合考虑后，决定要发布的用户故事。每个用户任务下的用户故事按照从上到下的顺序排列优先级。通常，选择那些对用户价值较大的用户故事进行发布。有了这个用户故事全景图，团队中的每个角色都能清晰地知道本次迭代中需要交付的用户故事内容，用户是如何使用的，以及产品能够给用户带来什么帮助。有了对用户故事的一致理解，后续大家在交付每个用户故事的过程中就有了统一的目标，不容易产生误解。3C 原则有了基于用户故事地图的全景视图，并不表示讲好用户故事，还需要遵循 3C 原则。3C 原则是在由 Ron Jeffries（罗恩·杰弗里斯） 等人在《极限编程实施》一书中提出的。3C 原则描述的是为了更有效的使用用户故事进行需求分析时应该遵循的原则： 卡片（Card）：在卡片上写下所期望的软件特性。 交谈（Conversation）：聚在一起对要开发的软件进行深入讨论。 确认（Confirmation）：对完成的特性进行确认。使用故事模板虽然有了上面的三个原则，但作为严谨的软件从业人员，这还不够。相对于产品和市场人员来说，很多开发人员不知道如何去讲用户故事，也不习惯讲用户故事。这时，你需要使用用户故事模板，这样就能够对卡片上的内容有一个指引，方便讨论。如下图所示：也就是说，在创建卡片的时候主要考虑以下三个问题： 用户是谁？ 用户需要什么特性？ 用户需要这个特性是要解决什么问题？在讨论之前，如果不能回答这三个问题，就无法知道我们交付这个用户故事的价值，也就不清楚这件事的意义，就无法交付对用户真正有用的东西。统一的故事模板，就是为了统一团队成员之间沟通交流的语言。而基于故事模板内容来引发讨论，能提升讨论的效果。为了达到更好的讨论效果，这里罗列几个讨论清单：用户故事及用户故事地图，在很多企业的研发团队中都有使用。、软件开发是一个团队项目，在团队中沟通协调是至关重要的。大多数时候，软件本身的功能并不复杂，但团队之间若因为沟通不到位，理解不一致，最终导致开发的产品与用户想要的不一样，可能会错过最佳的市场机会。这种因为疏忽，大意失荆州的事情是我们所有工作中都应该尽力去避免的。" }, { "title": "DevOps - 影响地图(02)", "url": "/posts/devops-2/", "categories": "DevOps", "tags": "DevOps, 影响地图", "date": "2020-02-05 12:33:00 +0000", "snippet": " 端到端：从需求提出到需求被发布到生产环境交付给用户的整个过程，可以理解为软件开发的全生命周期最佳实践：被业界广泛采用的、被证明有效的方式方法产品规划的必要性 ？首先，思考以下问题： 参与的软件产品开发中，有多少功能是用户真正需要的？ 参与的项目，有没有做到一半就取消的功能和产品？害，以上两种情况遇到的太多了。据 Standish Group 发布的 CHAOS Report 显示：截至 1994 年，在研究了 8000 多个软件开发项目后，只有 16% 的软件项目是成功，其他 84% 的项目不是失败了，就存在严重的问题。在十年后的 2004 年，当研究人员将项目样本增加到了 4 万个，发现这时软件项目的成功率提高到了 29%。为什么十年后软件开发的成功率会增加？其中一个原因就是精益思想和敏捷软件开发方法的广泛采用。但根据 2015 年的报告，从 2011 年到2015年，软件项目的成功率并未再有大的提升，基本是在 30% 左右。其中，在这些失败软件项目中有 31.1% 在未完成之前就取消了。软件项目的失败最终付出了金钱和时间的代价，最重要的是失去的机会是一去不复还的，有些企业失去抢占市场的先机，因此而倒闭。“项目中途取消”、“需求中途取消”、“项目延期超支”等问题的出现，最主要的原因有以下两点： 一点是“不了解用户真实需求”。在产品在开发阶段，不去调查用户需求，而是想当然的自我设定用户需求， 最终导致所开发的软件和用户想要的不一致 另一点是对产品缺乏整体的认识和理解。产品的规划没有重点，眉毛胡子一把抓，什么想都做，但最终什么都没做好，错失了产品投放市场的最佳机会由此可以看出，挖掘用户的真实需求是多么重要。如果一开始没有按照用户的需求进行开发，终究会落得中途废弃的下场。因此，需要找到一种工具或方法，能够挖掘出用户最真实的需求，能够帮助我们对产品进行定义和里程碑规划，确保交付的软件和用户的需求一致，并且是用户最高优先级的需求。这个工具就是 —— 影响地图。什么是影响地图 ？影响地图是一种战略规划方面的技术。通过清晰的假设、可视化的形式，建立产品功能与商业目标之间的关系，最终做出更合理的里程碑规划。避免开发人员在构建产品和交付过程中迷失方向。影响地图通过回答**为什么（why）、谁（who）、怎样（how）、什么（what） **这四个问题来一步一步构建完成一个思维导图。该思维导图一般是由技术人员、业务人员及其他相关人员共同协作完成。最终的思维导图框架如下图所示：1. 为什么（why）?开发过程中，首先思考的是：为什么要做这件事？为什么要开发这个功能？问题的答案正是要达到的目标。设立清晰的目标听起来简单，但真正做起来非常不易。有很多软件项目失败的原因就是目标不清晰。 这里的目标指的是业务目标（要实现的目标一定是业务目标）这个目标要有明确作用或者能够为企业带来利润、客户，或者能够为其降低成本。目标的设定非常关键，切记不可不切实际！！！好的目标应该符合 SMART 原则： Specific（明确的） Measurable（可度量的） Action-oriented（面向行动的） Realistic（现实的） Timely（有时限的）比如：3 个月新增用户 100 万或年底之前完成销售额 1000 万等。2. 谁（who）?地图的中间已经放好了要实现的目标，那么接下来的第一层就是表明实现这个目标和哪些人或角色有关系。这个关系可以是正面的，也可以是负面的。比如： 谁能帮助实现这个目标？ 谁会阻碍实现这个目标？这里要尽可能把所有相关联的人列举出来。防止开发过程中突然出现的角色对开发过程造成影响。罗列出的角色再分以下几种： 主要角色，软件的最终用户。 次要角色，软件的提供者，可能会包含产品经理、开发人员、测试人员、运维人员 场外角色，不直接参与的人员，比如高层管理者3. 怎样（how）?紧接着要考虑这些角色是如何影响最终要实现的目标的？也就是，考虑角色是如何帮助或阻碍目标的实现？这里就要思考角色会有哪些行为变化。可以根据行为的影响大小，实现难易进行优先级地排列。从而根据影响地图的层次结构清晰地掌握是谁造成了这些影响，以及他们如何影响目标的实现？注意，这里的“影响”影响到的并不是具体的产品功能，而是角色行为的变化。是行为与之前相比的不同之处。比如：页面响应时间缩短 30%，而不是页面响应时间本身。页面响应时间在之前的功能中也有，只是实现时间较长。如果要实现 why 的目标，需要将响应时间缩短30%，这才体现出与之前的变化。4. 什么（what） ？回答完前面三个问题，就可以讨论具体要做什么才能实现这个影响了。比如：是应该交付一个软件功能，还是组织一场线下活动？这一层就是我们的工作范围了。 这里罗列的内容并不是所有要交付的内容，而是交付内容的可选项我们需要在这一层筛选出有价值的部分，同时过滤掉没有明显价值的部分。有价值的部分就可以作为里程碑规划中的内容，也是我们应当优先完成的内容。比如 为页面数据添加缓存 减少计算逻辑的再次处理等。通过将交付的内容，内容产生的影响，影响实现的目标串联起来，我们能清楚地知道做这件事情的前因后果。从而在实际开发过程中合理分配资源，不会产生范围蔓延的问题。实践在工作中，有用户投诉说打开商品详情页特别慢，加载时间特别长。产品经理针对这个问题建了一个任务“商品详情页异步加载显示”并拖到了下一个迭代里。这种情况下，究竟要不要按产品经理说的去做？如果不做又有什么其他解决方法？下面，使用“影响地图”来分析一下这个需求。STEP 1：Why? 先确定任务目标在这个例子里，产品经理提出“商品详情页异步加载显示”的需求。透过现象看本质，产品经理的最终目的并不是要把商品详情页改造成异步加载的方式，而是要提高商品详情页的加载速度，异步加载只不过是实现这个目标的一种解决方案。因此，先定义好目标。STEP 2：Why? 先确定任务目标。根据经验，页面加载缓慢的因素有很多种。例如： 后台的接口慢； 前端的代码加载策略有问题； 系统的部署架构需要优化； 服务器的配置需要升级； 采用 CDN 进行静态资源分发等因此要实现这个目标，关联方就会涉及以上提到可能因素相关的所有人员： 后端开发人员； 前端开发人员； 架构师 设备管理员 购买设备和 CDN 服务时需要审批的领导等因此，将影响任务目标的角色放入影响地图，如下图所示：STEP 3：How？每种角色是如何影响这个目标的？为了达成目标，根据每种角色的职责和工作范围，思考他们用什么方式影响这个目标。比如： 后台开发人员提高后端接口响应时间； 前端开发人员进行前端代码优化； 架构师优化系统部署架构，通过负载均衡分散访问压力，静态资源文件单独存放加快下载速度； 设备管理员提升硬件性能； 领导需要加快方案落地速度等。这里要考虑的是角色行为对目前现状与改进后的差异。有些是从无到有，有些是提高，有些是减少……总之角色的行为要产生变化。这些变化要影响到目标的实现。如下图所示：STEP 4：What？具体做什么？这一步要求针对上面每种角色不同的影响，罗列具体的操作内容。也就是说，要做什么事情才能产生上面这个影响。 后台开发人员想要提高后端接口响应速度，可以通过添加分布式缓存、数据库添加索引、数据库读写分离等方式解决； 前端开发人员想要优化前端代码，可以通过添加浏览器缓存、采用懒加载方式和采用 CDN 访问静态资源等方式解决； 架构师想要优化服务器部署结构，可以通过改单点部署为集群部署，用负载均衡服务器分担单节点压力等方式解决； 设备管理员想要提升硬件设备性能，可以通过购买高性能服务器和更换万兆光纤等方式解决； 领导想要加快方案落地速度，可以通过及时审批（审查？）审批流程等方式解决。这些就是我们每个迭代需要的具体任务，在迭代结束时应该提供的交付物。如下图所示。STEP 5：选择最短路径通过上面 4 个步骤，形成了一份可供选择的任务清单。接下来应该做的并不是完成所有的任务，而是选择其中成本低、效果好的完成。这里需要我们预估每个交付物的效果和成本。如下图所示，可以看出效果较好，成本低的交付物有： 添加分布式缓存 数据库添加索引 添加浏览器缓存和采用懒加载的方式等选择这些路径完成，将为我们减轻不少负担。STEP 6：里程碑规划上一个步骤中选择的最短路径，可以作为第一个里程碑中需要交付的内容。如下图所示，标记为 ① 的任务是需要加入里程碑规划里的任务。待该里程碑完成后，根据度量指标衡量目标达成的效果，再从剩余的选项中继续选择成本低、效果好的任务添加到下一个里程碑规划中，直到目标达成。到此，利用影响地图对产品经理提出的需求进行了分析和里程碑规划。从而确定了需求目标、具体工作内容和里程碑规划。上面这几个图有几个特点： 结构化 整体性 协作性 动态性" }, { "title": "DevOps - 历史(01)", "url": "/posts/devops-1/", "categories": "DevOps", "tags": "DevOps", "date": "2020-02-05 02:33:00 +0000", "snippet": "谈到 DevOps ，就必须提到以下两个词： 精益软件开发 敏捷软件开发它们二者是 DevOps 发展的两大基础。DevOps 不仅是对二者进行扩展，还引入了很多重要并有用的原则。于是有些人认为 DevOps 是敏捷的一部分，其实这样的理解是错误的。DevOps 并不从属于二者，更像是在精益和敏捷经验基础上发展出的新生儿，是在前人的经验上发展，与时俱进的产物。精益软件开发精益思想诞生于 20 世纪中期的日本丰田汽车公司。 在美国汽车大行其道的时代，后期的日本汽车工业明显处于薄弱的阶段。成立于 1937 年的日本丰田汽车公司，就面临着不少危机。 二战后市场购买力低、车辆需求多样化、技术落后、资金短缺…… 丰田汽车的负责人大野耐一为了带领丰田生存。在经过 20 多年的不断探索之后，终于找到了适合日本国情的汽车生产方式：丰田生产系统（TPS）。 大野耐一将丰田生产系统描述为“一个绝对消除浪费的系统”，并实行多品种、小批量、高质量和低消耗的精益生产方式。 此后，精益生产方式成了日本汽车工业在市场竞争中战胜美国的重要法宝。也由此促使美国为代表的世界各地的专家、学者对开创大批量生产方式和精益生产方式的汽车工业进行研究。最终人们将这种生产方式命名为精益生产。“精”体现在对产品质量上追求“精益求精”，“益”体现在只有通过低成本、少消耗， 企业才能获得收益。2003年，由 Mary 和 Tom Poppendieck 编写的 《精益软件开发：软件开发管理者的敏捷工具箱》一书问世，该书第一次把精益思想映射到软件开发中。在2006年，作者在第二本书 《实施精益软件开发：从概念到价值实践》 中进一步完善了这种映射，并定义了 7 项原则： 消除浪费 内建质量 创建知识 推迟决策 快速交付 对人尊重 整体优化精益软件开发的作者认为，消除浪费是精益思想的精髓所在。在精益生产领域，一共定义了七种浪费。这七种浪费在软件开发中的对应如下： 序号 工业 软件开发 解释 1 库存 未完成的工作 一切已经开始但还未完成的东西。可能是未编码实现的需求，未进行测试、部署的代码。未完成的工作不能给最终的客户带来价值，但资源已经被消耗了 2 过度处理 不必要的流程 纯粹的浪费，没有增加任何价值。不必要的流程包括没有达成任何产出的过程，编写没有人读的文档，没有效果的汇报，可以自动化但却还是以手工处理的任务等。 3 过度生产 额外的特性 用户没有明确的需求。每一行代码都是有成本的，因此，用户不需要的功能就不要创建它。 4 运输 传递 中间交付物从一个阶段传递到下一个阶段。比如用户故事从业务人员传递到开发人员，系统设计从架构师传递到开发人员等。每次传递都会有知识丢失，因此要尽可能避免传递。 5 等待 延迟 指在任何环节中的等待，等待就会影响开发进度。与等待相关的延迟有：等待决策、等待资源分配或释放等。 6 移动 任务切换 在不同事情之间来回进行切换。每次切换都需要重新回忆之前任务的上下文，浪费大量时间。相信这个大家都深有体会。 7 缺陷 缺陷 缺陷在制造业和软件产品中都存在的。缺陷会导致返工，返工是没有增加任何价值的，是严重的浪费。 精益软件开发主要关注点是：在对客户有价值的上下文环境中消除浪费。为了帮助企业了解软解开发流程中发送的一切。这里可以采用 价值流图（Value Stream Map，VSM）这个精益工具，来用于检测和消除浪费。 VSM 是一种类似地图的图形，可视化地展示了从开始到结束的所有步骤。每一个活动都被标识了“增加价值的”“没有增加价值的”或者“没有增加价值但必需的”。使用 VSM，让我们更容易识别开发过程中的浪费和瓶颈，更好地采用精益软件开发方法。敏捷软件开发20 世纪中叶，软件开发方法中被广泛采用的是瀑布模型。将软件开发过程分解为一系列依次完成的单独阶段，包括需求、设计、实现、测试、部署和运维。整个开发过程就像瀑布一样，依次向下经过这些阶段。每个阶段都有一个开始点和结束点，上一个阶段结束后，便进入到下一个阶段，一旦进入到下一个阶段，就不能再回到上一个阶段。.这种模型的一个前提是需求不再发生变化，每个阶段通过文档进行流转，这也就导致了采用瀑布模型开发的软件项目存在大量的问题，比如进度超期、预算超支、不符合用户需求、需求中途被取消等。这些问题比较资深的程序员应该都遇到过。这是因为软件开发本身是一个高度动态的过程，软件开发过程中的变化是不可避免的。比如： 在需求阶段，由于用户也不知道他们想要什么，而这必然会带来需求的改变； 在编码阶段，可能会发现设计有问题需要重新设计； 在测试阶段，可能会发现代码逻辑有问题需要重新修改；而这必然导致要返回到上一个阶段或上上一个阶段进行返工修复，这些在瀑布模型下是不允许的或者成本是非常高的。瀑布模型的软件开发导致了项目失败率高、软件质量低下和客户满意度低。20 世纪 90 年代，人们对瀑布模式的软件开发方法越来越不满意。这种情况下，诞生了其他一些软件开发方法，包括： 极限编程（XP） Scrum 方法 动态系统开发方法（DSDM） 水晶方法（Crystal） 以及特征驱动开发（Feature Driven Development，FDD） 等。这些方法有很多相同之处，都是希望能够解决软件开发中遇到的问题。在 2001 年 2 月，17 位软件开发领域的思想家和领军人物，在美国犹他州的雪鸟滑雪场聚会，为期两天的聚会，几乎囊括了当时软件开发领域大多数知名大咖。这次著名的“雪鸟会议”最终产出了三个重要的结论： 决定使用“敏捷”这一术语来表示这些新出现的开发方法。 发表了《敏捷宣言》。 成立了敏捷联盟。其中，《敏捷宣言》定义了构成所有敏捷方法的 4 个核心价值观，成为日后指导软件开发的新的开发方法。 我们正通过亲身实践和帮助他人实践，来揭示更好的软件开发方法。通过这项工作，我们认为：个体和互动 胜于 过程和工具可工作的软件 胜于 面面俱到的文档客户合作 胜于 合同谈判响应变化 胜于 遵循计划虽然右边也具有价值，但是我们认为左边具有更大的价值。——引自《敏捷宣言》在这个《敏捷宣言》陈述的 4 项价值观中，有一点需要特别提醒注意：每一项都涉及左右两项内容，这里并不是说右边的没有价值，而是说左右两项都是有价值的，不过左边的比右边的更有价值。随后，社区中的项目管理者和开发人员开始采纳并发展敏捷方法，能够快速交付客户可用的软件。敏捷开发中的关键点有以下几点 开发人员与业务人员要相互合作，才能欣然面对需求的变化。即便到了开发后期也一样，为了客户的价值，通过敏捷来响应变化。 降低每个迭代交付的批量大小，减短交付周期，经常交付可工作的软件。通过使用敏捷方法，软件开发团队一般 2 到 4 周即可发布一个可工作的产品，并交付给客户。这相比以往几个月发布一个版本的节奏，发布周期缩短了很多。因此客户能够更早地使用产品，开发人员也能够更快地获得反馈，并由此对产品进行调整。可以看出，敏捷软件开发并不只是提高了交付软件的效率，更是响应了用户不断变化的需求。DevOps 的兴起在一些企业里，享受到了敏捷方法带来的好处，极大地提高了软件的交付速度。但是在有些企业里，在采用敏捷软件开发方法后的实际效果并没有达到预期。这并不是说敏捷方法不好，问题的关键在于敏捷所涉及的代码开发阶段只是软件端到端价值交付链中的一环。在实际开发过程中： 开发之前，还有需求收集、分析、拆解、设计等环节 开发之后，还有编译、构建，及将制品部署到测试环境、生产环境等环节只有部署到生产环境交付给用户使用，用户将使用效果反馈给开发人员，这才算是一个软件开发周期的完成。快速部署到生产环境这个环节在很多企业里也是很难实现的。因为，在大多数企业中服务器等硬件基础设施还相对薄弱，多是陈旧的老机器，新采购的预算申请较难且周期长。在这种情况下，为了准备一套测试环境短则数周，长则数月。这些问题严重阻碍了软件向测试环境和生产环境部署的效率。同时，为了保证生产环境的稳定性，运维部门的原则是能不变更就不变更。所有的这些问题，都使敏捷开发的收益难以有效体现出来。随着虚拟机和云计算技术的快速发展，提供 IT 基础设施的方式方法被彻底改变了。资源被池化，可以根据自己的需求动态分配物理资源，用完即可回收。基于基础设施即代码（Infrastructure as Code，IaC）的管理方式，只需要编写程序代码，就可以创建基础设施。同时，基础设施也能像代码一样进行管理，进行版本控制、变更追溯、回滚等操作。如今，Docker、K8s 等容器技术的兴起，更进一步提高了实施基础设施的效率。最终，敏捷软件开发方法的广泛普及，以及IT 基础设施即代码（IaC）的管理方式的出现，推动了IT管理方式的变革，这项运动被冠以”DevOps”之名。DevOps是一场运动，是推动企业内部IT管理方式变革的运动。DevOps是一个实践，包含了业界广泛采用的、卓有成效的软件开发方法。DevOps是一个思想，是对精益和敏捷思想的演进，并应用到IT端到端的价值链中。DevOps 缺少一个清晰的定义，每个人对 DevOps 的理解也都不一样。正如下面这张盲人摸象图，很形象地说明了当前大多数人对 DevOps 理解的现状。DevOps 是抽象的，不是具体的；是全局的，不是局部的。DevOps 告诉我们要做什么（What），但没告诉我们要如何做（How）。因为对于一个企业，IT 管理方式的变革跟企业内部的文化、流程、技术关系密切。当企业在应用DevOps的时候，要结合企业现状，有针对性的，制定适合企业内部的应用方案，切勿盲目追求，为了做而做。DevOps的兴起并席卷全球，是因为 DevOps 的确带来了好处，解决了企业面临的现实问题。“缩短市场响应时间”“减少技术债务”和“消除脆弱性”是 DevOp 着手处理的主要任务。这三个任务中任何一个的改进，都能为企业的业务带来明显的优势。 DevOps 运动的起点被认为是在 2009 年的 Velocity 大会。这次大会上，John Allspaw（约翰·沃斯帕）和 Paul Hammond（保罗·哈蒙德） 发表了主题为 “每天 10+ 次部署：Flickr 开发和运维的协作” 的演讲。同年，Patrick Debois 决定在比利时根特市举办首届 DevOpsDays 专题大会。 到了 2013年，参加这个活动的 Gene Kim（基恩·金） 的 《凤凰项目》 一书出版，DevOps 企业峰会也开始以每年两届的节奏举办。至此，DevOps 的概念以及社区，开始在全球如火如荼地发展起来。 2017年，中国首届 DevOpsDays 大会在北京召开，DevOps 之父 Patrick 也来到了现场，这场席卷全球的 DevOps 运动终于在中国落地生根。总结通过对软件开发方法演变的总结，发现直到今天，软件开发方法还是在不断演进，出现了很多新的实践，这些新出现的实践就被打上了 DevOps 的标签，成为组织成功的重要因素。DevOps 通过将敏捷软件开发方法和后续快速部署到生产环境的基础设施能力相结合，真正地打通端到端的软件开发流程，将开发和运维相互融合，最终高效的、高质量的交付用户价值，为企业赢得市场。" }, { "title": "垃圾回收 - 活跃性分析：GC Roots", "url": "/posts/gc-1/", "categories": "Java, JVM", "tags": "JVM, OOM, Reference Chain, OOM, Out Of Memory, 可达性分析, 引用计数法, GC", "date": "2019-12-15 15:33:00 +0000", "snippet": "堆（heap），是一个巨大的对象池。在这个对象池中管理着数量巨大的对象实例。而池中对象的引用层次，有的是很深的。一个被频繁调用的接口，每秒生成对象的速度，是非常客观的。对象之间的关系，形成了一张巨大的网。虽然 Java 一直在营造一种无限内存的氛围，但对象不能只增不减，于是需要垃圾回收。JVM 是怎么判断哪些对象应该被回收？哪些对象应该被保留？在古代，刑罚有诛九族之说，指的是有些人犯大事时，杀一人不足平民愤，会对其亲朋好友产生连带责任，诛九族时首先会追溯到一个共同的祖先，再往下细数连坐。堆上的垃圾回收也是同样的思路。JVM 的垃圾回收可达性分析JVM 的 GC 动作，是不受程序控制的，它会在满足条件的时候，自动触发。在发生 GC 的时候，对于一个对象，JVM 总能找到引用它的祖先。找到最后，如果发现这个祖先已经名存实亡，它们都会被清理掉。而能够躲过垃圾回收的那些祖先，比较特殊，它们的名字就叫作 GC Roots。从 GC Roots 向下追溯、搜索，会产生一个叫做 Reference Chain 的链条。当一个对象不能和任何一个 GC Root 产生关系时，就会无情的谋杀掉！！！垃圾回收就是围绕着 GC Roots 去做的。同时，它也有很多内存泄漏的根源。就像上图，Obj5、Obj6、Obj7，由于不能和 GC Root 产生关联，发送 GC 时，就会摧毁。GC Roots 有哪些GC Roots 是一组必须活跃的引用。换句话来说，就是程序接下来通过直接引用或间接引用，能够访问到的潜在被使用的对象。GC Roots 包括： Java 线程中，当前所有正在被调用的方法的引用类型参数、局部变量、临时值等。也就是栈帧相关的各种引用； 所有当前被加载的 Java 类； Java 类的引用类型静态变量； 运行时常量池里的引用类型变量（String 或 Class 类型）； JVM 内部数据结构的一些引用，比如 sun.jvm.hotspot.memory.Universe 类； 用于同步的监控对象，比如调用了对象的 wait() 方法； JNI handles，包括 global handles 和 local handles。以上 GC Roots 大体可以分为三类： 活动线程相关的各种引用； 类的静态变量的引用； JNI 引用。有两个注意点： 这里活跃的引用，不是对象，对象是不能作为 GC Roots 的； GC 过程是找出所有活跃的对象，并把其余空间认定为”无用”；而不是找出所有死掉的对象，并回收它们占用的空间。所以，哪怕 JVM 的堆非常大，基于 tracing 的 GC 方式，回收速度也相当快。引用级别能够找到 Reference Chain 的对象，就一定会存活 ？对象对于另外一个对象的引用，要看关系牢靠不牢靠，可能在链条的其中一环，就断掉了。根据发生 GC 时，这条链条的表现，可以对这个引用关系进行更加细致的划分。它们的关系，可以分为强引用、软引用、弱引用、虚引用等。强引用 Strong references当内存空间不足，系统支撑不住了，JVM 就会抛出 OutOfMemoryError 错误。即使程序会异常终止，这种对象也不会被回收。这种引用属于最普通最强硬的一种存在，只有在和 GC Roots 断绝关系时，才会被消灭掉。这种引用，每天的编码中都在使用。例如：new 一个普通对象：Object obj = new Object();这种方式可能是有问题的。例如系统正被大量用户（User）访问，需要记录这个 User 访问的时间，遗憾的是，User 对象里面并没有这个字段，所以决定将这些信息额外开辟一个空间进行存放。static Map&amp;lt;User,Long&amp;gt; userVisitMap = new HashMap();...userVisitMap.put(user, time)当使用完 User 对象，其实是期望它被回收掉的。但是，由于它被 userVisitMap 引用，没有其他手段 remove 掉它。这个时候，就发生了内存泄漏（memory leak）。这种情况还通常发生在一个没有设定上限的 Cache 系统，由于设置了不正确的引用方式，加上不正确的容量，很容易造成 OOM。软引用 Soft references软引用用于维护一些可有可无的对象。在内存足够的时候，软引用对象不会被回收，只有在内存不足的时候，系统则会回收软引用对象，如果回收了软引用对象之后仍然没有足够的内存，才会抛出内存溢出异常。可以看到，这种特性非常适合用在缓存技术上。比如网页缓存、图片缓存等。Guave 的 CacheBuilder ，提供了软引用和弱引用的设置方式。在这种场景中，软引用比强引用安全的多。软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，Java 虚拟机就会把这个软引用加入到与之关联的引用队列中。它的代码如下。软引用需要显式的声明，使用泛型来实现：// 伪代码Object object = new Object();SoftReference&amp;lt;Object&amp;gt; softRef = new SoftReference(Object);这里有一个相关的 JVM 参数。它的意思是：每 MB 堆空闲空间中 SoftReference 的存活空间。这个值的默认时间是 1 秒（1000）：-XX:SoftRefLRUPolicyMSPerMB=&amp;lt;N&amp;gt;这里特别的是：网上流传的一些优化方法，即把这个值设置成 0，这是错误的，因为很容易引发故障！！！这种偏门的优化手段，除非对原理相对了解的情况下，才能设置一些特殊的值。比如：0 值、无限大等。这种值在 JVM 的设置中，最好不要发生！！！弱引用 Weak references弱引用对象相比较弱引用，更加无用一些，生命周期更短！！！当 JVM 进行垃圾回收，无论内存是否充足，都会回收被弱引用关联的对象。弱引用拥有更短的生命周期，在 Java 中，用 java.lang.ref.WeakReference 类来表示。它的应用场景和软引用类型，可以在一些对内存更加敏感的系统里采用。它的使用方式类似于下面的代码：// 伪代码Object object = new Object();WeakReference&amp;lt;Object&amp;gt; softRef = new WeakReference(object);虚引用 Phantom References这是一种形同虚设的引用。在现实场景中用的不是很多。虚引用必须和引用队列（ReferenceQueue）联合使用。如果一个对象仅仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。实际上，虚引用的 get，总是返回 null。Object object = new Object();ReferenceQueue queue = new ReferenceQueue();// 虚引用，必须与一个引用队列关联PhantomReference pr = new PhantomReference(Object, queue);虚引用主要用来跟踪对象被垃圾回收的活动。当垃圾回收器准备回收一个对象的时候，如果发现它还有虚引用，就会在回收对象之前，将这个虚引用加入到与之关联的引用队列中。程序如果发现某个虚引用被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。下面的栗子，是一个用于监控 GC 发生的栗子：private static void startMonitoring(ReferenceQueue&amp;lt;MyObject&amp;gt; referenceQueue, Reference&amp;lt;MyObject&amp;gt; reference) { ExecutorService ex = Executors.newSingleThreadExecutor(); ex.execute(() -&amp;gt; { while (referenceQueue.poll() != reference) { // don&#39;t hang forever if (finishFlag) { break; } } System.out.println(&quot;-- ref gc&#39;ed --&quot;); }); ex.shutdown();}基于虚引用，有一个更加优雅的实现方式，就是 Java 9 以后新加入的 Cleaner ，用来替代 Object 类的 finalizer 方法。典型 OOM 场景OOM 的全称是 Out Of Memory，从内存区域划分图上，可以看出哪些内存区域会发生 OOM：可以看到除了程序计数器，其他区域都有 OOM 溢出的可能。但是最常见的还是发生在堆上。 区域 是否线程私有 是否会发生 OOM 程序计数器 是 否 虚拟机栈 是 是 本地方法栈 是 是 方法区 否 是 直接内存 否 是 堆 否 是 引起 OOM 有几个原因： 内存的容量太小了，需要扩容，或者需要调整堆的空间； 错误的引用方式，发生了内存泄漏。没有及时的切断与 GC Roots 的关系。比如线程池里的线程，在复用的情况下忘记清理 ThreadLocal 的内容； 接口没有进行范围校验，外部传参超出范围。比如数据库查询时的每页条数等。 对堆外内存无限制的使用。这种情况一旦发生更加严重，会造成操作系统内存耗尽典型的内存泄漏场景，原因在于对象没有及时的释放自己的引用。比如一个局部变量，被外部的静态集合引用。平常写代码时，一定要注意这种情况，千万不要为了方便把对象到处引用。即使引用了，也要在合适时机进行手动清理！！！引用计数法还有一种叫作引用计数法的方式，在判断对象的存活问题上，经常被提及。因为有循环依赖的硬伤，现在主流的 JVM，没有一个是采用引用计数法来实现 GC 的。引用计数法是在对象头里维护一个 counter 计数器，被引用一次数量 +1，引用失效记数 -1。计数器为 0 时，就被认为无效。（不经常用，忘掉！！！）总结 HostSpot 采用 tracing 的方式进行 GC，内存回收的速度与处于 living 状态的对象数量有关。 四种不同强度的引用类型，尤其是软引用和虚引用，在平常工作中使用还是比较多的。这里面最不常用的就是虚引用，但是它引申出来的 Cleaner 类，是用来替代 finalizer 方法的。" }, { "title": "JVM 是啥玩意", "url": "/posts/What-is-jvm/", "categories": "Java, JVM", "tags": "JVM", "date": "2019-11-20 12:13:00 +0000", "snippet": " 为什么 Java 研发系统需要 JVM ? JVM 的原理是什么 ？ 写的 Java 代码到底是如何运行起来的 ？想要完美回答这三个问题，就需要首先了解 JVM 是什么 ？ 它和 Java 有什么关系 ？ 又与 JDK 有什么渊源 ？为了弄清楚这写问题，我是从以下三个维度思考的： JVM 和操作系统的关系 ？ JVM、JRE、JDK 的关系 ？ Java 虚拟机规范和 Java 语言规范的关系 ？JVM 和 操作系统的关系在武侠小说中，想要炼制一把睥睨天下的宝剑，是需要下一番功夫的。除了上等的铸剑技术，还需要一鼎经百炼的剑炉，如果将工程师比作铸剑师，那 JVM 便是剑炉！JVM 的全称是 Java Vitual Machine ，也就是常说的 Java 虚拟机。JVM 可以识别以 .class后缀的文件（字节码文件），并且能够解析它（以.class后缀的文件），最终调用操作系统上的函数，完成我们想要的操作。一般而言，使用 C++ 编写的程序，编译成二进制文件后，能够被操作系统直接识别，因此可以直接执行；而使用 Java 编写的程序不一样，使用 javac命令编译成.class文件（字节码文件）之后，操作系统并不认识这些.class文件，还需要使用 Java 命令去主动执行它。为什么 Java 程序不能像 C++ 一样，直接在操作系统上运行编译后的二进制文件呢 ？而非要搞一个处于程序和操作系统中间层的虚拟机呢 ？这就是 JVM 过人之处！众所周知，Java 是一门抽象程度很高的编程语言，提供了自动内存管理等一系列的特性。这些特性直接在操作系统上实现是不可能的，所以需要 JVM 进行一番转换。可以做如下类比： JVM 等同于操作系统 Java 字节码（以 .class 后缀结尾的文件），等同于汇编语言对于 Java 字节码，比较容易读懂，这也从侧面反映 Java 语言的抽象程度比较高。可以把 JVM 认为是一个翻译器，会持续不断的翻译执行 Java 字节码，然后调用真正的操作系统的函数，这些操作系统函数是与平台息息相关的。可以借助下图，更好的理解。点击查看【processon】从上图可以看出，有了 JVM 这一层，Java 就可以实现跨平台了。JVM 只需要保证能够正确执行以.class文件，就可以运行在诸如 Linux、Windows、MacOS 等平台上了。Java 跨平台的意义在于一次编译，处处运行，能够做到这一点，JVM 功不可没。比如在 Maven 仓库下载同一版本的 jar 包就可以到处运行，不需要在每个平台上再编译一次。现在一些 JVM 的扩展语言，比如 Clojure、JRuby、Groovy 等，编译到最后都是.class文件，Java 语言的维护者，只需要控制好 JVM 这个解析器，就可以将这些扩展语言无缝的运行在 JVM 之上了。一句话总结 JVM 与操作系统之间的关系，如下：:::tipsJVM 上承开发语言，下接操作系统，它的中间接口就是字节码。:::而 Java 程序和 C++ 程序的不同如下图所示。点击查看【processon】通过上图的对比，可以看出 C++ 程序是编译成操作系统能够识别的 .exe文件； Java 程序是编译成 JVM 能够识别的 .class文件，然后由 JVM 负责调用系统函数执行程序JVM、JRE 、JDK 的关系通过上文，可以了解到 JVM 是 Java 程序能够运行的核心。换个角度想一想，JVM 没有了以 .class为后缀的文件，什么也干不了。需要为 JVM 提供生产原料，也就是以.class为后缀的文件。俗话说，巧妇难为无米之炊。JVM 虽然功能强大，但仍需要为它提供.class文件。仅仅有 JVM 的话，Java 程序是无法完成一次编译，处处运行的。它需要一个基本的类库，比如： 怎么操作文件 ？ 怎么连接网络等 ？Java 体系是非常慷慨的，会一次性将 JVM 运行所需要的类库都传递给它。:::tipsJVM 标准加上实现的一大堆基础类库，组成了 Java 运行时环境，也就是常说的 JRE ( Java Runtime Environment):::有了 JRE 之后，Java 程序就可以运行了 。:::tips可以观察 Java 目录，如果只需要执行一些 Java 程序，只需要一个 JRE 就足够了。:::对于 JDK 来说，就更庞大了。除了 JRE，JDK 还提供了一些非常好用的小工具，比如： javac java jar 等:::tipsJDK 是 Java 开发的核心:::JDK 的全称是 Java Development Kit . 其中，kit 这个单词是装备的意思，它就像一个无底洞，预示着我对它无休止的研究。JVM、JRE、JDK 三者之间的关系，可以用一个包含关系表示，如下图所示。点击查看【processon】Java 虚拟机规范和 Java 语言规范的关系通常谈到 JVM ，首先想到的是它的垃圾回收器。其实它还很多部分，比如： 对字节码进行解析的执行引擎广泛的讲，JVM 是一种规范，它是最官方、最为准确的文档；侠义的讲，日常开发，使用 Hotspot 更多一些，谈到 Java 虚拟机规范时，会将它们等同起来点击查看【processon】上图的左半部分是 Java 虚拟机规范，其实就是为输入和执行字节码提供一个运行环境。上图的右半部分是 Java 语言规范，比如 switch、for、泛型、lambda等相关程序。它们最终都会编译成字节码。而字节码是连接左右两部分的桥梁。如果 .class文件的规格是不变的，这两部分是可以独立进行优化的。不过，Java 也会偶尔扩充一下 .class文件的格式，增加一些字节码指令，以便支持更多的特性。如果把 Java 虚拟机看作一台抽象的计算机，它有自己的指令集以及各种运行时内存区域。Java 代码是如何运行起来的比如下面的 HelloWorld.java ，它遵循的就是 Java 语言规范。其中，调用了 System.out等模块，也就是 JRE 里提供的类库。public class HelloWorld { public static void main(String[] args) { System.out.println(&quot;Hello World&quot;); }}使用 JDK 的工具 javac进行编译后，会产生 HelloWorld的字节码。Java 字节码是沟通 JVM 和 Java 程序的桥梁，可以通过命令 [javap](https://www.cnblogs.com/frinder6/p/5440173.html)来看下字节码的结构：0 getstatic #2 &amp;lt;java/lang/System.out&amp;gt;3 ldc #3 &amp;lt;Hello World&amp;gt;5 invokevirtual #4 &amp;lt;java/io/PrintStream.println&amp;gt;8 returnJava 虚拟机采用基于栈的架构，其指令由操作码和操作数组成。这些字节码指令，叫作 opcode 。其中，getstatic、Idc、invokevirtual、return 等，就是 opcode。显然，Java 的字节码是非常容易理解的。可以继续使用 hexdump 看一下字节码的二进制内容。与以上字节码对应的二进制，如下：JVM 是依靠解析这些 opcode 和操作数来完成程序的执行的，当使用 Java 命令运行 .class文件的时候，相当于启动了一个 JVM 进程。而后 JVM 会翻译这些字节码，它主要有两种执行方式 常见的是解释执行将 opcode 和操作数翻译成机器代码； 另一种执行方式是 JIT，就是常说的即时编译，它会在一定条件下将字节码解释成机器码后再执行这些 .class文件会被加载、存放到 metaspace中，等待被调用，这里有一个类加载器的概念。JVM 的程序运行，都是在栈上完成的，这和其他普通程序的执行是类似的，同样分为堆和栈。比如 main 方法，就会将它分配给一个栈帧。当退出方法的时候，会弹出相应的栈帧，可以发现，大多数字节码指令，就是不断的对栈帧进行操作。而其它大块数据，是存放在堆上的。总之，Java 程序的运行如下图所示：点击查看【processon】选用的版本JVM 一个虚拟机规范，因此有非常多的实现。其中，最流行的就是 Oracle 的 HotSpot。总结由此，最开始的三个问题，就有了答案。 1 为什么 Java 研发需要 JVM JVM 解释的是类似于汇编语言的字节码，需要一个抽象的运行时环境。同时，这个虚拟环境也需要解决字节码加载、自动垃圾回收、并发等一系列问题。JVM 其实是一个规范，定义了 .class 文件的结构、加载机制、数据存储、运行时栈等诸多内容，最常用的 JVM 实现就是 Hotspot。 2 JVM 的运行原理 JVM 的生命周期是和 Java 程序的运行一样的，当程序运行结束，JVM 实例也跟着消失了。JVM 处于整个体系中的核心位置 Java 代码到底是如何运行起来的 一个 Java 程序，首先经过 javac 编译成 .class 文件，然后 JVM 将其加载到元数据(metaspace)区，执行引擎将会通过混合模式执行这些字节码。执行时，会翻译成操作系统相关的函数。JVM 作为 .class 文件的黑盒存在，输入字节码，调用操作系统函数。过程如下：Java 文件-&amp;gt;编译器&amp;gt;字节码-&amp;gt;JVM-&amp;gt;机器码。" }, { "title": "AQS 在 CountDownLatch 等类中的应用原理", "url": "/posts/aqs-in-implment/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-11-12 16:33:00 +0000", "snippet": "AQS 用法使用 AQS 来写一个自己的线程协作工具类，分为以下三步（也是 JDK 里利用 AQS 类的主要步骤）： 新建一个自己的线程协作工具类，在内部写一个 Sync 类，该 Sync 类继承 AbstractQueuedSynchronizer，即 AQS； 想好要设计的线程协作工具类的协作逻辑，在 Sync 类里，根据是否是独占，来重写对应的方法。如果是独占，则重写 tryAcquire 和tryRelease 等方法；如果是非独占，则重写 tryAcquireShared 和 tryReleaseShared 等方法； 在自己的线程协作工具类中，实现获取/释放的相关方法，并在里面调用 AQS 对应的方法，如果是独占则调用 acquire 或 release 等方法，非独占则调用 acquireShared 或 releaseShared 或 acquireSharedInterruptibly 等方法。通过这三步就可以实现对 AQS 的利用了。上面的第二步是根据某些条件来重写特定的一部分方法，这个做法是很少遇到过，或者，是不是有更好的做法？比如通过实现接口的方式，因为实现某一个接口之后，自然就知道需要重写其中哪些方法了，为什么要先继承类，然后自己去判断选择哪些方法进行重写呢？这不是自己给自己设置障碍吗？关于这个问题的答案，AQS 的原作者 Doug Lea 的论文中已经进行了说明，他认为如果是实现接口的话，那每一个抽象方法都需要实现。比如把整个 AQS 作为接口，那么需要实现的方法有很多，包括 tryAcquire、tryRelease、tryAcquireShared、tryReleaseShared 等，但是实际上并不是每个方法都需要重写，根据需求的不同，有选择的去实现一部分就足以了，所以就设计为不采用实现接口，而采用继承类并重写方法的形式。继承类后，是不强制要求重写方法的，所以如果一个方法都不重写，行不行呢？答案是不行的，如果不重写 tryAcquire 等方法，是不行的，因为在执行的时候会抛出异常，看下 AQS 对这些方法的默认的实现就知道了。下面有四个方法的代码，分别是 tryAcquire、tryRelease、tryAcquireShared 和 tryReleaseShared 方法：protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException();}protected boolean tryRelease(int arg) { throw new UnsupportedOperationException();}protected int tryAcquireShared(int arg) { throw new UnsupportedOperationException();}protected boolean tryReleaseShared(int arg) { throw new UnsupportedOperationException();}它们内部只有一行实现代码，就是直接抛出异常，所以要求在继承 AQS 之后，必须把相关方法去重写、覆盖，这样未来写的线程协作类才能正常的运行。AQS 在 CountDownLatch 的应用上面是 AQS 的基本流程，用例子来帮助理解 AQS 在 CountDownLatch 中的应用。在 CountDownLatch 里面有一个子类，该类的类名叫 Sync，这个类正是继承自 AQS。下面是 CountDownLatch 部分代码的截取：public class CountDownLatch { /** * Synchronization control For CountDownLatch. * Uses AQS state to represent count. */ private static final class Sync extends AbstractQueuedSynchronizer { private static final long serialVersionUID = 4982264981922014374L; Sync(int count) { setState(count); } int getCount() { return getState(); } protected int tryAcquireShared(int acquires) { return (getState() == 0) ? 1 : -1; } protected boolean tryReleaseShared(int releases) { // Decrement count; signal when transition to zero for (;;) { int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; } } } private final Sync sync; //省略其他代码...}可以很明显看到最开始一个 Sync 类继承了 AQS，这正是——“第一步，新建一个自己的线程协作工具类，在内部写一个 Sync 类，该 Sync 类继承 AbstractQueuedSynchronizer，即 AQS”。而在 CountDownLatch 里面还有一个 sync 的变量，正是 Sync 类的一个对象。同时，Sync 不但继承了 AQS 类，还重写了 tryAcquireShared 和 tryReleaseShared 方法，这正对应了“第二步，想好设计的线程协作工具类的协作逻辑，在 Sync 类里，根据是否是独占，来重写对应的方法。如果是独占，则重写 tryAcquire 或 tryRelease 等方法；如果是非独占，则重写 tryAcquireShared 和 tryReleaseShared 等方法”。这里的 CountDownLatch 属于非独占的类型，因此它重写了 tryAcquireShared 和 tryReleaseShared 方法。构造函数CountDownLatch 只有一个构造方法，传入的参数是需要“倒数”的次数，每次调用 countDown 方法就会倒数 1，直到达到了最开始设定的次数之后，相当于是“打开了门闩”，所以之前在等待的线程可以继续工作了。下构造函数的代码，如下public CountDownLatch(int count) { if (count &amp;lt; 0){ throw new IllegalArgumentException(&quot;count &amp;lt; 0&quot;); } this.sync = new Sync(count);}从代码中可以看到，当 count &amp;lt; 0 时会抛出异常，当 count &amp;gt; = 0，即代码 this.sync = new Sync(count ) ，往 Sync 中传入了 count，这个里的 Sync 的构造方法如下：Sync(int count) { setState(count);}该构造函数调用了 AQS 的 setState 方法，并且把 count 传进去了，而 setState 正是给 AQS 中的 state 变量赋值的，代码如下：protected final void setState(int newState) { state = newState;}所以通过 CountDownLatch 构造函数将传入的 count 最终传递到 AQS 内部的 state 变量，给 state 赋值，state 就代表还需要倒数的次数。getCount该方法的作用是获取当前剩余的还需要“倒数”的数量，getCount 方法的源码如下：public long getCount() { return sync.getCount();}该方法 return 的是 sync 的 getCount：int getCount() { return getState();}getCount 方法调用的是 AQS 的 getState：protected final int getState() { return state;}如代码所示，protected final int getState 方法直接 return 的就是 state 的值，所以最终它获取到的就在 AQS 中 state 变量的值。countDown该方法就是 CountDownLatch 的“释放”方法，下面来看下源码：public void countDown() { sync.releaseShared(1);}在 countDown 方法中调用的是 sync 的 releaseShared 方法：public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false;}可以看出，releaseShared 先进行 if 判断，判断 tryReleaseShared 方法的返回结果，因此先把目光聚焦到 tryReleaseShared 方法中，tryReleaseShared 源码如下所示 ：protected boolean tryReleaseShared(int releases) { // Decrement count; signal when transition to zero for (;;) { int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; }}方法内是一个 for 的死循环，在循环体中，最开始是通过 getState 拿到当前 state 的值并赋值给变量 c，这个 c 可以理解为是 count 的缩写，如果此时 c = 0，则意味着已经倒数为零了，会直接会执行下面的 return false 语句，一旦 tryReleaseShared 方法返回 false，再往上看上一层的 releaseShared 方法，就会直接跳过整个 if (tryReleaseShared(arg)) 代码块，直接返回 false，相当于 releaseShared 方法不产生效果，也就意味着 countDown 方法不产生效果。再回到 tryReleaseShared 方法中往下看 return false 下面的语句，如果 c 不等于 0，在这里会先把 c-1 的值赋给 nextc，然后再利用 CAS 尝试把 nextc 赋值到 state 上。如果赋值成功就代表本次 countDown 方法操作成功，也就意味着把 AQS 内部的 state 值减了 1。最后，是 return nextc == 0，如果 nextc 为 0，意味着本次倒数后恰好达到了规定的倒数次数，门闩应当在此时打开，所以 tryReleaseShared 方法会返回 true，那么再回到之前的 releaseShared 方法中，可以看到，接下来会调用 doReleaseShared 方法，效果是对之前阻塞的线程进行唤醒，让它们继续执行。如果结合具体的数来分析，可能会更清晰。假设 c = 2，则代表需要倒数的值是 2，nextc = c-1，所以 nextc 就是 1，然后利用 CAS 尝试把 state 设置为 1，假设设置成功，最后会 return nextc == 0，此时 nextc 等于 1，不等于 0，所以返回 false，也就意味着 countDown 之后成功修改了 state 的值，把它减 1 了，但并没有唤醒线程。下一次执行 countDown时，c 的值就是 1，而 nextc = c - 1，所以 nextc 等于 0，若这时 CAS 操作成功，最后 return nextc == 0，所以方法返回 true，一旦 tryReleaseShared 方法 return true，则 releaseShared 方法会调用 doReleaseShared 方法，把所有之前阻塞的线程都唤醒。await该方法是 CountDownLatch 的“获取”方法，调用 await 方法会把线程阻塞，直到倒数为 0 才能继续执行。await 方法和 countDown 是配对的，追踪源码可以看到 await 方法的实现：public void await() throws InterruptedException { sync.acquireSharedInterruptibly(1);}它会调用 sync 的 acquireSharedInterruptibly ，并且传入 1。acquireSharedInterruptibly 方法源码如下所示：public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) { throw new InterruptedException(); } if (tryAcquireShared(arg) &amp;lt; 0){ doAcquireSharedInterruptibly(arg); }}可以看到，它除了对于中断的处理之外，比较重要的就是 tryAcquireShared 方法。这个方法很简单，它会直接判断 getState 的值是不是等于 0，如果等于 0 就返回 1，不等于 0 则返回 -1。protected int tryAcquireShared(int acquires) { return (getState() == 0) ? 1 : -1;}getState 方法获取到的值是剩余需要倒数的次数，如果此时剩余倒数的次数大于 0，那么 getState 的返回值自然不等于 0，因此 tryAcquireShared 方法会返回 -1，一旦返回 -1，再看到 if (tryAcquireShared(arg) &amp;lt; 0) 语句中，就会符合 if 的判断条件，并且去执行 doAcquireSharedInterruptibly 方法，然后会让线程进入阻塞状态。另一种情况，当 state 如果此时已经等于 0 了，那就意味着倒数其实结束了，不需要再去等待了，就是说门闩是打开状态，所以说此时 getState 返回 0，tryAcquireShared 方法返回 1 ，一旦返回 1，对于 acquireSharedInterruptibly 方法而言相当于立刻返回，也就意味着 await 方法会立刻返回，那么此时线程就不会进入阻塞状态了，相当于倒数已经结束，立刻放行了。这里的 await 和 countDown 方法，正对应了本讲一开始所介绍的“第三步，在自己的线程协作工具类中，实现获取/释放的相关方法，并在里面调用 AQS 对应的方法，如果是独占则调用 acquire 或 release 等方法，非独占则调用 acquireShared 或 releaseShared 或 acquireSharedInterruptibly 等方法。”总结当线程调用 CountDownLatch 的 await 方法时，便会尝试获取“共享锁”，不过一开始通常获取不到锁，于是线程被阻塞。“共享锁”可获取到的条件是“锁计数器”的值为 0，而“锁计数器”的初始值为 count，当每次调用 CountDownLatch 对象的 countDown 方法时，也可以把“锁计数器” -1。通过这种方式，调用 count 次 countDown 方法之后，“锁计数器”就为 0 了，于是之前等待的线程就会继续运行了，并且此时如果再有线程想调用 await 方法时也会被立刻放行，不会再去做任何阻塞操作了。" }, { "title": "AQS 的内部原理", "url": "/posts/aqs-theory/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-11-11 15:33:00 +0000", "snippet": "AQS 的内部比较复杂，代码很长而且非常不容易读懂。因此从 AQS 最核心的三个部分作为切入点，打开 AQS 的大门。AQS 最核心的三大部分就是： 状态； 队列； 期望协作工具类去实现的获取/释放等重要方法。state 状态如果我们的 AQS 想要去管理或者想作为协作工具类的一个基础框架，那么它必然要管理一些状态，而这个状态在 AQS 内部就是用 state 变量去表示的。它的定义如下：/** * The synchronization state. */private volatile int state;state 的含义并不是一成不变的，会根据具体实现类的作用不同而表示不同的含义，下面举几个例子： 在信号量里面，state 表示的是剩余许可证的数量。如果最开始把 state 设置为 10，这就代表许可证初始一共有 10 个，然后当某一个线程取走一个许可证之后，这个 state 就会变为 9，所以信号量的 state 相当于是一个内部计数器； 在 CountDownLatch 工具类里面。state 表示的是需要“倒数”的数量。如果最开始把它设置为 5，当每次调用 CountDown 方法时，state 就会减 1，一直减到 0 的时候就代表这个门闩被放开； 在 ReentrantLock 工具类里面。state 表示的是锁的占有情况。如果最开始是 0，表示没有任何线程占有锁；如果 state 变成 1，则就代表这个锁已经被某一个线程所持有了。 为什么还会变成 2、3、4 呢？为什么会往上加呢？因为 ReentrantLock 是可重入的，同一个线程再次拥有这把锁就叫重入。如果这个锁被同一个线程多次获取，那么 state 就会逐渐的往上加，state 的值表示重入的次数。在释放的时候也是逐步递减，比如一开始是 4，释放一次就变成了 3，再释放一次变成了 2，这样进行的减操作，即便是减到 2 或者 1 了，都不代表这个锁是没有任何线程持有，只有当它减到 0 的时候，此时恢复到最开始的状态了，则代表现在没有任何线程持有这个锁了。所以，state 等于 0 表示锁不被任何线程所占有，代表这个锁当前是处于释放状态的，其他线程此时就可以来尝试获取了。 以上就是三个栗子是 state 在不同类中不同含义的一个具体表现。如果未来有新的工具要利用到 AQS，它一定也需要利用 state，为这个类表示它所需要的业务逻辑和状态。state 的修改因为 state 是会被多个线程共享的，所以会被并发地修改。因此所有去修改 state 的方法都必须要保证 state 是线程安全的。可是 state 本身，仅仅是被 volatile 修饰的，volatile 本身不足以保证线程安全。AQS 在修改 state 的时候具体利用了什么样的设计来保证并发安全 ？有两个和 state 相关的方法，分别是 compareAndSetState 及 setState，它们的实现已经由 AQS 去完成了，也就是说，直接调用这两个方法就可以对 state 进行线程安全的修改。这两个方法的源码实现如下：compareAndSetState() 方法protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update);}该方法里面只有一行代码，即return unsafe.compareAndSwapInt(this, stateOffset, expect, update)，它利用了 Unsafe 里面的 CAS 操作，利用 CPU 指令的原子性保证了这个操作的原子性，与原子类去保证线程安全的原理是一致的。setState() 方法protected final void setState(int newState) { state = newState;}可以看到，它修改 state 值的时候非常直截了当，直接把 state = newState，这样就直接赋值了。这里并没有进行任何的并发安全处理，没有加锁也没有 CAS，那如何能保证线程安全呢？这里就要说到 volatile 的作用了，volatile 适用于两种场景，其中一种场景就是，当对基本类型的变量进行直接赋值时，如果加了 volatile 就可以保证它的线程安全（这是 volatile 的非常典型的使用场景）。state 是 int 类型的，属于基本类型，并且这里的 setState 方法内是对 state 直接赋值的，它不涉及读取之前的值，也不涉及在原来值的基础上再修改，所以仅仅利用 volatile 就可以保证在这种情况下的并发安全，这就是 setState 方法线程安全的原因。总之，在 AQS 中有 state 这样的一个属性，是被 volatile 修饰的，会被并发修改，代表当前工具类的某种状态，在不同的类中代表不同的含义。FIFO 队列FIFO 队列，即先进先出队列。最主要的作用是：存储等待的线程。假设很多线程都想要同时抢锁，那么大部分的线程是抢不到的，那怎么去处理这些抢不到锁的线程呢？就得需要有一个队列来存放、管理它们。所以 AQS 的一大功能就是充当线程的“排队管理器”。当多个线程去竞争同一把锁的时候，就需要用排队机制把那些没能拿到锁的线程串在一起；当前面的线程释放锁之后，这个管理器就会挑选一个合适的线程来尝试抢刚刚释放的那把锁（从队列中取得话，是第一个）。所以 AQS 就一直在维护这个队列，并把等待的线程都放到队列里面。这个队列内部是双向链表的形式，其数据结构看似简单，但是要想维护成一个线程安全的双向队列却非常复杂，因为要考虑很多的多线程并发问题。下面是 AQS 作者 Doug Lea 给出的关于这个队列的图示：在队列中，分别用 head 和 tail 来表示头节点和尾节点，两者在初始化的时候都指向了一个空节点。头节点可以理解为“当前持有锁的线程”，而在头节点之后的线程就被阻塞了，它们会等待被唤醒，唤醒也是由 AQS 负责操作的。获取/释放方法除了 state 和队列之外，还有一部分非常重要，就是获取和释放相关的重要方法，这些方法是协作工具类的逻辑的具体体现，需要每一个协作工具类自己去实现，所以在不同的工具类中，它们的实现和含义各不相同。获取方法获取操作依赖 state 变量的值。根据 state 值不同，协作工具类也会有不同的逻辑，并且在获取的时候也经常会阻塞。比如： ReentrantLock 中的 lock 方法，是其中一个“获取方法” 执行时当发现 state 不等于 0，并且当前线程不是持有锁的线程，代表这个锁已经被其他线程所持有了。此时，当然就获取不到锁，于是就让该线程进入阻塞状态； Semaphore 中的 acquire 方法，是其中一个“获取方法”， 作用是获取许可证。能不能获取到这个许可证，取决于 state 的值。 当 state 值是正数，代表还有剩余的许可证，数量足够的话，就可以成功获取； 当 state 是 0，代表已经没有更多的空余许可证了，此时这个线程就获取不到许可证，会进入阻塞状态，所以这里同样也是和 state 的值相关的； CountDownLatch 获取方法是 await 方法（包含重载方法） 作用是“等待，直到倒数结束”。 执行 await 的时候会判断 state 的值，如果 state 不等于 0，线程就陷入阻塞状态，直到其他线程执行倒数方法把 state 减为 0，此时就代表现在这个门闩放开了，所以之前阻塞的线程就会被唤醒。 “获取方法”在不同的类中代表不同的含义，往往和 state 值相关，并经常让线程进入阻塞状态，这也同样证明了 state 状态在 AQS 类中的重要地位。释放方法释放方法是站在获取方法的对立面的，通常和刚才的获取方法配合使用。获取方法可能会让线程阻塞，比如说获取不到锁就会让线程进入阻塞状态，但是释放方法通常是不会阻塞线程的。比如在 Semaphore 信号量里面，释放就是 release 方法（包含重载方法），release() 方法的作用是去释放一个许可证，会让 state 加 1；而在 CountDownLatch 里面，释放就是 countDown 方法，作用是倒数一个数，让 state 减 1。所以也可以看出，在不同的实现类里面，他们对于 state 的操作是截然不同的，需要由每一个协作类根据自己的逻辑去具体实现。资源： AQS 作者本人 Doug Lea 所写的一篇论文； Javadoop 博客对于 AQS 的源码分析。总结AQS 最重要的三个部分： state，一个数值，在不同的类中表示不同的含义，往往代表一种状态； FIFO 队列，该队列用来存放线程； “获取/释放”的相关方法，利用 AQS 的工具类根据具体的业务逻辑去实现。" }, { "title": "AQS 的作用和重要性", "url": "/posts/aqs-effect/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-11-10 15:23:00 +0000", "snippet": "AQS 的重要性如上图所示，AQS 在 ReentrantLock、ReentrantReadWriteLock、Semaphore、CountDownLatch、ThreadPoolExcutor 的 Worker 中都有运用（JDK 1.8），AQS 是这些类的底层原理。以上这些类，大多数经常使用的类，可见 JUC 包里很多重要的工具类背后都离不开 AQS 框架，因此 AQS 的重要性不言而喻。锁和协作类有共同点：阀门功能从熟悉的类作为学习 AQS 的切入点。ReentrantLock 和 Semaphore，二者之间的共同点是：当作一个阀门来使用比如：把 Semaphore 的许可证数量设置为 1，那么由于它只有一个许可证，所以只能允许一个线程通过，并且当之前的线程归还许可证后，会允许其他线程继续获得许可证。这点和 ReentrantLock 很像，只有一个线程能获得锁，并且当这个线程释放锁之后，会允许其他的线程获得锁。如果线程发现当前没有额外的许可证时，或者当前得不到锁，那么线程就会被阻塞，并且等到后续有许可证或者锁释放出来后，被唤醒，所以这些环节都是比较类似的。除了 ReentrantLock 和 Semaphore 之外，CountDownLatch、ReentrantReadWriteLock 等工具类都有类似的让线程“协作”的功能，它们背后都是利用 AQS 来实现的。为什么需要 AQS上面刚讲的那些协作类，它们有很多工作是类似的，所以如果能把实现类似工作的代码给提取出来，变成一个新的底层工具类（或称为框架）的话，就可以直接使用这个工具类来构建上层代码了，而这个工具类其实就是 AQS。有了 AQS 之后，对于 ReentrantLock 和 Semaphore 等线程协作工具类而言，它们就不需要关心这么多的线程调度细节，只需要实现它们各自的设计逻辑即可。如果没有 AQS如果没有 AQS，就需要每个线程协作工具类自己去实现至少以下内容，包括： 状态的原子性管理 线程的阻塞与解除阻塞 队列的管理这里的状态对于不同的工具类而言，代表不同的含义，比如对于 ReentrantLock 而言，它需要维护锁被重入的次数，但是保存重入次数的变量是会被多线程同时操作的，就需要进行处理，以便保证线程安全。不仅如此，对于那些未抢到锁的线程，还应该让它们陷入阻塞，并进行排队，并在合适的时机唤醒。所以说这些内容其实是比较繁琐的，而且也是比较重复的，而这些工作目前都由 AQS 来承担了。如果没有 AQS，就需要 ReentrantLock 等类来自己实现相关的逻辑，但是让每个线程协作工具类自己去正确并且高效地实现这些内容，是相当有难度的。AQS 可以帮我们把 “脏活累活” 都搞定，所以对于 ReentrantLock 和 Semaphore 等类而言，它们只需要关注自己特有的业务逻辑即可。正所谓是“哪有什么岁月静好，不过是有人替你负重前行”。AQS 的作用AQS 是一个用于构建锁、同步器等线程协作工具类的框架。有了 AQS 以后，很多用于线程协作的工具类就都可以很方便的被写出来；有了 AQS 之后，可以让更上层的开发极大的减少工作量，避免重复造轮子，同时也避免了上层因处理不当而导致的线程安全问题，因为 AQS 把这些事情都做好了。总之，有了 AQS 之后，构建线程协作工具类就容易多了。" }, { "title": "String 是不可变的", "url": "/posts/final-string/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-30 15:33:00 +0000", "snippet": "在 Java 中，字符串是一个常量，一旦创建了一个 String 对象，就无法改变它的值，它的内容也就不可能发生变化（不考虑反射这种特殊行为）。举个例子，比如给字符串 s 赋值为“happymaya”，然后再尝试给它赋一个新值，正如下面这段代码所示：public class StringDemo { public static void main(String[] args) { String a = &quot;happymaya&quot;; a = &quot;ls&quot;; }}看上去好像是改变了字符串的值，但其背后实际上是新建了一个新的字符串“la”，并且把 s 的引用指向这个新创建出来的字符串“ls”，原来的字符串对象“happymaya”保持不变。同样，如果调用 String 的 subString() 或 replace() 等方法，同时把 s 的引用指向这个新创建出来的字符串，这样都没有改变原有字符串对象的内容，因为这些方法只不过是建了一个新的字符串而已。例如下面这个例子：String a = &quot;happymaya&quot;;a = a.substring(0,4);代码中，利用 a.subString(0, 4) 会建立一个新的字符串“lago”这四个字母，比原来少了一个字母，但是这并不会影响到原有的“lagou”这个五个字母的字符串，也就是说，现在内存中同时存在“lagou”和“lago”这两个对象。那这背后是什么原因呢？我们来看下 String 类的部分重要源码：public final class String implements java.io.Serializable, Comparable&amp;lt;String&amp;gt;, CharSequence { /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0}首先，可以看到这里面有个非常重要的属性，即 private final 的 char 数组，数组名字叫 value。它存储着字符串的每一位字符，同时 value 数组是被 final 修饰的，也就是说，这个 value 一旦被赋值，引用就不能修改了；并且在 String 的源码中可以发现，除了构造函数之外，并没有任何其他方法会修改 value 数组里面的内容，而且 value 的权限是 private，外部的类也访问不到，所以最终使得 value 是不可变的。那么有没有可能存在这种情况：其他类继承了 String 类，然后重写相关的方法，就可以修改 value 的值呢？这样的话它不就是可变的了吗？这个问题很好，不过这一点也不用担心，因为 String 类是被 final 修饰的，所以这个 String 类是不会被继承的，因此没有任何人可以通过扩展或者覆盖行为来破坏 String 类的不变性。这就是 String 具备不变性的原因。String 不可变的好处那我们就考虑一下，为什么当时的 Java 语言设计者会把它设计成这样？当然我们不是 String 的设计者本人，也无从考究他们当时的真实想法。不过我们可以思考一下，如果把 String 设计为不可变的，会带来哪些好处呢？我经过总结，主要有以下这四个好处。字符串常量池String 不可变的第一个好处是可以使用字符串常量池。在 Java 中有字符串常量池的概念，比如两个字符串变量的内容一样，那么就会指向同一个对象，而不需创建第二个同样内容的新对象，例如：String s1 = &quot;lagou&quot;;String s2 = &quot;lagou&quot;;在图中可以看到，左边这两个引用都指向常量池中的同一个“lagou”，正是因为这样的机制，再加上 String 在程序中的应用是如此广泛，我们就可以节省大量的内存空间。如果想利用常量池这个特性，这就要求 String 必须具备不可变的性质，否则的话会出问题，我们来看下面这个例子：String s1 = &quot;lagou&quot;;String s2 = &quot;lagou&quot;;s1 = &quot;LAGOU&quot;;System.out.println(s2);想一下，假设 String 对象是可变的，那么把 s1 指向的对象从小写的“lagou”修改为大写的“LAGOU”之后，s2 理应跟着变化，那么此时打印出来的 s2 也会是大写的：LAGOU这就和预期不符了，同样也就没办法实现字符串常量池的功能了，因为对象内容可能会不停变化，没办法再实现复用了。假设这个小写的“lagou”对象已经被许多变量引用了，如果使用其中任何一个引用更改了对象值，那么其他的引用指向的内容是不应该受到影响的。实际上，由于 String 具备不可变的性质，所以上面的程序依然会打印出小写的“lagou”，不变性使得不同的字符串之间不会相互影响，符合我们预期。用作 HashMap 的 keyString 不可变的第二个好处就是它可以很方便地用作 HashMap （或者 HashSet） 的 key。通常建议把不可变对象作为 HashMap的 key，比如 String 就很合适作为 HashMap 的 key。对于 key 来说，最重要的要求就是它是不可变的，这样我们才能利用它去检索存储在 HashMap 里面的 value。由于 HashMap 的工作原理是 Hash，也就是散列，所以需要对象始终拥有相同的 Hash 值才能正常运行。如果 String 是可变的，这会带来很大的风险，因为一旦 String 对象里面的内容变了，那么 Hash 码自然就应该跟着变了，若再用这个 key 去查找的话，就找不回之前那个 value 了。缓存 HashCodeString 不可变的第三个好处就是缓存 HashCode。在 Java 中经常会用到字符串的 HashCode，在 String 类中有一个 hash 属性，代码如下：/** Cache the hash code for the String */private int hash;这是一个成员变量，保存的是 String 对象的 HashCode。因为 String 是不可变的，所以对象一旦被创建之后，HashCode 的值也就不可能变化了，我们就可以把 HashCode 缓存起来。这样的话，以后每次想要用到 HashCode 的时候，不需要重新计算，直接返回缓存过的 hash 的值就可以了，因为它不会变，这样可以提高效率，所以这就使得字符串非常适合用作 HashMap 的 key。而对于其他的不具备不变性的普通类的对象而言，如果想要去获取它的 HashCode ，就必须每次都重新算一遍，相比之下，效率就低了。线程安全String 不可变的第四个好处就是线程安全，因为具备不变性的对象一定是线程安全的，我们不需要对其采取任何额外的措施，就可以天然保证线程安全。由于 String 是不可变的，所以它就可以非常安全地被多个线程所共享，这对于多线程编程而言非常重要，避免了很多不必要的同步操作。 “缓存 HashCode”和“多线程安全”思路参考自 Deecyn：https://juejin.cn/post/6844904006909689864" }, { "title": "为什么加了 final 却依然无法拥有“不变性”", "url": "/posts/final-fail/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-29 15:33:00 +0000", "snippet": "什么是不变性什么是不变性（Immutable）。如果对象在被创建之后，其状态就不能修改了，那么它就具备“不变性”。举个例子，比如下面这个 Person 类：public class Person { final int id = 1; final int age = 18;}如果创建一个 person 对象，那么里面的属性会有两个，即 id 和 age，并且由于它们都是被 final 修饰的，所以一旦这个 person 对象被创建好，那么它里面所有的属性，即 id 和 age 就都是不能变的。如果想改变其中属性的值就会报错，代码如下所示：public class Person { final int id = 1; final int age = 18; public static void main(String[] args) { Person p = new Person();// p.age=5; // 编译错误，无法修改 final 变量的值 }}final 修饰对象时，只是引用不可变这里有个非常重要的注意点，当用 final 去修饰一个指向对象类型（而不是指向 8 种基本数据类型，例如 int 等）的变量时候，那么 final 起到的作用只是保证这个变量的引用不可变，而对象本身的内容依然是可以变化的。final 修饰的变量意味着一旦被赋值就不能修改，也就是只能被赋值一次，如果尝试对已经被 final 修饰过的变量再次赋值的话，则会报编译错误。/** * 描述：final变量一旦被赋值就不能被修改 */public class FinalVarCantChange { private final int finalVar = 0; private final Random random = new Random(); private final int array[] = {1,2,3}; public static void main(String[] args) { FinalVarCantChange finalVarCantChange = new FinalVarCantChange();// finalVarCantChange.finalVar = 9; // 编译错误，不允许修改final的变量(基本类型)// finalVarCantChange.random = null; // 编译错误，不允许修改final的变量(对象)// finalVarCantChange.array =new int[5]; // 编译错误，不允许修改final的变量（数组） }}首先在这里分别创建了一个 int 类型的变量、一个 Random 类型的变量，还有一个是数组，它们都是被 final 修饰的；后尝试对它们进行修改，比如把 int 变量的值改成 9，或者把 random 变量置为 null，或者给数组重新指定一个内容，这些代码都无法通过编译。这就证明了“被 final 修饰的变量意味着一旦被赋值就不能修改”，而这个规则对于基本类型的变量是没有歧义的，但是对于对象类型而言，final 其实只是保证这个变量的引用不可变，而对象本身依然是可以变化的。这一点同样适用于数组，因为在 Java 中数组也是对象。看一看以下 Java 程序的输出：/** * 描述：final变量一旦被赋值就不能被修改 */public class FinalVarCantChange { private final int finalVar = 0; private final Random random = new Random(); private final int array[] = {1,2,3}; public static void main(String[] args) { FinalVarCantChange finalVarCantChange = new FinalVarCantChange(); for (int i = 0; i &amp;lt; finalVarCantChange.array.length; i++) { finalVarCantChange.array[i] = finalVarCantChange.array[i] * 10; } }}现在想证明的是，数组对象里面的内容可以修改，所以接下来我们就用 for 循环把它里面的内容都乘以 10，最后打印出来结果如下：10 20 30 40 50可以看到，它打印出来的是 10 20 30 40 50，而不是最开始的 1 2 3 4 5，这就证明了，虽然数组 arr 被 final 修饰了，它的引用不能被修改，但是里面的内容依然是可以被修改的。同样，对于非数组的对象而言也是如此，我们来看下面的例子：/** * 描述：final变量一旦被赋值就不能被修改 */public class FinalVarCantChange { public static void main(String[] args) { FinalVarCantChange finalVarCantChange = new FinalVarCantChange(); int a[] = {1, 2, 4}; for (int i = 0; i &amp;lt; a.length; i++) { a[i] = a[i] * 10; } }}这个 Test 类中有一个 int 类型的 p 属性，我们在 main 函数中新建了 Test 的实例 t 之后，把它用 final 修饰，然后去尝试改它里面成员变量 p 的值，并打印出结果，程序会打印出“30”。一开始 p 的值是 20，但是最后修改完毕变成了 30，说明这次修改是成功的。以上我们就得出了一个结论，final 修饰一个指向对象的变量的时候，对象本身的内容依然是可以变化的。final 和不可变的关系具体对比一下 final 和不变性。关键字 final 可以确保变量的引用保持不变，但是不变性意味着对象一旦创建完毕就不能改变其状态，它强调的是对象内容本身，而不是引用，所以 final 和不变性这两者是很不一样的。对于一个类的对象而言，你必须要保证它创建之后所有内部状态（包括它的成员变量的内部属性等）永远不变，才是具有不变性的，这就要求所有成员变量的状态都不允许发生变化。有一种说法就认为：“要想保证对象具有不变性的最简单的办法，就是把类中所有属性都声明为 final”，这条规则是不完全正确的，它通常只适用于类的所有属性都是基本类型的情况，比如前面 Person 的例子：Person 类里面有 final int id 和 final int age 两个属性，都是基本类型的，且都加了 final，所以 Person 类的对象确实是具备不变性的。但是如果一个类里面有一个 final 修饰的成员变量，并且这个成员变量不是基本类型，而是对象类型，那么情况就不一样了。有了前面基础之后，我们知道，对于对象类型的属性而言，我果给它加了 final，它内部的成员变量还是可以变化的，因为 final 只能保证其引用不变，不能保证其内容不变。所以这个时候若一旦某个对象类型的内容发生了变化，就意味着这整个类都不具备不变性了。所以就得出了这个结论：不变性并不意味着，简单地使用 final 修饰所有类的属性，这个类的对象就具备不变性了。那就会有一个很大的疑问，假设我的类里面有一个对象类型的成员变量，那要怎样做才能保证整个对象是不可变的呢？举个例子，即一个包含对象类型的成员变量的类的对象，具备不可变性的例子。代码如下：public class ImmutableDemo { private final Set&amp;lt;String&amp;gt; lessons = new HashSet&amp;lt;&amp;gt;(); public ImmutableDemo() { lessons.add(&quot;为何说只有 1 种实现线程的方法？&quot;); lessons.add(&quot;如何正确停止线程？为什么 volatile 标记位的停止方法是错误的？&quot;); lessons.add(&quot;线程是如何在 6 种状态之间转换的？&quot;); } public boolean isLesson(String name) { return lessons.contains(name); }}在这个类中有一个 final 修饰的、且也是 private 修饰的的一个 Set 对象，叫作 lessons，它是个 HashSet；然后在构造函数中往这个 HashSet 里面加了三个值，分别是第 01、02、03 讲的题目；类中还有一个方法，即 isLesson，去判断传入的参数是不是属于本课前 3 讲的标题，isLesson 方法就是利用 lessons.contains 方法去判断的，如果包含就返回 true，否则返回 false。这个类的内容就是这些了，没有其他额外的代码了。在这种情况下，尽管 lessons 是 Set 类型的，尽管它是一个对象，但是对于 ImmutableDemo 类的对象而言，就是具备不变性的。因为 lessons 对象是 final 且 private 的，所以引用不会变，且外部也无法访问它，而且 ImmutableDemo 类也没有任何方法可以去修改 lessons 里包含的内容，只是在构造函数中对 lessons 添加了初始值，所以 ImmutableDemo 对象一旦创建完成，也就是一旦执行完构造方法，后面就再没有任何机会可以修改 lessons 里面的数据了。而对于 ImmutableDemo 类而言，它就只有这么一个成员变量，而这个成员变量一旦构造完毕之后又不能变，所以就使得这个 ImmutableDemo 类的对象是具备不变性的，这就是一个很好的“包含对象类型的成员变量的类的对象，具备不可变性”的例子。" }, { "title": "final 的三种用法", "url": "/posts/final-three-uses/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-28 15:33:00 +0000", "snippet": "final 的作用final 是 Java 中的一个关键字。简单来说，final 的作用意味着“这是无法改变的”。final 关键字一共有三种用法： 修饰变量； 修饰方法； 修饰类。在修饰不同的地方时，效果、含义和侧重点也会有所不同。final 修饰变量作用意味着被修饰得变量一旦被赋值就不能被修改了，也就是说只能被赋值一次，直到天涯海角也不会“变心”。如果尝试对一个已经赋值过 final 的变量再次赋值，就会报编译错误。下面这段代码示例：/** * &amp;lt;h1&amp;gt;final 的三种用法&amp;lt;/h1&amp;gt; * &amp;lt;ol&amp;gt; * &amp;lt;li&amp;gt;修饰变量&amp;lt;/li&amp;gt; * &amp;lt;li&amp;gt;修饰方法&amp;lt;/li&amp;gt; * &amp;lt;li&amp;gt;修饰类&amp;lt;/li&amp;gt; * &amp;lt;/ol&amp;gt; * */public class ThreeUsesOfFinal { /** * &amp;lt;h2&amp;gt;修饰变量&amp;lt;/h2&amp;gt; * &amp;lt;p&amp;gt;final 变量一旦被赋值，就不能被修改&amp;lt;/p&amp;gt; */ public final int finalVar = 0; public static void main(String[] args) { ThreeUsesOfFinal threeUsesOfFinal = new ThreeUsesOfFinal(); // threeUsesOfFinal.finalVar = 9; // 无法将值赋给 final 变量 &#39;finalVar&#39; }}在这个例子中，有一个 final 修饰的 int，这个变量叫作 finalVar，然后在 main 函数中，新建了这个类的实例，并且尝试去修改它的值，此时会报编译错误，所以这体现了 final 修饰变量的一个最主要的作用：一旦被赋值就不能被修改了。目的使用 final 修饰变量，主要有两个目的： 设计角度。比如希望创建一个一旦被赋值就不能改变的量，那么就可以使用 final 关键字。就像声明常量的时候，通常都是带 final 的： public static final int YEAR = 2021; 此时 YEAR 是固定写死的，为了防止它被修改，就给它加上了 final 关键字，这样可以让这个常量更加清晰，也更不容易出错。 线程安全的角度。不可变的对象天生就是线程安全的，所以不需要额外进行同步等处理，这些开销是没有的。如果 final 修饰的是基本数据类型，那么它自然就具备了不可变这个性质，所以自动保证了线程安全，这样的话，未来去使用它也就非常放心了。 这就是使用 final 去修饰变量的两个目的。赋值时机变量可以分为以下三种： 成员变量，类中的非 static 修饰的属性； 静态变量，类中的被 static 修饰的属性； 局部变量，方法中的变量。这三种不同情况的变量，被 final 修饰后，赋值时机也各不相同。成员变量成员变量指的是一个类中的非 static 属性。对于这种成员变量而言，被 final 修饰后，有三种赋值时机（或者叫作赋值途径）。 在声明变量的等号右边直接赋值，例如：```java/** final 的三种用法 修饰变量 修饰方法 修饰类 &amp;lt;/ol&amp;gt; * */ public class ThreeUsesOfFinal { /** 修饰变量 final 变量一旦被赋值，就不能被修改 复制时机： */ public final int finalVar = 0; public static void main(String[] args) { ThreeUsesOfFinal threeUsesOfFinal = new ThreeUsesOfFinal(); // threeUsesOfFinal.finalVar = 9; // 无法将值赋给 final 变量 ‘finalVar’ } }```在这个类中有 private final int finalVar = 0 ，在声明变量的时候就已经赋值了。 在构造函数中赋值，例如：```java/** final 的三种用法 修饰变量 修饰方法 修饰类 &amp;lt;/ol&amp;gt; * */ public class ThreeUsesOfFinal { // 首先声明了变量，但是没有赋值 public final int finalVar2; ThreeUsesOfFinal() { // 在本类的构造函数中，进行赋值 finalVar2 = 0; } } 在类的构造代码块中赋值（不常用），例如：```java/** final 的三种用法 修饰变量 修饰方法 修饰类 &amp;lt;/ol&amp;gt; * */ public class ThreeUsesOfFinal { public final int finalVar3; { finalVar3 = 0; } } ​ 同样也声明了一个变量 private final int finalVar，且没有把它赋值， 然后在下面的一个由大括号括起来的类的构造代码块中，对变量进行 了赋值，这也是合理的赋值时机。注意，三种赋值时机，必须从中挑一种来完成对 final 变量的赋值。如果不是 final 的普通变量，当然可以不用在这三种情况下赋值，完全可以在其他的时机赋值；如果不准备使用这个变量，那么自始至终不赋值甚至也是可以的。对于 final 修饰的成员变量而言，必须在三种情况中任选一种来进行赋值，而不能一种都不挑、完全不赋值，那是不行的，这是 final 语法所规定的。空白 final如果声明了 final 变量之后，并没有立刻在等号右侧对它赋值，这种情况就被称为“空白 final”。这样做的好处：增加了 final 变量的灵活性，比如可以在构造函数中根据不同的情况，对 final 变量进行不同的赋值，这样的话，被 final 修饰的变量就不会变得死板，同时又能保证在赋值后保持不变。我们用下面这个代码来说明：/** * &amp;lt;h1&amp;gt;空白 final 提供了灵活性&amp;lt;/h1&amp;gt; */public class BlankFinal { // 空白 final private final int a; // 不传参数，把 a 赋值为默认值 0 public BlankFinal() { this.a = 0; } // 穿参数，则把 a 赋值为传入的参数 public BlankFinal(int a) { this.a = a; }}在这个代码中，有一个 private final 的 int 变量叫作 a，该类有两个构造函数： 第一个构造函数是把 a 赋值为 0； 第二个构造函数是把 a 赋值为传进来的参数。所以调用不同的构造函数，就会有不同的赋值情况。这样一来，利用这个规则，就可以根据业务去给 final 变量设计更灵活的赋值逻辑。利用空白 final 的一大好处，就是可以让这个 final 变量的值并不是说非常死板，不是绝对固定的，而是可以根据情况进行灵活的赋值，只不过一旦赋值后，就不能再更改了。静态变量静态变量是类中的 static 属性，它被 final 修饰后，只有两种赋值时机。第一种同样是在声明变量的等号右边直接赋值，例如：/** * &amp;lt;h1&amp;gt;final 的三种用法&amp;lt;/h1&amp;gt; * &amp;lt;ol&amp;gt; * &amp;lt;li&amp;gt;修饰变量&amp;lt;/li&amp;gt; * &amp;lt;li&amp;gt;修饰方法&amp;lt;/li&amp;gt; * &amp;lt;li&amp;gt;修饰类&amp;lt;/li&amp;gt; * &amp;lt;/ol&amp;gt; * */public class ThreeUsesOfFinal { private static final int a = 0;}第二种赋值时机就是它可以在一个静态的 static 初始代码块中赋值，这种用法不是很多，例如：/** * &amp;lt;h1&amp;gt;final 的三种用法&amp;lt;/h1&amp;gt; * &amp;lt;ol&amp;gt; * &amp;lt;li&amp;gt;修饰变量&amp;lt;/li&amp;gt; * &amp;lt;li&amp;gt;修饰方法&amp;lt;/li&amp;gt; * &amp;lt;li&amp;gt;修饰类&amp;lt;/li&amp;gt; * &amp;lt;/ol&amp;gt; * */public class ThreeUsesOfFinal { private static final int a = 0; { a = 0; }}在这个类中有一个变量 private static final int a，然后有一个 static，接着是大括号，这是静态初始代码块的语法，在这里面我们对 a 进行了赋值，这种赋值时机也是允许的。以上就是静态 final 变量的两种赋值时机。需要注意的是，不能用普通的非静态初始代码块来给静态的 final 变量赋值。同样有一点比较特殊的是，这个 static 的 final 变量不能在构造函数中进行赋值。局部变量局部变量指的是方法中的变量，如果你把它修饰为了 final，它的含义依然是一旦赋值就不能改变。但是它的赋值时机和前两种变量是不一样的，因为它是在方法中定义的，所以它没有构造函数，也同样不存在初始代码块，所以对应的这两种赋值时机就都不存在了。实际上，对于 final 的局部变量而言，它是不限定具体赋值时机的，只要求我们在使用之前必须对它进行赋值即可。这个要求和方法中的非 final 变量的要求也是一样的，对于方法中的一个非 final 修饰的普通变量而言，它其实也是要求在使用这个变量之前对它赋值。我们来看下面这个代码的例子：/** * 描述： 本地变量的赋值时机：使用前赋值即可 */public class LocalVarAssignment1 { public void foo() { final int a = 0;//等号右边直接赋值 }}class LocalVarAssignment2 { public void foo() { final int a;//这是允许的，因为a没有被使用 }}class LocalVarAssignment3 { public void foo() { final int a; a = 0;//使用前赋值 System.out.println(a); }}首先我们来看下第一个类，即 LocalVarAssignment1，然后在 foo() 方法中有一个 final 修饰的 int a，最后这里直接在等号右边赋值。下面看第二个类，由于我们后期没有使用到这个 final 修饰的局部变量 a，所以这里实际上自始至终都没有对 a 进行赋值，即便它是 final 的，也可以对它不赋值，这种行为是语法所允许的。第三种情况就是先创造出一个 final int a，并且不在等号右边对它进行赋值，然后在使用之前对 a 进行赋值，最后再使用它，这也是允许的。总结一下，对于这种局部变量的 final 变量而言，它的赋值时机就是要求在使用之前进行赋值，否则使用一个未赋值的变量，自然会报错。特殊用法：final 修饰参数关键字 final 还可以用于修饰方法中的参数。在方法的参数列表中是可以把参数声明为 final 的，这意味着我们没有办法在方法内部对这个参数进行修改。例如：/** * 描述： final参数 */public class FinalPara { public void withFinal(final int a) { System.out.println(a);//可以读取final参数的值// a = 9; //编译错误，不允许修改final参数的值 }}在这个代码中有一个 withFinal 方法，而且这个方法的入参 a 是被 final 修饰的。接下来，我们首先把入参的 a 打印出来，这是允许的，意味着我们可以读取到它的值；但是接下来我们假设想在方法中对这个 a 进行修改，比如改成 a = 9，这就会报编译错误，因此不允许修改 final 参数的值。以上我们就把 final 修饰变量的情况都讲完了，其核心可以用一句话总结：一旦被赋值就不能被修改了。final 修饰方法选择用 final 修饰方法的原因之一是为了提高效率。因为在早期的 Java 版本中，会把 final 方法转为内嵌调用，可以消除方法调用的开销，以提高程序的运行效率。不过在后期的 Java 版本中，JVM 会对此自动进行优化，所以不需要程序员去使用 final 修饰方法来进行这些优化了，即便使用也不会带来性能上的提升。目前使用 final 去修饰方法的唯一原因，就是想把这个方法锁定，意味着任何继承类都不能修改这个方法的含义，也就是说，被 final 修饰的方法不可以被重写，不能被 override。/** * final 修饰的方法不允许被重写 */public class FinalMethod { public void drink() {} public final void eat(){}}class SubClass extends FinalMethod { @Override public void drink() { // 非 final 方法允许被重写 } void eat();}在这个代码中一共有两个类： 第一个是 FinalMethod，它里面有一个 drink 方法和 eat 方法，其中 eat 方法是被 final 修饰的； 第二个类 SubClass 继承了前面的 FinalMethod 类。然后尝试对 drink 方法进行 Override，这当然是可以的，因为它是非 final 方法；接着尝试对 eat 方法进行 Override，你会发现，在下面的子类中去重写这个 eat 方法是不行的，会报编译错误，因为不允许重写 final 方法。同时这里还有一个注意点，在下方又写了一个 public final SubClass () {}，这是一个构造函数，这里也是编译不通过的，因为构造方法不允许被 final 修饰。特例：final 的 private方法这里有一个特例，那就是用 final 去修饰 private 方法。/** * private 方法隐式指定为 final */public class PrivateFinalMethod { private final void privateEat(){}}class SubClass2 extends PrivateFinalMethod { // 编译通过，但这并不是真正的重写 private final void privateEat(){}}在这个代码例子中，首先有个 PrivateFinalMethod 类，它有个 final 修饰的方法，但是注意这个方法是 private 的。接下来，下面的 SubClass2 extends 第一个 PrivateFinalMethod 类，也就是说继承了第一个类；然后子类中又写了一个 private final void privateEat() 方法，而且这个时候编译是通过的，也就是说，子类有一个方法名字叫 privateEat，而且是 final 修饰的。同样的，这个方法一模一样的出现在了父类中，那是不是说这个子类 SubClass2 成功的重写了父类的 privateEat 方法呢？是不是意味着我们之前讲的“被 final 修饰的方法，不可被重写”，这个结论是有问题的呢？其实之前讲的结论依然是对的，但是类中的所有 private 方法都是隐式的指定为自动被 final 修饰的，额外的给它加上 final 关键字并不能起到任何效果。由于这个方法是 private 类型的，所以对于子类而言，根本就获取不到父类的这个方法，就更别说重写了。在上面这个代码例子中，其实子类并没有真正意义上的去重写父类的 privateEat 方法，子类和父类的这两个 privateEat 方法彼此之间是独立的，只是方法名碰巧一样而已。为了证明这一点，尝试在子类的 privateEat 方法上加个 Override 注解，这个时候就会提示Method does not override method from its superclass，意思是“该方法没有重写父类的方法”，就证明了这不是一次真正的重写。final 修饰类final 修饰类的含义很明确，就是这个类“不可被继承”。// 测试 final 修饰类的效果public final class FinalClassDemo{ // code} class A extends FinalClassDemo{} //编译错误，无法继承final的类//class A extends FinalClassDemo {}有一个 final 修饰的类叫作 FinalClassDemo，然后尝试写 class A extends FinalClassDemo，结果会报编译错误，因为语法规定无法继承 final 类，给类加上 final 的目的是什么呢？如果这样设计，就代表自己不会继承这个类，也不允许其他人来继承，也就是不可能有子类的出现，这在一定程度上可以保证线程安全。比如，非常经典的 String 类就是被 final 修饰的，所以自始至终也没有看到过哪个类是继承自 String 类的，这对于保证 String 的不可变性是很重要的。有个注意点，假设某个类加上了 final 关键字，不代表里面的成员变量自动被加上 final。事实上，这两者之间不存在相互影响的关系，也就是说，类是 final 的，不代表里面的属性就会自动加上 final。不过 final 修饰方法的含义就是这个方法不允许被重写，而现在如果给这个类都加了 final，那这个类连子类都不会有，就更不可能发生重写方法的情况。所以，在 final 的类里面，所有的方法，不论是 public、private 还是其他权限修饰符修饰的，都会自动的、隐式的被指定为是 final 修饰的。如果真的要使用 final 类或者方法的话，需要注明原因。因为未来代码的维护者，可能不是很理解为什么在这里使用了 final，因为使用后，对他来说是有影响的，比如用 final 修饰方法，就不能去重写了，或者说用 final 修饰了类，就不能去继承了。所以为了防止后续维护者有困惑，有必要或者说有义务说明原因，这样也不至于发生后续维护上的一些问题。在很多情况下，并需要不急着把这个类或者方法声明为 final，可以到开发的中后期再去决定这件事情，这样的话，就能更清楚的明白各个类之间的交互方式，或者是各个方法之间的关系。所以可能会发现根本就不需要去使用 final 来修饰，或者不需要把范围扩得太大，可以重构代码，把 final 应用在更小范围的类或方法上，这样造成更小的影响。总结final 用在变量、方法或者类上时，含义截然不同的： 修饰变量，意味着一旦被赋值就不能被修改； 修饰方法，意味着不能被重写； 修饰类，意味着不能被继承。" }, { "title": "哲学家就餐问题", "url": "/posts/dining-philosophers-problem/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-27 15:33:00 +0000", "snippet": "问题描述哲学家就餐问题（Dining philosophers problem），又称为刀叉问题或吃面问题【是关于死锁的经典问题】，如下图所示：有 5 个哲学家，他们面前都有一双筷子，即左手有一根筷子，右手有一根筷子（这个问题有多个版本的描述，可以说是筷子，也可以说是一刀一叉，这并不重要），重要的是必须要同时持有左右两边的两个才行，也就是说，哲学家左手要拿到一根筷子，右手也要拿到一根筷子，在这种情况下哲学家才能吃饭。为什么选择哲学家呢？因为哲学家的特点是喜欢思考，所以把哲学家一天的行为抽象为思考，然后吃饭，并且他们吃饭的时候要用一双筷子，而不能只用一根筷子。主流程哲学家就餐的主流程： 哲学家如果想吃饭，他会先尝试拿起左手的筷子； 然后再尝试拿起右手的筷子； 如果某一根筷子被别人使用了，他就得等待他人用完，用完之后他人自然会把筷子放回原位， 接着他把筷子拿起来就可以吃了（不考虑卫生问题）。伪代码哲学家就餐主流程的伪代码，如下所示：while(true) { // 思考人生、宇宙、万物... think(); // 思考后感到饿了，需要拿筷子开始吃饭 pick_up_left_chopstick(); pick_up_right_chopstick(); eat(); put_down_right_chopstick(); put_down_left_chopstick();     // 吃完饭后，继续思考人生、宇宙、万物...} while(true) 代表整个是一个无限循环； 每个循环中，哲学家首先会开始思考，思考一段时间之后（这个时间长度可以是随机的），他感到饿了，就准备开始吃饭； 在吃饭之前必须先拿到左手的筷子，再拿到右手的筷子，然后才开始吃饭；吃完之后，先放回右手的筷子，再放回左手的筷子； 由于这是个 while 循环，所以他就会继续思考人生，开启下一个循环。这就是整个过程。风险这里存在的风险呢？就是发生死锁的风险。如下图所示：根据我们的逻辑规定，在拿起左手边的筷子之后，下一步是去拿右手的筷子。大部分情况下，右边的哲学家正在思考，所以当前哲学家的右手边的筷子是空闲的，或者如果右边的哲学家正在吃饭，那么当前的哲学家就等右边的哲学家吃完饭并释放筷子，于是当前哲学家就能拿到了他右手边的筷子了。但是，如果每个哲学家都同时拿起左手的筷子，那么就形成了环形依赖，在这种特殊的情况下，每个人都拿着左手的筷子，都缺少右手的筷子，那么就没有人可以开始吃饭了，自然也就没有人会放下手中的筷子。这就陷入了死锁，形成了一个相互等待的情况。代码如下所示：package cn.happymaya.base.diedlock;public class DiningPhilosophers { static class Philosopher implements Runnable { private Object leftChopstick; private Object rightChopstick; public Philosopher(Object leftChopstick, Object rightChopstick) { this.leftChopstick = leftChopstick; this.rightChopstick = rightChopstick; } @Override public void run() { try { while (true) { doAction(&quot;思考人生、宇宙、万物、灵魂...&quot;); synchronized (rightChopstick) { doAction(&quot;拿起右边的筷子&quot;); doAction(&quot;吃饭&quot;); doAction(&quot;放下右边的筷子&quot;); } doAction(&quot;放下左边的筷子&quot;); } } catch (InterruptedException e) { e.printStackTrace(); } } private void doAction(String action) throws InterruptedException { System.out.println(Thread.currentThread().getName() + &quot; &quot; + action); Thread.sleep((long)(Math.random() * 10)); } } public static void main(String[] args) { Philosopher[] philosophers = new Philosopher[5]; Object[] chopsticks = new Object[philosophers.length]; for (int i = 0; i &amp;lt; chopsticks.length; i++) { chopsticks[i] = new Object(); } for (int i = 0; i &amp;lt; philosophers.length; i++) { Object leftChopstick = chopsticks[i]; Object rightChopstick = chopsticks[(i + 1) % chopsticks.length]; philosophers[i] = new Philosopher(rightChopstick, leftChopstick); new Thread(philosophers[i], &quot;哲学家&quot; + (i + 1) + &quot;号&quot;).start(); } }}在这个代码中，有一个内部类叫作 Philosophers，是哲学家的意思。在创建这个哲学家实例，也就是调用构造方法的时候，需要传入两个参数，分别是左手的筷子和右手的筷子。Philosophers 类实现了 Runnable 接口，在它的 run 方法中是无限循环，每个循环中，会多次调用 doAction 方法。在这里的 doAction 方法的定义在下方，这个方法实际上就是把当前输入的字符串给打印出来，并且去进行一段随机时间的休眠。这里的随机休眠是为了模拟真实的场景，因为每个哲学家的思考、吃饭和拿筷子的时间会各不相同。同样，在线上的实际场景中，这个时间也肯定是不相同的，所以我们用随机数来模拟。while 中的代码，哲学家会首先思考人生，然后获取左边筷子这把锁，并打印出“拿起左边的筷子”；接着他去获取右边筷子这把锁，并会打印出“拿起右边的筷子”、“吃饭”，并且“放下右边的筷子”，接下来，他会退出右边筷子的这个同步代码块，释放锁；最后打印出“放下左边的筷子”，随即退出左边筷子的这个同步代码块，释放锁。这样就完成了这个过程，当然他会继续进行 while 循环。main 方法，main 方法中新建了 5 个哲学家，并按照哲学家的数量去新建对应数量的筷子，并且把它们都初始化出来。筷子只用于充当锁对象，所以就把它定义为一个普通的 Object 类型。接下来，初始化哲学家。初始化哲学家需要两个入参，分别是左手筷子和右手筷子，在这里会选取之前定义好的 chopsticks 数组中的对象来给 leftChopstick 和 rightChopstick 进行合理的赋值。当然有一种特殊情况，那就是考虑到最后一个哲学家右手的筷子，由于它已经转完了桌子的一圈，所以他实际上拿的还是第一根筷子，在这里会进行一个取余操作。创建完哲学家之后，就会把它作为 Runnable 对象，传入 Thread，创建一个线程并启动。在 for 循环执行完毕之后，5 个哲学家都启动了起来，于是他们就开始思考并且吃饭。其中一种可能的执行结果如下所示：哲学家1号 思考人生、宇宙、万物...哲学家3号 思考人生、宇宙、万物...哲学家2号 思考人生、宇宙、万物...哲学家4号 思考人生、宇宙、万物...哲学家5号 思考人生、宇宙、万物...哲学家4号 拿起左边的筷子哲学家5号 拿起左边的筷子哲学家1号 拿起左边的筷子哲学家3号 拿起左边的筷子哲学家2号 拿起左边的筷子哲学家 1、3、2、4、5 几乎同时开始思考，然后，假设他们思考的时间比较相近，于是他们都在几乎同一时刻想开始吃饭，都纷纷拿起左手的筷子，这时就陷入了死锁状态，没有人可以拿到右手的筷子，也就没有人可以吃饭，于是陷入了无穷等待，这就是经典的哲学家就餐问题。多种解决方案要想解决死锁问题，只要破坏死锁四个必要条件的任何一个都可以。1. 服务员检查引入服务员检查机制。比如引入一个服务员，当每次哲学家要吃饭时，他需要先询问服务员：我现在能否去拿筷子吃饭？此时，服务员先判断他拿筷子有没有发生死锁的可能，假如有的话，服务员会说：现在不允许你吃饭。这是一种解决方案。2. 领导调节根据死锁检测和恢复策略，可以引入一个领导，这个领导进行定期巡视。如果他发现已经发生死锁了，就会剥夺某一个哲学家的筷子，让他放下。这样一来，由于这个人的牺牲，其他的哲学家就都可以吃饭了。这也是一种解决方案。3. 改变一个哲学家拿筷子的顺序利用死锁避免策略，那就是从逻辑上去避免死锁的发生，比如改变其中一个哲学家拿筷子的顺序。可以让 4 个哲学家都先拿左边的筷子再拿右边的筷子，但是有一名哲学家与他们相反，他是先拿右边的再拿左边的，这样一来就不会出现循环等待同一边筷子的情况，也就不会发生死锁了。死锁解决把“改变一个哲学家拿筷子的顺序”这件事情用代码来写一下，修改后的 main 方法如下：public static void main(String[] args) { Philosopher[] philosophers = new Philosopher[5]; Object[] chopsticks = new Object[philosophers.length]; for (int i = 0; i &amp;lt; chopsticks.length; i++) { chopsticks[i] = new Object(); } for (int i = 0; i &amp;lt; philosophers.length; i++) { Object leftChopstick = chopsticks[i]; Object rightChopstick = chopsticks[(i + 1) % chopsticks.length]; philosophers[i] = new Philosopher(rightChopstick, leftChopstick); new Thread(philosophers[i], &quot;哲学家&quot; + (i + 1) + &quot;号&quot;).start(); }}最主要的变化是，实例化哲学家对象的时候，传入的参数原本都是先传入左边的筷子再传入右边的，但是当我们发现他是最后一个哲学家的时候，也就是 if (i == philosophers.length - 1) ，在这种情况下，给它传入的筷子顺序恰好相反，这样一来，他拿筷子的顺序也就相反了，他会先首先拿起的是右手边的筷子，然后拿起的是左手边的筷子。那么这个程序运行的结果，是所有哲学家都可以正常地去进行思考和就餐了，并且不会发生死锁。" }, { "title": "解决死锁问题的策略", "url": "/posts/solve-problem-of-dead-locks/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-26 15:33:00 +0000", "snippet": "线上发生死锁应该怎么办修复死锁的最好时机在于“防患于未然”，而不是事后补救。就好比发生火灾时，一旦着了大火，想要不造成损失去扑灭几乎已经不可能了。死锁也是一样的，如果线上发生死锁问题，为了尽快减小损失，最好的办法是保存 JVM 信息、日志等“案发现场”的数据，然后立刻重启服务，来尝试修复死锁。为什么说重启服务能解决这个问题呢？因为发生死锁往往要有很多前提条件的，并且当并发度足够高的时候才有可能会发生死锁，所以重启后再次立刻发生死锁的几率并不是很大。重启服务器之后，就可以暂时保证线上服务的可用，然后利用保存过的案发现场的信息，排查死锁、修改代码，最终重新发布。常见修复策略三种主要的修复策略，分别是： 避免策略 检测与恢复策略 鸵鸟策略它们侧重各不相同。避免策略避免策略最主要的思路是： 优化代码逻辑，从根本上消除发生死锁的可能性。通常而言，发生死锁的一个主要原因顺序相反的去获取不同的锁。因此通过调整锁的获取顺序来避免死锁。转账时避免死锁转账时发生死锁的情况（主要看的是如何避免死锁，不是转账的业务逻辑）。(1) 发生了死锁转账系统为了保证线程安全，在转账前需要首先获取到两把锁（两个锁对象），分别是被转出的账户和被转入的账户。如果不做这一层限制，那么在某一个线程修改余额的期间，可能会有其他线程同时修改该变量，可能导致线程安全问题。所以在没有获取到这两把锁之前，是不能对余额进行操作的；只有获取到这两把锁之后，才能进行接下来真正的转账操作。当然，如果要转出的余额大于账户的余额，也不能转账，因为不允许余额变成负数。而这期间就隐藏着发生死锁的可能，代码如下：public class TransferMoney implements Runnable { int flag; static Account a = new Account(500); static Account b = new Account(500); static class Account { public Account(int balance) { this.balance = balance; } int balance; } @Override    public void run() { if (flag == 1) { transferMoney(a, b, 200); } if (flag == 0) { transferMoney(b, a, 200); } }    public static void transferMoney(Account from, Account to, int amount) { //先获取两把锁，然后开始转账 synchronized (to) { synchronized (from) { if (from.balance - amount &amp;lt; 0) { System.out.println(&quot;余额不足，转账失败。&quot;); return;                } from.balance -= amount; to.balance += amount; System.out.println(&quot;成功转账&quot; + amount + &quot;元&quot;);            } } }    public static void main(String[] args) throws InterruptedException { TransferMoney r1 = new TransferMoney();        TransferMoney r2 = new TransferMoney();        r1.flag = 1;        r2.flag = 0;        Thread t1 = new Thread(r1);        Thread t2 = new Thread(r2);        t1.start();        t2.start();        t1.join();        t2.join();        System.out.println(&quot;a的余额&quot; + a.balance);        System.out.println(&quot;b的余额&quot; + b.balance); }}在代码中，首先定义了 int 类型的 flag，它是一个标记位，用于控制不同线程执行不同逻辑。然后建了两个 Account 对象 a 和 b，代表账户，它们最初都有 500 元的余额。我们接下来看 run 方法，该方法里面会根据 flag 值，来决定传入 transferMoney 方法的参数的顺序，如果 flag 为 1，那么就代表从 a 账户转给 b 账户 200元；相反，如果 flag 为 0，那么它就从 b 账户转给 a 账户 200 元。我们再来看一下 transferMoney 转账方法，这个方法会先尝试获取两把锁，也就是 synchronized (to) 和 synchronized (from)。当都获取成功之后，它首先会判断余额是不是足以转出本次的转账金额，如果不足的话，则直接用 return 来退出；如果余额足够，就对转出账户进行减余额，对被转入的账户加余额，最后打印出“成功转账 XX 元”的消息。在主函数中我们新建了两个 TransferMoney 对象，并且把它们的 flag 分别设置为 1 和 0，然后分别传入两个线程中，并把它们都启动起来，最后，打印出各自的余额。执行结果如下：成功转账200元成功转账200元a的余额500b的余额500代码是可以正常执行的，打印结果也是符合逻辑的。此时并没有发生死锁，因为每个锁的持有时间很短，同时释放也很快，所以在低并发的情况下，不容易发生死锁的现象。那我们对代码做一些小调整，让它发生死锁。如果我们在两个 synchronized 之间加上一个 Thread.sleep(500)，来模拟银行网络迟延等情况，那么 transferMoney 方法就变为：public static void transferMoney(Account from, Account to, int amount) {    //先获取两把锁，然后开始转账    synchronized (to) {        try {            Thread.sleep(500);        } catch (InterruptedException e) {            e.printStackTrace();        }        synchronized (from) {            if (from.balance - amount &amp;lt; 0) {                System.out.println(&quot;余额不足，转账失败。&quot;);                return;            }            from.balance -= amount;            to.balance += amount;            System.out.println(&quot;成功转账&quot; + amount + &quot;元&quot;);        }    }}可以看到 transferMoney 的变化就在于，在两个 synchronized 之间，也就是获取到第一把锁后、获取到第二把锁前，我们加了睡眠 500 毫秒的语句。此时再运行程序，会有很大的概率发生死锁，从而导致控制台中不打印任何语句，而且程序也不会停止。我们分析一下它为什么会发生死锁，最主要原因就是，两个不同的线程获取两个锁的顺序是相反的（第一个线程获取的这两个账户和第二个线程获取的这两个账户顺序恰好相反，第一个线程的“转出账户”正是第二个线程的“转入账户”），所以我们就可以从这个“相反顺序”的角度出发，来解决死锁问题。（2）实际上不在乎获取锁的顺序转账时，并不在乎两把锁的相对获取顺序。转账的时候，我们无论先获取到转出账户锁对象，还是先获取到转入账户锁对象，只要最终能拿到两把锁，就能进行安全的操作。所以我们来调整一下获取锁的顺序，使得先获取的账户和该账户是“转入”或“转出”无关，而是使用 HashCode 的值来决定顺序，从而保证线程安全。修复之后的 transferMoney 方法如下：public static void transferMoney(Account from, Account to, int amount) {    int fromHash = System.identityHashCode(from);    int toHash = System.identityHashCode(to);    if (fromHash &amp;lt; toHash) {        synchronized (from) {            synchronized (to) {                if (from.balance - amount &amp;lt; 0) {                    System.out.println(&quot;余额不足，转账失败。&quot;);                    return;                }                from.balance -= amount;                to.balance += amount;                System.out.println(&quot;成功转账&quot; + amount + &quot;元&quot;);            }        }    } else if (fromHash &amp;gt; toHash) {        synchronized (to) {            synchronized (from) {                if (from.balance - amount &amp;lt; 0) {                    System.out.println(&quot;余额不足，转账失败。&quot;);                    return;                }                from.balance -= amount;                to.balance += amount;                System.out.println(&quot;成功转账&quot; + amount + &quot;元&quot;);            }        }    }}可以看到，分别计算出这两个 Account 的 HashCode，然后根据 HashCode 的大小来决定获取锁的顺序。这样一来，不论是哪个线程先执行，不论是转出还是被转入，它获取锁的顺序都会严格根据 HashCode 的值来决定，那么大家获取锁的顺序就一样了，就不会出现获取锁顺序相反的情况，也就避免了死锁。（3）有主键就更安全、方便用主键决定锁获取顺序的方式，它会更加的安全方便。刚才使用 HashCode 作为排序的标准，因为 HashCode 比较通用，每个对象都有，不过这依然有极小的概率会发生 HashCode 相同的情况。在实际生产中，需要排序的往往是一个实体类，而一个实体类一般都会有一个主键 ID，主键 ID 具有唯一、不重复的特点，所以如果这个类包含主键属性的话就方便多了，就没必要去计算 HashCode，直接使用它的主键 ID 来进行排序，由主键 ID 大小来决定获取锁的顺序，就可以确保避免死锁。检测与恢复策略死锁检测算法和避免死锁的策略不一样，避免死锁是通过逻辑让死锁不发生。检测与恢复策略，是先允许系统发生死锁，然后再解除。例如系统可以在每次调用锁的时候，都记录下来调用信息，形成一个“锁的调用链路图”，然后隔一段时间就用死锁检测算法来检测一下，搜索这个图中是否存在环路，一旦发生死锁，就可以用死锁恢复机制，比如剥夺某一个资源，来解开死锁，进行恢复。所以它的思路和之前的死锁避免策略是有很大不同的。在检测到死锁发生后，如何解开死锁呢：方法1——线程终止第一种解开死锁的方法是线程（或进程，下同）终止，在这里，系统会逐个去终止已经陷入死锁的线程，线程被终止，同时释放资源，这样死锁就会被解开。当然这个终止是需要讲究顺序的，一般有以下几个考量指标。（1）优先级一般来说，终止时会考虑到线程或者进程的优先级，先终止优先级低的线程。例如，前台线程会涉及界面显示，这对用户而言是很重要的，所以前台线程的优先级往往高于后台线程。（2）已占用资源、还需要的资源同时也会考虑到某个线程占有的资源有多少，还需要的资源有多少？如果某线程已经占有了一大堆资源，只需要最后一点点资源就可以顺利完成任务，那么系统可能就不会优先选择终止这样的线程，会选择终止别的线程来优先促成该线程的完成。（3）已经运行时间另外还可以考虑的一个因素就是已经运行的时间，比如当前这个线程已经运行了很多个小时，甚至很多天了，很快就能完成任务了，那么终止这个线程可能不是一个明智的选择，我们可以让那些刚刚开始运行的线程终止，并在之后把它们重新启动起来，这样成本更低。这里会有各种各样的算法和策略，我们根据实际业务去进行调整就可以了。方法2——资源抢占第二个解开死锁的方法就是资源抢占。其实，我们不需要把整个的线程终止，而是只需要把它已经获得的资源进行剥夺，比如让线程回退几步、 释放资源，这样一来就不用终止掉整个线程了，这样造成的后果会比刚才终止整个线程的后果更小一些，成本更低。当然这种方式也有一个缺点，那就是如果算法不好的话，我们抢占的那个线程可能一直是同一个线程，就会造成线程饥饿。也就是说，这个线程一直被剥夺它已经得到的资源，那么它就长期得不到运行。以上就是死锁的检测与恢复策略。鸵鸟策略鸵鸟策略以鸵鸟命名，因为鸵鸟有一个特点，就是遇到危险的时候，它会把头埋到沙子里，这样一来它就看不到危险了。鸵鸟策略的意思就是，如果系统发生死锁的概率不高，并且一旦发生其后果不是特别严重的话，选择先忽略它。直到死锁发生的时候，再人工修复，比如重启服务，这并不是不可以的。如果系统用的人比较少，比如是内部的系统，那么在并发量极低的情况下，它可能几年都不会发生死锁。对此考虑到投入产出比，自然也没有必要去对死锁问题进行特殊的处理，这是需要根据业务场景进行合理选择的。总结在线上发生死锁的时： 应该在保存了重要数据后，优先恢复线上服务； 三种具体的修复策略： 避免策略，其主要思路就是去改变锁的获取顺序，防止相反顺序获取锁这种情况的发生； 检测与恢复策略，它是允许死锁发生，但是一旦发生之后它有解决方案； 鸵鸟策略。 " }, { "title": "使用命令和代码定位死锁", "url": "/posts/positioning-dead-lock/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-25 14:36:44 +0000", "snippet": "即便很小心地编写代码，也必不可免地依然有可能会发生死锁。一旦死锁发生，第一步要做的就是把它给找到，因为在找到并定位到死锁之后，才能有接下来的补救措施，比如解除死锁、解除死锁之后恢复、对代码进行优化等；若找不到死锁的话，后面的步骤就无从谈起了。命令：jstack这个命令看到 Java 线程的一些相关信息。如果是比较明显的死锁关系，这个工具就可以直接检测出来；如果死锁不明显，那么它无法直接检测出来，不过也可以借此来分析线程状态，进而就可以发现锁的相互依赖关系，所以这也很有利于找到死锁的方式。首先，下面是必然发生死锁的 MustDeadLock 类：/** * 描述：必定死锁的情况 */public class MustDeadLock implements Runnable { public int flag; static Object o1 = new Object(); static Object o2 = new Object(); public void run() { System.out.println(&quot;线程&quot;+Thread.currentThread().getName() + &quot;的flag为&quot; + flag); if (flag == 1) { synchronized (o1) { try { Thread.sleep(500); } catch (Exception e) { e.printStackTrace(); } synchronized (o2) { System.out.println(&quot;线程1获得了两把锁&quot;); } } } if (flag == 2) { synchronized (o2) { try { Thread.sleep(500); } catch (Exception e) { e.printStackTrace(); } synchronized (o1) { System.out.println(&quot;线程2获得了两把锁&quot;); } } } } public static void main(String[] argv) { MustDeadLock r1 = new MustDeadLock(); MustDeadLock r2 = new MustDeadLock(); r1.flag = 1; r2.flag = 2; Thread t1 = new Thread(r1, &quot;t1&quot;); Thread t2 = new Thread(r2, &quot;t2&quot;); t1.start(); t2.start(); }}由于它发生了死锁，在没有干预的情况下，程序在运行后就不会停止；然后打开终端，执行 ${JAVA_HOME}/bin/jps 这个命令，就可以查看到当前 Java 程序的 pid，我的执行结果如下：56402 MustDeadLock56403 Launcher56474 Jps55051 KotlinCompileDaemon有多行，可以看到第一行是 MustDeadLock 这类的 pid 56402；然后继续执行下一个命令， ${JAVA_HOME}/bin/jstack pid最后它会打印出很多信息，就包含了线程获取锁的信息，比如哪个线程获取哪个锁，它获得的锁是在哪个语句中获得的，它正在等待或者持有的锁是什么等，这些重要信息都会打印出来，部分如下：Found one Java-level deadlock:=============================&quot;t2&quot;:  waiting to lock monitor 0x00007fa06c004a18 (object 0x000000076adabaf0, a java.lang.Object),  which is held by &quot;t1&quot;&quot;t1&quot;:  waiting to lock monitor 0x00007fa06c007358 (object 0x000000076adabb00, a java.lang.Object),  which is held by &quot;t2&quot;Java stack information for the threads listed above:===================================================&quot;t2&quot;: at lesson67.MustDeadLock.run(MustDeadLock.java:31) - waiting to lock &amp;lt;0x000000076adabaf0&amp;gt; (a java.lang.Object) - locked &amp;lt;0x000000076adabb00&amp;gt; (a java.lang.Object) at java.lang.Thread.run(Thread.java:748)&quot;t1&quot;: at lesson67.MustDeadLock.run(MustDeadLock.java:19) - waiting to lock &amp;lt;0x000000076adabb00&amp;gt; (a java.lang.Object) - locked &amp;lt;0x000000076adabaf0&amp;gt; (a java.lang.Object) at java.lang.Thread.run(Thread.java:748)Found 1 deadlock在这里它首先会打印“Found one Java-level deadlock”，表明“找到了一个死锁”，然后是更详细的信息，从中间这部分的信息中可以看出： t2 线程想要去获取这个尾号为 af0 的锁对象，但是它被 t1 线程持有，同时 t2 持有尾号为 b00 的锁对象； 相反，t1 想要获取尾号为 b00 的锁对象，但是它被 t2 线程持有，同时 t1 持有的却是尾号为 af0 的锁对象，这就形成了一个依赖环路，发生了死锁。 最后它还打印出了“Found 1 deadlock.”，可以看出，jstack 工具不但找到了死锁，甚至还把哪个线程、想要获取哪个锁、形成什么样的环路都输出了，有了这样的信息之后，死锁就非常容易定位了，所以接下来进一步修改代码，来避免死锁了。以上就是利用 jstack 来定位死锁的方法，jstack 可以用来帮助我们分析线程持有的锁和需要的锁，然后分析出是否有循环依赖形成死锁的情况。代码：ThreadMXBean下面我们再看一下用代码来定位死锁的方式。我们会用到 ThreadMXBean 工具类，代码示例如下：package cn.happymaya.base.diedlock;public class DetectDeadLock implements Runnable { public int flag; static Object o1 = new Object(); static Object o2 = new Object(); public void run() { System.out.println(Thread.currentThread().getName()+&quot; flag = &quot; + flag); if (flag == 1) { synchronized (o1) { try { Thread.sleep(500); } catch (Exception e) { e.printStackTrace(); } synchronized (o2) { System.out.println(&quot;线程1获得了两把锁&quot;); } } } if (flag == 2) { synchronized (o2) { try { Thread.sleep(500); } catch (Exception e) { e.printStackTrace(); } synchronized (o1) { System.out.println(&quot;线程2获得了两把锁&quot;); } } } } public static void main(String[] argv) throws InterruptedException { DetectDeadLock r1 = new DetectDeadLock(); DetectDeadLock r2 = new DetectDeadLock(); r1.flag = 1; r2.flag = 2; Thread t1 = new Thread(r1,&quot;t1&quot;); Thread t2 = new Thread(r2,&quot;t2&quot;); t1.start(); t2.start(); Thread.sleep(1000); ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean(); long[] deadlockedThreads = threadMXBean.findDeadlockedThreads(); if (deadlockedThreads != null &amp;amp;&amp;amp; deadlockedThreads.length &amp;gt; 0) { for (int i = 0; i &amp;lt; deadlockedThreads.length; i++) { ThreadInfo threadInfo = threadMXBean.getThreadInfo(deadlockedThreads[i]); System.out.println(&quot;线程id为&quot;+threadInfo.getThreadId()+&quot;,线程名为&quot; + threadInfo.getThreadName()+&quot;的线程已经发生死锁，需要的锁正被线程&quot;+threadInfo.getLockOwnerName()+&quot;持有。&quot;); } } }}这个类是在前面 MustDeadLock 类的基础上做了升级，MustDeadLock 类的主要作用就是让线程 1 和线程 2 分别以不同的顺序来获取到 o1 和 o2 这两把锁，并且形成死锁。在 main 函数中，在启动 t1 和 t2 之后的代码，用 Thread.sleep(1000) 来确保已经形成死锁，然后利用 ThreadMXBean 来检查死锁。通过 ThreadMXBean 的 findDeadlockedThreads 方法，可以获取到一个 deadlockedThreads 的数组，然后进行判断，当这个数组不为空且长度大于 0 的时候，逐个打印出对应的线程信息。比如打印线程 id，也打印出了线程名，同时打印出了它所需要的那把锁正被哪个线程所持有，那么这一部分代码的运行结果如下。t1 flag = 1t2 flag = 2线程 id 为 12，线程名为 t2 的线程已经发生死锁，需要的锁正被线程 t1 持有。线程 id 为 11，线程名为 t1 的线程已经发生死锁，需要的锁正被线程 t2 持有。一共有四行语句，前两行是“t1 flag = 1“、“t2 flag = 2”，这是发生死锁之前所打印出来的内容；然后的两行语句就是检测到的死锁的结果，可以看到，它打印出来的是“线程 id 为 12，线程名为 t2 的线程已经发生了死锁，需要的锁正被线程 t1 持有。”同样的，它也会打印出“线程 id 为 11，线程名为 t1 的线程已经发生死锁，需要的锁正被线程 t2 持有。”可以看出，ThreadMXBean 也可以帮我们找到并定位死锁，如果我们在业务代码中加入这样的检测，那我们就可以在发生死锁的时候及时地定位，同时进行报警等其他处理，也就增强了我们程序的健壮性。总结两种方式来定位代码中的死锁，在发生死锁的时： 用 jstack 命令； 在代码中利用 ThreadMXBean 去找死锁。" }, { "title": "发生死锁的四个必要条件", "url": "/posts/four-conditions-dead-locks/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-24 12:45:10 +0000", "snippet": "要想发生死锁有 4 个缺一不可的必要条件： 互斥条件，每个资源每次只能被一个线程（或进程，下同）使用，为什么资源不能同时被多个线程或进程使用呢？这是因为如果每个人都可以拿到想要的资源，那就不需要等待，所以是不可能发生死锁的。 请求与保持条件，当一个线程试图获取资源，但发生了阻塞，则需对已获得的资源保持不放。如果在请求资源时阻塞了，并且会自动释放手中资源（例如锁）的话，那别人自然就能拿到我刚才释放的资源，也就不会形成死锁。 不剥夺条件，指线程已获得的资源，在未使用完之前，不会被强行剥夺。比如我们在上一课时中介绍的数据库的例子，它就有可能去强行剥夺某一个事务所持有的资源，这样就不会发生死锁了。所以要想发生死锁，必须满足不剥夺条件，也就是说当现在的线程获得了某一个资源后，别人就不能来剥夺这个资源，这才有可能形成死锁。 循环等待条件，通俗得讲就是多个线程之间必须形成“循环等待”，才有可能形成死锁，比如在两个线程之间，这种“循环等待”就意味着它们互相持有对方所需的资源、互相等待；而在三个或更多线程中，则需要形成环路，例如依次请求下一个线程已持有的资源等。栗子上次写的的必然死锁的例子中，看看它是否一一满足了这 4 个条件，案例代码如下所示：/** * 描述：必定死锁的情况 */public class MustDeadLock implements Runnable { public int flag; static Object o1 = new Object(); static Object o2 = new Object(); public void run() { System.out.println(&quot;线程&quot;+Thread.currentThread().getName() + &quot;的flag为&quot; + flag); if (flag == 1) { synchronized (o1) { try { Thread.sleep(500); } catch (Exception e) { e.printStackTrace(); } synchronized (o2) { System.out.println(&quot;线程1获得了两把锁&quot;); } } } if (flag == 2) { synchronized (o2) { try { Thread.sleep(500); } catch (Exception e) { e.printStackTrace(); } synchronized (o1) { System.out.println(&quot;线程2获得了两把锁&quot;); } } } } public static void main(String[] argv) { MustDeadLock r1 = new MustDeadLock(); MustDeadLock r2 = new MustDeadLock(); r1.flag = 1; r2.flag = 2; Thread t1 = new Thread(r1, &quot;t1&quot;); Thread t2 = new Thread(r2, &quot;t2&quot;); t1.start(); t2.start(); }}第 1 个互斥条件，很显然，使用的是 synchronized 互斥锁，它的锁对象 o1、o2 只能同时被一个线程所获得，所以是满足互斥条件的。第 2 个是请求与保持条件，可以看到，同样是满足的。比如，线程 1 在获得 o1 这把锁之后想去尝试获取 o2 这把锁 ，这时它被阻塞了，但是它并不会自动去释放 o1 这把锁，而是对已获得的资源保持不放。第 3 个是不剥夺条件，在我们这个代码程序中，JVM 并不会主动把某一个线程所持有的锁剥夺，所以也满足不剥夺条件。第 4 个是循环等待条件，可以看到在我们的例子中，这两个线程都想获取对方已持有的资源，也就是说线程 1 持有 o1 去等待 o2，而线程 2 则是持有 o2 去等待 o1，这是一个环路，此时就形成了一个循环等待。可以看出，例子中确实满足这 4 个必要条件，可以从这 4 个发生死锁的必要条件出发，来解决死锁的问题，只要破坏任意一个条件就可以消除死锁，这是解决死锁策略中重点要考虑的内容。总结发生死锁，必须满足 4 个条件： 互斥条件 请求与保持条件 不剥夺条件 循环等待条件" }, { "title": "一个必然死锁的栗子", "url": "/posts/example-dead-lock/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-23 15:33:00 +0000", "snippet": "死锁发生的场景发生在并发中死锁一定发生在并发场景中。为了保证线程安全，有时会给程序使用各种能保证并发安全的工具，尤其是锁，但是如果在使用过程中处理不得当，就有可能会导致发生死锁的情况。互不相让死锁是一种状态，当两个（或多个）线程（或进程）相互持有对方所需要的资源，却又都不主动释放自己手中所持有的资源，导致大家都获取不到自己想要的资源，所有相关的线程（或进程）都无法继续往下执行，在未改变这种状态之前都不能向前推进，我们就把这种状态称为死锁状态，认为它们发生了死锁。通俗的讲，死锁就是两个或多个线程（或进程）被无限期地阻塞，相互等待对方手中资源的一种状态。生活中的例子一种生活中发生死锁的情况，如下图所示：这张漫画展示了两个绅士分别向对方鞠躬的场景，为了表示礼貌，他们弯下腰之后谁也不愿意先起身，都希望对方起身之后我再起身。可是这样一来，就没有任何人可以先起身，起身这个动作就一直无法继续执行，两人形成了相互等待的状态，所以这就是一种典型的死锁！两个线程的例子两个线程发生死锁的情况，如下图所示：假设有两个线程，分别是线程 A 和线程 B： 假设线程 A 现在持有了锁 A，线程 B 持有了锁 B； 然后线程 A 尝试去获取锁 B，当然它获取不到，因为线程 B 还没有释放锁 B。然后线程 B 又来尝试获取锁 A，同样线程 B 也获取不到锁 A，因为锁 A 已经被线程 A 持有了； 这样一来，线程 A 和线程 B 就发生了死锁，因为它们都相互持有对方想要的资源，却又不释放自己手中的资源，形成相互等待，而且会一直等待下去。多个线程造成死锁的情况死锁不仅仅存在于两个线程的场景，在多个线程中也同样存在。如果多个线程之间的依赖关系是环形，存在环路的依赖关系，那么也可能会发生死锁，如下图所示：在这个例子中： 首先线程 1 持有了锁 A；然后线程 2 持有了锁 B，最后线程 3 持有了锁 C，现在每个线程都分别持有一把锁； 接下来线程 1 想要去持有锁 B，可是它获取不到，因为现在锁 B 正在线程 2 的手里；接下来线程 2 又去尝试获取锁 C， 它同样也获取不到，因为现在锁 C 在线程 3 的手里；然后线程 3 去尝试获取锁 A ，当然它也获取不到，因为锁 A 现在在线程 1 的手里； 这样一来线程 1、线程 2 和线程 3 相互之间就形成了一个环，这就是在多线程中发生死锁的情况。所以不仅是两个线程，多个线程同样也有可能会发生死锁的情况。死锁的影响死锁的影响在不同系统中是不一样的，影响的大小一部分取决于当前这个系统或者环境对死锁的处理能力。数据库中例如，在数据库系统软件的设计中，考虑了监测死锁以及从死锁中恢复的情况。在执行一个事务的时候可能需要获取多把锁，并一直持有这些锁直到事务完成。在某个事务中持有的锁可能在其他事务中也需要，因此在两个事务之间有可能发生死锁的情况，一旦发生了死锁，如果没有外部干涉，那么两个事务就会永远的等待下去。但数据库系统不会放任这种情况发生，当数据库检测到这一组事务发生了死锁时，根据策略的不同，可能会选择放弃某一个事务，被放弃的事务就会释放掉它所持有的锁，从而使其他的事务继续顺利进行。此时程序可以重新执行被强行终止的事务，而这个事务现在就可以顺利执行了，因为所有跟它竞争资源的事务都已经在刚才执行完毕，并且释放资源了。JVM 中在 JVM 中，对于死锁的处理能力就不如数据库那么强大了。如果在 JVM 中发生了死锁，JVM 并不会自动进行处理，所以一旦死锁发生，就会陷入无穷的等待。几率不高但危害大死锁的问题和其他的并发安全问题一样，是概率性的，也就是说，即使存在发生死锁的可能性，也并不是 100% 会发生的。如果每个锁的持有时间很短，那么发生冲突的概率就很低，所以死锁发生的概率也很低。但是在线上系统里，可能每天有几千万次的“获取锁”、“释放锁”操作，在巨量的次数面前，整个系统发生问题的几率就会被放大，只要有某几次操作是有风险的，就可能会导致死锁的发生。也正是因为死锁“不一定会发生”的特点，导致提前找出死锁成为了一个难题。压力测试虽然可以检测出一部分可能发生死锁的情况，但是并不足以完全模拟真实、长期运行的场景，因此没有办法把所有潜在可能发生死锁的代码都找出来。一旦发生了死锁，根据发生死锁的线程的职责不同，就可能会造成子系统崩溃、性能降低甚至整个系统崩溃等各种不良后果。而且死锁往往发生在高并发、高负载的情况下，因为可能会直接影响到很多用户，造成一系列的问题。以上就是死锁发生几率不高但是危害大的特点。发生死锁的例子下面我们举一个必然会发生死锁的例子，代码如下所示：/** * 描述：必定死锁的情况 */public class MustDeadLock implements Runnable { public int flag; static Object o1 = new Object(); static Object o2 = new Object(); public void run() { System.out.println(&quot;线程&quot;+Thread.currentThread().getName() + &quot;的flag为&quot; + flag); if (flag == 1) { synchronized (o1) { try { Thread.sleep(500); } catch (Exception e) { e.printStackTrace(); } synchronized (o2) { System.out.println(&quot;线程1获得了两把锁&quot;); } } } if (flag == 2) { synchronized (o2) { try { Thread.sleep(500); } catch (Exception e) { e.printStackTrace(); } synchronized (o1) { System.out.println(&quot;线程2获得了两把锁&quot;); } } } } public static void main(String[] argv) { MustDeadLock r1 = new MustDeadLock(); MustDeadLock r2 = new MustDeadLock(); r1.flag = 1; r2.flag = 2; Thread t1 = new Thread(r1, &quot;t1&quot;); Thread t2 = new Thread(r2, &quot;t2&quot;); t1.start(); t2.start(); }}可以看到，在这段代码中有一个 int 类型的 flag，它是一个标记位，然后我们新建了 o1 和 o2、作为 synchronized 的锁对象。在 run 方法里面，它会首先打印出当前线程的名字，然后打印出当前线程 flag 的值是多少。如果 flag 等于 1，就会先获取 o1 这把锁，然后休眠 500 毫秒，再去尝试获取 o2 这把锁并且打印出”线程1获得了两把锁”。如果 flag 等于 2，那么情况恰恰相反，线程会先获取 o2 这把锁，然后休眠 500 毫秒，再去获取 o1 这把锁，并且打印出”线程2获得了两把锁”。在 main 方法中新建了两个本类的实例，也就是两个 Runnable 对象，并且把它们的 flag 分别改为 1 和 2：r1 的 flag 设置为 1，r2 的 flag 设置为 2。然后新建两个线程，分别去执行这两个 Runnable 对象，执行 r1 和 r2 这两个线程的名字分别叫做 t1 和 t2，最后把两个线程给启动起来。程序的一种执行结果：线程t1的flag为1线程t2的flag为2这里的重点就在于程序执行到此时还在继续执行，并没停止，并且它永远不会打印出“线程 1 获得了两把锁”或“线程 2 获得了两把锁”这样的语句，此时这里就发生了死锁。对发生死锁这个过程进行分析下面我们对上面发生死锁的过程进行分析： 当第 1 个线程运行的时候，它会发现自己的 flag 是 1 ，所以它会尝试先获得 o1 这把锁，然后休眠 500 毫秒。 在线程 1 启动并休眠的期间，线程 2 同样会启动起来。由于线程 2 的 flag 是 2，所以它会进入到下面 的 if (flag == 2) 对应的代码块中，然后线程 2 首先会去获取 o2 这把锁。也就是说在线程 1 启动并获取到 o1 这把锁之后进行休眠的期间，线程 2 获取到了 o2 这把锁，然后线程 2 也开始 500 毫秒的休眠。 当线程 1 的 500 毫秒休眠时间结束后，它将尝试去获取 o2 这把锁，此时 o2 这个锁正被线程 2 持有，所以线程 1 无法获取到的 o2 紧接着线程 2 也会苏醒过来，它将尝试获取 o1 这把锁，此时 o1 已被线 所以现在的状态是，线程 1 卡在获取 o2 这把锁的位置，而线程 2 卡在获取 o1 这把锁的位置，这样一来线程 1 和线程 2 就形成了相互等待，需要对方持有的资源才能继续执行，从而形成了死锁。在这个例子里，如果线程 2 比线程 1 先启动，情况也是类似的，最终也会形成死锁。这就是一个“必然发生死锁的例子”。" }, { "title": "CSA 的三个缺点", "url": "/posts/cas-shortcoming/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-22 12:33:00 +0000", "snippet": "CAS 是有很多优点的，比如可以避免加互斥锁，可以提高程序的运行效率，但是同样 CAS 也有非常明显的缺点。所以在使用 CAS 的时候应该同时考虑到它的优缺点，合理地进行技术选型。ABA 问题CAS 最大的缺点就是 ABA 问题。描述决定 CAS 是否进行 swap 的判断标准是： 当前的值和预期的值是否一致； 如果一致，就认为在此期间这个数值没有发生过变动，这在大多数情况下是没有问题的。场景但是在有的业务场景下，想确切知道从上一次看到这个值以来到现在，这个值是否发生过变化。例如，这个值假设从 A 变成了 B，再由 B 变回了 A，此时，不仅认为它发生了变化，并且会认为它变化了两次。在这种场景下，使用 CAS，就看不到这两次的变化，因为仅判断当前的值和预期的值是否一致就是不够的了。CAS 检查的并不是值有没有发生过变化，而是去比较这当前的值和预期值是不是相等，如果变量的值从旧值 A 变成了新值 B 再变回旧值 A，由于最开始的值 A 和现在的值 A 是相等的，所以 CAS 会认为变量的值在此期间没有发生过变化。所以，CAS 并不能检测出在此期间值是不是被修改过，它只能检查出现在的值和最初的值是不是一样。再一个例子：假设第一个线程拿到的初始值是 100，然后进行计算，在计算的过程中，有第二个线程把初始值改为了 200，然后紧接着又有第三个线程把 200 改回了 100。等到第一个线程计算完毕去执行 CAS 的时候，它会比较当前的值是不是等于最开始拿到的初始值 100，此时会发现确实是等于 100，所以线程一就认为在此期间值没有被修改过，就理所当然的把这个 100 改成刚刚计算出来的新值，但实际上，在此过程中已经有其他线程把这个值修改过了，这样就会发生 ABA 问题。如果发生了 ABA 问题，那么线程一就根本无法知晓在计算过程中是否有其他线程把这个值修改过，由于第一个线程发现当前值和预期值是相等的，所以就会认为在此期间没有线程修改过变量的值，所以它接下来的一些操作逻辑，是按照在此期间这个值没被修改过”的逻辑去处理的，比如它可能会打印日志：“本次修改十分顺利”，但是它本应触发其他的逻辑，比如当它发现了在此期间有其他线程修改过这个值，其实本应该打印的是“本次修改过程受到了干扰”。解决方法那么如何解决这个问题呢？添加一个版本号就可以解决。在变量值自身之外，再添加一个版本号，那么这个值的变化路径就从 A→B→A 变成了 1A→2B→3A，这样一来，就可以通过对比版本号来判断值是否变化过，这比我们直接去对比两个值是否一致要更靠谱，所以通过这样的思路就可以解决 ABA 的问题了。在 atomic 包中提供了 AtomicStampedReference 这个类，它是专门用来解决 ABA 问题的，解决思路正是利用版本号，AtomicStampedReference 会维护一种类似 &amp;lt;Object,int&amp;gt; 的数据结构，其中的 int 就是用于计数的，也就是版本号，它可以对这个对象和 int 版本号同时进行原子更新，从而也就解决了 ABA 问题。因为我们去判断它是否被修改过，不再是以值是否发生变化为标准，而是以版本号是否变化为标准，即使值一样，它们的版本号也是不同的。以上就是对 CAS 的第一个缺点—— ABA 问题的介绍。自旋时间过长由于单次 CAS 不一定能执行成功，所以 CAS 往往是配合着循环来实现的，有的时候甚至是死循环，不停地进行重试，直到线程竞争不激烈的时候，才能修改成功。可是如果应用场景本身就是高并发的场景，就有可能导致 CAS 一直都操作不成功，这样的话，循环时间就会越来越长。而且在此期间，CPU 资源也是一直在被消耗的，这会对性能产生很大的影响。所以这就要求我们，要根据实际情况来选择是否使用 CAS，在高并发的场景下，通常 CAS 的效率是不高的。范围不能灵活控制CAS 的第三个缺点就是不能灵活控制线程安全的范围。通常去执行 CAS 的时候，是针对某一个，而不是多个共享变量的，这个变量可能是 Integer 类型，也有可能是 Long 类型、对象类型等等，但是不能针对多个共享变量同时进行 CAS 操作，因为这多个变量之间是独立的，简单的把原子操作组合到一起，并不具备原子性。因此如果想对多个对象同时进行 CAS 操作并想保证线程安全的话，是比较困难的。有一个解决方案，那就是利用一个新的类，来整合刚才这一组共享变量，这个新的类中的多个成员变量就是刚才的那多个共享变量，然后再利用 atomic 包中的 AtomicReference 来把这个新对象整体进行 CAS 操作，这样就可以保证线程安全。相比之下，如果使用其他的线程安全技术，那么调整线程安全的范围就可能变得非常容易，比如用 synchronized 关键字时，把更多的代码加锁，那么只需要把更多的代码放到同步代码块里面就可以了。总结CAS 的三个缺点： ABA 问题； 自旋时间过长； 线程安全的范围不能灵活控制" }, { "title": "CAS 与乐观锁的关系，什么时候用 CAS", "url": "/posts/cas-use/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-21 13:33:00 +0000", "snippet": "并发容器Doug Lea 大神在 JUC 包中大量使用了 CAS 技术，该技术既能保证安全性，又不需要使用互斥锁，能大大提升工具类的性能。下面通过两个例子来展示 CAS 在并发容器中的使用情况。案例一：ConcurrentHashMap先来看看并发容器 ConcurrentHashMap 的例子，我们截取部分 putVal 方法的代码，如下所示：final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&amp;lt;K,V&amp;gt;[] tab = table;;) { Node&amp;lt;K,V&amp;gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp;amp; hash)) == null) { if (casTabAt(tab, i, null, new Node&amp;lt;K,V&amp;gt;(hash, key, value, null))) break; // no lock when adding to empty bin } //以下部分省略 ...}在第 10 行，有一个醒目的方法，它就是 “casTabAt”，这个方法名就带有 “CAS”，可以猜测它一定是和 CAS 密不可分了，下面给出 casTabAt 方法的代码实现：static final &amp;lt;K,V&amp;gt; boolean casTabAt(Node&amp;lt;K,V&amp;gt;[] tab, int i, Node&amp;lt;K,V&amp;gt; c, Node&amp;lt;K,V&amp;gt; v) { return U.compareAndSwapObject(tab, ((long)i &amp;lt;&amp;lt; ASHIFT) + ABASE, c, v);}该方法里面只有一行代码，即调用变量 U 的 compareAndSwapObject 的方法，那么，这个变量 U 是什么类型的呢？U 的定义是：private static final sun.misc.Unsafe U可以看出，U 是 Unsafe 类型的，Unsafe 类包含 compareAndSwapInt、compareAndSwapLong、compareAndSwapObject 等和 CAS 密切相关的 native 层的方法，其底层正是利用 CPU 对 CAS 指令的支持实现的。上面介绍的 casTabAt 方法，不仅被用在了 ConcurrentHashMap 的 putVal 方法中，还被用在了 merge、compute、computeIfAbsent、transfer 等重要的方法中，所以 ConcurrentHashMap 对于 CAS 的应用是比较广泛的。案例二：ConcurrentLinkedQueue接下来，我们来看并发容器的第二个案例。非阻塞并发队列 ConcurrentLinkedQueue 的 offer 方法里也有 CAS 的身影，offer 方法的代码如下所示：public boolean offer(E e) { checkNotNull(e); final Node&amp;lt;E&amp;gt; newNode = new Node&amp;lt;E&amp;gt;(e); for (Node&amp;lt;E&amp;gt; t = tail, p = t;;) { Node&amp;lt;E&amp;gt; q = p.next; if (q == null) { if (p.casNext(null, newNode)) { if (p != t) casTail(t, newNode); return true; } } else if (p == q) p = (t != (t = tail)) ? t : head; else p = (p != t &amp;amp;&amp;amp; t != (t = tail)) ? t : q; }}可以看出，在 offer 方法中，有一个 for 循环，这是一个死循环，在第 8 行有一个与 CAS 相关的方法，是 casNext 方法，用于更新节点。那么如果执行 p 的 casNext 方法失败的话，casNext 会返回 false，那么显然代码会继续在 for 循环中进行下一次的尝试。所以在这里也可以很明显的看出 ConcurrentLinkedQueue 的 offer 方法使用到了 CAS。以上就是 CAS 在并发容器中应用的两个例子，我们再来看一看 CAS 在数据库中有哪些应用。数据库在我们的数据库中，也存在对乐观锁和 CAS 思想的应用。在更新数据时，可以利用 version 字段在数据库中实现乐观锁和 CAS 操作，而在获取和修改数据时都不需要加悲观锁。具体思路如下：当我们获取完数据，并计算完毕，准备更新数据时，会检查现在的版本号与之前获取数据时的版本号是否一致，如果一致就说明在计算期间数据没有被更新过，可以直接更新本次数据；如果版本号不一致，则说明计算期间已经有其他线程修改过这个数据了，那就可以选择重新获取数据，重新计算，然后再次尝试更新数据。假设取出数据的时候 version 版本为 1，相应的 SQL 语句示例如下所示：UPDATE student SET name = ‘小王’, version = 2 WHERE  id = 10 AND version = 1这样一来就可以用 CAS 的思想去实现本次的更新操作，它会先去比较 version 是不是最开始获取到的 1，如果和初始值相同才去进行 name 字段的修改，同时也要把 version 的值加一。原子类在原子类中，例如 AtomicInteger，也使用了 CAS，原子类的内容我们在第 39 课时中已经具体分析过了，现在我们复习一下和 CAS 相关的重点内容，也就是 AtomicInteger 的 getAndAdd 方法，该方法代码如下所示：public final int getAndAdd(int delta) { return unsafe.getAndAddInt(this, valueOffset, delta);}从上面的三行代码中可以看到，return 的内容是 Unsafe 的 getAndAddInt 方法的执行结果，接下来我们来看一下 getAndAddInt 方法的具体实现，代码如下所示：public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;}在这里，我们看到上述方法中有对 var5 的赋值，调用了 unsafe 的 getIntVolatile(var1, var2) 方法，这是一个 native 方法，作用是获取变量 var1 中偏移量 var2 处的值。这里传入 var1 的是 AtomicInteger 对象的引用，而 var2 就是 AtomicInteger 里面所存储的数值（也就是 value）的偏移量 valueOffset，所以此时得到的 var5 实际上代表当前时刻下的原子类中存储的数值。接下来重点来了，我们看到有一个 compareAndSwapInt 方法，这里会传入多个参数，分别是 var1、var2、 var5、var5 + var4，其实它们代表 object、offset、expectedValue 和 newValue。 第一个参数 object 就是将要修改的对象，传入的是 this，也就是 atomicInteger 这个对象本身； 第二个参数是 offset，也就是偏移量，借助它就可以获取到 value 的数值； 第三个参数 expectedValue，代表“期望值”，传入的是刚才获取到的 var5； 而最后一个参数 newValue 是希望修改为的新值 ，等于之前取到的数值 var5 再加上 var4，而 var4 就是我们之前所传入的 delta，delta 就是我们希望原子类所改变的数值，比如可以传入 +1，也可以传入 -1。所以 compareAndSwapInt 方法的作用就是，判断如果现在原子类里 value 的值和之前获取到的 var5 相等的话，那么就把计算出来的 var5 + var4 给更新上去，所以说这行代码就实现了 CAS 的过程。一旦 CAS 操作成功，就会退出这个 while 循环，但是也有可能操作失败。如果操作失败就意味着在获取到 var5 之后，并且在 CAS 操作之前，value 的数值已经发生变化了，证明有其他线程修改过这个变量。这样一来，就会再次执行循环体里面的代码，重新获取 var5 的值，也就是获取最新的原子变量的数值，并且再次利用 CAS 去尝试更新，直到更新成功为止，所以这是一个死循环。总之，Unsafe 的 getAndAddInt 方法是通过循环 + CAS 的方式来实现的，在死循环过程中，每次都会调用compareAndSwapInt方法来试图把value值给更新上去，如果更新失败，也会一直重试，直到更新成功。" }, { "title": "CAS", "url": "/posts/cas/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-20 15:33:00 +0000", "snippet": "CAS 简介CAS 是面试中的常客，因为它是原子类的底层原理，同时也是乐观锁的原理，CAS 的英文全称是 Compare-And-Swap，中文叫做“比较并交换”，是一种思想、一种算法。在多线程的情况下，各个代码的执行顺序是不能确定的，所以为了保证并发安全，可以使用互斥锁。而 CAS 的特点是避免使用互斥锁，当多个线程同时使用 CAS 更新同一个变量时，只有其中一个线程能够操作成功，而其他线程都会更新失败。不过和同步互斥锁不同的是，更新失败的线程并不会被阻塞，而是被告知这次由于竞争而导致的操作失败，但还可以再次尝试。CAS 被广泛应用在并发编程领域中，以实现那些不会被打断的数据交换操作，从而就实现了无锁的线程安全。CAS 的思路在大多数处理器的指令中，都会实现 CAS 相关的指令，这一条指令就可以完成“比较并交换”的操作，也正是由于这是一条（而不是多条）CPU 指令，所以 CAS 相关的指令是具备原子性的，这个组合操作在执行期间不会被打断，这样就能保证并发安全。由于这个原子性是由 CPU 保证的，所以无需我们程序员来操心。CAS 最核心的思路就是，判断当前的内存值V是否和预期值A相同，只有相同的时候，才将内存值修改为新值B，否则就不做变动。 CAS 会提前假定当前内存值 V 应该等于值 A，而值 A 往往是之前读取到当时的内存值 V； 在执行 CAS 时，如果发现当前的内存值 V 恰好是值 A 的话，那 CAS 就会把内存值 V 改成值 B，而值 B 往往是在拿到值 A 后，在值 A 的基础上经过计算而得到的。 如果执行 CAS 时发现此时内存值 V 不等于值 A，则说明在刚才计算 B 的期间内，内存值已经被其他线程修改过了，那么本次 CAS 就不应该再修改了，可以避免多人同时修改导致出错。这就是 CAS 的主要思路和流程。JDK 正是利用了这些 CAS 指令，可以实现并发的数据结构，比如 AtomicInteger 等原子类。利用 CAS 实现的无锁算法，就像谈判的时候，用一种非常乐观的方式去协商，彼此之间很友好，这次没谈成，还可以重试。CAS 的思路和互斥锁是两种完全不同的思路，如果是互斥锁，不存在协商机制，大家都会尝试抢占资源，如果抢到了，在操作完成前，会把这个资源牢牢的攥在自己的手里。当然，利用 CAS 和利用互斥锁，都可以保证并发安全，它们是实现同一目标的不同手段。栗子用图解和例子的方式，让 CAS 的过程变得更加清晰，如下图所示：假设有两个线程，分别使用两个 CPU，它们都想利用 CAS 来改变右边的变量的值。我们先来看线程 1，它使用 CPU 1，假设它先执行，它期望当前的值是 100，并且想将其改成 150。在执行的时候，它会去检查当前的值是不是 100，发现真的是 100，所以可以改动成功，而当改完之后，右边的值就会从 100 变成 150。如上图所示，假设现在才刚刚轮到线程 2 所使用的 CPU 2 来执行，它想要把这个值从 100 改成 200，所以它也希望当前值是 100，可实际上当前值是 150，所以它会发现当前值不是自己期望的值，所以并不会真正的去继续把 100 改成 200，也就是说整个操作都是没有效果的，此次没有修改成功，CAS 操作失败。当然，接下来线程 2 还可以有其他的操作，这需要根据业务需求来决定，比如重试、报错或者干脆跳过执行。举一个例子，在秒杀场景下，多个线程同时执行秒杀，只要有一个执行成功就够了，剩下的线程当发现自己 CAS 失败了，其实说明兄弟线程执行成功了，也就没有必要继续执行了，这就是跳过操作。所以业务逻辑不同，就会有不同的处理方法，但无论后续怎么处理，之前的那一次 CAS 操作是已经失败了的。CAS 的语义有了下面的等价代码之后，理解起来会比前面的图示和文字更加容易，因为代码实际上是一目了然的。接下来我们把 CAS 拆开，看看它内部究竟做了哪些事情。CAS 的等价语义的代码，如下所示：/** * 描述：模拟CAS操作，等价代码 */public class SimulatedCAS {    private int value;    public synchronized int compareAndSwap(int expectedValue, int newValue) {        int oldValue = value;        if (oldValue == expectedValue) {            value = newValue;        }        return oldValue;    }}在这段代码中有一个 compareAndSwap 方法，在这个方法里有两个入参，第 1 个入参期望值 expectedValue，第 2 个入参是 newValue，它就是我们计算好的新的值，我们希望把这个新的值去更新到变量上去。你一定注意到了， compareAndSwap 方法是被 synchronized 修饰的，我们用同步方法为 CAS 的等价代码保证了原子性。接下来我将讲解，在 compareAndSwap 方法里都做了哪些事情。需要先拿到变量的当前值，所以代码里用就会用 int oldValue = value 把变量的当前值拿到。然后就是 compare，也就是“比较”，所以此时会用 if (oldValue == expectedValue) 把当前值和期望值进行比较，如果它们是相等的话，那就意味着现在的值正好就是我们所期望的值，满足条件，说明此时可以进行 swap，也就是交换，所以就把 value 的值修改成 newValue，最后再返回 oldValue，完成了整个 CAS 过程。CAS 最核心的思想就在上面这个流程中体现了，可以看出，compare 指的就是 if 里的比较，比较 oldValue 是否等于 expectedValue；同样，swap 实际上就是把 value 改成 newValue，并且返回 oldValue。所以这整个 compareAndSwap 方法就还原了 CAS 的语义，也象征了 CAS 指令在背后所做的工作。案例演示：两个线程竞争 CAS，其中一个落败有了这前面的等价代码之后，我们再来深入介绍一个具体的案例：两个线程执行 CAS，尝试修改数据，第一个线程能修改成功，而第二个线程由于来晚了，会发现数据已经被修改过了，就不再修改了。我们通过 debug 的方式可以看到 CAS 在执行过程中的具体情况。下面我们用代码来演示一下 CAS 在两个线程竞争的时候，会发生的情况，同时我也录制了一段视频，你也可以直接跳过文字版看视频演示。我们看下面的这段代码：package cn.happymaya.base.cas;public class DebugCAS implements Runnable { private volatile int value; public synchronized int compareAndSwap(int expectedValue, int newValue) { int oldValue = value; if (oldValue == expectedValue) { value = newValue; System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;执行成功&quot;); } return oldValue; } public static void main(String[] args) throws InterruptedException { DebugCAS r = new DebugCAS(); r.value = 100; Thread t1 = new Thread(r, &quot;Thread 1&quot;); Thread t2 = new Thread(r, &quot;Thread 2&quot;); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(r.value); } @Override public void run() { compareAndSwap(100, 150); }}这里的 compareAndSwap 方法就是刚才所讲过的 CAS 的等价语义的代码，然后在此基础上加了一行代码，如果执行成功的话，它会打印出是哪个线程执行成功。而在我们的 main() 方法里面，首先把 DebugCAS 类实例化出来，并把 value 的值修改为 100，这样它的初始值就为 100，接着我们新建两个线程 Thread t1 和 Thread t2，把它们启动起来，并且主线程等待两个线程执行完毕之后，去打印出最后 value 的值。新建的这两个线程都做了什么内容呢？在 run() 方法里面可以看到，就是执行 compareAndSwap 方法，并且期望的值是 100，希望改成的值是 150，那么当两个线程都去执行 run() 方法的时候，可以预见到的是，只会有一个线程执行成功，另外一个线程不会打印出“执行成功”这句话，因为当它执行的时候会发现，当时的值已经被修改过了，不是 100 了。首先，我们不打断点，直接执行看看运行的结果：线程Thread 1执行成功150可以看到，Thread 1 执行成功，且最终的结果是 150。在这里，打印“Thread 1 执行成功”这句话的概率比打印“Thread 2 执行成功”这句话的概率要大得多，因为 Thread 1 是先 start 的。下面我们用 debug 的方法来看看内部究竟是如何执行的。我们先在“if (oldValue == expectedValue){”这一行打断点，然后用 Debug 的形式去运行。可以看到，此时程序已经停留在打断点的地方了，停留的是 Thread 1（在 Debugger 里可以显示出来当前线程的名字和状态），而 Thread 2 此时的状态是 Monitor （对应 Java 线程的 Blocked 状态），其含义是没有拿到这把锁 synchronized，正在外面等待这把锁。现在 Thread 1 进到 compareAndSwap 方法里了，我们可以很清楚地看到，oldValue 值是 100，而 expectedValue 的值也是 100，所以它们是相等的。继续让代码单步运行，因为满足 if 判断条件，所以可以进到 if 语句中，所以接下来会把 value 改成 newValue，而 newValue 的值正是 150。在修改完成后，还会打印出“线程Thread 1执行成功”这句话，如下图所示。接下来我们按下左侧的执行按钮，就轮到 Thread 2 了，此时情景就不同了。可以看到，oldValue 拿到的值是 150，因为 value 的值已经被 Thread 1 修改过了，所以，150 与 Thread 2 所期望的 expectedValue 的值 100 是不相等的，从而会跳过整个 if 语句，也就不能打印出“Thread 2 执行成功”这句话，最后会返回 oldValue，其实对这个值没有做任何的修改。到这里，两个线程就执行完毕了。在控制台，只打印出 Thread 1 执行成功，而没有打印出 Thread 2 执行成功。其中的原因，我们通过 Debug 的方式已经知晓了。以上代码通过 Debug 的方式，看到了当两个线程去竞争 CAS 时，其中一个成功、另一个失败的情况。总结CAS 的核心思想是通过将内存中的值与指定数据进行比较，当这两个数值一样时，才将内存中的数据替换为新的值，整个过程是具备原子性的。" }, { "title": "单例模式的双重检查锁模式为什么必须加 volatile", "url": "/posts/singular-volatile/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-19 15:33:00 +0000", "snippet": "什么是单例模式单例模式指的是，保证一个类只有一个实例，并且提供一个可以全局访问的入口。为什么需要使用单例模式那么我们为什么需要单例呢？其中一个理由，那就是为了节省内存、节省计算。在很多情况下，只需要一个实例就够了，如果出现更多的实例，反而纯属浪费。举一个例子来说明这个情况，以一个初始化比较耗时的类来说，代码如下所示：public class ExpensiveResource {    public ExpensiveResource() {        field1 = // 查询数据库        field2 = // 然后对查到的数据做大量计算        field3 = // 加密、压缩等耗时操作    }}这个类在构造的时候，需要查询数据库并对查到的数据做大量计算，所以在第一次构造时，我们花了很多时间来初始化这个对象。但是假设数据库里的数据是不变的，我们就可以把这个对象保存在内存中，那么以后开发的时候就可以直接用这同一个实例了，不需要再次构建新实例。如果每次都重新生成新的实例，则会造成更多的浪费，实在没有必要。接下来看看需要单例的第二个理由，那就是为了保证结果的正确。**比如我们需要一个全局的计数器，用来统计人数，如果有多个实例，反而会造成混乱。另外呢，就是为了方便管理。**很多工具类，我们只需要一个实例，那么我们通过统一的入口，比如通过 getInstance 方法去获取这个单例是很方便的，太多实例不但没有帮助，反而会让人眼花缭乱。一般单例模式的类结构如下图所示：有一个私有的 Singleton 类型的 singleton 对象；同时构造方法也是私有的，为了防止他人调用构造函数来生成实例；另外还会有一个 public 的 getInstance 方法，可通过这个方法获取到单例。双重检查锁模式的写法单例模式有多种写法，本文重点关注使用 volatile 强相关的双重检查锁模式的写法，代码如下所示：public class Singleton {    private static volatile Singleton singleton;    private Singleton() {    }    public static Singleton getInstance() {        if (singleton == null) {            synchronized (Singleton.class) {                if (singleton == null) {                    singleton = new Singleton();                }            }        }        return singleton;    }}在这里我将重点讲解 getInstance 方法，方法中首先进行了一次 if (singleton == null) 的检查，然后是 synchronized 同步块，然后又是一次 if (singleton == null) 的检查，最后是 singleton = new Singleton() 来生成实例。我们进行了两次 if (singleton == null) 检查，这就是“双重检查锁”这个名字的由来。这种写法是可以保证线程安全的，假设有两个线程同时到达 synchronized 语句块，那么实例化代码只会由其中先抢到锁的线程执行一次，而后抢到锁的线程会在第二个 if 判断中发现 singleton 不为 null，所以跳过创建实例的语句。再后面的其他线程再来调用 getInstance 方法时，只需判断第一次的 if (singleton == null) ，然后会跳过整个 if 块，直接 return 实例化后的对象。这种写法的优点是不仅线程安全，而且延迟加载、效率也更高。这里就涉及到了一个常见的问题，面试官可能会问你，“为什么要 double-check？去掉任何一次的 check 行不行？”我们先来看第二次的 check，这时你需要考虑这样一种情况，有两个线程同时调用 getInstance 方法，由于 singleton 是空的 ，因此两个线程都可以通过第一重的 if 判断；然后由于锁机制的存在，会有一个线程先进入同步语句，并进入第二重 if 判断 ，而另外的一个线程就会在外面等待。不过，当第一个线程执行完 new Singleton() 语句后，就会退出 synchronized 保护的区域，这时如果没有第二重 if (singleton == null) 判断的话，那么第二个线程也会创建一个实例，此时就破坏了单例，这肯定是不行的。而对于第一个 check 而言，如果去掉它，那么所有线程都会串行执行，效率低下，所以两个 check 都是需要保留的。在双重检查锁模式中为什么需要使用 volatile 关键字相信细心的你可能看到了，我们在双重检查锁模式中，给 singleton 这个对象加了 volatile 关键字，那为什么要用 volatile 呢？主要就在于 singleton = new Singleton() ，它并非是一个原子操作，事实上，在 JVM 中上述语句至少做了以下这 3 件事： 第一步是给 singleton 分配内存空间； 然后第二步开始调用 Singleton 的构造函数等，来初始化 singleton； 最后第三步，将 singleton 对象指向分配的内存空间（执行完这步 singleton 就不是 null 了）。这里需要留意一下 1-2-3 的顺序，因为存在指令重排序的优化，也就是说第2 步和第 3 步的顺序是不能保证的，最终的执行顺序，可能是 1-2-3，也有可能是 1-3-2。如果是 1-3-2，那么在第 3 步执行完以后，singleton 就不是 null 了，可是这时第 2 步并没有执行，singleton 对象未完成初始化，它的属性的值可能不是我们所预期的值。假设此时线程 2 进入 getInstance 方法，由于 singleton 已经不是 null 了，所以会通过第一重检查并直接返回，但其实这时的 singleton 并没有完成初始化，所以使用这个实例的时候会报错，详细流程如下图所示：线程 1 首先执行新建实例的第一步，也就是分配单例对象的内存空间，由于线程 1 被重排序，所以执行了新建实例的第三步，也就是把 singleton 指向之前分配出来的内存地址，在这第三步执行之后，singleton 对象便不再是 null。这时线程 2 进入 getInstance 方法，判断 singleton 对象不是 null，紧接着线程 2 就返回 singleton 对象并使用，由于没有初始化，所以报错了。最后，线程 1 “姗姗来迟”，才开始执行新建实例的第二步——初始化对象，可是这时的初始化已经晚了，因为前面已经报错了。使用了 volatile 之后，相当于是表明了该字段的更新可能是在其他线程中发生的，因此应确保在读取另一个线程写入的值时，可以顺利执行接下来所需的操作。在 JDK 5 以及后续版本所使用的 JMM 中，在使用了 volatile 后，会一定程度禁止相关语句的重排序，从而避免了上述由于重排序所导致的读取到不完整对象的问题的发生。使用 volatile 的意义主要在于它可以防止避免拿到没完成初始化的对象，从而保证了线程安全。 参考：小宝马的爸爸 - 梦想的家园《单例模式（Singleton）》：https://www.cnblogs.com/BoyXiao/archive/2010/05/07/1729376.htmlJark’s Blog《如何正确地写出单例模式》：http://wuchong.me/blog/2014/08/28/how-to-correctly-write-singleton-pattern/Hollis Chuang《为什么我墙裂建议大家使用枚举来实现单例》：https://www.hollischuang.com/archives/2498Hollis Chuang《深度分析Java的枚举类型—-枚举的线程安全性及序列化问题》：https://www.hollischuang.com/archives/197" }, { "title": "volatile", "url": "/posts/volatile/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-18 15:33:00 +0000", "snippet": "volatile 是什么volatile，是 Java 中的一个关键字，是一种同步机制。当某个变量是共享变量，且这个变量是被 volatile 修饰的，那么在修改了这个变量的值之后，再读取该变量的值时，可以保证获取到的是修改后的最新的值，而不是过期的值。相比于 synchronized 或者 Lock，volatile 是更轻量的，因为使用 volatile 不会发生上下文切换等开销很大的情况，不会让线程阻塞。但正是由于它的开销相对比较小，所以它的效果，也就是能力，相对也小一些。虽然说 volatile 是用来保证线程安全的，但是它做不到像 synchronized 那样的同步保护，volatile 仅在很有限的场景中才能发挥作用，所以下面就让我们来看一下它的适用场景，我们会先给出不适合使用 volatile 的场景，再给出两种适合使用 volatile 的场景。volatile 的适用场合不适用：a++volatile 不适合运用于需要保证原子性的场景，比如更新的时候需要依赖原来的值，而最典型的就是 a++ 的场景，仅靠 volatile 是不能保证 a++ 的线程安全的。代码如下所示：public class DontVolatile implements Runnable { volatile int a; AtomicInteger realA = new AtomicInteger(); public static void main(String[] args) throws InterruptedException {        Runnable r =  new DontVolatile();        Thread thread1 = new Thread(r);        Thread thread2 = new Thread(r);        thread1.start();        thread2.start();        thread1.join();        thread2.join();        System.out.println(((DontVolatile) r).a);        System.out.println(((DontVolatile) r).realA.get());    }    @Override    public void run() {        for (int i = 0; i &amp;lt; 1000; i++) {            a++;            realA.incrementAndGet();        }    }}在这段代码中，我们有一个 volatile 修饰的 int 类型的 a 变量，并且下面还有一个原子类的 realA，原子类是可以保证线程安全的，所以我们就用它来和 volatile int a 做对比，看一看它们实际效果上的差别。在 main 函数中，我们新建了两个线程，并且让它们运行。这两个线程运行的内容就是去执行 1000 次的累加操作，每次累加操作会对 volatile 修饰的变量 a 进行自加操作，同时还会对原子类 realA 进行自加操作。当这两个线程都运行完毕之后，我们把结果给打印出来，其中一种运行结果如下：19882000会发现最终的 a 值和 realA 值分别为 1988 和 2000。可以看出，即便变量 a 被 volatile 修饰了，即便它最终一共执行了 2000 次的自加操作（这一点可以由原子类的最终值来印证），但是依然有一些自加操作失效了，所以最终它的结果是不到 2000 的，这就证明了 volatile 不能保证原子性，那么它究竟适合运用于什么场景呢？适用场合1：布尔标记位如果某个共享变量自始至终只是被各个线程所赋值或读取，而没有其他的操作（比如读取并在此基础上进行修改这样的复合操作）的话，那么我们就可以使用 volatile 来代替 synchronized 或者代替原子类，因为赋值操作自身是具有原子性的，volatile 同时又保证了可见性，这就足以保证线程安全了。一个比较典型的场景就是布尔标记位的场景，例如 volatile boolean flag。因为通常情况下，boolean 类型的标记位是会被直接赋值的，此时不会存在复合操作（如 a++），只存在单一操作，就是去改变 flag 的值，而一旦 flag 被 volatile 修饰之后，就可以保证可见性了，那么这个 flag 就可以当作一个标记位，此时它的值一旦发生变化，所有线程都可以立刻看到，所以这里就很适合运用 volatile 了。代码示例：public class YesVolatile1 implements Runnable {    volatile boolean done = false;    AtomicInteger realA = new AtomicInteger();    public static void main(String[] args) throws InterruptedException {        Runnable r =  new YesVolatile1();        Thread thread1 = new Thread(r);        Thread thread2 = new Thread(r);        thread1.start();        thread2.start();        thread1.join();        thread2.join();        System.out.println(((YesVolatile1) r).done);        System.out.println(((YesVolatile1) r).realA.get());    }    @Override    public void run() {        for (int i = 0; i &amp;lt; 1000; i++) {            setDone();            realA.incrementAndGet();        }    }    private void setDone() {        done = true;    }}这段代码和前一段代码非常相似，唯一不同之处在于，我们把 volatile int a 改成了 volatile boolean done，并且在 1000 次循环的操作过程中调用的是 setDone() 方法，而这个 setDone() 方法就是把 done 这个变量设置为 true，而不是根据它原来的值再做判断，例如原来是 false，就设置成 true，或者原来是 true，就设置成 false，这些复杂的判断是没有的，setDone() 方法直接就把变量 done 的值设置为 true。那么这段代码最终运行的结果如下：true2000无论运行多少次，控制台都会打印出 true 和 2000，打印出的 2000 已经印证出确实是执行了 2000 次操作，而最终的 true 结果证明了，在这种场景下，volatile 起到了保证线程安全的作用。第二个例子区别于第一个例子最大的不同点就在于，第一个例子的操作是 a++，这是个复合操作，不具备原子性，而在本例中的操作仅仅是把 done 设置为 true，这样的赋值操作本身就是具备原子性的，所以在这个例子中，它是适合运用 volatile 的。适用场合 2：作为触发器第二个适合用 volatile 的场景：作为触发器，保证其他变量的可见性。下面是 Brian Goetz 提供的一个经典例子：Map configOptions;char[] configText;volatile boolean initialized = false;···// In thread AconfigOptions = new HashMap();configText = readConfigFile(fileName);processConfigOptions(configText, configOptions);initialized = true;···// In thread Bwhile (!initialized) sleep();// use configOptions在这段代码中可以看到，我们有一个 map 叫作 configOptions，还有一个 char 数组叫作 configText，然后会有一个被 volatile 修饰的 boolean initialized，最开始等于 false。再下面的这四行代码是由线程 A 所执行的，它所做的事情就是初始化 configOptions，再初始化 configText，再把这两个值放到一个方法中去执行，实际上这些都代表了初始化的行为。那么一旦这些方法执行完毕之后，就代表初始化工作完成了，线程 A 就会把 initialized 这个变量设置为 true。而对于线程 B 而言，它一开始会在 while 循环中反复执行 sleep 方法（例如休眠一段时间），直到 initialized 这个变量变成 true，线程 B 才会跳过 sleep 方法，继续往下执行。重点来了，一旦 initialized 变成了 true，此时对于线程 B 而言，它就会立刻使用这个 configOptions，所以这就要求此时的 configOptions 是初始化完毕的，且初始化的操作的结果必须对线程 B 可见，否则线程 B 在执行的时候就可能报错。你可能会担心，因为这个 configOptions 是在线程 A 中修改的，那么在线程 B 中读取的时候，会不会发生可见性问题，会不会读取的不是初始化完毕后的值？如果我们不使用 volatile，那么确实是存在这个问题的。但是现在我们用了被 volatile 修饰的 initialized 作为触发器，所以这个问题被解决了。根据happens-before 关系的单线程规则，线程 A 中 configOptions 的初始化 happens-before 对 initialized 变量的写入，而线程 B 中对 initialzed 的读取 happens-before 对 configOptions 变量的使用，同时根据 happens-before 关系的 volatile 规则，线程 A 中对 initialized 的写入为 true 的操作 happens-before 线程 B 中随后对 initialized 变量的读取。如果我们分别有操作 A 和操作 B，我们用 hb(A, B) 来表示 A happens-before B。而 Happens-before 是有可传递性质的，如果hb(A, B)，且hb(B, C)，那么可以推出hb(A, C)。所以根据上面的条件，我们可以得出结论：线程 A 中对于 configOptions 的初始化 happens-before 线程 B 中 对于 configOptions 的使用。所以对于线程 B 而言，既然它已经看到了 initialized 最新的值，那么它同样就能看到包括 configOptions 在内的这些变量初始化后的状态，所以此时线程 B 使用 configOptions 是线程安全的。这种用法就是把被 volatile 修饰的变量作为触发器来使用，保证其他变量的可见性，这种用法也是非常值得掌握的，可以作为面试时的亮点。volatile 的作用volatile 一共有两层作用： 保证可见性。Happens-before 关系中对于 volatile 是这样描述的：对一个 volatile 变量的写操作 happen-before 后面对该变量的读操作。 这就代表了如果变量被 volatile 修饰，那么每次修改之后，接下来在读取这个变量的时候一定能读取到该变量最新的值。 禁止重排序。 as-if-serial 语义：不管怎么重排序，（单线程）程序的执行结果不会改变。在满足 as-if-serial 语义的前提下，由于编译器或 CPU 的优化，代码的实际执行顺序可能与我们编写的顺序是不同的，这在单线程的情况下是没问题的，但是一旦引入多线程，这种乱序就可能会导致严重的线程安全问题。用了 volatile 关键字就可以在一定程度上禁止这种重排序。 volatile 和 synchronized 的关系volatile 和 synchronized 的关系： 相似性：volatile 可以看作是一个轻量版的 synchronized，比如一个共享变量如果自始至终只被各个线程赋值和读取，而没有其他操作的话，那么就可以用 volatile 来代替 synchronized 或者代替原子变量，足以保证线程安全。实际上，对 volatile 字段的每次读取或写入都类似于“半同步”——读取 volatile 与获取 synchronized 锁有相同的内存语义，而写入 volatile 与释放 synchronized 锁具有相同的语义。 不可代替：但是在更多的情况下，volatile 是不能代替 synchronized 的，volatile 并没有提供原子性和互斥性。 性能方面：volatile 属性的读写操作都是无锁的，正是因为无锁，所以不需要花费时间在获取锁和释放锁上，所以说它是高性能的，比 synchronized 性能更好。 valatile关键字，可以保证可见性，但如果修饰一个对象，但对象的属性改了，能保证吗？如果想保证，怎么做？—— 对象里面的属性不保证，如果想要保证，需要把属性用volatile修饰。 没有说明为什么 volatile已经保证了可见性的前提下，为什么不是2000 —— volatile 不能保证原子性，a++ 本身不是原子操作，如果不能保证原子性，将出现线程安全问题。" }, { "title": "happens-before", "url": "/posts/happens-before/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-17 15:33:00 +0000", "snippet": "happens-before 关系Happens-before 关系是用来描述和可见性相关问题的：如果第一个操作 happens-before 第二个操作（也可以描述为，第一个操作和第二个操作之间满足 happens-before 关系），那么就说第一个操作对于第二个操作一定是可见的，也就是第二个操作在执行时就一定能保证看见第一个操作执行的结果。不具备 happens-before 关系的例子一个不具备 happens-before 关系的例子，从宏观上进一步理解 happens-before 关系想要表达的内容。代码如下：public class Visibility { int x = 0; public void write() { x = 1; } public void read() { int y = x; }}代码很简单，类里面有一个 int x 变量 ，初始值为 0，而 write 方法的作用是把 x 的值改写为 1， 而 read 方法的作用则是读取 x 的值。如果有两个线程，分别执行 write 和 read 方法，那么由于这两个线程之间没有相互配合的机制，所以 write 和 read 方法内的代码不具备 happens-before 关系，其中的变量的可见性无法保证，下面我们用例子说明这个情况。比如，假设线程 1 已经先执行了 write 方法，修改了共享变量 x 的值，然后线程 2 执行 read 方法去读取 x 的值，此时我们并不能确定线程 2 现在是否能读取到之前线程 1 对 x 所做的修改，线程 2 有可能看到这次修改，所以读到的 x 值是 1，也有可能看不到本次修改，所以读到的 x 值是最初始的 0。既然存在不确定性，那么 write 和 read 方法内的代码就不具备 happens-before 关系。相反，如果第一个操作 happens-before 第二个操作，那么第一个操作对于第二个操作而言一定是可见的。Happens-before 关系的规则如果分别有操作 x 和操作 y，用 hb(x, y) 来表示 x happens-before y。单线程规则在一个单独的线程中，按照程序代码的顺序，先执行的操作 happen-before 后执行的操作。也就是说，如果操作 x 和操作 y 是同一个线程内的两个操作，并且在代码里 x 先于 y 出现，那么有 hb(x, y)，正如下图所示：这一个 happens-before 的规则非常重要，因为如果对于同一个线程内部而言，后面语句都不能保证可以看见前面的语句的执行结果的话，那会造成非常严重的后果，程序的逻辑性就无法保证了。这里有一个注意点，我们之前讲过重排序，那是不是意味着 happens-before 关系的规则和重排序冲突，为了满足 happens-before 关系，就不能重排序了？答案是否定的。其实只要重排序后的结果依然符合 happens-before 关系，也就是能保证可见性的话，那么就不会因此限制重排序的发生。比如，单线程内，语句 1 在语句 2 的前面，所以根据“单线程规则”，语句 1 happens-before 语句 2，但是并不是说语句 1 一定要在语句 2 之前被执行，例如语句 1 修改的是变量 a 的值，而语句 2 的内容和变量 a 无关，那么语句 1 和语句 2 依然有可能被重排序。当然，如果语句 1 修改的是变量 a，而语句 2 正好是去读取变量 a 的值，那么语句 1 就一定会在语句 2 之前执行了。锁操作规则（synchronized 和 Lock 接口等）如果操作 A 是解锁，而操作 B 是对同一个锁的加锁，那么 hb(A, B) 。正如下图所示：从上图中可以看到，有线程 A 和线程 B 这两个线程。线程 A 在解锁之前的所有操作，对于线程 B 的对同一个锁的加锁之后的所有操作而言，都是可见的。这就是锁操作的 happens-before 关系的规则。volatile 变量规则对一个 volatile 变量的写操作 happen-before 后面对该变量的读操作。这就代表了如果变量被 volatile 修饰，那么每次修改之后，其他线程在读取这个变量的时候一定能读取到该变量最新的值。我们之前介绍过 volatile 关键字，知道它能保证可见性，而这正是由本条规则所规定的。线程启动规则Thread 对象的 start 方法 happen-before 此线程 run 方法中的每一个操作。如下图所示：在图中的例子中，左侧区域是线程 A 启动了一个子线程 B，而右侧区域是子线程 B，那么子线程 B 在执行 run 方法里面的语句的时候，它一定能看到父线程在执行 threadB.start() 前的所有操作的结果。线程 join 规则join 可以让线程之间等待，假设线程 A 通过调用 threadB.start() 启动了一个新线程 B，然后调用 threadB.join() ，那么线程 A 将一直等待到线程 B 的 run 方法结束（不考虑中断等特殊情况），然后 join 方法才返回。在 join 方法返回后，线程 A 中的所有后续操作都可以看到线程 B 的 run 方法中执行的所有操作的结果，也就是线程 B 的 run 方法里面的操作 happens-before 线程 A 的 join 之后的语句。如下图所示：中断规则对线程 interrupt 方法的调用 happens-before 检测该线程的中断事件。也就是说，如果一个线程被其他线程 interrupt，那么在检测中断时（比如调用 Thread.interrupted 或者 Thread.isInterrupted 方法）一定能看到此次中断的发生，不会发生检测结果不准的情况。并发工具类的规则 线程安全的并发容器（如 HashTable）在 get 某个值时一定能看到在此之前发生的 put 等存入操作的结果。也就是说，线程安全的并发容器的存入操作 happens-before 读取操作。 信号量（Semaphore）它会释放许可证，也会获取许可证。这里的释放许可证的操作 happens-before 获取许可证的操作，也就是说，如果在获取许可证之前有释放许可证的操作，那么在获取时一定可以看到。 Future：Future 有一个 get 方法，可以用来获取任务的结果。那么，当 Future 的 get 方法得到结果的时候，一定可以看到之前任务中所有操作的结果，也就是说 Future 任务中的所有操作 happens-before Future 的 get 操作。 线程池：要想利用线程池，就需要往里面提交任务（Runnable 或者 Callable），这里面也有一个 happens-before 关系的规则，那就是提交任务的操作 happens-before 任务的执行。 happens-before 规则属于 JMM" }, { "title": "主内存和工作内存", "url": "/posts/main-memory-work-memory/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-16 15:33:00 +0000", "snippet": "CPU 有多级缓存，导致读的数据过期由于 CPU 的处理速度很快，相比之下，内存的速度就显得很慢，所以为了提高 CPU 的整体运行效率，减少空闲时间，在 CPU 和内存之间会有 cache 层，也就是缓存层的存在。虽然缓存的容量比内存小，但是缓存的速度却比内存的速度要快得多，其中 L1 缓存的速度仅次于寄存器的速度。结构示意图如下所示：在图中，从下往上分别是内存，L3 缓存、L2 缓存、L1 缓存，寄存器，然后最上层是 CPU 的 4个核心。从内存，到 L3 缓存，再到 L2 和 L1 缓存，它们距离 CPU 的核心越来越近了，越靠近核心，其容量就越小，但是速度也越快。正是由于缓存层的存在，才让我们的 CPU 能发挥出更好的性能。其实，线程间对于共享变量的可见性问题，并不是直接由多核引起的，而是由我们刚才讲到的这些 L3 缓存、L2 缓存、L1 缓存，也就是多级缓存引起的：每个核心在获取数据时，都会将数据从内存一层层往上读取，同样，后续对于数据的修改也是先写入到自己的 L1 缓存中，然后等待时机再逐层往下同步，直到最终刷回内存。假设 core 1 修改了变量 a 的值，并写入到了 core 1 的 L1 缓存里，但是还没来得及继续往下同步，由于 core 1 有它自己的的 L1 缓存，core 4 是无法直接读取 core 1 的 L1 缓存的值的，那么此时对于 core 4 而言，变量 a 的值就不是 core 1 修改后的最新的值，core 4 读取到的值可能是一个过期的值，从而引起多线程时可见性问题的发生。JMM的抽象：主内存和工作内存什么是主内存和工作内存Java 作为高级语言，屏蔽了 L1 缓存、L2 缓存、L3 缓存，也就是多层缓存的这些底层细节，用 JMM 定义了一套读写数据的规范。我们不再需要关心 L1 缓存、L2 缓存、L3 缓存等多层缓存的问题，我们只需要关心 JMM 抽象出来的主内存和工作内存的概念。为了更方便你去理解，可参考下图（来自程晓明《深入理解 Java 内存模型》https://www.infoq.cn/article/java-memory-model-1/）：每个线程只能够直接接触到工作内存，无法直接操作主内存，而工作内存中所保存的正是主内存的共享变量的副本，主内存和工作内存之间的通信是由 JMM 控制的。主内存和工作内存的关系JMM 有以下规定：（1）所有的变量都存储在主内存中，同时每个线程拥有自己独立的工作内存，而工作内存中的变量的内容是主内存中该变量的拷贝；（2）线程不能直接读 / 写主内存中的变量，但可以操作自己工作内存中的变量，然后再同步到主内存中，这样，其他线程就可以看到本次修改；（3） 主内存是由多个线程所共享的，但线程间不共享各自的工作内存，如果线程间需要通信，则必须借助主内存中转来完成。听到这里，你对上图的理解可能会更深刻一些，从图中可以看出，每个工作内存中的变量都是对主内存变量的一个拷贝，相当于是一个副本。而且图中没有一条线是可以直接连接各个工作内存的，因为工作内存之间的通信，都需要通过主内存来中转。正是由于所有的共享变量都存在于主内存中，每个线程有自己的工作内存，其中存储的是变量的副本，所以这个副本就有可能是过期的，我们来举个例子：如果一个变量 x 被线程 A 修改了，只要还没同步到主内存中，线程 B 就看不到，所以此时线程 B 读取到的 x 值就是一个过期的值，这就导致了可见性问题。" }, { "title": "内存可见性", "url": "/posts/memory-visibility/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-15 15:33:00 +0000", "snippet": "案例一我们来看看下面的代码，有一个变量 x，它是 int 类型的，如下所示：public class Visibility {    int x = 0;    public void write() {        x = 1;    }    public void read() {        int y = x;    }}这是一段很简单的代码，类中有两个方法： write 方法，作用是给 x 赋值，代码中，把 x 赋值为 1，由于 x 的初始值是 0，所以执行 write 方法相当于改变了 x 的值； read 方法，作用是把 x 读取出来，读取的时候我们用了一个新的 int 类型变量的 y 来接收 x 的值。我们假设有两个线程来执行上述代码，第 1 个线程执行的是 write 方法，第 2 个线程执行的是 read 方法。下面我们来分析一下，代码在实际运行过程中的情景是怎么样的，如下图所示：那么，假设线程 1 的工作内存还未同步给主内存，此时假设线程 2 开始读取，那么它读到的 x 值不是 1，而是 0，也就是说虽然此时线程 1 已经把 x 的值改动了，但是对于第 2 个线程而言，根本感知不到 x 的这个变化，这就产生了可见性问题。案例二在如下所示的代码中，有两个变量 a 和 b， 并且把它们赋初始值为 10 和 20。/** * 描述：     演示可见性带来的问题 */public class VisibilityProblem {    int a = 10;    int b = 20;    private void change() {        a = 30;        b = a;    }    private void print() {        System.out.println(&quot;b=&quot; + b + &quot;;a=&quot; + a);    }    public static void main(String[] args) {        while (true) {            VisibilityProblem problem = new VisibilityProblem();            new Thread(new Runnable() {                @Override                public void run() {                    try {                        Thread.sleep(1);                    } catch (InterruptedException e) {                        e.printStackTrace();                    }                    problem.change();                }            }).start();            new Thread(new Runnable() {                @Override                public void run() {                    try {                        Thread.sleep(1);                    } catch (InterruptedException e) {                        e.printStackTrace();                    }                    problem.print();                }            }).start();        }    }}在类中，有两个方法： change 方法，把 a 改成 30，然后把 b 赋值为 a 的值； print 方法，先打印出 b 的值，然后再打印出 a 的值。接下来我们来看一下 main 函数，在 main 函数中同样非常简单。首先有一个 while 的死循环，在这个循环中，我们新建两个线程，并且让它们先休眠一毫秒，然后再分别去执行 change 方法和 print 方法。休眠一毫秒的目的是让它们执行这两个方法的时间，尽可能的去靠近。下面我们运行这段代码并分析一下可能出现的情况。 第 1 种情况：是最普通的情况了。假设第 1 个线程，也就是执行 change 的线程先运行，并且运行完毕了，然后第 2 个线程开始运行，那么第 2 个线程自然会打印出 b = 30;a = 30 的结果。 第 2 种情况：与第 1 种情况相反。因为线程先 start，并不代表它真的先执行，所以第 2 种情况是第 2 个线程先打印，然后第 1 个线程再去进行 change，那么此时打印出来的就是 a 和 b 的初始值，打印结果为 b = 20;a = 10。 第 3 种情况：它们几乎同时运行，所以会出现交叉的情况。比如说当第 1 个线程的 change 执行到一半，已经把 a 的值改为 30 了，而 b 的值还未来得及修改，此时第 2 个线程就开始打印了，所以此时打印出来的 b 还是原始值 20，而 a 已经变为了 30， 即打印结果为 b = 20;a = 30。这些都很好理解，但是有一种情况不是特别容易理解，那就是打印结果为 b = 30;a = 10，我们来想一下，为什么会发生这种情况？ 首先打印出来的是 b = 30，这意味着 b 的值被改变了，也就是说 b = a 这个语句已经执行了； 如果 b = a 要想执行，那么前面 a = 30 也需要执行，此时 b 才能等于 a 的值，也就是 30； 这也就意味着 change 方法已经执行完毕了。可是在这种情况下再打印 a，结果应该是 a = 30，而不应该打印出 a = 10。因为在刚才 change 执行的过程中，a 的值已经被改成 30 了，不再是初始值的 10。所以，如果出现了打印结果为 b = 30;a = 10 这种情况，就意味着发生了可见性问题：a 的值已经被第 1 个线程修改了，但是其他线程却看不到，由于 a 的最新值却没能及时同步过来，所以才会打印出 a 的旧值。发生上述情况的几率不高。我把发生时的截屏用图片的形式展示给你看看，如下所示：解决问题那么我们应该如何避免可见性问题呢？在案例一中，我们可以使用 volatile 来解决问题，我们在原来的代码的基础上给 x 变量加上 volatile 修饰，其他的代码不变。加了 volatile 关键字之后，只要第 1 个线程修改完了 x 的值，那么当第 2 个线程想读取 x 的时候，它一定可以读取到 x 的最新的值，而不可能读取到旧值。同理，我们也可以用 volatile 关键字来解决案例二的问题，如果我们给 a 和 b 加了 volatile 关键字后，无论运行多长时间，也不会出现 b = 30;a = 10 的情况，这是因为 volatile 保证了只要 a 和 b 的值发生了变化，那么读取的线程一定能感知到。能够保证可见性的措施除了 volatile 关键字可以让变量保证可见性外，synchronized、Lock、并发集合等一系列工具都可以在一定程度上保证可见性，具体保证可见性的时机和手段，我将在第 61 课时 happens-before 原则中详细展开讲解。synchronized 不仅保证了原子性，还保证了可见性下面再来分析一下之前所使用过的 synchronized 关键字，在理解了可见性问题之后，相信你对 synchronized 的理解会更加深入。关于 synchronized 这里有一个特别值得说的点，我们之前可能一致认为，使用了 synchronized 之后，它会设立一个临界区，这样在一个线程操作临界区内的数据的时候，另一个线程无法进来同时操作，所以保证了线程安全。其实这是不全面的，这种说法没有考虑到可见性问题，完整的说法是：synchronized 不仅保证了临界区内最多同时只有一个线程执行操作，同时还保证了在前一个线程释放锁之后，之前所做的所有修改，都能被获得同一个锁的下一个线程所看到，也就是能读取到最新的值。因为如果其他线程看不到之前所做的修改，依然也会发生线程安全问题。" }, { "title": "原子性和原子操作", "url": "/posts/java-atom-notice/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-14 15:33:00 +0000", "snippet": "什么是原子性和原子操作在编程中，具备原子性的操作被称为原子操作。原子操作是指一系列的操作，要么全部发生，要么全部不发生，不会出现执行一半就终止的情况。比如转账行为就是一个原子操作，该过程包含扣除余额、银行系统生成转账记录、对方余额增加等一系列操作。虽然整个过程包含多个操作，但由于这一系列操作被合并成一个原子操作，所以它们要么全部执行成功，要么全部不执行，不会出现执行一半的情况。比如我的余额已经扣除，但是对方的余额却不增加，这种情况是不会出现的，所以说转账行为是具备原子性的。而具有原子性的原子操作，天然具备线程安全的特性。下面我们举一个不具备原子性的例子，比如 i++ 这一行代码在 CPU 中执行时，可能会从一行代码变为以下的 3 个指令： 第一个步骤是读取； 第二个步骤是增加； 第三个步骤是保存。这就说明 i++ 是不具备原子性的，同时也证明了 i++ 不是线程安全的，正如第 06 课时中所介绍的那样。下面我们简单的复习一下，如何发生的线程不安全问题，如下所示：我们根据箭头指向依次看，线程 1 首先拿到 i=1 的结果，然后进行 i+1 操作，但假设此时 i+1 的结果还没有来得及被保存下来，线程 1 就被切换走了，于是 CPU 开始执行线程 2，它所做的事情和线程 1 是一样的 i++ 操作，但此时我们想一下，它拿到的 i 是多少？实际上和线程 1 拿到的 i 结果一样，同样是 1，为什么呢？因为线程 1 虽然对 i 进行了 +1 操作，但结果没有保存，所以线程 2 看不到修改后的结果。然后假设等线程 2 对 i 进行 +1 操作后，又切换到线程 1，让线程 1 完成未完成的操作，即将 i+1 的结果 2 保存下来，然后又切换到线程 2 完成 i=2 的保存操作，虽然两个线程都执行了对 i 进行 +1 的操作，但结果却最终保存了 i=2，而不是我们期望的 i=3，这样就发生了线程安全问题，导致数据结果错误，这也是最典型的线程安全问题。Java 中的原子操作有哪些在了解了原子操作的特性之后，让我们来看一下 Java 中有哪些操作是具备原子性的。Java 中的以下几种操作是具备原子性的，属于原子操作： 除了 long 和 double 之外的基本类型（int、byte、boolean、short、char、float）的读/写操作，都天然的具备原子性； 所有引用 reference 的读/写操作； 加了 volatile 后，所有变量的读/写操作（包含 long 和 double）。这也就意味着 long 和 double 加了 volatile 关键字之后，对它们的读写操作同样具备原子性； 在 java.concurrent.Atomic 包中的一部分类的一部分方法是具备原子性的，比如 AtomicInteger 的 incrementAndGet 方法。long 和 double 的原子性在前面，我们讲述了 long 和 double 和其他的基本类型不太一样，好像不具备原子性，这是什么原因造成的呢？ 官方文档对于上述问题的描述，如下所示：Non-Atomic Treatment of double and longFor the purposes of the Java programming language memory model, a single write to a non-volatile long or double value is treated as two separate writes: one to each 32-bit half. This can result in a situation where a thread sees the first 32 bits of a 64-bit value from one write, and the second 32 bits from another write.Writes and reads of volatile long and double values are always atomic.Writes to and reads of references are always atomic, regardless of whether they are implemented as 32-bit or 64-bit values.Some implementations may find it convenient to divide a single write action on a 64-bit long or double value into two write actions on adjacent 32-bit values. For efficiency’s sake, this behavior is implementation-specific; an implementation of the Java Virtual Machine is free to perform writes to long and double values atomically or in two parts.Implementations of the Java Virtual Machine are encouraged to avoid splitting 64-bit values where possible. Programmers are encouraged to declare shared 64-bit values as volatile or synchronize their programs correctly to avoid possible complications.从刚才的 JVM 规范中我们可以知道，long 和 double 的值需要占用 64 位的内存空间，而对于 64 位值的写入，可以分为两个 32 位的操作来进行。这样一来，本来是一个整体的赋值操作，就可能被拆分为低 32 位和高 32 位的两个操作。如果在这两个操作之间发生了其他线程对这个值的读操作，就可能会读到一个错误、不完整的值。JVM 的开发者可以自由选择是否把 64 位的 long 和 double 的读写操作作为原子操作去实现，并且规范推荐 JVM 将其实现为原子操作。当然，JVM 的开发者也有权利不这么做，这同样是符合规范的。规范同样规定，如果使用 volatile 修饰了 long 和 double，那么其读写操作就必须具备原子性了。同时，规范鼓励程序员使用 volatile 关键字对这个问题加以控制，由于规范规定了对于 volatile long 和 volatile double 而言，JVM 必须保证其读写操作的原子性，所以加了 volatile 之后，对于程序员而言，就可以确保程序正确。实际开发中此时，你可能会有疑问，比如，如果之前对于上述问题不是很了解，在开发过程中没有给 long 和 double 加 volatile，好像也没有出现过问题？而且，在以后的开发过程中，是不是必须给 long 和 double 加 volatile 才是安全的？其实在实际开发中，读取到“半个变量”的情况非常罕见，这个情况在目前主流的 Java 虚拟机中不会出现。因为 JVM 规范虽然不强制虚拟机把 long 和 double 的变量写操作实现为原子操作，但它其实是“强烈建议”虚拟机去把该操作作为原子操作来实现的。而在目前各种平台下的主流虚拟机的实现中，几乎都会把 64 位数据的读写操作作为原子操作来对待，因此我们在编写代码时一般不需要为了避免读到“半个变量”而把 long 和 double 声明为 volatile 的。原子操作 + 原子操作 != 原子操作值得注意的是，简单地把原子操作组合在一起，并不能保证整体依然具备原子性。比如连续转账两次的操作行为，显然不能合并当做一个原子操作，虽然每一次转账操作都是具备原子性的，但是将两次转账合为一次的操作，这个组合就不具备原子性了，因为在两次转账之间可能会插入一些其他的操作，例如系统自动扣费等，导致第二次转账失败，而且第二次转账失败并不会影响第一次转账成功。 volatile修饰的变量不是不保证原子性吗？—— 只能保证读取/写入的原子性，不保证组合操作的原子性。 上面的例子i=2的结果和int的读写具有原子性是不是冲突了啊？ —— 不冲突，读和写分别具有原子性，但组合后不具备原子性。 除了 long 和 double 之外的基本类型（int、byte、boolean、short、char、float）的读/写操作，都天然的具备原子性； 那为啥多线程的时候加操作还需要加锁？我是不是有什么地方理解错了？—— 需要加锁是因为通常涉及到组合操作，不是单纯的一次性的读和写。 int i=1；属于写操作，可以单独看作有原子性，这样理解对吗？但是它在内部有三个过程 加载-赋值-存储，这个过程也具备是吧 —— 这是具备原子性的。 讲错了吧 ，volatile修饰的变量不是不保证原子性吗？ —— 只能保证读取/写入的原子性，不保证组合操作的原子性。 int a=1 具备原子性吗？是的" }, { "title": "什么是重排序", "url": "/posts/common-sort/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-13 15:33:00 +0000", "snippet": "什么是重排序假设写了一个 Java 程序，包含一系列的语句，我们会默认期望这些语句的实际运行顺序和写的代码顺序一致。但实际上，编译器、JVM 或者 CPU 都有可能出于优化等目的，对于实际指令执行的顺序进行调整，这就是重排序。重排序的好处：提高处理速度为什么要重排序？这样做有什么好处呢？图中左侧是 3 行 Java 代码，右侧是这 3 行代码可能被转化成的指令。可以看出 a = 100 对应的是 Load a、Set to 100、Store a，意味着从主存中读取 a 的值，然后把值设置为 100，并存储回去，同理， b = 5 对应的是下面三行 Load b、Set to 5、Store b，最后的 a = a + 10，对应的是 Load a、Set to 110、Store a。如果你仔细观察，会发现这里有两次“Load a”和两次“Store a”，说明存在一定的重排序的优化空间。经过重排序之后，情况如下图所示：重排序后， a 的两次操作被放到一起，指令执行情况变为 Load a、Set to 100、Set to 110、 Store a。下面和 b 相关的指令不变，仍对应 Load b、 Set to 5、Store b。可以看出，重排序后 a 的相关指令发生了变化，节省了一次 Load a 和一次 Store a。重排序通过减少执行指令，从而提高整体的运行速度，这就是重排序带来的优化和好处。重排序的 3 种情况 编译器优化编译器（包括 JVM、JIT 编译器等）出于优化的目的，例如当前有了数据 a，把对 a 的操作放到一起效率会更高，避免读取 b 后又返回来重新读取 a 的时间开销，此时在编译的过程中会进行一定程度的重排。不过重排序并不意味着可以任意排序，它需要需要保证重排序后，不改变单线程内的语义，否则如果能任意排序的话，程序早就逻辑混乱了。 CPU 重排序CPU 同样会有优化行为，这里的优化和编译器优化类似，都是通过乱序执行的技术来提高整体的执行效率。所以即使之前编译器不发生重排，CPU 也可能进行重排，我们在开发中，一定要考虑到重排序带来的后果。 内存的“重排序”内存系统内不存在真正的重排序，但是内存会带来看上去和重排序一样的效果，所以这里的“重排序”打了双引号。由于内存有缓存的存在，在 JMM 里表现为主存和本地内存，而主存和本地内存的内容可能不一致，所以这也会导致程序表现出乱序的行为。 例如，线程 1 修改了 a 的值，但是修改后没有来得及把新结果写回主存或者线程 2 没来得及读到最新的值，所以线程 2 看不到刚才线程 1 对 a 的修改，此时线程 2 看到的 a 还是等于初始值。但是线程 2 却可能看到线程 1 修改 a 之后的代码执行效果，表面上看起来像是发生了重顺序。 参考：程晓明《深入理解 Java 内存模型》 https://www.infoq.cn/article/java-memory-model-1/" }, { "title": "Java 内存模型", "url": "/posts/java-memory-model/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-12 14:33:00 +0000", "snippet": "Java 并发的底层原理，那么 Java 内存模型的知识非常重要，同时也是一个分水岭，可以区分出是仅停留在如何使用并发工具，还是能更进一步，知其所以然。Java 内存结构和Java模型给区分开来。和面试常考的 JVM 中的堆、栈、方法区、常量池等相关的概念实际上是“JVM内存结构”，并不是Java 内存模型。我们都知道，编写的 Java 代码，最终还是要转化为 CPU 指令才能执行的。为了理解 Java 内存模型的作用，我们首先就来回顾一下从 Java 代码到最终执行的 CPU 指令的大致流程： 最开始，我们编写的 Java 代码，是 *.java 文件； 在编译（包含词法分析、语义分析等步骤）后，在刚才的 .java 文件之外，会多出一个新的 Java 字节码文件（.class）； JVM 会分析刚才生成的字节码文件（*.class），并根据平台等因素，把字节码文件转化为具体平台上的机器指令； 机器指令则可以直接在 CPU 上运行，也就是最终的程序执行。为什么需要 JMM（Java Memory Model，Java 内存模型）在更早期的语言中，其实是不存在内存模型的概念的。所以程序最终执行的效果会依赖于具体的处理器，而不同的处理器的规则又不一样，不同的处理器之间可能差异很大，因此同样的一段代码，可能在处理器 A 上运行正常，而在处理器 B 上运行的结果却不一致。同理，在没有 JMM 之前，不同的 JVM 的实现，也会带来不一样的“翻译”结果。所以 Java 非常需要一个标准，来让 Java 开发者、编译器工程师和 JVM 工程师能够达成一致。达成一致后，我们就可以很清楚的知道什么样的代码最终可以达到什么样的运行效果，让多线程运行结果可以预期，这个标准就是 JMM，这就是需要 JMM 的原因。突破 Java 代码的层次，开始往下钻研，研究从 Java 代码到 CPU 指令的这个转化过程要遵守哪些和并发相关的原则和规范，这就是 JMM 的重点内容。如果不加以规范，那么同样的 Java 代码，完全可能产生不一样的执行效果，那是不可接受的，这也违背了 Java “书写一次、到处运行”的特点。JMM 是什么JMM 是规范JMM 是和多线程相关的一组规范，需要各个 JVM 的实现来遵守 JMM 规范，以便于开发者可以利用这些规范，更方便地开发多线程程序。这样一来，即便同一个程序在不同的虚拟机上运行，得到的程序结果也是一致的。如果没有 JMM 内存模型来规范，那么很可能在经过了不同 JVM 的“翻译”之后，导致在不同的虚拟机上运行的结果不一样，那是很大的问题。因此，JMM 与处理器、缓存、并发、编译器有关。它解决了 CPU 多级缓存、处理器优化、指令重排等导致的结果不可预期的问题。JMM 是工具类和关键字的原理之前我们使用了各种同步工具和关键字，包括 volatile、synchronized、Lock 等，其实它们的原理都涉及 JMM。正是 JMM 的参与和帮忙，才让各个同步工具和关键字能够发挥作用，帮我们开发出并发安全的程序。比如我们写了关键字 synchronized，JVM 就会在 JMM 的规则下，“翻译”出合适的指令，包括限制指令之间的顺序，以便在即使发生了重排序的情况下，也能保证必要的“可见性”，这样一来，不同的 JVM 对于相同的代码的执行结果就变得可预期了，我们 Java 程序员就只需要用同步工具和关键字就可以开发出正确的并发程序了，这都要感谢 JMM。JMM 里最重要 3 点内容，分别是：重排序、原子性、内存可见性。" }, { "title": "Condition 接口", "url": "/posts/condition-object-wait()-notify()/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-10 13:33:00 +0000", "snippet": "作用假设线程 1 需要等待某些条件满足后，才能继续运行，这个条件会根据业务场景不同，有不同的可能性，比如等待某个时间点到达或者等待某些任务处理完毕。在这种情况下，我们就可以执行 Condition 的 await 方法，一旦执行了该方法，这个线程就会进入 WAITING 状态。通常会有另外一个线程，我们把它称作线程 2，它去达成对应的条件，直到这个条件达成之后，那么，线程 2 调用 Condition 的 signal 方法 [或 signalAll 方法]，代表“这个条件已经达成了，之前等待这个条件的线程现在可以苏醒了”。这个时候，JVM 就会找到等待该 Condition 的线程，并予以唤醒，根据调用的是 signal 方法或 signalAll 方法，会唤醒 1 个或所有的线程。于是，线程 1 在此时就会被唤醒，然后它的线程状态又会回到 Runnable 可执行状态。栗子用一个代码来说明这个问题，如下所示：public class ConditionDemo { private ReentrantLock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); void method1() throws InterruptedException { lock.lock(); try { System.out.println(Thread.currentThread().getName()+&quot;:条件不满足，开始await&quot;); condition.await(); System.out.println(Thread.currentThread().getName()+&quot;:条件满足了，开始执行后续的任务&quot;); } finally { lock.unlock(); } } void method2() throws InterruptedException {        lock.lock();        try{            System.out.println(Thread.currentThread().getName()+&quot;:需要5秒钟的准备时间&quot;);            Thread.sleep(5000);            System.out.println(Thread.currentThread().getName()+&quot;:准备工作完成，唤醒其他的线程&quot;);            condition.signal();        }finally {            lock.unlock();        }    }    public static void main(String[] args) throws InterruptedException {        ConditionDemo conditionDemo = new ConditionDemo();        new Thread(new Runnable() {            @Override            public void run() {                try {                    conditionDemo.method2();                } catch (InterruptedException e) {                    e.printStackTrace();                }            }        }).start();        conditionDemo.method1();    }}在这个代码中，有以下三个方法。 method1，它代表主线程将要执行的内容，首先获取到锁，打印出“条件不满足，开始 await”，然后调用 condition.await() 方法，直到条件满足之后，则代表这个语句可以继续向下执行了，于是打印出“条件满足了，开始执行后续的任务”，最后会在 finally 中解锁。 method2，它同样也需要先获得锁，然后打印出“需要 5 秒钟的准备时间”，接着用 sleep 来模拟准备时间；在时间到了之后，则打印出“准备工作完成”，最后调用 condition.signal() 方法，把之前已经等待的线程唤醒。 main 方法，它的主要作用是执行上面这两个方法，它先去实例化我们这个类，然后再用子线程去调用这个类的 method2 方法，接着用主线程去调用 method1 方法。最终这个代码程序运行结果如下所示：main:条件不满足，开始 awaitThread-0:需要 5 秒钟的准备时间Thread-0:准备工作完成，唤醒其他的线程main:条件满足了，开始执行后续的任务同时也可以看到，打印这行语句它所运行的线程，第一行语句和第四行语句打印的是在 main 线程中，也就是在主线程中去打印的，而第二、第三行是在子线程中打印的。这个代码就模拟了我们前面所描述的场景。注意点下面我们来看一下，在使用 Condition 的时候有哪些注意点。 线程 2 解锁后，线程 1 才能获得锁并继续执行线程 2 对应刚才代码中的子线程，而线程 1 对应主线程。这里需要额外注意，并不是说子线程调用了 signal 之后，主线程就可以立刻被唤醒去执行下面的代码了，而是说在调用了 signal 之后，还需要等待子线程完全退出这个锁，即执行 unlock 之后，这个主线程才有可能去获取到这把锁，并且当获取锁成功之后才能继续执行后面的任务。刚被唤醒的时候主线程还没有拿到锁，是没有办法继续往下执行的。 signalAll() 和 signal() 区别signalAll() 会唤醒所有正在等待的线程，而 signal() 只会唤醒一个线程。用 Condition 和 wait/notify 实现简易版阻塞队列在第 05 讲，讲过如何用 Condition 和 wait/notify 来实现生产者/消费者模式，其中的精髓就在于用 Condition 和 wait/notify 来实现简易版阻塞队列，我们来分别回顾一下这两段代码。用 Condition 实现简易版阻塞队列代码如下所示：public class MyBlockingQueueForCondition { private Queue queue; private int max = 16; private ReentrantLock lock = new ReentrantLock(); private Condition notEmpty = lock.newCondition(); private Condition notFull = lock.newCondition(); public MyBlockingQueueForCondition(int size) { this.max = size; queue = new LinkedList(); } public void put(Object o) throws InterruptedException { lock.lock(); try { while (queue.size() == max) { notFull.await(); } queue.add(o); notEmpty.signalAll(); } finally { lock.unlock(); } } public Object take() throws InterruptedException { lock.lock(); try { while (queue.size() == 0) { notEmpty.await(); } Object item = queue.remove(); notFull.signalAll(); return item; } finally { lock.unlock(); } }}在上面的代码中，首先定义了一个队列变量 queue，其最大容量是 16；然后定义了一个 ReentrantLock 类型的 Lock 锁，并在 Lock 锁的基础上创建了两个 Condition，一个是 notEmpty，另一个是 notFull，分别代表队列没有空和没有满的条件；最后，声明了 put 和 take 这两个核心方法。用 wait/notify 实现简易版阻塞队列我们再来看看如何使用 wait/notify 来实现简易版阻塞队列，代码如下：class MyBlockingQueueForWaitNotify { private int maxSize; private LinkedList&amp;lt;Object&amp;gt; storage; public MyBlockingQueueForWaitNotify (int size) { this.maxSize = size; storage = new LinkedList&amp;lt;&amp;gt;(); } public synchronized void put() throws InterruptedException { while (storage.size() == maxSize) { this.wait(); } storage.add(new Object()); this.notifyAll(); } public synchronized void take() throws InterruptedException { while (storage.size() == 0) { this.wait(); } System.out.println(storage.remove()); this.notifyAll(); }}如代码所示，最主要的部分仍是 put 与 take 方法。我们先来看 put 方法，该方法被 synchronized 保护，while 检查 List 是否已满，如果不满就往里面放入数据，并通过 notifyAll() 唤醒其他线程。同样，take 方法也被 synchronized 修饰，while 检查 List 是否为空，如果不为空则获取数据并唤醒其他线程。在第 05 讲，有对这两段代码的详细讲解，遗忘的小伙伴可以到前面复习一下。Condition 和 wait/notify的关系对比上面两种实现方式的 put 方法，会发现非常类似，此时让我们把这两段代码同时列在屏幕中，然后进行对比：左：public void put(Object o) throws InterruptedException { lock.lock(); try { while (queue.size() == max) { condition1.await(); } queue.add(o); condition2.signalAll(); } finally { lock.unlock(); }}右：public synchronized void put() throws InterruptedException { while (storage.size() == maxSize) { this.wait(); } storage.add(new Object()); this.notifyAll();}可以看出，左侧是 Condition 的实现，右侧是 wait/notify 的实现：lock.lock() 对应进入 synchronized 方法condition.await() 对应 object.wait()condition.signalAll() 对应 object.notifyAll()lock.unlock() 对应退出 synchronized 方法实际上，如果说 Lock 是用来代替 synchronized 的，那么 Condition 就是用来代替相对应的 Object 的 wait/notify/notifyAll，所以在用法和性质上几乎都一样。Condition 把 Object 的 wait/notify/notifyAll 转化为了一种相应的对象，其实现的效果基本一样，但是把更复杂的用法，变成了更直观可控的对象方法，是一种升级。await 方法会自动释放持有的 Lock 锁，和 Object 的 wait 一样，不需要自己手动释放锁。另外，调用 await 的时候必须持有锁，否则会抛出异常，这一点和 Object 的 wait 一样。 在 Debug 模式下，有可能method2的方法会先执行造成死锁，一直等待。—— 可能会一直等待，但是不属于死锁，因为await会释放锁。" }, { "title": "CyclicBarrier 和 CountdownLatch", "url": "/posts/CyclicBarrier-CountdownLatch/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-10 09:33:00 +0000", "snippet": "CyclicBarrier作用CyclicBarrier 和 CountDownLatch 确实有一定的相似性，它们都能阻塞一个或者一组线程，直到某种预定的条件达到之后，这些之前在等待的线程才会统一出发，继续向下执行。正因为它们有这个相似点，你可能会认为它们的作用是完全一样的，其实并不是。CyclicBarrier 可以构造出一个集结点，当某一个线程执行 await() 的时候，它就会到这个集结点开始等待，等待这个栅栏被撤销。直到预定数量的线程都到了这个集结点之后，这个栅栏就会被撤销，之前等待的线程就在此刻统一出发，继续去执行剩下的任务。举一个生活中的例子。假设我们班级春游去公园里玩，并且会租借三人自行车，每个人都可以骑，但由于这辆自行车是三人的，所以要凑齐三个人才能骑一辆，而且从公园大门走到自行车驿站需要一段时间。那么我们模拟这个场景，写出如下代码：public class CyclicBarrierDemo {    public static void main(String[] args) {        CyclicBarrier cyclicBarrier = new CyclicBarrier(3);        for (int i = 0; i &amp;lt; 6; i++) {            new Thread(new Task(i + 1, cyclicBarrier)).start();        }    }    static class Task implements Runnable {        private int id;        private CyclicBarrier cyclicBarrier;        public Task(int id, CyclicBarrier cyclicBarrier) {            this.id = id;            this.cyclicBarrier = cyclicBarrier;        }        @Override        public void run() {            System.out.println(&quot;同学&quot; + id + &quot;现在从大门出发，前往自行车驿站&quot;);            try {                Thread.sleep((long) (Math.random() * 10000));                System.out.println(&quot;同学&quot; + id + &quot;到了自行车驿站，开始等待其他人到达&quot;);                cyclicBarrier.await();                System.out.println(&quot;同学&quot; + id + &quot;开始骑车&quot;);            } catch (InterruptedException e) {                e.printStackTrace();            } catch (BrokenBarrierException e) {                e.printStackTrace();            }        }    }}在这段代码中可以看到，首先建了一个参数为 3 的 CyclicBarrier，参数为 3 的意思是需要等待 3 个线程到达这个集结点才统一放行；然后我们又在 for 循环中去开启了 6 个线程，每个线程中执行的 Runnable 对象就在下方的 Task 类中，直接看到它的 run 方法，它首先会打印出”同学某某现在从大门出发，前往自行车驿站”，然后是一个随机时间的睡眠，这就代表着从大门开始步行走到自行车驿站的时间，由于每个同学的步行速度不一样，所以时间用随机值来模拟。当同学们都到了驿站之后，比如某一个同学到了驿站，首先会打印出“同学某某到了自行车驿站，开始等待其他人到达”的消息，然后去调用 CyclicBarrier 的 await() 方法。一旦它调用了这个方法，它就会陷入等待，直到三个人凑齐，才会继续往下执行，一旦开始继续往下执行，就意味着 3 个同学开始一起骑车了，所以打印出“某某开始骑车”这个语句。结果如下所示：同学 1 现在从大门出发，前往自行车驿站同学 3 现在从大门出发，前往自行车驿站同学 2 现在从大门出发，前往自行车驿站同学 4 现在从大门出发，前往自行车驿站同学 5 现在从大门出发，前往自行车驿站同学 6 现在从大门出发，前往自行车驿站同学 5 到了自行车驿站，开始等待其他人到达同学 2 到了自行车驿站，开始等待其他人到达同学 3 到了自行车驿站，开始等待其他人到达同学 3 开始骑车同学 5 开始骑车同学 2 开始骑车同学 6 到了自行车驿站，开始等待其他人到达同学 4 到了自行车驿站，开始等待其他人到达同学 1 到了自行车驿站，开始等待其他人到达同学 1 开始骑车同学 6 开始骑车同学 4 开始骑车可以看到 6 个同学纷纷从大门出发走到自行车驿站，因为每个人的速度不一样，所以会有 3 个同学先到自行车驿站，不过在这 3 个先到的同学里面，前面 2 个到的都必须等待第 3 个人到齐之后，才可以开始骑车。后面的同学也一样，由于第一辆车已经被骑走了，第二辆车依然也要等待 3 个人凑齐才能统一发车。要想实现这件事情，如果你不利用 CyclicBarrier 去做的话，逻辑可能会非常复杂，因为你也不清楚哪个同学先到、哪个后到。而用了 CyclicBarrier 之后，可以非常简洁优雅的实现这个逻辑，这就是它的一个非常典型的应用场景。执行动作 barrierActionpublic CyclicBarrier(int parties, Runnable barrierAction)：当 parties 线程到达集结点时，继续往下执行前，会执行这一次这个动作。接下来我们再介绍一下它的一个额外功能，就是执行动作 barrierAction 功能。CyclicBarrier 还有一个构造函数是传入两个参数的，第一个参数依然是 parties，代表需要几个线程到齐；第二个参数是一个 Runnable 对象，它就是我们下面所要介绍的 barrierAction。当预设数量的线程到达了集结点之后，在出发的时候，便会执行这里所传入的 Runnable 对象，那么假设我们把刚才那个代码的构造函数改成如下这个样子：CyclicBarrier cyclicBarrier = new CyclicBarrier(3, new Runnable() { @Override public void run() { System.out.println(&quot;凑齐3人了，出发！&quot;); }});可以看出，我们传入了第二个参数，它是一个 Runnable 对象，在这里传入了这个 Runnable 之后，这个任务就会在到齐的时候去打印”凑齐3人了，出发！”。上面的代码如果改成这个样子，则执行结果如下所示：同学 1 现在从大门出发，前往自行车驿站同学 3 现在从大门出发，前往自行车驿站同学 2 现在从大门出发，前往自行车驿站同学 4 现在从大门出发，前往自行车驿站同学 5 现在从大门出发，前往自行车驿站同学 6 现在从大门出发，前往自行车驿站同学 2 到了自行车驿站，开始等待其他人到达同学 4 到了自行车驿站，开始等待其他人到达同学 6 到了自行车驿站，开始等待其他人到达凑齐 3 人了，出发！同学 6 开始骑车同学 2 开始骑车同学 4 开始骑车同学 1 到了自行车驿站，开始等待其他人到达同学 3 到了自行车驿站，开始等待其他人到达同学 5 到了自行车驿站，开始等待其他人到达凑齐 3 人了，出发！同学 5 开始骑车同学 1 开始骑车同学 3 开始骑车可以看出，三个人凑齐了一组之后，就会打印出“凑齐 3 人了，出发！”这样的语句，该语句恰恰是我们在这边传入 Runnable 所执行的结果。值得注意的是，这个语句每个周期只打印一次，不是说你有几个线程在等待就打印几次，而是说这个任务只在“开闸”的时候执行一次。CyclicBarrier 和 CountDownLatch 的异同下面我们来总结一下 CyclicBarrier 和 CountDownLatch 有什么异同。相同点：都能阻塞一个或一组线程，直到某个预设的条件达成发生，再统一出发。但是它们也有很多不同点，具体如下。 作用对象不同：CyclicBarrier 要等固定数量的线程都到达了栅栏位置才能继续执行，而 CountDownLatch 只需等待数字倒数到 0，也就是说 CountDownLatch 作用于事件，但 CyclicBarrier 作用于线程；CountDownLatch 是在调用了 countDown 方法之后把数字倒数减 1，而 CyclicBarrier 是在某线程开始等待后把计数减 1。 可重用性不同：CountDownLatch 在倒数到 0 并且触发门闩打开后，就不能再次使用了，除非新建一个新的实例；而 CyclicBarrier 可以重复使用，在刚才的代码中也可以看出，每 3 个同学到了之后都能出发，并不需要重新新建实例。CyclicBarrier 还可以随时调用 reset 方法进行重置，如果重置时有线程已经调用了 await 方法并开始等待，那么这些线程则会抛出 BrokenBarrierException 异常。 执行动作不同：CyclicBarrier 有执行动作 barrierAction，而 CountDownLatch 没这个功能。" }, { "title": "CountDownLatch", "url": "/posts/CountDownLatch-sort/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-09 15:33:00 +0000", "snippet": "我们先来介绍一下 CountDownLatch，它是 JDK 提供的并发流程控制的工具类，它是在 java.util.concurrent 包下，在 JDK1.5 以后加入的。下面举个例子来说明它主要在什么场景下使用。比如我们去游乐园坐激流勇进，有的时候游乐园里人不是那么多，这时，管理员会让你稍等一下，等人坐满了再开船，这样的话可以在一定程度上节约游乐园的成本。座位有多少，就需要等多少人，这就是 CountDownLatch 的核心思想，等到一个设定的数值达到之后，才能出发。流程图激流勇进的用流程图的方式来表示：可以看到，最开始 CountDownLatch 设置的初始值为 3，然后 T0 线程上来就调用 await 方法，它的作用是让这个线程开始等待，等待后面的 T1、T2、T3，它们每一次调用 countDown 方法，3 这个数值就会减 1，也就是从 3 减到 2，从 2 减到 1，从 1 减到 0，一旦减到 0 之后，这个 T0 就相当于达到了自己触发继续运行的条件，于是它就恢复运行了。主要方法介绍CountDownLatch 的主要方法： 构造函数：public CountDownLatch(int count) {}；它的构造函数是传入一个参数，该参数 count 是需要倒数的数值； await()：调用 await() 方法的线程开始等待，直到倒数结束，也就是 count 值为 0 的时候才会继续执行； await(long timeout, TimeUnit unit)：await() 有一个重载的方法，里面会传入超时参数，这个方法的作用和 await() 类似，但是这里可以设置超时时间，如果超时就不再等待了。 countDown()：把数值倒数 1，也就是将 count 值减 1，直到减为 0 时，之前等待的线程会被唤起。CountDownLatch 的两个典型用法。 一个线程等待其他多个线程都执行完毕，再继续自己的工作在实际场景中，很多情况下需要我们初始化一系列的前置条件（比如建立连接、准备数据），在这些准备条件都完成之前，是不能进行下一步工作的，所以这就是利用 CountDownLatch 的一个很好场景，我们可以让应用程序的主线程在其他线程都准备完毕之后再继续执行。那就是运动员跑步的场景，比如在比赛跑步时有 5 个运动员参赛，终点有一个裁判员，什么时候比赛结束呢？那就是当所有人都跑到终点之后，这相当于裁判员等待 5 个运动员都跑到终点，宣布比赛结束。我们用代码的形式来写出运动员跑步的场景，代码如下：public class RunDemo1 { public static void main(String[] args) throws InterruptedException { CountDownLatch latch = new CountDownLatch(5); ExecutorService service = Executors.newFixedThreadPool(5); for (int i = 0; i &amp;lt; 5; i++) { final int no = i + 1; Runnable runnable = new Runnable() { @Override public void run() { try { Thread.sleep((long) (Math.random() * 10000)); System.out.println(no + &quot;号运动员完成了比赛。&quot;); } catch (InterruptedException e) { e.printStackTrace(); } finally { latch.countDown(); } } }; service.submit(runnable); } System.out.println(&quot;等待5个运动员都跑完.....&quot;); latch.await(); System.out.println(&quot;所有人都跑完了，比赛结束。&quot;); }}在这段代码中，我们新建了一个初始值为 5 的 CountDownLatch，然后建立了一个固定 5 线程的线程池，用一个 for 循环往这个线程池中提交 5 个任务，每个任务代表一个运动员，这个运动员会首先随机等待一段时间，代表他在跑步，然后打印出他完成了比赛，在跑完了之后，同样会调用 countDown 方法来把计数减 1。之后我们再回到主线程，主线程打印完“等待 5 个运动员都跑完”这句话后，会调用 await() 方法，代表让主线程开始等待，在等待之前的那几个子线程都执行完毕后，它才会认为所有人都跑完了比赛。这段程序的运行结果如下所示：等待5个运动员都跑完.....4号运动员完成了比赛。3号运动员完成了比赛。1号运动员完成了比赛。5号运动员完成了比赛。2号运动员完成了比赛。所有人都跑完了，比赛结束。可以看出，直到 5 个运动员都完成了比赛之后，主线程才会继续，而且由于子线程等待的时间是随机的，所以各个运动员完成比赛的次序也是随机的。用法二：多个线程等待某一个线程的信号，同时开始执行这和第一个用法有点相反，我们再列举一个实际的场景，比如在运动会上，刚才说的是裁判员等运动员，现在是运动员等裁判员。在运动员起跑之前都会等待裁判员发号施令，一声令下运动员统一起跑，我们用代码把这件事情描述出来，如下所示：public class RunDemo2 { public static void main(String[] args) throws InterruptedException { System.out.println(&quot;运动员有5秒的准备时间&quot;); CountDownLatch countDownLatch = new CountDownLatch(1); ExecutorService service = Executors.newFixedThreadPool(5); for (int i = 0; i &amp;lt; 5; i++) { final int no = i + 1; Runnable runnable = new Runnable() { @Override public void run() { System.out.println(no + &quot;号运动员准备完毕，等待裁判员的发令枪&quot;); try { countDownLatch.await(); System.out.println(no + &quot;号运动员开始跑步了&quot;); } catch (InterruptedException e) { e.printStackTrace(); } } }; service.submit(runnable); } Thread.sleep(5000); System.out.println(&quot;5秒准备时间已过，发令枪响，比赛开始！&quot;); countDownLatch.countDown(); }}在这段代码中，首先打印出了运动员有 5 秒的准备时间，然后新建了一个 CountDownLatch，其倒数值只有 1；接着，同样是一个 5 线程的线程池，并且用 for 循环的方式往里提交 5 个任务，而这 5 个任务在一开始时就让它调用 await() 方法开始等待。再回到主线程。主线程会首先等待 5 秒钟，这意味着裁判员正在做准备工作，比如他会喊“各就各位，预备”这样的话语；然后 5 秒之后，主线程会打印出“5 秒钟准备时间已过，发令枪响，比赛开始”的信号，紧接着会调用 countDown 方法，一旦主线程调用了该方法，那么之前那 5 个已经调用了 await() 方法的线程都会被唤醒，所以这段程序的运行结果如下：运动员有5秒的准备时间2 号运动员准备完毕，等待裁判员的发令枪1 号运动员准备完毕，等待裁判员的发令枪3 号运动员准备完毕，等待裁判员的发令枪4 号运动员准备完毕，等待裁判员的发令枪5 号运动员准备完毕，等待裁判员的发令枪5 秒准备时间已过，发令枪响，比赛开始！2 号运动员开始跑步了1 号运动员开始跑步了5 号运动员开始跑步了4 号运动员开始跑步了3 号运动员开始跑步了可以看到，运动员首先会有 5 秒钟的准备时间，然后 5 个运动员分别都准备完毕了，等待发令枪响，紧接着 5 秒之后，发令枪响，比赛开始，于是 5 个子线程几乎同时开始跑步了。注意点CountDownLatch 的注意点： 两种用法，这两种用法并不是孤立的，甚至可以把这两种用法结合起来，比如利用两个 CountDownLatch，第一个初始值为多个，第二个初始值为 1，这样就可以应对更复杂的业务场景了； CountDownLatch 是不能够重用的，比如已经完成了倒数，那可不可以在下一次继续去重新倒数呢？这是做不到的，如果你有这个需求的话，可以考虑使用 CyclicBarrier 或者创建一个新的 CountDownLatch 实例。总结CountDownLatch 类在创建实例的时候，需要在构造函数中传入倒数次数，然后由需要等待的线程去调用 await 方法开始等待，而每一次其他线程调用了 countDown 方法之后，计数便会减 1，直到减为 0 时，之前等待的线程便会继续运行。 countdownlatch和信号量有一丢丢像。一个上来先等待，满足条件再放行，另一个先放行，满足条件再等待。不过前者无法复用 CountDownLatch 到底是如何编排线程执行顺序的呢？我个人理解 CountDownLatch 其实没有控制线程执行顺序，只是当 count 到 0 时去notify其他wait的线程 - 是的，“安排线程执行顺序”指的就是先让一部分线程等待，倒数结束时，再被唤醒放开的意思，这是一种执行顺序上的配合。" }, { "title": "Semaphore 信号量", "url": "/posts/FixedThreadPool-signal/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-08 15:33:00 +0000", "snippet": "控制并发流程的工具类，作用就是更容易地让线程之间相互配合，比如让线程 A 等待线程 B 执行完毕后再继续执行，来满足业务逻辑。本课时我们从 Semaphore（信号量）开始介绍。从图中可以看出，信号量的一个最主要的作用就是，来控制那些需要限制并发访问量的资源。具体来讲，信号量会维护“许可证”的计数，而线程去访问共享资源前，必须先拿到许可证。线程可以从信号量中去“获取”一个许可证，一旦线程获取之后，信号量持有的许可证就转移过去了，所以信号量手中剩余的许可证要减一。同理，线程也可以“释放”一个许可证，如果线程释放了许可证，这个许可证相当于被归还给信号量了，于是信号量中的许可证的可用数量加一。当信号量拥有的许可证数量减到 0 时，如果下个线程还想要获得许可证，那么这个线程就必须等待，直到之前得到许可证的线程释放，它才能获取。由于线程在没有获取到许可证之前不能进一步去访问被保护的共享资源，所以这就控制了资源的并发访问量，这就是整体思路。应用实例、使用场景背景在这个场景中，我们的服务是中间这个方块儿，左侧是请求，右侧是我们所依赖的那个慢服务。出于种种原因（比如计算量大、依赖的下游服务多等），右边的慢服务速度很慢，并且它可以承受的请求数量也很有限，一旦有太多的请求同时到达它这边，可能会导致它这个服务不可用，会压垮它。所以我们必须要保护它，不能让太多的线程同时去访问。那怎么才能做到这件事情呢？在讲解怎么做到这个事情之前，我们先来看一看，在通常的场景下，我们用一个普通线程池能不能做到这件事情。public class SemaphoreDemo1 { public static void main(String[] args) { ExecutorService service = Executors.newFixedThreadPool(50); for (int i = 0; i &amp;lt; 1000; i++) { service.submit(new Task()); } service.shutdown(); } static class Task implements Runnable { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot;调用了慢服务&quot;); try { // 模拟慢服务 Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } }}在这段代码中，有一个固定 50 个线程的线程池，然后给线程池提交 1000 个任务，并且每一个任务所执行的内容，就是去休眠 3 秒钟，来模拟调用这个慢服务的过程。我们启动这个程序，会发现打印出来的结果如下所示：pool-1-thread-2调用了慢服务pool-1-thread-4调用了慢服务pool-1-thread-3调用了慢服务pool-1-thread-1调用了慢服务pool-1-thread-5调用了慢服务pool-1-thread-6调用了慢服务...（包含了pool-1-thread-1到pool-1-thread-50这50个线程）它会从线程 1 一直到线程 50 都去调用这个慢服务，当然实际调用顺序每次都会不一样，但是这 50 个线程都会去几乎同时调用这个慢服务，在这种情况下，就会导致我们的慢服务崩溃。所以，必须严格限制能够同时到达该服务的请求数。比如，我们想限制同时不超过 3 个请求来访问该服务，该怎么实现呢？并且这里有一点值得注意，我们的前提条件是，线程池中确实有 50 个线程，线程数肯定超过了 3 个，那么怎么进一步控制这么多的线程不同时访问慢服务呢？我们可以通过信号量来解决这个问题。正常情况下获取许可证这张图的方框代表一个许可证为 3 的信号量，每一个绿色的长条代表一个许可证（permit）。现在我们拥有 3 个许可证，并且信号量的特点是非常“慷慨”，只要它持有许可证，别人想请求的话它都会分发的。假设此时 Thread 1 来请求了，在这种情况下，信号量就会把一个许可证给到这边的第一个线程 Thread 1。于是 Thread 1 获得了许可证，变成了下图这个样子：Thread 1 拿到许可证之后就拥有了访问慢服务的资格，它紧接着就会去访问我们的慢服务，同时，我们的信号量手中持有的许可证也减为了 2。假设这个慢服务速度很慢，可能长时间内不返回，所以在没返回之前，Thread 1 也会不释放许可证，在此期间第二个线程又来请求了：同理，此时由于信号量手中持有两个许可证，还是可以满足 Thread 2 的需求的，所以就把第二个许可证给了第二个线程。这样一来，第二个线程也拿到了我们的许可证，可以访问右边的慢服务了，如图所示：同理，在前两个线程返回前，第三个线程也过来了，也是按照同样的方式获得了许可证，并且访问慢服务：没许可证时，会阻塞前来请求的线程至此，我们信号量中的许可证已经没有了，因为原有的 3 个都分给这 3 个线程了。在这种情况下，信号量就可以进一步发挥作用了，此时假设第 4 个线程再来请求找我们信号量拿许可证，由于此时线程 1、线程 2、线程 3 都正在访问“慢服务”，还没归还许可证，而信号量自身也没有更多的许可证了，所以在这个时候就会发生这样的一种情况：线程 4 在找我们用 acquire 方法请求许可证的时候，它会被阻塞，意味着线程 4 没有拿到许可证，也就没有被允许访问“慢服务”，也就是说此时“慢服务”依然只能被前面的 3 个线程访问，这样就达到我们最开始的目的了：限制同时最多有 3 个线程调用我们的慢服务。有线程释放信号量后假设此时线程 1 因为最早去的，它执行完了这个任务，于是返回了。返回的时候它会调用 release 方法，表示“我处理完了我的任务，我想把许可证还回去”，所以，此时线程 1 就释放了之前持有的许可证，把它还给了我们的信号量，于是信号量所持有的许可证数量从 0 又变回了 1，如图所示：此时由于许可证已经归还给了信号量，那么刚才找我们要许可证的线程 4 就可以顺利地拿到刚刚释放的这个许可证了。于是线程 4 也就拥有了访问慢服务的访问权，接下来它也会去访问这个慢服务。不过要注意，此时线程 1 先归还了许可证给信号量，再由信号量把这个许可证转给线程 4，所以，此时同时访问慢服务的依然只有 3 个线程，分别是线程 2、3 和 4，因为之前的线程 1 已经完成任务并且离开了。不过此时访问慢服务的就变成了线程 4、5、6，可以看出，总的数量从来没有超过 3 个。在这个例子中，线程 4 一开始获取许可证的时候被阻塞了，那个时候即使有线程 5 和线程 6 甚至线程 100 都来执行 acquire 方法的话，信号量也会把这些通通给阻塞住，这样就起到了信号量最主要的控制并发量的作用。用法使用流程讲完了场景之后，我们来看一下具体的用法，使用流程主要分为以下三步。首先初始化一个信号量，并且传入许可证的数量，这是它的带公平参数的构造函数：public Semaphore(int permits, boolean fair)，传入两个参数，第一个参数是许可证的数量，另一个参数是是否公平。如果第二个参数传入 true，则代表它是公平的策略，会把之前已经等待的线程放入到队列中，而当有新的许可证到来时，它会把这个许可证按照顺序发放给之前正在等待的线程；如果这个构造函数第二个参数传入 false，则代表非公平策略，也就有可能插队，就是说后进行请求的线程有可能先得到许可证。第二个流程是在建立完这个构造函数，初始化信号量之后，我们就可以利用 acquire() 方法。在调用慢服务之前，让线程来调用 acquire 方法或者 acquireUninterruptibly方法，这两个方法的作用是要获取许可证，这同时意味着只有这个方法能顺利执行下去的话，它才能进一步访问这个代码后面的调用慢服务的方法。如果此时信号量已经没有剩余的许可证了，那么线程就会等在 acquire 方法的这一行代码中，所以它也不会进一步执行下面调用慢服务的方法。我们正是用这种方法，保护了我们的慢服务。acquire() 和 acquireUninterruptibly() 的区别是：是否能响应中断。acquire() 是可以支持中断的，也就是说，它在获取信号量的期间，假设这个线程被中断了，那么它就会跳出 acquire() 方法，不再继续尝试获取了。而 acquireUninterruptibly() 方法是不会被中断的。第三步就是在任务执行完毕之后，调用 release() 来释放许可证，比如说我们在执行完慢服务这行代码之后，再去执行 release() 方法，这样一来，许可证就会还给我们的信号量了。其他主要方法介绍除了这几个主要方法以外，还有一些其他的方法。（1）public boolean tryAcquire()tryAcquire 和之前介绍锁的 trylock 思维是一致的，是尝试获取许可证，相当于看看现在有没有空闲的许可证，如果有就获取，如果现在获取不到也没关系，不必陷入阻塞，可以去做别的事。（2）public boolean tryAcquire(long timeout, TimeUnit unit)同样有一个重载的方法，它里面传入了超时时间。比如传入了 3 秒钟，则意味着最多等待 3 秒钟，如果等待期间获取到了许可证，则往下继续执行；如果超时时间到，依然获取不到许可证，它就认为获取失败，且返回 false。（3）availablePermits()这个方法用来查询可用许可证的数量，返回一个整型的结果。示例代码下面我们来看一段示例代码：public class SemaphoreDemo2 { static Semaphore semaphore = new Semaphore(3); public static void main(String[] args) { ExecutorService service = Executors.newFixedThreadPool(50); for (int i = 0; i &amp;lt; 1000; i++) { service.submit(new Task()); } service.shutdown(); } static class Task implements Runnable { @Override public void run() { try { semaphore.acquire(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot;拿到了许可证，花费2秒执行慢服务&quot;); try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;慢服务执行完毕，&quot; + Thread.currentThread().getName() + &quot;释放了许可证&quot;); semaphore.release(); } }}在这段代码中我们新建了一个数量为 3 的信号量，然后又有一个和之前一样的固定 50 线程的线程池，并且往里面放入 1000 个任务。每个任务在执行模拟慢服务之前，会先用信号量的 acquire 方法获取到信号量，然后再去执行这 2 秒钟的慢服务，最后利用 release() 方法来释放许可证。代码执行结果如下：pool-1-thread-1 拿到了许可证，花费 2 秒执行慢服务pool-1-thread-2 拿到了许可证，花费 2 秒执行慢服务pool-1-thread-3 拿到了许可证，花费 2 秒执行慢服务慢服务执行完毕，pool-1-thread-1 释放了许可证慢服务执行完毕，pool-1-thread-2 释放了许可证慢服务执行完毕，pool-1-thread-3 释放了许可证pool-1-thread-4 拿到了许可证，花费 2秒执行慢服务pool-1-thread-5 拿到了许可证，花费 2 秒执行慢服务pool-1-thread-6 拿到了许可证，花费 2 秒执行慢服务慢服务执行完毕，pool-1-thread-4 释放了许可证慢服务执行完毕，pool-1-thread-5 释放了许可证慢服务执行完毕，pool-1-thread-6 释放了许可证...它会先让线程 1、2、3 拿到许可证，然后分别去执行这 2 秒钟的慢服务，直到执行完毕则会释放许可证，后面的线程才能进一步拿到许可证来执行服务。当前面 3 个线程还没有执行完毕，也就是还没有释放许可证的时候，后面的线程其实已经来请求了，它们也会尝试调用 acquire 方法，只不过这个时候会被阻塞住。通过执行结果可以看出，同时最多只有 3 个线程可以访问我们的慢服务。特殊用法：一次性获取或释放多个许可证信号量的一种特殊用法，那就是它可以一次性释放或者获取多个许可证。比如 semaphore.acquire(2)，里面传入参数 2，这就叫一次性获取两个许可证。同时释放也是一样的，semaphore.release(3) 相当于一次性释放三个许可证。为什么要这样做呢？我们列举一个使用场景。比如说第一个任务 A（Task A ）会调用很耗资源的方法一 method1()，而任务 B 调用的是方法二 method 2，但这个方法不是特别消耗资源。在这种情况下，假设我们一共有 5 个许可证，只能允许同时有 1 个线程调用方法一，或者同时最多有 5 个线程调用方法二，但是方法一和方法二不能同时被调用。所以，我们就要求 Task A 在执行之前要一次性获取到 5 个许可证才能执行，而 Task B 只需要获取一个许可证就可以执行了。这样就避免了任务 A 和 B 同时运行，同时又很好的兼顾了效率，不至于同时只允许一个线程访问方法二，那样的话也存在浪费资源的情况，所以这就相当于我们可以根据自己的需求合理地利用信号量的许可证来分配资源。注意点信号量还有几个注意点： 获取和释放的许可证数量尽量保持一致，否则比如每次都获取 2 个但只释放 1 个甚至不释放，那么信号量中的许可证就慢慢被消耗完了，最后导致里面没有许可证了，那其他的线程就再也没办法访问了； 在初始化的时候可以设置公平性，如果设置为 true 则会让它更公平，但如果设置为 false 则会让总的吞吐量更高。 信号量是支持跨线程、跨线程池的，而且并不是哪个线程获得的许可证，就必须由这个线程去释放。事实上，对于获取和释放许可证的线程是没有要求的，比如线程 A 获取了然后由线程 B 释放，这完全是可以的，只要逻辑合理即可。信号量能被 FixedThreadPool 替代吗？信号量能不能被 FixedThreadPool 代替呢？这个问题相当于，信号量是可以限制同时访问的线程数，那为什么不直接用固定数量线程池去限制呢？这样不是更方便吗？比如说线程池里面有 3 个线程，那自然最多只有 3 个线程去访问了。这是一个很好的问题，在实际业务中会遇到这样的情况：假如，在调用慢服务之前需要有个判断条件，比如只想在每天的零点附近去访问这个慢服务时受到最大线程数的限制（比如 3 个线程），而在除了每天零点附近的其他大部分时间，是希望让更多的线程去访问的。所以在这种情况下就应该把线程池的线程数量设置为 50 ，甚至更多，然后在执行之前加一个 if 判断，如果符合时间限制了（比如零点附近），再用信号量去额外限制，这样做是比较合理的。再比如说在大型应用程序中会有不同类型的任务，它们也是通过不同的线程池来调用慢服务的。因为调用方不只是一处，可能是 Tomcat 服务器或者网关，就不应该限制，或者说也无法做到限制它们的线程池的大小。但可以做的是，在执行任务之前用信号量去限制一下同时访问的数量，因为我们的信号量具有跨线程、跨线程池的特性，所以即便这些请求来自于不同的线程池，也可以限制它们的访问。如果用 FixedThreadPool 去限制，那就做不到跨线程池限制了，这样的话会让功能大大削弱。基于以上的理由，如果想要限制并发访问的线程数，用信号量是更合适的。 信号量的实际实现原理是一个计数器，一个等待队列，acquire是计数器减，release是计数器加，当减到0时再请求会进入等待队列阻塞。所以跨线程跨线程池是完全没问题的，只要操作的是同一个信号量对象！ 信号量有点类似于限流手段中的令牌桶算法 为什么获取许可证的线程和释放许可证的线程可以允许不是同一个线程呢？ 线程 A 未获取许可证却归还许可证很怪 信号量是用在慢服务中的业务代码中去控制 当前同时访问慢服务的 线程个数（请求数）。例如 微服务中的服务调用方， 服务提供方， 则信号量可用在服务提供方的业务代码中去实现一个限流处理？" }, { "title": "利用 CompletableFuture 实现“旅游平台”问题", "url": "/posts/CompletableFuture-example/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-07 05:33:00 +0000", "snippet": "如果想要搭建一个旅游平台，经常会有这样的需求：用户想同时获取多家航空公司的航班信息。比如，从北京到上海的机票钱是多少？有很多家航空公司都有这样的航班信息，所以应该把所有航空公司的航班、票价等信息都获取到，然后再聚合。由于每个航空公司都有自己的服务器，所以分别去请求它们的服务器就可以了，比如请求国航、海航、东航等，如下图所示：串行一种比较简单原始的方式是：用串行的方式来解决这个问题。 假设想获取价格，要先去访问国航，在这里叫作 website 1； 然后再去访问海航 website 2，以此类推。 当每一个请求发出去之后，等它响应回来以后，才能去请求下一个网站，这就是串行的方式。这样做的效率非常低下，比如航空公司比较多，假设每个航空公司都需要 1 秒钟的话，用户肯定等不及，所以这种方式是不可取的。并行对刚才的思路进行改进，最主要的思路就是把串行改成并行，如下图所示： 并行获取这些机票信息； 然后再把机票信息给聚合起来，这样的话，效率会成倍的提高。这种并行虽然提高了效率，但也有一个缺点，那就是会“一直等到所有请求都返回”。如果有一个网站特别慢，那么你不应该被那个网站拖累，比如说某个网站打开需要二十秒，那肯定是等不了这么长时间的，所以需要一个功能，就是有超时的获取。有超时的并行获取如上图所示，就属于有超时的并行获取，同样也在并行的去请求各个网站信息。但是规定了一个时间的超时，比如 3 秒钟，那么到 3 秒钟的时候如果都已经返回了那当然最好，把它们收集起来即可；但是如果还有些网站没能及时返回，我们就把这些请求给忽略掉，这样一来用户体验就比较好了，它最多只需要等固定的 3 秒钟就能拿到信息，虽然拿到的可能不是最全的，但是总比一直等更好。想要实现这个目标有几种实现方案，我们一个一个的来看看。线程池的实现第一个实现方案是用线程池，我们来看一下代码。public class ThreadPoolDemo { ExecutorService threadPool = Executors.newFixedThreadPool(3); public static void main(String[] args) throws InterruptedException { ThreadPoolDemo threadPoolDemo = new ThreadPoolDemo(); System.out.println(threadPoolDemo.getPrices()); } private Set&amp;lt;Integer&amp;gt; getPrices() throws InterruptedException { Set&amp;lt;Integer&amp;gt; prices = Collections.synchronizedSet(new HashSet&amp;lt;Integer&amp;gt;()); threadPool.submit(new Task(123, prices)); threadPool.submit(new Task(456, prices)); threadPool.submit(new Task(789, prices)); Thread.sleep(3000); return prices; } private class Task implements Runnable { Integer productId; Set&amp;lt;Integer&amp;gt; prices; public Task(Integer productId, Set&amp;lt;Integer&amp;gt; prices) { this.productId = productId; this.prices = prices; } @Override public void run() { int price = 0; try { Thread.sleep((long) (Math.random() * 4000)); price= (int) (Math.random() * 4000); } catch (InterruptedException e) { e.printStackTrace(); } prices.add(price); } }}在代码中，新建了一个线程安全的 Set，它是用来存储各个价格信息的，把它命名为 Prices，然后往线程池中去放任务。线程池是在类的最开始时创建的，是一个固定 3 线程的线程池。而这个任务在下方的 Task 类中进行了描述，在这个 Task 中我们看到有 run 方法，在该方法里面，我们用一个随机的时间去模拟各个航空网站的响应时间，然后再去返回一个随机的价格来表示票价，最后把这个票价放到 Set 中。这就是我们 run 方法所做的事情。再回到 getPrices 函数中，我们新建了三个任务，productId 分别是 123、456、789，这里的 productId 并不重要，因为我们返回的价格是随机的，为了实现超时等待的功能，在这里调用了 Thread 的 sleep 方法来休眠 3 秒钟，这样做的话，它就会在这里等待 3 秒，之后直接返回 prices。此时，如果前面响应速度快的话，prices 里面最多会有三个值，但是如果每一个响应时间都很慢，那么可能 prices 里面一个值都没有。不论你有多少个，它都会在休眠结束之后，也就是执行完 Thread 的 sleep 之后直接把 prices 返回，并且最终在 main 函数中把这个结果给打印出来。我们来看一下可能的执行结果，一种可能性就是有 3 个值，即 [3815, 3609, 3819]（数字是随机的）；有可能是 1 个 [3496]、或 2 个 [1701, 2730]，如果每一个响应速度都特别慢，可能一个值都没有。这就是用线程池去实现的最基础的方案。CountDownLatch在这里会有一个优化的空间，比如说网络特别好时，每个航空公司响应速度都特别快，根本不需要等三秒，有的航空公司可能几百毫秒就返回了，那么我们也不应该让用户等 3 秒。所以需要进行一下这样的改进，看下面这段代码：public class CountDownLatchDemo { ExecutorService threadPool = Executors.newFixedThreadPool(3); public static void main(String[] args) throws InterruptedException { CountDownLatchDemo countDownLatchDemo = new CountDownLatchDemo(); System.out.println(countDownLatchDemo.getPrices()); } private Set&amp;lt;Integer&amp;gt; getPrices() throws InterruptedException { Set&amp;lt;Integer&amp;gt; prices = Collections.synchronizedSet(new HashSet&amp;lt;Integer&amp;gt;()); CountDownLatch countDownLatch = new CountDownLatch(3); threadPool.submit(new Task(123, prices, countDownLatch)); threadPool.submit(new Task(456, prices, countDownLatch)); threadPool.submit(new Task(789, prices, countDownLatch)); countDownLatch.await(3, TimeUnit.SECONDS); return prices; } private class Task implements Runnable { Integer productId; Set&amp;lt;Integer&amp;gt; prices; CountDownLatch countDownLatch; public Task(Integer productId, Set&amp;lt;Integer&amp;gt; prices, CountDownLatch countDownLatch) { this.productId = productId; this.prices = prices; this.countDownLatch = countDownLatch; } @Override public void run() { int price = 0; try { Thread.sleep((long) (Math.random() * 4000)); price = (int) (Math.random() * 4000); } catch (InterruptedException e) { e.printStackTrace(); } prices.add(price); countDownLatch.countDown(); } }}这段代码使用 CountDownLatch 实现了这个功能，整体思路和之前是一致的，不同点在于我们新增了一个 CountDownLatch，并且把它传入到了 Task 中。在 Task 中，获取完机票信息并且把它添加到 Set 之后，会调用 countDown 方法，相当于把计数减 1。这样一来，在执行 countDownLatch.await(3,TimeUnit.SECONDS) 这个函数进行等待时，如果三个任务都非常快速地执行完毕了，那么三个线程都已经执行了 countDown 方法，那么这个 await 方法就会立刻返回，不需要傻等到 3 秒钟。如果有一个请求特别慢，相当于有一个线程没有执行 countDown 方法，来不及在 3 秒钟之内执行完毕，那么这个带超时参数的 await 方法也会在 3 秒钟到了以后，及时地放弃这一次等待，于是就把 prices 给返回了。所以这样一来，我们就利用 CountDownLatch 实现了这个需求，也就是说我们最多等 3 秒钟，但如果在 3 秒之内全都返回了，我们也可以快速地去返回，不会傻等，提高了效率。CompletableFuture我们再来看一下用 CompletableFuture 来实现这个功能的用法，代码如下所示：public class CompletableFutureDemo {    public static void main(String[] args)            throws Exception {        CompletableFutureDemo completableFutureDemo = new CompletableFutureDemo();        System.out.println(completableFutureDemo.getPrices());    }    private Set&amp;lt;Integer&amp;gt; getPrices() {        Set&amp;lt;Integer&amp;gt; prices = Collections.synchronizedSet(new HashSet&amp;lt;Integer&amp;gt;());        CompletableFuture&amp;lt;Void&amp;gt; task1 = CompletableFuture.runAsync(new Task(123, prices));        CompletableFuture&amp;lt;Void&amp;gt; task2 = CompletableFuture.runAsync(new Task(456, prices));        CompletableFuture&amp;lt;Void&amp;gt; task3 = CompletableFuture.runAsync(new Task(789, prices));        CompletableFuture&amp;lt;Void&amp;gt; allTasks = CompletableFuture.allOf(task1, task2, task3);        try {            allTasks.get(3, TimeUnit.SECONDS);        } catch (InterruptedException e) {        } catch (ExecutionException e) {        } catch (TimeoutException e) {        }        return prices;    }    private class Task implements Runnable {        Integer productId;        Set&amp;lt;Integer&amp;gt; prices;        public Task(Integer productId, Set&amp;lt;Integer&amp;gt; prices) {            this.productId = productId;            this.prices = prices;        }        @Override        public void run() {            int price = 0;            try {                Thread.sleep((long) (Math.random() * 4000));                price = (int) (Math.random() * 4000);            } catch (InterruptedException e) {                e.printStackTrace();            }            prices.add(price);        }    }}这里不再使用线程池了，在 getPrices 方法中，用了 CompletableFuture 的 runAsync 方法，这个方法会异步的去执行任务。有三个任务，并且在执行这个代码之后会分别返回一个 CompletableFuture 对象，把它们命名为 task 1、task 2、task 3，然后执行 CompletableFuture 的 allOf 方法，并且把 task 1、task 2、task 3 传入。这个方法的作用是把多个 task 汇总，然后可以根据需要去获取到传入参数的这些 task 的返回结果，或者等待它们都执行完毕等。就把这个返回值叫作 allTasks，并且在下面调用它的带超时时间的 get 方法，同时传入 3 秒钟的超时参数。这样一来它的效果就是，如果在 3 秒钟之内这 3 个任务都可以顺利返回，也就是这个任务包括的那三个任务，每一个都执行完毕的话，则这个 get 方法就可以及时正常返回，并且往下执行，相当于执行到 return prices。在下面的这个 Task 的 run 方法中，该方法如果执行完毕的话，对于 CompletableFuture 而言就意味着这个任务结束，它是以这个作为标记来判断任务是不是执行完毕的。但是如果有某一个任务没能来得及在 3 秒钟之内返回，那么这个带超时参数的 get 方法便会抛出 TimeoutException 异常，同样会被我们给 catch 住。这样一来它就实现了这样的效果：会尝试等待所有的任务完成，但是最多只会等 3 秒钟，在此之间，如及时完成则及时返回。那么所以我们利用 CompletableFuture，同样也可以解决旅游平台的问题。它的运行结果也和之前是一样的，有多种可能性。当然除了这几种实现方案之外，还会有其他的实现方案……" }, { "title": "Future 的注意点", "url": "/posts/future-notice/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-06 15:33:00 +0000", "snippet": "1. 当 for 循环批量获取 Future 的结果时容易 block，get 方法调用时应使用 timeout 限制对于 Future 而言，第一个注意点就是，当 for 循环批量获取 Future 的结果时容易 block，在调用 get 方法时，应该使用 timeout 来限制。下面我们具体看看这是一个什么情况。首先，假设一共有四个任务需要执行，我们都把它放到线程池中，然后它获取的时候是按照从 1 到 4 的顺序，也就是执行 get() 方法来获取的，代码如下所示：public class FutureDemo { public static void main(String[] args) { // 创建线程池 ExecutorService service = Executors.newFixedThreadPool(10); // 提交任务，并用 Future 接收返回结果 ArrayList&amp;lt;Future&amp;gt; allFutures = new ArrayList&amp;lt;&amp;gt;(); for (int i = 0; i &amp;lt; 4; i++) { Future&amp;lt;String&amp;gt; future; if (i == 0 || i == 1) { future = service.submit(new SlowTask()); } else { future = service.submit(new FastTask()); } allFutures.add(future); } for (int i = 0; i &amp;lt; 4; i++) { Future&amp;lt;String&amp;gt; future = allFutures.get(i); try { String result = future.get(); System.out.println(result); } catch (InterruptedException e) {  e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } } service.shutdown(); } static class SlowTask implements Callable&amp;lt;String&amp;gt; { @Override public String call() throws Exception { Thread.sleep(5000); return &quot;速度慢的任务&quot;; } } static class FastTask implements Callable&amp;lt;String&amp;gt; { @Override public String call() throws Exception { return &quot;速度快的任务&quot;; } }}可以看出，在代码中我们新建了线程池，并且用一个 list 来保存 4 个 Future。其中，前两个 Future 所对应的任务是慢任务，也就是代码下方的 SlowTask，而后两个 Future 对应的任务是快任务。慢任务在执行的时候需要 5 秒钟的时间才能执行完毕，而快任务很快就可以执行完毕，几乎不花费时间。在提交完这 4 个任务之后，我们用 for 循环对它们依次执行 get 方法，来获取它们的执行结果，然后再把这个结果打印出来。执行结果如下：速度慢的任务速度慢的任务速度快的任务速度快的任务这里有一个问题，即第三个的任务量是比较小的，它可以很快返回结果，紧接着第四个任务也会返回结果。但是由于前两个任务速度很慢，所以我们在利用 get 方法执行时，会卡在第一个任务上。也就是说，虽然此时第三个和第四个任务很早就得到结果了，但我们在此时使用这种 for 循环的方式去获取结果，依然无法及时获取到第三个和第四个任务的结果。直到 5 秒后，第一个任务出结果了，我们才能获取到，紧接着也可以获取到第二个任务的结果，然后才轮到第三、第四个任务。假设由于网络原因，第一个任务可能长达 1 分钟都没办法返回结果，那么这个时候，我们的主线程会一直卡着，影响了程序的运行效率。此时我们就可以用 Future 的带超时参数的 get(long timeout, TimeUnit unit) 方法来解决这个问题。这个方法的作用是，如果在限定的时间内没能返回结果的话，那么便会抛出一个 TimeoutException 异常，随后就可以把这个异常捕获住，或者是再往上抛出去，这样就不会一直卡着了。2. Future 的生命周期不能后退Future 的生命周期不能后退，一旦完成了任务，它就永久停在了“已完成”的状态，不能从头再来，也不能让一个已经完成计算的 Future 再次重新执行任务。这一点和线程、线程池的状态是一样的，线程和线程池的状态也是不能后退的。关于线程的状态和流转路径，第 03 讲已经讲过了，如图所示。这个图也是我们当时讲解所用的图，如果有些遗忘，可以回去复习一下当时的内容。这一讲，我推荐你采用看视频的方式，因为视频中会把各个路径都标明清楚，看起来会更加清晰。Future 产生新的线程了吗Future 是否产生新的线程了？有一种说法是，除了继承 Thread 类和实现 Runnable 接口之外，还有第三种产生新线程的方式，那就是采用 Callable 和 Future，这叫作有返回值的创建线程的方式。这种说法是不正确的。其实 Callable 和 Future 本身并不能产生新的线程，它们需要借助其他的比如 Thread 类或者线程池才能执行任务。例如，在把 Callable 提交到线程池后，真正执行 Callable 的其实还是线程池中的线程，而线程池中的线程是由 ThreadFactory 产生的，这里产生的新线程与 Callable、Future 都没有关系，所以 Future 并没有产生新的线程。 通过submit(Callable)的方法产生的任务会进入ExecutorService的阻塞队列吗，如果进入了那么取消的话是直接从队列移除吗？—— 是会进入的，取消不会直接移除，而是等轮到执行的时候再根据状态来执行不同的策略。" }, { "title": "Future", "url": "/posts/future-main-function/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-06 02:13:22 +0000", "snippet": "Future 的作用Future 最主要的作用是，比如当做一定运算的时候，运算过程可能比较耗时，有时会去查数据库，或是繁重的计算，比如压缩、加密等，在这种情况下，如果我们一直在原地等待方法返回，显然是不明智的，整体程序的运行效率会大大降低。我们可以把运算的过程放到子线程去执行，再通过 Future 去控制子线程执行的计算过程，最后获取到计算结果。这样一来就可以把整个程序的运行效率提高，是一种异步的思想。Callable 和 Future 的关系接下来我们介绍下 Callable 和 Future 的关系，前面讲过，Callable 接口相比于 Runnable 的一大优势是可以有返回结果，那这个返回结果怎么获取呢？就可以用 Future 类的 get 方法来获取 。因此，Future 相当于一个存储器，它存储了 Callable 的 call 方法的任务结果。除此之外，我们还可以通过 Future 的 isDone 方法来判断任务是否已经执行完毕了，还可以通过 cancel 方法取消这个任务，或限时获取任务的结果等，总之 Future 的功能比较丰富。有了这样一个从宏观上的概念之后，我们就来具体看一下 Future 类的主要方法。Future 的方法和用法首先看一下 Future 接口的代码，一共有 5 个方法，代码如下所示：public interface Future&amp;lt;V&amp;gt; { boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;}其中，第 5 个方法是对第 4 个方法的重载，方法名一样，但是参数不一样。get() 方法：获取结果get 方法最主要的作用就是获取任务执行的结果，该方法在执行时的行为取决于 Callable 任务的状态，可能会发生以下 5 种情况： 最常见的就是当执行 get 的时候，任务已经执行完毕了，可以立刻返回，获取到任务执行的结果； 任务还没有结果，这是有可能的，比如我们往线程池中放一个任务，线程池中可能积压了很多任务，还没轮到我去执行的时候，就去 get 了，在这种情况下，相当于任务还没开始；还有一种情况是任务正在执行中，但是执行过程比较长，所以我去 get 的时候，它依然在执行的过程中。无论是任务还没开始或在进行中，我们去调用 get 的时候，都会把当前的线程阻塞，直到任务完成再把结果返回回来； 任务执行过程中抛出异常，一旦这样，我们再去调用 get 的时候，就会抛出 ExecutionException 异常，不管我们执行 call 方法时里面抛出的异常类型是什么，在执行 get 方法时所获得的异常都是 ExecutionException； 任务被取消了，如果任务被取消，我们用 get 方法去获取结果时则会抛出 CancellationException； 任务超时，我们知道 get 方法有一个重载方法，那就是带延迟参数的，调用了这个带延迟参数的 get 方法之后，如果 call 方法在规定时间内正常顺利完成了任务，那么 get 会正常返回；但是如果到达了指定时间依然没有完成任务，get 方法则会抛出 TimeoutException，代表超时了。下面用图的形式让过程更清晰：在图中，右侧是一个线程池，线程池中有一些线程来执行任务。重点在图的左侧，可以看到有一个 submit 方法，该方法往线程池中提交了一个 Task，这个 Task 实现了 Callable 接口，当我们去给线程池提交这个任务的时候，调用 submit 方法会立刻返回一个 Future 类型的对象，这个对象目前内容是空的，其中还不包含计算结果，因为此时计算还没有完成。当计算一旦完成时，也就是当我们可以获取结果的时候，线程池便会把这个结果填入到之前返回的 Future 中去（也就是 f 对象），而不是在此时新建一个新的 Future。这时就可以利用 Future 的 get 方法来获取到任务的执行结果了。我们来看一个代码示例：/** * 描述：演示一个 Future 的使用方法 */public class OneFuture { public static void main(String[] args) { ExecutorService service = Executors.newFixedThreadPool(10); Future&amp;lt;Integer&amp;gt; future = service.submit(new CallableTask()); try { System.out.println(future.get()); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } service.shutdown(); static class CallableTask implements Callable&amp;lt;Integer&amp;gt; { @Override public Integer call() throws Exception { Thread.sleep(3000); return new Random().nextInt(); } } }}在这段代码中，main 方法新建了一个 10 个线程的线程池，并且用 submit 方法把一个任务提交进去。这个任务如代码的最下方所示，它实现了 Callable 接口，它所做的内容就是先休眠三秒钟，然后返回一个随机数。接下来我们就直接把 future.get 结果打印出来，其结果是正常打印出一个随机数，比如 100192 等。这段代码对应了我们刚才那个图示的讲解，这也是 Future 最常用的一种用法。isDone() 方法：判断是否执行完毕下面我们再接着看看 Future 的一些其他方法，比如说 isDone() 方法，该方法是用来判断当前这个任务是否执行完毕了。需要注意的是，这个方法如果返回 true 则代表执行完成了；如果返回 false 则代表还没完成。但这里如果返回 true，并不代表这个任务是成功执行的，比如说任务执行到一半抛出了异常。那么在这种情况下，对于这个 isDone 方法而言，它其实也是会返回 true 的，因为对它来说，虽然有异常发生了，但是这个任务在未来也不会再被执行，它确实已经执行完毕了。所以 isDone 方法在返回 true 的时候，不代表这个任务是成功执行的，只代表它执行完毕了。我们用一个代码示例来看一看，代码如下所示：public class GetException {    public static void main(String[] args) {   ExecutorService service = Executors.newFixedThreadPool(20);   Future&amp;lt;Integer&amp;gt; future = service.submit(new CallableTask());   try {  for (int i = 0; i &amp;lt; 5; i++) { System.out.println(i); Thread.sleep(500);  }  System.out.println(future.isDone());  future.get();   } catch (InterruptedException e) {  e.printStackTrace();   } catch (ExecutionException e) {  e.printStackTrace();   }    }    static class CallableTask implements Callable&amp;lt;Integer&amp;gt; {   @Override   public Integer call() throws Exception {  throw new IllegalArgumentException(&quot;Callable抛出异常&quot;);   }    }}在这段代码中，可以看到有一个线程池，并且往线程池中去提交任务，这个任务会直接抛出一个异常。那么接下来我们就用一个 for 循环去休眠，同时让它慢慢打印出 0 ~ 4 这 5 个数字，这样做的目的是起到了一定的延迟作用。在这个执行完毕之后，再去调用 isDone() 方法，并且把这个结果打印出来，然后再去调用 future.get()。这段代码的执行结果是这样的：01234truejava.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Callable抛出异常...这里要注意，我们知道这个异常实际上是在任务刚被执行的时候就抛出了，因为我们的计算任务中是没有其他逻辑的，只有抛出异常。我们再来看，控制台是什么时候打印出异常的呢？它是在 true 打印完毕后才打印出异常信息的，也就是说，在调用 get 方法时打印出的异常。这段代码证明了三件事情：第一件事情，即便任务抛出异常，isDone 方法依然会返回 true；第二件事情，虽然抛出的异常是 IllegalArgumentException，但是对于 get 而言，它抛出的异常依然是 ExecutionException；第三个事情，虽然在任务执行一开始时就抛出了异常，但是真正要等到我们执行 get 的时候，才看到了异常。cancel 方法：取消任务的执行如果不想执行某个任务了，则可以使用 cancel 方法，会有以下三种情况： 第一种情况最简单，那就是当任务还没有开始执行时，一旦调用 cancel，这个任务就会被正常取消，未来也不会被执行，那么 cancel 方法返回 true； 第二种情况也比较简单。如果任务已经完成，或者之前已经被取消过了，那么执行 cancel 方法则代表取消失败，返回 false。因为任务无论是已完成还是已经被取消过了，都不能再被取消了； 第三种情况比较特殊，就是这个任务正在执行，这个时候执行 cancel 方法是不会直接取消这个任务的，而是会根据我们传入的参数做判断。cancel 方法是必须传入一个参数，该参数叫作 mayInterruptIfRunning，它是什么含义呢？如果传入的参数是 true，执行任务的线程就会收到一个中断的信号，正在执行的任务可能会有一些处理中断的逻辑，进而停止，这个比较好理解。如果传入的是 false 则就代表不中断正在运行的任务，也就是说，本次 cancel 不会有任何效果，同时 cancel 方法会返回 false。那么如何选择传入 true 还是 false 呢？传入 true 适用的情况是，明确知道这个任务能够处理中断。传入 false 适用于什么情况呢？ 如果我们明确知道这个线程不能处理中断，那应该传入 false。 我们不知道这个任务是否支持取消（是否能响应中断），因为在大多数情况下代码是多人协作的，对于这个任务是否支持中断，不一定有十足的把握，那么在这种情况下也应该传入 false。 如果这个任务一旦开始运行，我们就希望它完全的执行完毕。在这种情况下，也应该传入 false。这就是传入 true 和 false 的不同含义和选择方法。isCancelled() 方法：判断是否被取消最后一个方法是 isCancelled 方法，判断是否被取消，它和 cancel 方法配合使用，比较简单。以上就是关于 Future 的主要方法的介绍了。用 FutureTask 来创建 Future除了用线程池的 submit 方法会返回一个 future 对象之外，同样还可以用 FutureTask 来获取 Future 类和任务的结果。FutureTask 首先是一个任务（Task），然后具有 Future 接口的语义，因为它可以在将来（Future）得到执行的结果。FutureTask 的代码实现：public class FutureTask&amp;lt;V&amp;gt; implements RunnableFuture&amp;lt;V&amp;gt;{ ...}可以看到，它实现了一个接口，这个接口叫作 RunnableFuture。我们再来看一下 RunnableFuture 接口的代码实现：public interface RunnableFuture&amp;lt;V&amp;gt; extends Runnable, Future&amp;lt;V&amp;gt; { void run();}可以看出，它是 extends Runnable 和 Future 这两个接口的，它们的关系如下图所示：既然 RunnableFuture 继承了 Runnable 接口和 Future 接口，而 FutureTask 又实现了 RunnableFuture 接口，所以 FutureTask 既可以作为 Runnable 被线程执行，又可以作为 Future 得到 Callable 的返回值。典型用法是，把 Callable 实例当作 FutureTask 构造函数的参数，生成 FutureTask 的对象，然后把这个对象当作一个 Runnable 对象，放到线程池中或另起线程去执行，最后还可以通过 FutureTask 获取任务执行的结果。代码如下：/** * 描述：演示 FutureTask 的用法 */public class FutureTaskDemo { public static void main(String[] args) { Task task = new Task(); FutureTask&amp;lt;Integer&amp;gt; integerFutureTask = new FutureTask&amp;lt;&amp;gt;(task); new Thread(integerFutureTask).start(); try { System.out.println(&quot;task运行结果：&quot;+integerFutureTask.get()); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } }}class Task implements Callable&amp;lt;Integer&amp;gt; { @Override public Integer call() throws Exception { System.out.println(&quot;子线程正在计算&quot;); int sum = 0; for (int i = 0; i &amp;lt; 100; i++) { sum += i; } return sum; }}在这段代码中可以看出，首先创建了一个实现了 Callable 接口的 Task，然后把这个 Task 实例传入到 FutureTask 的构造函数中去，创建了一个 FutureTask 实例，并且把这个实例当作一个 Runnable 放到 new Thread() 中去执行，最后再用 FutureTask 的 get 得到结果，并打印出来。执行结果是 4950，正是任务里 0+1+2+…+99 的结果。 有个疑问，既然希望全部执行完，那为啥还要取消？ —— 复杂业务场景中，取消的需求是会经常发生的，比如取消订单，取消预约，都有的。" }, { "title": "Callable？Runnable 的缺陷", "url": "/posts/Callable-Runnable/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-04 07:33:00 +0000", "snippet": "先来看一下，为什么需要 Callable？要想回答这个问题，我们先来看看现有的 Runnable 有哪些缺陷？不能返回一个返回值第一个缺陷，对于 Runnable 而言，它不能返回一个返回值，虽然可以利用其他的一些办法，比如在 Runnable 方法中写入日志文件或者修改某个共享的对象的办法，来达到保存线程执行结果的目的，但这种解决问题的行为千曲百折，属于曲线救国，效率着实不高。实际上，在很多情况下执行一个子线程时，我们都希望能得到执行的任务的结果，也就是说，我们是需要得到返回值的，比如请求网络、查询数据库等。可是 Runnable 不能返回一个返回值，这是它第一个非常严重的缺陷。不能抛出 checked Exception第二个缺陷就是不能抛出 checked Exception，如下面这段代码所示：public class RunThrowException {    /**    * 普通方法内可以 throw 异常，并在方法签名上声明 throws    */   public void normalMethod() throws Exception {       throw new IOException();   }    Runnable runnable = new Runnable() {       /**        *  run方法上无法声明 throws 异常，且run方法内无法 throw 出 checked Exception，除非使用try catch进行处理        */       @Override       public void run() {           try {               throw new IOException();           } catch (IOException e) {               e.printStackTrace();           }       }   }}在这段代码中，有两个方法，第一个方法是一个普通的方法，叫作 normalMethod，可以看到，在它的方法签名中有 throws Exception，并且在它的方法内也 throw 了一个 new IOException()。然后在下面的的代码中，我们新建了一个 Runnable 对象，同时重写了它的 run 方法，我们没有办法在这个 run 方法的方法签名上声明 throws 一个异常出来。同时，在这个 run 方法里面也没办法 throw 一个 checked Exception，除非如代码所示，用 try catch 包裹起来，但是如果不用 try catch 是做不到的。这就是对于 Runnable 而言的两个重大缺陷。为什么有这样的缺陷为什么有这样的缺陷呢？我们来看一下 Runnable 接口的定义：public interface Runnable {   public abstract void run();}代码比较短小，Runnable 是一个 interface，并且里面只有一个方法，叫作 public abstract void run()。这个方法已经规定了 run() 方法的返回类型是 void，而且这个方法没有声明抛出任何异常。所以，当实现并重写这个方法时，我们既不能改返回值类型，也不能更改对于异常抛出的描述，因为在实现方法的时候，语法规定是不允许对这些内容进行修改的。回顾课程之前小节的众多代码，从来没有出现过可以在 run 方法中返回一个返回值这样的情况。Runnable 为什么设计成这样我们再深入思考一层，为什么 Java 要把它设计成这个样子呢？假设 run() 方法可以返回返回值，或者可以抛出异常，也无济于事，因为我们并没有办法在外层捕获并处理，这是因为调用 run() 方法的类（比如 Thread 类和线程池）是 Java 直接提供的，而不是我们编写的。所以就算它能有一个返回值，我们也很难把这个返回值利用到，如果真的想弥补 Runnable 的这两个缺陷，可以用下面的补救措施——使用 Callable。Callable 接口Callable 是一个类似于 Runnable 的接口，实现 Callable 接口的类和实现 Runnable 接口的类都是可以被其他线程执行的任务。 我们看一下 Callable 的源码：public interface Callable&amp;lt;V&amp;gt; {     V call() throws Exception;}可以看出它也是一个 interface，并且它的 call 方法中已经声明了 throws Exception，前面还有一个 V 泛型的返回值，这就和之前的 Runnable 有很大的区别。实现 Callable 接口，就要实现 call 方法，这个方法的返回值是泛型 V，如果把 call 中计算得到的结果放到这个对象中，就可以利用 call 方法的返回值来获得子线程的执行结果了。Callable 和 Runnable 的不同之处最后总结一下 Callable 和 Runnable 的不同之处： 方法名，Callable 规定的执行方法是 call()，而 Runnable 规定的执行方法是 run()； 返回值，Callable 的任务执行后有返回值，而 Runnable 的任务执行后是没有返回值的； 抛出异常，call() 方法可抛出异常，而 run() 方法是不能抛出受检查异常的； 和 Callable 配合的有一个 Future 类，通过 Future 可以了解任务执行情况，或者取消任务的执行，还可获取任务执行的结果，这些功能都是 Runnable 做不到的，Callable 的功能要比 Runnable 强大。以上就是本课时的内容了。首先介绍了 Runnable 的两个缺陷，第一个是没有返回值，第二个是不能抛出受检查异常；然后分析了为什么会有这样的缺陷，以及为什么设计成这样；接下来分析了 Callable 接口，并且把 Callable 接口和 Runnable 接口的区别进行了对比和总结。 既然Callable接口这么强大，那Callable是不是用来代替Runnable的？—— 不是的，它们并存，例如Thread类初始化时不接受Callable作为参数。" }, { "title": "Thread、 ThreadLocal 及 ThreadLocalMap 三者之间的关系", "url": "/posts/ThreadLocal-3/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-02 09:33:00 +0000", "snippet": "最直观、最容易理解的图画的方式来看看它们三者的关系：我们看到最左下角的 Thread 1，这是一个线程，它的箭头指向了 ThreadLocalMap 1，其要表达的意思是，每个 Thread 对象中都持有一个 ThreadLocalMap 类型的成员变量，在这里 Thread 1 所拥有的成员变量就是 ThreadLocalMap 1。而这个 ThreadLocalMap 自身类似于是一个 Map，里面会有一个个 key value 形式的键值对。那么我们就来看一下它的 key 和 value 分别是什么。可以看到这个表格的左侧是 ThreadLocal 1、ThreadLocal 2…… ThreadLocal n，能看出这里的 key 就是 ThreadLocal 的引用。而在表格的右侧是一个一个的 value，这就是我们希望 ThreadLocal 存储的内容，例如 user 对象等。这里需要重点看到它们的数量对应关系：一个 Thread 里面只有一个ThreadLocalMap ，而在一个 ThreadLocalMap 里面却可以有很多的 ThreadLocal，每一个 ThreadLocal 都对应一个 value。因为一个 Thread 是可以调用多个 ThreadLocal 的，所以 Thread 内部就采用了 ThreadLocalMap 这样 Map 的数据结构来存放 ThreadLocal 和 value。通过这张图片，我们就可以搞清楚 Thread、 ThreadLocal 及 ThreadLocalMap 三者在宏观上的关系了。源码分析知道了它们的关系之后，我们再来进行源码分析，来进一步地看到它们内部的实现。get 方法首先我们来看一下 get 方法，源码如下所示：public T get() {    //获取到当前线程    Thread t = Thread.currentThread();    //获取到当前线程内的 ThreadLocalMap 对象，每个线程内都有一个 ThreadLocalMap 对象    ThreadLocalMap map = getMap(t);    if (map != null) {        //获取 ThreadLocalMap 中的 Entry 对象并拿到 Value        ThreadLocalMap.Entry e = map.getEntry(this);        if (e != null) {            @SuppressWarnings(&quot;unchecked&quot;)            T result = (T)e.value;            return result;        }    }    //如果线程内之前没创建过 ThreadLocalMap，就创建    return setInitialValue();}这是 ThreadLocal 的 get 方法，可以看出它利用了 Thread.currentThread 来获取当前线程的引用，并且把这个引用传入到了 getMap 方法里面，来拿到当前线程的 ThreadLocalMap。然后就是一个 if ( map != null ) 条件语句，那我们先来看看 if (map == null) 的情况，如果 map == null，则说明之前这个线程中没有创建过 ThreadLocalMap，于是就去调用 setInitialValue 来创建；如果 map != null，我们就应该通过 this 这个引用（也就是当前的 ThreadLocal 对象的引用）来获取它所对应的 Entry，同时再通过这个 Entry 拿到里面的 value，最终作为结果返回。值得注意的是，这里的 ThreadLocalMap 是保存在线程 Thread 类中的，而不是保存在 ThreadLocal 中的。getMap 方法下面我们来看一下 getMap 方法，源码如下所示：ThreadLocalMap getMap(Thread t) {    return t.threadLocals;}可以看到，这个方法很清楚地表明了 Thread 和 ThreadLocalMap 的关系，可以看出 ThreadLocalMap 是线程的一个成员变量。这个方法的作用就是获取到当前线程内的 ThreadLocalMap 对象，每个线程都有 ThreadLocalMap 对象，而这个对象的名字就叫作 threadLocals，初始值为 null，代码如下：ThreadLocal.ThreadLocalMap threadLocals = null;set 方法下面我们再来看一下 set 方法，源码如下所示：public void set(T value) {    Thread t = Thread.currentThread();    ThreadLocalMap map = getMap(t);    if (map != null)        map.set(this, value);    else        createMap(t, value);}set 方法的作用是把我们想要存储的 value 给保存进去。可以看出，首先，它还是需要获取到当前线程的引用，并且利用这个引用来获取到 ThreadLocalMap ；然后，如果 map == null 则去创建这个 map，而当 map != null 的时候就利用 map.set 方法，把 value 给 set 进去。可以看出，map.set(this, value) 传入的这两个参数中，第一个参数是 this，就是当前 ThreadLocal 的引用，这也再次体现了，在 ThreadLocalMap 中，它的 key 的类型是 ThreadLocal；而第二个参数就是我们所传入的 value，这样一来就可以把这个键值对保存到 ThreadLocalMap 中去了。ThreadLocalMap 类，也就是 Thread.threadLocals下面我们来看一下 ThreadLocalMap 这个类，下面这段代码截取自定义在 ThreadLocal 类中的 ThreadLocalMap 类：static class ThreadLocalMap {    static class Entry extends WeakReference&amp;lt;ThreadLocal&amp;lt;?&amp;gt;&amp;gt; {        /** The value associated with this ThreadLocal. */        Object value;        Entry(ThreadLocal&amp;lt;?&amp;gt; k, Object v) {            super(k);            value = v;        }    }   private Entry[] table;//...}ThreadLocalMap 类是每个线程 Thread 类里面的一个成员变量，其中最重要的就是截取出的这段代码中的 Entry 内部类。在 ThreadLocalMap 中会有一个 Entry 类型的数组，名字叫 table。我们可以把 Entry 理解为一个 map，其键值对为： 键，当前的 ThreadLocal； 值，实际需要存储的变量，比如 user 用户对象或者 simpleDateFormat 对象等。ThreadLocalMap 既然类似于 Map，所以就和 HashMap 一样，也会有包括 set、get、rehash、resize 等一系列标准操作。但是，虽然思路和 HashMap 是类似的，但是具体实现会有一些不同。比如其中一个不同点就是，我们知道 HashMap 在面对 hash 冲突的时候，采用的是拉链法。它会先把对象 hash 到一个对应的格子中，如果有冲突就用链表的形式往下链，如下图所示：但是 ThreadLocalMap 解决 hash 冲突的方式是不一样的，它采用的是线性探测法。如果发生冲突，并不会用链表的形式往下链，而是会继续寻找下一个空的格子。这是 ThreadLocalMap 和 HashMap 在处理冲突时不一样的点。总结一个 Thread 有一个 ThreadLocalMap，而 ThreadLocalMap 的 key 就是一个个的 ThreadLocal，它们就是用这样的关系来存储并维护内容的。之后我们对于 ThreadLocal 的一些重要方法进行了源码分析。 既然key都是this，而map的key是不允许重复的，那一个线程中多个value怎么存？ —— 一个线程如果要对应多个value，需要用多个ThreadLocal ThreadLocalMap的Entry是弱引用，会发生使用的时候已被回收吗讲师回复： 不会的。怎么不会呢？那为什么是弱引用？这个能违反JVM吗？不会的原因是？（我不懂，望指教）—— 不违反JVM。当还有强引用的时候，不会被回收，只有弱引用的时候，会被回收。 ThreadLocalMap的Entry是弱引用，会发生使用的时候已被回收吗 —— 不会的 线性探测时找不到空格子了怎么处理 —— 会进行扩容" }, { "title": "每次用完 ThreadLocal 一定要 remove()", "url": "/posts/ThreadLocal-4/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-02 03:34:22 +0000", "snippet": "什么是内存泄漏内存泄漏指的是，当某一个对象不再有用的时候，占用的内存却不能被回收，这就叫作内存泄漏。因为通常情况下，如果一个对象不再有用，那么我们的垃圾回收器 GC，就应该把这部分内存给清理掉。这样的话，就可以让这部分内存后续重新分配到其他的地方去使用；否则，如果对象没有用，但一直不能被回收，这样的垃圾对象如果积累的越来越多，则会导致我们可用的内存越来越少，最后发生内存不够用的 OOM 错误。下面我们来分析一下，在 ThreadLocal 中这样的内存泄漏是如何发生的。Key 的泄漏在上一讲中，我们分析了 ThreadLocal 的内部结构，知道了每一个 Thread 都有一个 ThreadLocal.ThreadLocalMap 这样的类型变量，该变量的名字叫作 threadLocals。线程在访问了 ThreadLocal 之后，都会在它的 ThreadLocalMap 里面的 Entry 中去维护该 ThreadLocal 变量与具体实例的映射。我们可能会在业务代码中执行了 ThreadLocal instance = null 操作，想清理掉这个 ThreadLocal 实例，但是假设我们在 ThreadLocalMap 的 Entry 中强引用了 ThreadLocal 实例，那么，虽然在业务代码中把 ThreadLocal 实例置为了 null，但是在 Thread 类中依然有这个引用链的存在。GC 在垃圾回收的时候会进行可达性分析，它会发现这个 ThreadLocal 对象依然是可达的，所以对于这个 ThreadLocal 对象不会进行垃圾回收，这样的话就造成了内存泄漏的情况。JDK 开发者考虑到了这一点，所以 ThreadLocalMap 中的 Entry 继承了 WeakReference 弱引用，代码如下所示：static class Entry extends WeakReference&amp;lt;ThreadLocal&amp;lt;?&amp;gt;&amp;gt; {    /** The value associated with this ThreadLocal. */    Object value;    Entry(ThreadLocal&amp;lt;?&amp;gt; k, Object v) {        super(k);        value = v;    }}可以看到，这个 Entry 是 extends WeakReference。弱引用的特点是，如果这个对象只被弱引用关联，而没有任何强引用关联，那么这个对象就可以被回收，所以弱引用不会阻止 GC。因此，这个弱引用的机制就避免了 ThreadLocal 的内存泄露问题。这就是为什么 Entry 的 key 要使用弱引用的原因。Value 的泄漏可是，如果我们继续研究的话会发现，虽然 ThreadLocalMap 的每个 Entry 都是一个对 key 的弱引用，但是这个 Entry 包含了一个对 value 的强引用，还是刚才那段代码：static class Entry extends WeakReference&amp;lt;ThreadLocal&amp;lt;?&amp;gt;&amp;gt; {    /** The value associated with this ThreadLocal. */    Object value;    Entry(ThreadLocal&amp;lt;?&amp;gt; k, Object v) {        super(k);        value = v;    }}可以看到，value = v 这行代码就代表了强引用的发生。正常情况下，当线程终止，key 所对应的 value 是可以被正常垃圾回收的，因为没有任何强引用存在了。但是有时线程的生命周期是很长的，如果线程迟迟不会终止，那么可能 ThreadLocal 以及它所对应的 value 早就不再有用了。在这种情况下，我们应该保证它们都能够被正常的回收。为了更好地分析这个问题，我们用下面这张图来看一下具体的引用链路（实线代表强引用，虚线代表弱引用）：可以看到，左侧是引用栈，栈里面有一个 ThreadLocal 的引用和一个线程的引用，右侧是我们的堆，在堆中是对象的实例。我们重点看一下下面这条链路：Thread Ref → Current Thread → ThreadLocalMap → Entry → Value → 可能泄漏的value实例。这条链路是随着线程的存在而一直存在的，如果线程执行耗时任务而不停止，那么当垃圾回收进行可达性分析的时候，这个 Value 就是可达的，所以不会被回收。但是与此同时可能我们已经完成了业务逻辑处理，不再需要这个 Value 了，此时也就发生了内存泄漏问题。JDK 同样也考虑到了这个问题，在执行 ThreadLocal 的 set、remove、rehash 等方法时，它都会扫描 key 为 null 的 Entry，如果发现某个 Entry 的 key 为 null，则代表它所对应的 value 也没有作用了，所以它就会把对应的 value 置为 null，这样，value 对象就可以被正常回收了。但是假设 ThreadLocal 已经不被使用了，那么实际上 set、remove、rehash 方法也不会被调用，与此同时，如果这个线程又一直存活、不终止的话，那么刚才的那个调用链就一直存在，也就导致了 value 的内存泄漏。如何避免内存泄露分析完这个问题之后，该如何解决呢？解决方法就是我们本课时的标题：调用 ThreadLocal 的 remove 方法。调用这个方法就可以删除对应的 value 对象，可以避免内存泄漏。我们来看一下 remove 方法的源码：public void remove() {    ThreadLocalMap m = getMap(Thread.currentThread());    if (m != null)        m.remove(this);}可以看出，它是先获取到 ThreadLocalMap 这个引用的，并且调用了它的 remove 方法。这里的 remove 方法可以把 key 所对应的 value 给清理掉，这样一来，value 就可以被 GC 回收了。所以，在使用完了 ThreadLocal 之后，我们应该手动去调用它的 remove 方法，目的是防止内存泄漏的发生。 注：第一张图片和引用链相关内容，参考自https://blog.csdn.net/zhongxiangbo/article/details/70859181，" }, { "title": "ThreadLocal 使用场景", "url": "/posts/ThreadLocal-1/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-01 07:33:11 +0000", "snippet": "学习一个工具之前，首先应该知道这个工具的作用，能带来哪些好处，而不是一上来就闷头进入工具的 API、用法等，否则就算我们把某个工具的用法学会了，也不知道应该在什么场景下使用。所以，我们先来看看究竟哪些场景下需要用到 ThreadLocal。在通常的业务开发中，ThreadLocal 有两种典型的使用场景。场景1，ThreadLocal 用作保存每个线程独享的对象，为每个线程都创建一个副本，这样每个线程都可以修改自己所拥有的副本, 而不会影响其他线程的副本，确保了线程安全。场景2，ThreadLocal 用作每个线程内需要独立保存信息，以便供其他方法更方便地获取该信息的场景。每个线程获取到的信息可能都是不一样的，前面执行的方法保存了信息后，后续方法可以通过 ThreadLocal 直接获取到，避免了传参，类似于全局变量的概念。典型场景1这种场景通常用于保存线程不安全的工具类，典型的需要使用的类就是 SimpleDateFormat。场景介绍在这种情况下，每个 Thread 内都有自己的实例副本，且该副本只能由当前 Thread 访问到并使用，相当于每个线程内部的本地变量，这也是 ThreadLocal 命名的含义。因为每个线程独享副本，而不是公用的，所以不存在多线程间共享的问题。我们来做一个比喻，比如饭店要做一道菜，但是有 5 个厨师一起做，这样的话就很乱了，因为如果一个厨师已经放过盐了，假如其他厨师都不知道，于是就都各自放了一次盐，导致最后的菜很咸。这就好比多线程的情况，线程不安全。我们用了 ThreadLocal 之后，相当于每个厨师只负责自己的一道菜，一共有 5 道菜，这样的话就非常清晰明了了，不会出现问题。SimpleDateFormat 的进化之路1. 2 个线程都要用到 SimpleDateFormat下面我们用一个案例来说明这种典型的第一个场景。假设有个需求，即 2 个线程都要用到 SimpleDateFormat。代码如下所示：public class ThreadLocalDemo01 {    public static void main(String[] args) throws InterruptedException {        new Thread(() -&amp;gt; {            String date = new ThreadLocalDemo01().date(1);            System.out.println(date);        }).start();        Thread.sleep(100);        new Thread(() -&amp;gt; {            String date = new ThreadLocalDemo01().date(2);            System.out.println(date);        }).start();    }    public String date(int seconds) {        Date date = new Date(1000 * seconds);        SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;mm:ss&quot;);        return simpleDateFormat.format(date);    }}在以上代码中可以看出，两个线程分别创建了一个自己的 SimpleDateFormat 对象，如图所示：这样一来，有两个线程，那么就有两个 SimpleDateFormat 对象，它们之间互不干扰，这段代码是可以正常运转的，运行结果是：  00:01  00:022. 10 个线程都要用到 SimpleDateFormat假设我们的需求有了升级，不仅仅需要 2 个线程，而是需要 10 个，也就是说，有 10 个线程同时对应 10 个 SimpleDateFormat 对象。我们就来看下面这种写法：public class ThreadLocalDemo02 {    public static void main(String[] args) throws InterruptedException {        for (int i = 0; i &amp;lt; 10; i++) {            int finalI = i;            new Thread(() -&amp;gt; {                String date = new ThreadLocalDemo02().date(finalI);                System.out.println(date);            }).start();            Thread.sleep(100);        }    }    public String date(int seconds) {        Date date = new Date(1000 * seconds);        SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;mm:ss&quot;);        return simpleDateFormat.format(date);    }}上面的代码利用了一个 for 循环来完成这个需求。for 循环一共循环 10 次，每一次都会新建一个线程，并且每一个线程都会在 date 方法中创建一个 SimpleDateFormat 对象，示意图如下：可以看出一共有 10 个线程，对应 10 个 SimpleDateFormat 对象。代码的运行结果：00:0000:0100:0200:0300:0400:0500:0600:0700:0800:093. 需求变成了 1000 个线程都要用到 SimpleDateFormat但是线程不能无休地创建下去，因为线程越多，所占用的资源也会越多。假设我们需要 1000 个任务，那就不能再用 for 循环的方法了，而是应该使用线程池来实现线程的复用，否则会消耗过多的内存等资源。在这种情况下，我们给出下面这个代码实现的方案：public class ThreadLocalDemo03 {    public static ExecutorService threadPool = Executors.newFixedThreadPool(16);    public static void main(String[] args) throws InterruptedException {        for (int i = 0; i &amp;lt; 1000; i++) {            int finalI = i;            threadPool.submit(new Runnable() {                @Override                public void run() {                    String date = new ThreadLocalDemo03().date(finalI);                    System.out.println(date);                }            });        }        threadPool.shutdown();    }    public String date(int seconds) {        Date date = new Date(1000 * seconds);        SimpleDateFormat dateFormat = new SimpleDateFormat(&quot;mm:ss&quot;);        return dateFormat.format(date);    }}可以看出，我们用了一个 16 线程的线程池，并且给这个线程池提交了 1000 次任务。每个任务中它做的事情和之前是一样的，还是去执行 date 方法，并且在这个方法中创建一个 simpleDateFormat 对象。程序的一种运行结果是（多线程下，运行结果不唯一）：00:0000:0700:0400:02...16:2916:2816:2716:2616:39程序运行结果正确，把从 00:00 到 16:39 这 1000 个时间给打印了出来，并且没有重复的时间。我们把这段代码用图形化给表示出来，如图所示：图的左侧是一个线程池，右侧是 1000 个任务。我们刚才所做的就是每个任务都创建了一个 simpleDateFormat 对象，也就是说，1000 个任务对应 1000 个 simpleDateFormat 对象。但是这样做是没有必要的，因为这么多对象的创建是有开销的，并且在使用完之后的销毁同样是有开销的，而且这么多对象同时存在在内存中也是一种内存的浪费。现在我们就来优化一下。既然不想要这么多的 simpleDateFormat 对象，最简单的就是只用一个就可以了。4. 所有的线程都共用一个 simpleDateFormat 对象我们用下面的代码来演示只用一个 simpleDateFormat 对象的情况：public class ThreadLocalDemo04 {    public static ExecutorService threadPool = Executors.newFixedThreadPool(16);    static SimpleDateFormat dateFormat = new SimpleDateFormat(&quot;mm:ss&quot;);    public static void main(String[] args) throws InterruptedException {        for (int i = 0; i &amp;lt; 1000; i++) {            int finalI = i;            threadPool.submit(new Runnable() {                @Override                public void run() {                    String date = new ThreadLocalDemo04().date(finalI);                    System.out.println(date);                }            });        }        threadPool.shutdown();    }    public String date(int seconds) {        Date date = new Date(1000 * seconds);        return dateFormat.format(date);    }}在代码中可以看出，其他的没有变化，变化之处就在于，我们把这个 simpleDateFormat 对象给提取了出来，变成 static 静态变量，需要用的时候直接去获取这个静态对象就可以了。看上去省略掉了创建 1000 个 simpleDateFormat 对象的开销，看上去没有问题，我们用图形的方式把这件事情给表示出来：从图中可以看出，我们有不同的线程，并且线程会执行它们的任务。但是不同的任务所调用的 simpleDateFormat 对象都是同一个，所以它们所指向的那个对象都是同一个，但是这样一来就会有线程不安全的问题。5. 线程不安全，出现了并发安全问题控制台会打印出（多线程下，运行结果不唯一）：00:0400:0400:0500:04...16:1516:1416:13执行上面的代码就会发现，控制台所打印出来的和我们所期待的是不一致的。我们所期待的是打印出来的时间是不重复的，但是可以看出在这里出现了重复，比如第一行和第二行都是 04 秒，这就代表它内部已经出错了。6. 加锁出错的原因就在于，simpleDateFormat 这个对象本身不是一个线程安全的对象，不应该被多个线程同时访问。所以我们就想到了一个解决方案，用 synchronized 来加锁。于是代码就修改成下面的样子：public class ThreadLocalDemo05 {    public static ExecutorService threadPool = Executors.newFixedThreadPool(16);    static SimpleDateFormat dateFormat = new SimpleDateFormat(&quot;mm:ss&quot;);    public static void main(String[] args) throws InterruptedException {        for (int i = 0; i &amp;lt; 1000; i++) {            int finalI = i;            threadPool.submit(new Runnable() {                @Override                public void run() {                    String date = new ThreadLocalDemo05().date(finalI);                    System.out.println(date);                }            });        }        threadPool.shutdown();    }    public String date(int seconds) {        Date date = new Date(1000 * seconds);        String s = null;        synchronized (ThreadLocalDemo05.class) {            s = dateFormat.format(date);        }        return s;    }}可以看出在 date 方法中加入了 synchronized 关键字，把 simpleDateFormat 的调用给上了锁。运行这段代码的结果（多线程下，运行结果不唯一）：00:0000:0100:06...15:5616:3716:36这样的结果是正常的，没有出现重复的时间。但是由于我们使用了 synchronized 关键字，就会陷入一种排队的状态，多个线程不能同时工作，这样一来，整体的效率就被大大降低了。有没有更好的解决方案呢？我们希望达到的效果是，既不浪费过多的内存，同时又想保证线程安全。经过思考得出，可以让每个线程都拥有一个自己的 simpleDateFormat 对象来达到这个目的，这样就能两全其美了。7. 使用 ThreadLocal那么，要想达到这个目的，我们就可以使用 ThreadLocal。示例代码如下所示：public class ThreadLocalDemo06 {    public static ExecutorService threadPool = Executors.newFixedThreadPool(16);    public static void main(String[] args) throws InterruptedException {        for (int i = 0; i &amp;lt; 1000; i++) {            int finalI = i;            threadPool.submit(new Runnable() {                @Override                public void run() {                    String date = new ThreadLocalDemo06().date(finalI);                    System.out.println(date);                }            });        }        threadPool.shutdown();    }    public String date(int seconds) {        Date date = new Date(1000 * seconds);        SimpleDateFormat dateFormat = ThreadSafeFormatter.dateFormatThreadLocal.get();        return dateFormat.format(date);    }}class ThreadSafeFormatter {    public static ThreadLocal&amp;lt;SimpleDateFormat&amp;gt; dateFormatThreadLocal = new ThreadLocal&amp;lt;SimpleDateFormat&amp;gt;() {        @Override        protected SimpleDateFormat initialValue() {            return new SimpleDateFormat(&quot;mm:ss&quot;);        }    };}在这段代码中，我们使用了 ThreadLocal 帮每个线程去生成它自己的 simpleDateFormat 对象，对于每个线程而言，这个对象是独享的。但与此同时，这个对象就不会创造过多，一共只有 16 个，因为线程只有 16 个。代码运行结果（多线程下，运行结果不唯一）：00:0500:0400:01...16:3716:3616:32用图来看一下当前的这种状态：在图中的左侧可以看到，这个线程池一共有 16 个线程，对应 16 个 simpleDateFormat 对象。而在这个图画的右侧是 1000 个任务，任务是非常多的，和原来一样有 1000 个任务。但是这里最大的变化就是，虽然任务有 1000 个，但是我们不再需要去创建 1000 个 simpleDateFormat 对象了。即便任务再多，最终也只会有和线程数相同的 simpleDateFormat 对象。这样既高效地使用了内存，又同时保证了线程安全。以上就是第一种非常典型的适合使用 ThreadLocal 的场景。典型场景2每个线程内需要保存类似于全局变量的信息（例如在拦截器中获取的用户信息），可以让不同方法直接使用，避免参数传递的麻烦却不想被多线程共享（因为不同线程获取到的用户信息不一样）。例如，用 ThreadLocal 保存一些业务内容（用户权限信息、从用户系统获取到的用户名、用户ID 等），这些信息在同一个线程内相同，但是不同的线程使用的业务内容是不相同的。在线程生命周期内，都通过这个静态 ThreadLocal 实例的 get() 方法取得自己 set 过的那个对象，避免了将这个对象（如 user 对象）作为参数传递的麻烦。我们用图画的形式举一个实例：比如说我们是一个用户系统。假设不使用 ThreadLocal，那么当一个请求进来的时候，一个线程会负责执行这个请求，然后这个请求就会依次调用 service-1()、service-2()、service-3()、service-4()，这 4 个方法可能是分布在不同的类中的。在 service-1() 的时候它会创建一个 user 的对象，用于保存比如说这个用户的用户名等信息，后面 service-2/3/4() 都需要用到这个对象的信息，比如说 service-2() 代表下订单、service-3() 代表发货、service-4() 代表完结订单，在这种情况下，每一个方法都需要用户信息，所以就需要把这个 user 对象层层传递下去，从 service-1() 传到 service-2()，再从 service-2() 传到 service-3()，以此类推。这样做会导致代码非常冗余，那有没有什么办法可以解决这个问题呢？我们首先想到的方法就是使用一个 HashMap，如下图所示：比如说我们使用了这样的 Map 之后，就不需要把 user 对象层层传递了，而是在执行 service-1() 的时候，把这个用户信息给 put 进去，然后后面需要拿用户信息的时候，直接从静态的 User map 里面 get 就可以了。这样一来，无论你执行哪个方法，都可以直接获取到这个用户信息。当然，我们也要考虑到 web 服务器通常都是多线程的，当多个线程同时工作的时候，我们也需要保证线程安全。所以在这里，如果我们使用 HashMap 是不够的，因为它是线程不安全的，那我们就可以使用 synchronized，或者直接把 HashMap 替换成 ConcurrentHashMap，用类似的方法来保证线程安全，这样的改进如下图所示：在这个图中，可以看出有两个线程，并且每个线程所做的事情都是访问 service-1/2/3/4()。那么当它们同时运行的时候，都会同时访问这个 User map，于是就需要 User map 是线程安全的。无论我们使用 synchronized 还是使用 ConcurrentHashMap，它对性能都是有所影响的，因为即便是使用性能比较好的 ConcurrentHashMap，它也是包含少量的同步，或者是 cas 等过程。相比于完全没有同步，它依然是有性能损耗的。所以在此一个更好的办法就是使用 ThreadLocal。这样一来，我们就可以在不影响性能的情况下，也无需层层传递参数，就可以达到保存当前线程所对应的用户信息的目的。如下图所示：在这个图中可以看出，同样是多个线程同时去执行，但是这些线程同时去访问这个 ThreadLocal 并且能利用 ThreadLocal 拿到只属于自己的独享对象。这样的话，就无需任何额外的措施，保证了线程安全，因为每个线程是独享 user 对象的。代码如下所示：public class ThreadLocalDemo07 {    public static void main(String[] args) {        new Service1().service1();    }}class Service1 {    public void service1() {        User user = new User(&quot;拉勾教育&quot;);        UserContextHolder.holder.set(user);        new Service2().service2();    }}class Service2 {    public void service2() {        User user = UserContextHolder.holder.get();        System.out.println(&quot;Service2拿到用户名：&quot; + user.name);        new Service3().service3();    }}class Service3 {    public void service3() {        User user = UserContextHolder.holder.get();        System.out.println(&quot;Service3拿到用户名：&quot; + user.name);        UserContextHolder.holder.remove();    }}class UserContextHolder {    public static ThreadLocal&amp;lt;User&amp;gt; holder = new ThreadLocal&amp;lt;&amp;gt;();}class User {    String name;    public User(String name) {        this.name = n    }}在这个代码中我们可以看出，我们有一个 UserContextHolder，里面保存了一个 ThreadLocal，在调用 Service1 的方法的时候，就往里面存入了 user 对象，而在后面去调用的时候，直接从里面用 get 方法取出来就可以了。没有参数层层传递的过程，非常的优雅、方便。代码运行结果：Service2拿到用户名：superhscService3拿到用户名：superhsc总结ThreadLocal 的两个典型的使用场景。场景1，ThreadLocal 用作保存每个线程独享的对象，为每个线程都创建一个副本，每个线程都只能修改自己所拥有的副本, 而不会影响其他线程的副本，这样就让原本在并发情况下，线程不安全的情况变成了线程安全的情况。场景2，ThreadLocal 用作每个线程内需要独立保存信息的场景，供其他方法更方便得获取该信息，每个线程获取到的信息都可能是不一样的，前面执行的方法设置了信息后，后续方法可以通过 ThreadLocal 直接获取到，避免了传参。" }, { "title": "ThreadLocal 是用来解决共享资源的多线程访问的", "url": "/posts/ThreadLocal-2/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-10-01 06:33:00 +0000", "snippet": "ThreadLocal 是不是用来解决共享资源的多线程访问的。这是一个常见的面试问题，如果被问到了 ThreadLocal，则有可能在你介绍完它的作用、注意点等内容之后，再问你：ThreadLocal 是不是用来解决共享资源的多线程访问的呢？假如遇到了这样的问题，其思路一定要清晰。面试时被问到应如何回答这道题的答案很明确——不是，ThreadLocal 并不是用来解决共享资源问题的。虽然 ThreadLocal 确实可以用于解决多线程情况下的线程安全问题，但其资源并不是共享的，而是每个线程独享的。所以这道题其实是有一定陷阱成分在内的。ThreadLocal 解决线程安全问题的时候，相比于使用“锁”而言，换了一个思路，把资源变成了各线程独享的资源，非常巧妙地避免了同步操作。具体而言，它可以在 initialValue 中 new 出自己线程独享的资源，而多个线程之间，它们所访问的对象本身是不共享的，自然就不存在任何并发问题。这是 ThreadLocal 解决并发问题的最主要思路。如果我们把放到 ThreadLocal 中的资源用 static 修饰，让它变成一个共享资源的话，那么即便使用了 ThreadLocal，同样也会有线程安全问题。比如我们对第 44 讲中的例子进行改造，如果我们在 SimpleDateFormat 之前加上一个 static 关键字来修饰，并且把这个静态对象放到 ThreadLocal 中去存储的话，代码如下所示：public class ThreadLocalStatic {    public static ExecutorService threadPool = Executors.newFixedThreadPool(16);    static SimpleDateFormat dateFormat = new SimpleDateFormat(&quot;mm:ss&quot;);    public static void main(String[] args) throws InterruptedException {        for (int i = 0; i &amp;lt; 1000; i++) {            int finalI = i;            threadPool.submit(new Runnable() {                @Override                public void run() {                    String date = new ThreadLocalStatic().date(finalI);                    System.out.println(date);                }            });        }        threadPool.shutdown();    }    public String date(int seconds) {        Date date = new Date(1000 * seconds);        SimpleDateFormat dateFormat = ThreadSafeFormatter.dateFormatThreadLocal.get();        return dateFormat.format(date);    }}class ThreadSafeFormatter {    public static ThreadLocal&amp;lt;SimpleDateFormat&amp;gt; dateFormatThreadLocal = new ThreadLocal&amp;lt;SimpleDateFormat&amp;gt;() {        @Override        protected SimpleDateFormat initialValue() {            return ThreadLocalStatic.dateFormat;        }    }}那么在多线程中去获取这个资源并且同时使用的话，同样会出现时间重复的问题，运行结果如下。00:1500:1500:0500:16...可以看出，00:15 被多次打印了，发生了线程安全问题。也就是说，如果我们需要放到 ThreadLocal 中的这个对象是共享的，是被 static 修饰的，那么此时其实根本就不需要用到 ThreadLocal，即使用了 ThreadLocal 并不能解决线程安全问题。相反，我们对于这种共享的变量，如果想要保证它的线程安全，应该用其他的方法，比如说可以使用 synchronized 或者是加锁等其他的方法来解决线程安全问题，而不是使用 ThreadLocal，因为这不是 ThreadLocal 应该使用的场景。这个问题回答到这里，可能会引申出下面这个问题。ThreadLocal 和 synchronized 是什么关系面试官可能会问：你既然说 ThreadLocal 和 synchronized 它们两个都能解决线程安全问题，那么 ThreadLocal 和 synchronized 是什么关系呢？我们先说第一种情况。当 ThreadLocal 用于解决线程安全问题的时候，也就是把一个对象给每个线程都生成一份独享的副本的，在这种场景下，ThreadLocal 和 synchronized 都可以理解为是用来保证线程安全的手段。例如，在第 44 讲 SimpleDateFormat 的例子中，我们既使用了 synchronized 来达到目的，也使用了 ThreadLocal 作为实现方案。但是效果和实现原理不同： ThreadLocal 是通过让每个线程独享自己的副本，避免了资源的竞争。 synchronized 主要用于临界资源的分配，在同一时刻限制最多只有一个线程能访问该资源。相比于 ThreadLocal 而言，synchronized 的效率会更低一些，但是花费的内存也更少。在这种场景下，ThreadLocal 和 synchronized 虽然有不同的效果，不过都可以达到线程安全的目的。但是对于 ThreadLocal 而言，它还有不同的使用场景。比如当 ThreadLocal 用于让多个类能更方便地拿到我们希望给每个线程独立保存这个信息的场景下时（比如每个线程都会对应一个用户信息，也就是 user 对象），在这种场景下，ThreadLocal 侧重的是避免传参，所以此时 ThreadLocal 和 synchronized 是两个不同维度的工具。总结ThreadLocal 是不是用来解决共享资源的多线程访问的问题的，答案是“不是”，因为对于 ThreadLocal 而言，每个线程中的资源并不共享；然后我们又介绍了 ThreadLocal 和 synchronized 的关系。 变量副本和每个线程都创建一个对象啥区别？副本也需要占用内存 —— 区别在于对象的数量。ThreadLocal 只会创建线程数个变量，而每个任务都创建变量的话，数量就远大于线程数了。 每次都newSimpleDateFormat（）是线程安全的吧 —— 是的，但是没必要，开销大。" }, { "title": "Java 8 中 Adder 和 Accumulator 有什么区别", "url": "/posts/Java8-Adder-Accumulator/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-30 15:33:00 +0000", "snippet": "Adder 的介绍我们要知道 Adder 和 Accumulator 都是 Java 8 引入的，是相对比较新的类。对于 Adder 而言，比如最典型的 LongAdder，我们在第 40 讲的时候已经讲解过了，在高并发下 LongAdder 比 AtomicLong 效率更高，因为对于 AtomicLong 而言，它只适合用于低并发场景，否则在高并发的场景下，由于 CAS 的冲突概率大，会导致经常自旋，影响整体效率。而 LongAdder 引入了分段锁的概念，当竞争不激烈的时候，所有线程都是通过 CAS 对同一个 Base 变量进行修改，但是当竞争激烈的时候，LongAdder 会把不同线程对应到不同的 Cell 上进行修改，降低了冲突的概率，从而提高了并发性。Accumulator 的介绍那么 Accumulator 又是做什么的呢？Accumulator 和 Adder 非常相似，实际上 Accumulator 就是一个更通用版本的 Adder，比如 LongAccumulator 是 LongAdder 的功能增强版，因为 LongAdder 的 API 只有对数值的加减，而 LongAccumulator 提供了自定义的函数操作。我这样讲解可能有些同学还是不太理解，那就让我们用一个非常直观的代码来举例说明一下，代码如下：public class LongAccumulatorDemo {    public static void main(String[] args) throws InterruptedException {        LongAccumulator accumulator = new LongAccumulator((x, y) -&amp;gt; x + y, 0);        ExecutorService executor = Executors.newFixedThreadPool(8);        IntStream.range(1, 10).forEach(i -&amp;gt; executor.submit(() -&amp;gt; accumulator.accumulate(i)));        Thread.sleep(2000);        System.out.println(accumulator.getThenReset());    }}在这段代码中： 首先新建了一个 LongAccumulator，同时给它传入了两个参数； 然后又新建了一个 8 线程的线程池，并且利用整形流也就是 IntStream 往线程池中提交了从 1 ~ 9 这 9 个任务； 之后等待了两秒钟，这两秒钟的作用是等待线程池的任务执行完毕； 最后把 accumulator 的值打印出来。这段代码的运行结果是 45，代表 0+1+2+3+…+8+9=45 的结果，这个结果怎么理解呢？我们先重点看看新建的 LongAccumulator 的这一行语句：LongAccumulator accumulator = new LongAccumulator((x, y) -&amp;gt; x + y, 0);案例分析我们来看一下上面这段代码执行的过程，当执行 accumulator.accumulate(1) 的时候，首先要知道这时候 x 和 y 是什么，第一次执行时， x 是 LongAccumulator 构造函数中的第二个参数，也就是 0，而第一次执行时的 y 值就是本次 accumulator.accumulate(1) 方法所传入的 1；然后根据表达式 x+y，计算出 0+1=1，这个结果会赋值给下一次计算的 x，而下一次计算的 y 值就是 accumulator.accumulate(2) 传入的 2，所以下一次的计算结果是 1+2=3。我们在 IntStream.range(1, 10).forEach(i -&amp;gt; executor.submit(() -&amp;gt; accumulator.accumulate(i))); 这一行语句中实际上利用了整型流，分别给线程池提交了从 1 ~ 9 这 9 个任务，相当于执行了：accumulator.accumulate(1);accumulator.accumulate(2);accumulator.accumulate(3);...accumulator.accumulate(8);accumulator.accumulate(9);那么根据上面的这个推演，就可以得出它的内部运行，这也就意味着，LongAccumulator 执行了：0+1=1;1+2=3;3+3=6;6+4=10;10+5=15;15+6=21;21+7=28;28+8=36;36+9=45;这里需要指出的是，这里的加的顺序是不固定的，并不是说会按照顺序从 1 开始逐步往上累加，它也有可能会变，比如说先加 5、再加 3、再加 6。但总之，由于加法有交换律，所以最终加出来的结果会保证是 45。这就是这个类的一个基本的作用和用法。拓展功能我们继续看一下它的功能强大之处。举几个例子，刚才我们给出的表达式是 x + y，其实同样也可以传入 x * y，或者写一个 Math.min(x, y)，相当于求 x 和 y 的最小值。同理，也可以去求 Math.max(x, y)，相当于求一个最大值。根据业务的需求来选择就可以了。代码如下：LongAccumulator counter = new LongAccumulator((x, y) -&amp;gt; x + y, 0);LongAccumulator result = new LongAccumulator((x, y) -&amp;gt; x * y, 0);LongAccumulator min = new LongAccumulator((x, y) -&amp;gt; Math.min(x, y), 0);LongAccumulator max = new LongAccumulator((x, y) -&amp;gt; Math.max(x, y), 0);这时你可能会有一个疑问：在这里为什么不用 for 循环呢？比如说我们之前的例子，从 0 加到 9，我们直接写一个 for 循环不就可以了吗？确实，用 for 循环也能满足需求，但是用 for 循环的话，它执行的时候是串行，它一定是按照 0+1+2+3+…+8+9 这样的顺序相加的，但是 LongAccumulator 的一大优势就是可以利用线程池来为它工作。一旦使用了线程池，那么多个线程之间是可以并行计算的，效率要比之前的串行高得多。这也是为什么刚才说它加的顺序是不固定的，因为我们并不能保证各个线程之间的执行顺序，所能保证的就是最终的结果是确定的。适用场景接下来我们说一下 LongAccumulator 的适用场景。第一点需要满足的条件，就是需要大量的计算，并且当需要并行计算的时候，我们可以考虑使用 LongAccumulator。当计算量不大，或者串行计算就可以满足需求的时候，可以使用 for 循环；如果计算量大，需要提高计算的效率时，我们则可以利用线程池，再加上 LongAccumulator 来配合的话，就可以达到并行计算的效果，效率非常高。第二点需要满足的要求，就是计算的执行顺序并不关键，也就是说它不要求各个计算之间的执行顺序，也就是说线程 1 可能在线程 5 之后执行，也可能在线程 5 之前执行，但是执行的先后并不影响最终的结果。一些非常典型的满足这个条件的计算，就是类似于加法或者乘法，因为它们是有交换律的。同样，求最大值和最小值对于顺序也是没有要求的，因为最终只会得出所有数字中的最大值或者最小值，无论先提交哪个或后提交哪个，都不会影响到最终的结果。 accumulator用法比较复杂，如果Adder可以满足场景，那么用adder更易懂。" }, { "title": "AtomicInteger 和 synchronized 的异同点", "url": "/posts/autoicInteger-sysnchronized/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-29 15:33:00 +0000", "snippet": "用原子类和 synchronized 关键字来解决一个经典的线程安全问题，给出具体的代码对比，然后再分析它们背后的区别。代码对比首先，原始的线程不安全的情况的代码如下所示：public class Lesson42 implements Runnable { static int value = 0; public static void main(String[] args) throws InterruptedException { Runnable runnable = new Lesson42(); Thread thread1 = new Thread(runnable); Thread thread2 = new Thread(runnable); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(value); } @Override public void run() { for (int i = 0; i &amp;lt; 10000; i++) { value++; } }}在代码中我们新建了一个 value 变量，并且在两个线程中对它进行同时的自加操作，每个线程加 10000 次，然后我们用 join 来确保它们都执行完毕，最后打印出最终的数值。因为 value++ 不是一个原子操作，所以上面这段代码是线程不安全的（具体分析详见第 6 讲），所以代码的运行结果会小于 20000，例如会输出 14611 等各种数字。我们首先给出方法一，也就是用原子类来解决这个问题，代码如下所示：public class Lesson42Atomic implements Runnable { static AtomicInteger atomicInteger = new AtomicInteger(); public static void main(String[] args) throws InterruptedException { Runnable runnable = new Lesson42Atomic(); Thread thread1 = new Thread(runnable); Thread thread2 = new Thread(runnable); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(atomicInteger.get()); } @Override public void run() { for (int i = 0; i &amp;lt; 10000; i++) { atomicInteger.incrementAndGet(); } }}用原子类之后，我们的计数变量就不再是一个普通的 int 变量了，而是 AtomicInteger 类型的对象，并且自加操作也变成了 incrementAndGet 法。由于原子类可以确保每一次的自加操作都是具备原子性的，所以这段程序是线程安全的，所以以上程序的运行结果会始终等于 20000。下面我们给出方法二，我们用 synchronized 来解决这个问题，代码如下所示：public class Lesson42Syn implements Runnable { static int value = 0; public static void main(String[] args) throws InterruptedException { Runnable runnable = new Lesson42Syn(); Thread thread1 = new Thread(runnable); Thread thread2 = new Thread(runnable); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(value); } @Override public void run() { for (int i = 0; i &amp;lt; 10000; i++) { synchronized (this) { value++; } } }}它与最开始的线程不安全的代码的区别在于，在 run 方法中加了 synchronized 代码块，就可以非常轻松地解决这个问题，由于 synchronized 可以保证代码块内部的原子性，所以以上程序的运行结果也始终等于 20000，是线程安全的。方案对比下面我们就对这两种不同的方案进行分析。第一点，我们来看一下它们背后原理的不同。在第 21 课时中我们详细分析了 synchronized 背后的 monitor 锁，也就是 synchronized 原理，同步方法和同步代码块的背后原理会有少许差异，但总体思想是一致的：在执行同步代码之前，需要首先获取到 monitor 锁，执行完毕后，再释放锁。而我们在第 39 课时中介绍了原子类，它保证线程安全的原理是利用了 CAS 操作。从这一点上看，虽然原子类和 synchronized 都能保证线程安全，但是其实现原理是大有不同的。第二点不同是使用范围的不同。对于原子类而言，它的使用范围是比较局限的。因为一个原子类仅仅是一个对象，不够灵活。而 synchronized 的使用范围要广泛得多。比如说 synchronized 既可以修饰一个方法，又可以修饰一段代码，相当于可以根据我们的需要，非常灵活地去控制它的应用范围。所以仅有少量的场景，例如计数器等场景，我们可以使用原子类。而在其他更多的场景下，如果原子类不适用，那么我们就可以考虑用 synchronized 来解决这个问题。第三个区别是粒度的区别。原子变量的粒度是比较小的，它可以把竞争范围缩小到变量级别。通常情况下，synchronized 锁的粒度都要大于原子变量的粒度。如果我们只把一行代码用 synchronized 给保护起来的话，有一点杀鸡焉用牛刀的感觉。第四点是它们性能的区别，同时也是悲观锁和乐观锁的区别。因为 synchronized 是一种典型的悲观锁，而原子类恰恰相反，它利用的是乐观锁。所以，我们在比较 synchronized 和 AtomicInteger 的时候，其实也就相当于比较了悲观锁和乐观锁的区别。从性能上来考虑的话，悲观锁的操作相对来讲是比较重量级的。因为 synchronized 在竞争激烈的情况下，会让拿不到锁的线程阻塞，而原子类是永远不会让线程阻塞的。不过，虽然 synchronized 会让线程阻塞，但是这并不代表它的性能就比原子类差。因为悲观锁的开销是固定的，也是一劳永逸的。随着时间的增加，这种开销并不会线性增长。而乐观锁虽然在短期内的开销不大，但是随着时间的增加，它的开销也是逐步上涨的。所以从性能的角度考虑，它们没有一个孰优孰劣的关系，而是要区分具体的使用场景。在竞争非常激烈的情况下，推荐使用 synchronized；而在竞争不激烈的情况下，使用原子类会得到更好的效果。值得注意的是，synchronized 的性能随着 JDK 的升级，也得到了不断的优化。synchronized 会从无锁升级到偏向锁，再升级到轻量级锁，最后才会升级到让线程阻塞的重量级锁。因此synchronized 在竞争不激烈的情况下，性能也是不错的，不需要“谈虎色变”。 悲观锁开销是一次性的获取锁的开销，可能涉及线程状态转换，而乐观锁的开销是一次次的尝试获取锁。" }, { "title": "volatile 和原子类的异同", "url": "/posts/atomic-volatile/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-28 15:33:00 +0000", "snippet": "案例说明 volatile 和原子类的异同我们首先看一个案例。如图所示，有两个线程。在图中左上角可以看出，有一个公共的 boolean flag 标记位，最开始赋值为 true，然后线程 2 会进入一个 while 循环，并且根据这个 flag 也就是标记位的值来决定是否继续执行或着退出。最开始由于 flag 的值是 true，所以首先会在这里执行一定时期的循环。然后假设在某一时刻，线程 1 把这个 flag 的值改为 false 了，它所希望的是，线程 2 看到这个变化后停止运行。但是这样做其实是有风险的，线程 2 可能并不能立刻停下来，也有可能过一段时间才会停止，甚至在最极端的情况下可能永远都不会停止。为了理解发生这种情况的原因，我们首先来看一下 CPU 的内存结构，这里是一个双核的 CPU 的简单示意图：可以看出，线程 1 和线程 2 分别在不同的 CPU 核心上运行，每一个核心都有自己的本地内存，并且在下方也有它们共享的内存。最开始它们都可以读取到 flag 为 true ，不过当线程 1 这个值改为 false 之后，线程 2 并不能及时看到这次修改，因为线程 2 不能直接访问线程 1 的本地内存，这样的问题就是一个非常典型的可见性问题。要想解决这个问题，我们只需要在变量的前面加上 volatile 关键字修饰，只要我们加上这个关键字，那么每一次变量被修改的时候，其他线程对此都可见，这样一旦线程 1 改变了这个值，那么线程 2 就可以立刻看到，因此就可以退出 while 循环了。之所以加了关键字之后就就可以让它拥有可见性，原因在于有了这个关键字之后，线程 1 的更改会被 flush 到共享内存中，然后又会被 refresh 到线程 2 的本地内存中，这样线程 2 就能感受到这个变化了，所以 volatile 这个关键字最主要是用来解决可见性问题的，可以一定程度上保证线程安全。现在让我们回顾一下很熟悉的多线程同时进行 value++ 的场景，如图所示：如果它被初始化为每个线程都加 1000 次，最终的结果很可能不是 2000。由于 value++ 不是原子的，所以在多线程的情况下，会出现线程安全问题。但是如果我们在这里使用 volatile 关键字，能不能解决问题呢？很遗憾，答案是即便使用了 volatile 也是不能保证线程安全的，因为这里的问题不单单是可见性问题，还包含原子性问题。我们有多种办法可以解决这里的问题，第 1 种是使用 synchronized 关键字，如图所示：这样一来，两个线程就不能同时去更改 value 的数值，保证了 value++ 语句的原子性，并且 synchronized 同样保证了可见性，也就是说，当第 1 个线程修改了 value 值之后，第 2 个线程可以立刻看见本次修改的结果。解决这个问题的第 2 个方法，就是使用我们的原子类，如图所示：比如用一个 AtomicInteger，然后每个线程都调用它的 incrementAndGet 方法。在利用了原子变量之后就无需加锁，我们可以使用它的 incrementAndGet 方法，这个操作底层由 CPU 指令保证原子性，所以即便是多个线程同时运行，也不会发生线程安全问题。原子类和 volatile 的使用场景那下面我们就来说一下原子类和 volatile 各自的使用场景。我们可以看出，volatile 和原子类的使用场景是不一样的，如果我们有一个可见性问题，那么可以使用 volatile 关键字，但如果我们的问题是一个组合操作，需要用同步来解决原子性问题的话，那么可以使用原子变量，而不能使用 volatile 关键字。通常情况下，volatile 可以用来修饰 boolean 类型的标记位，因为对于标记位来讲，直接的赋值操作本身就是具备原子性的，再加上 volatile 保证了可见性，那么就是线程安全的了。而对于会被多个线程同时操作的计数器 Counter 的场景，这种场景的一个典型特点就是，它不仅仅是一个简单的赋值操作，而是需要先读取当前的值，然后在此基础上进行一定的修改，再把它给赋值回去。这样一来，我们的 volatile 就不足以保证这种情况的线程安全了。我们需要使用原子类来保证线程安全。 直接使用synchronized加锁，不适用volatile修饰value，对value进行自增或自减操作，同样可以保证最终value值的准确性吧？！ 可以volatile和AtomicInteger组合使用吗？—— 不需要，单独使用AtomicInteger已经可以保证原子性了 volatile修饰变量，要保证赋值的准确性，就直接设置值，不依赖其他变量的值来修改当前volatile修饰的变量值 如果只使用了synchronized同步，但变量没有用 volite 修饰，也可以保证可见性。" }, { "title": "解决 AtomicInteger 在高并发下性能不好的手段", "url": "/posts/AtomicInteger/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-27 15:33:00 +0000", "snippet": "JDK1.5 中新增了并发情况下使用的 Integer/Long 所对应的原子类 AtomicInteger 和 AtomicLong。在并发的场景下，如果我们需要实现计数器，可以利用 AtomicInteger 和 AtomicLong，这样一来，就可以避免加锁和复杂的代码逻辑，有了它们之后，我们只需要执行对应的封装好的方法，例如对这两个变量进行原子的增操作或原子的减操作，就可以满足大部分业务场景的需求。不过，虽然它们很好用，但是如果你的业务场景是并发量很大的，那么你也会发现，这两个原子类实际上会有较大的性能问题，这是为什么呢？就让我们从一个例子看起。AtomicLong 存在的问题/*** 描述：     在16个线程下使用AtomicLong*/public class AtomicLongDemo {    public static void main(String[] args) throws InterruptedException {       AtomicLong counter = new AtomicLong(0);       ExecutorService service = Executors.newFixedThreadPool(16);       for (int i = 0; i &amp;lt; 100; i++) {           service.submit(new Task(counter));       }        Thread.sleep(2000);       System.out.println(counter.get());   }    static class Task implements Runnable {        private final AtomicLong counter;        public Task(AtomicLong counter) {           this.counter = counter;       }        @Override       public void run() {           counter.incrementAndGet();       }   }}在这段代码中可以看出，我们新建了一个原始值为 0 的 AtomicLong。然后，有一个线程数为 16 的线程池，并且往这个线程池中添加了 100 次相同的一个任务。那我们往下看这个任务是什么。在下面的 Task 类中可以看到，这个任务实际上就是每一次去调用 AtomicLong 的 incrementAndGet 方法，相当于一次自加操作。这样一来，整个类的作用就是把这个原子类从 0 开始，添加 100 个任务，每个任务自加一次。这段代码的运行结果毫无疑问是 100，虽然是多线程并发访问，但是 AtomicLong 依然可以保证 incrementAndGet 操作的原子性，所以不会发生线程安全问题。不过如果我们深入一步去看内部情景的话，你可能会感到意外。我们把模型简化成只有两个线程在同时工作的并发场景，因为两个线程和更多个线程本质上是一样的。如图所示：我们可以看到在这个图中，每一个线程是运行在自己的 core 中的，并且它们都有一个本地内存是自己独用的。在本地内存下方，有两个 CPU 核心共用的共享内存。对于 AtomicLong 内部的 value 属性而言，也就是保存当前 AtomicLong 数值的属性，它是被 volatile 修饰的，所以它需要保证自身可见性。这样一来，每一次它的数值有变化的时候，它都需要进行 flush 和 refresh。比如说，如果开始时，ctr 的数值为 0 的话，那么如图所示，一旦 core 1 把它改成 1 的话，它首先会在左侧把这个 1 的最新结果给 flush 到下方的共享内存。然后，再到右侧去往上 refresh 到核心 2 的本地内存。这样一来，对于核心 2 而言，它才能感知到这次变化。由于竞争很激烈，这样的 flush 和 refresh 操作耗费了很多资源，而且 CAS 也会经常失败。LongAdder 带来的改进和原理在 JDK 8 中又新增了 LongAdder 这个类，这是一个针对 Long 类型的操作工具类。那么既然已经有了 AtomicLong，为何又要新增 LongAdder 这么一个类呢？我们同样是用一个例子来说明。下面这个例子和刚才的例子很相似，只不过我们把工具类从 AtomicLong 变成了 LongAdder。其他的不同之处还在于最终打印结果的时候，调用的方法从原来的 get 变成了现在的 sum 方法。而其他的逻辑都一样。我们来看一下使用 LongAdder 的代码示例：/*** 描述：     在16个线程下使用LongAdder*/public class LongAdderDemo {    public static void main(String[] args) throws InterruptedException {       LongAdder counter = new LongAdder();       ExecutorService service = Executors.newFixedThreadPool(16);       for (int i = 0; i &amp;lt; 100; i++) {           service.submit(new Task(counter));       }        Thread.sleep(2000);       System.out.println(counter.sum());   }   static class Task implements Runnable {        private final LongAdder counter;        public Task(LongAdder counter) {           this.counter = counter;       }        @Override       public void run() {           counter.increment();       }   }}代码的运行结果同样是 100，但是运行速度比刚才 AtomicLong 的实现要快。下面我们解释一下，为什么高并发下 LongAdder 比 AtomicLong 效率更高。因为 LongAdder 引入了分段累加的概念，内部一共有两个参数参与计数：第一个叫作 base，它是一个变量，第二个是 Cell[] ，是一个数组。其中的 base 是用在竞争不激烈的情况下的，可以直接把累加结果改到 base 变量上。那么，当竞争激烈的时候，就要用到我们的 Cell[] 数组了。一旦竞争激烈，各个线程会分散累加到自己所对应的那个 Cell[] 数组的某一个对象中，而不会大家共用同一个。这样一来，LongAdder 会把不同线程对应到不同的 Cell 上进行修改，降低了冲突的概率，这是一种分段的理念，提高了并发性，这就和 Java 7 的 ConcurrentHashMap 的 16 个 Segment 的思想类似。竞争激烈的时候，LongAdder 会通过计算出每个线程的 hash 值来给线程分配到不同的 Cell 上去，每个 Cell 相当于是一个独立的计数器，这样一来就不会和其他的计数器干扰，Cell 之间并不存在竞争关系，所以在自加的过程中，就大大减少了刚才的 flush 和 refresh，以及降低了冲突的概率，这就是为什么 LongAdder 的吞吐量比 AtomicLong 大的原因，本质是空间换时间，因为它有多个计数器同时在工作，所以占用的内存也要相对更大一些。那么 LongAdder 最终是如何实现多线程计数的呢？答案就在最后一步的求和 sum 方法，执行 LongAdder.sum() 的时候，会把各个线程里的 Cell 累计求和，并加上 base，形成最终的总和。代码如下：public long sum() {   Cell[] as = cells; Cell a;   long sum = base;   if (as != null) {       for (int i = 0; i &amp;lt; as.length; ++i) {           if ((a = as[i]) != null)               sum += a.value;       }   }   return sum;}在这个 sum 方法中可以看到，思路非常清晰。先取 base 的值，然后遍历所有 Cell，把每个 Cell 的值都加上去，形成最终的总和。由于在统计的时候并没有进行加锁操作，所以这里得出的 sum 不一定是完全准确的，因为有可能在计算 sum 的过程中 Cell 的值被修改了。那么我们已经了解了，为什么 AtomicLong 或者说 AtomicInteger 它在高并发下性能不好，也同时看到了性能更好的 LongAdder。下面我们就分析一下，对它们应该如何选择。如何选择在低竞争的情况下，AtomicLong 和 LongAdder 这两个类具有相似的特征，吞吐量也是相似的，因为竞争不高。但是在竞争激烈的情况下，LongAdder 的预期吞吐量要高得多，经过试验，LongAdder 的吞吐量大约是 AtomicLong 的十倍，不过凡事总要付出代价，LongAdder 在保证高效的同时，也需要消耗更多的空间。AtomicLong 可否被 LongAdder 替代？那么我们就要考虑了，有了更高效的 LongAdder，那 AtomicLong 可否不使用了呢？是否凡是用到 AtomicLong 的地方，都可以用 LongAdder 替换掉呢？答案是不是的，这需要区分场景。LongAdder 只提供了 add、increment 等简单的方法，适合的是统计求和计数的场景，场景比较单一，而 AtomicLong 还具有 compareAndSet 等高级方法，可以应对除了加减之外的更复杂的需要 CAS 的场景。结论：如果我们的场景仅仅是需要用到加和减操作的话，那么可以直接使用更高效的 LongAdder，但如果我们需要利用 CAS 比如 compareAndSet 等操作的话，就需要使用 AtomicLong 来完成。 “老师，LongAdder 既然最后在相加的时候可能不准确，那不也是线程不安全的么，为什么还要使用呢？” 答案在这里：https://www.cnblogs.com/thisiswhy/p/13176237.html LongAdder既然最后在相加的时候可能不准确，那不也是线程不安全的么，为什么还要使用呢？ 当在多线程的情况下对一个共享数据进行更新（写）操作，比如实现一些统计信息类的需求，LongAdder 的表现比它的老大哥 AtomicLong 表现的更好。在并发不高的时候，两个类都差不多。但是高并发时 LongAdder 的吞吐量明显高一点，它也占用更多的空间。这是一种空间换时间的思想。 因为它在多线程并发情况下，没有一个准确的返回值，所以当你需要根据返回值去搞事情的时候，你就要仔细思考思考，这个返回值你是要精准的，还是大概的统计类的数据就行。 比如说，如果你是用来做序号生成器，所以你需要一个准确的返回值，那么还是用 AtomicLong 更加合适。 如果你是用来做计数器，这种写多读少的场景。比如接口访问次数的统计类需求，不需要时时刻刻的返回一个准确的值，那就上 LongAdder 吧。 总之，AtomicLong 是可以保证每次都有准确值，而 LongAdder 是可以保证最终数据是准确的。高并发的场景下 LongAdder 的写性能比 AtomicLong 高。" }, { "title": "原子类是如何使用 CSA 保持线程安全", "url": "/posts/cas-thread-safe/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-26 15:33:00 +0000", "snippet": "什么是原子类？原子类有什么作用？要想回答这个问题，首先我们需要知道什么是原子类，以及它有什么作用。在编程领域里，原子性意味着“一组操作要么全都操作成功，要么全都失败，不能只操作成功其中的一部分”。而 java.util.concurrent.atomic 下的类，就是具有原子性的类，可以原子性地执行添加、递增、递减等操作。比如之前多线程下的线程不安全的 i++ 问题，到了原子类这里，就可以用功能相同且线程安全的 getAndIncrement 方法来优雅地解决。原子类的作用和锁有类似之处，是为了保证并发情况下线程安全。不过原子类相比于锁，有一定的优势： 粒度更细：原子变量可以把竞争范围缩小到变量级别，通常情况下，锁的粒度都要大于原子变量的粒度。 效率更高：除了高度竞争的情况之外，使用原子类的效率通常会比使用同步互斥锁的效率更高，因为原子类底层利用了 CAS 操作，不会阻塞线程。6 类原子类纵览下面我们来看下一共有哪些原子类，原子类一共可以分为以下这 6 类，我们来逐一介绍： 类型 具体类 Atomic* 基本类型原子类 AtomicInteger、AtomicLong、AtomicBoolean Atomic*Array 数组类型原子类 AtomicIntegerArray、AtomicLongArray、AtomicReferenceArray Atomic*Reference 引用类型原子类 AtomicReference、AtomicStampedReference、AtomicMarkableReference Atomic*FieldUpdater 升级类型原子类 AtomicIntegerfieldupdater、AtomicLongFieldUpdater、AtomicReferenceFieldUpdater Adder 累加器 LongAdder、DoubleAdde Accumulator 积累器 LongAccumulator、DoubleAccumulator Atomic\\ 基本类型原子类首先看到第一类 Atomic*，我们把它称为基本类型原子类，它包括三种，分别是 AtomicInteger、AtomicLong 和 AtomicBoolean。我们来介绍一下最为典型的 AtomicInteger。对于这个类型而言，它是对于 int 类型的封装，并且提供了原子性的访问和更新。也就是说，我们如果需要一个整型的变量，并且这个变量会被运用在并发场景之下，我们可以不用基本类型 int，也不使用包装类型 Integer，而是直接使用 AtomicInteger，这样一来就自动具备了原子能力，使用起来非常方便。AtomicInteger 类常用方法AtomicInteger 类有以下几个常用的方法： public final int get() //获取当前的值因为它本身是一个 Java 类，而不再是一个基本类型，所以要想获取值还是需要一些方法，比如通过 get 方法就可以获取到当前的值。 public final int getAndSet(int newValue) //获取当前的值，并设置新的值接下来的几个方法和它平时的操作相关： public final int getAndIncrement() //获取当前的值，并自增 public final int getAndDecrement() //获取当前的值，并自减 public final int getAndAdd(int delta) //获取当前的值，并加上预期的值这个参数就是我想让当前这个原子类改变多少值，可以是正数也可以是负数，如果是正数就是增加，如果是负数就是减少。而刚才的 getAndIncrement 和 getAndDecrement 修改的数值默认为 +1 或 -1，如果不能满足需求，我们就可以使用 getAndAdd 方法来直接一次性地加减我们想要的数值。 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值更新为输入值（update）这个方法也是 CAS 的一个重要体现。Array 数组类型原子类下面我们来看第二大类 AtomicArray 数组类型原子类，数组里的元素，都可以保证其原子性，比如 AtomicIntegerArray 相当于把 AtomicInteger 聚合起来，组合成一个数组。这样一来，我们如果想用一个每一个元素都具备原子性的数组的话， 就可以使用 AtomicArray。它一共分为 3 种，分别是： AtomicIntegerArray：整形数组原子类； AtomicLongArray：长整形数组原子类； AtomicReferenceArray ：引用类型数组原子类。Atomic\\Reference 引用类型原子类下面我们介绍第三种 AtomicReference 引用类型原子类。AtomicReference 类的作用和AtomicInteger 并没有本质区别， AtomicInteger 可以让一个整数保证原子性，而AtomicReference 可以让一个对象保证原子性。这样一来，AtomicReference 的能力明显比 AtomicInteger 强，因为一个对象里可以包含很多属性。在这个类别之下，除了 AtomicReference 之外，还有： AtomicStampedReference：它是对 AtomicReference 的升级，在此基础上还加了时间戳，用于解决 CAS 的 ABA 问题。 AtomicMarkableReference：和 AtomicReference 类似，多了一个绑定的布尔值，可以用于表示该对象已删除等场景。Atomic\\FieldUpdater 原子更新器第四类我们将要介绍的是 Atomic\\FieldUpdater，我们把它称为原子更新器，一共有三种，分别是。 AtomicIntegerFieldUpdater：原子更新整形的更新器； AtomicLongFieldUpdater：原子更新长整形的更新器； AtomicReferenceFieldUpdater：原子更新引用的更新器。如果我们之前已经有了一个变量，比如是整型的 int，实际它并不具备原子性。可是木已成舟，这个变量已经被定义好了，此时我们有没有办法可以让它拥有原子性呢？办法是有的，就是利用 Atomic*FieldUpdater，如果它是整型的，就使用 AtomicIntegerFieldUpdater 把已经声明的变量进行升级，这样一来这个变量就拥有了 CAS 操作的能力。这里的非互斥同步手段，是把我们已经声明好的变量进行 CAS 操作以达到同步的目的。那么你可能会想，既然想让这个变量具备原子性，为什么不在一开始就声明为 AtomicInteger？这样也免去了升级的过程，难道是一开始设计的时候不合理吗？这里有以下几种情况：第一种情况是出于历史原因考虑，那么如果出于历史原因的话，之前这个变量已经被声明过了而且被广泛运用，那么修改它成本很高，所以我们可以利用升级的原子类。另外还有一个使用场景，如果我们在大部分情况下并不需要使用到它的原子性，只在少数情况，比如每天只有定时一两次需要原子操作的话，我们其实没有必要把原来的变量声明为原子类型的变量，因为 AtomicInteger 比普通的变量更加耗费资源。所以如果我们有成千上万个原子类的实例的话，它占用的内存也会远比我们成千上万个普通类型占用的内存高。所以在这种情况下，我们可以利用 AtomicIntegerFieldUpdater 进行合理升级，节约内存。下面我们看一段代码：public class AtomicIntegerFieldUpdaterDemo implements Runnable{   static Score math;   static Score computer;   public static AtomicIntegerFieldUpdater&amp;lt;Score&amp;gt; scoreUpdater = AtomicIntegerFieldUpdater.newUpdater(Score.class, &quot;score&quot;);   @Override   public void run() {       for (int i = 0; i &amp;lt; 1000; i++) {           computer.score++;           scoreUpdater.getAndIncrement(math);       }   }   public static class Score {       volatile int score;   }    public static void main(String[] args) throws InterruptedException {       math =new Score();       computer =new Score();       AtomicIntegerFieldUpdaterDemo2 r = new AtomicIntegerFieldUpdaterDemo2();       Thread t1 = new Thread(r);       Thread t2 = new Thread(r);       t1.start();       t2.start();       t1.join();       t2.join();       System.out.println(&quot;普通变量的结果：&quot;+ computer.score);       System.out.println(&quot;升级后的结果：&quot;+ math.score);   }}这段代码就演示了这个类的用法，比如说我们有两个类，它们都是 Score 类型的，Score 类型内部会有一个分数，也叫作 core，那么这两个分数的实例分别叫作数学 math 和计算机 computer，然后我们还声明了一个 AtomicIntegerFieldUpdater，在它构造的时候传入了两个参数，第一个是 Score.class，这是我们的类名，第二个是属性名，叫作 score。接下来我们看一下 run 方法，run 方法里面会对这两个实例分别进行自加操作。第一个是 computer，这里的 computer 我们调用的是它内部的 score，也就是说我们直接调用了 int 变量的自加操作，这在多线程下是线程非安全的。第二个自加是利用了刚才声明的 scoreUpdater 并且使用了它的 getAndIncrement 方法并且传入了 math，这是一种正确使用AtomicIntegerFieldUpdater 的用法，这样可以线程安全地进行自加操作。接下来我们看下 main 函数。在 main 函数中，我们首先把 math 和 computer 定义了出来，然后分别启动了两个线程，每个线程都去执行我们刚才所介绍过的 run 方法。这样一来，两个 score，也就是 math 和 computer 都会分别被加 2000 次，最后我们在 join 等待之后把结果打印了出来，这个程序的运行结果如下：普通变量的结果：1942升级后的结果：2000可以看出，正如我们所预料的那样，普通变量由于不具备线程安全性，所以在多线程操作的情况下，它虽然看似进行了 2000 次操作，但有一些操作被冲突抵消了，所以最终结果小于 2000。可是使用 AtomicIntegerFieldUpdater 这个工具之后，就可以做到把一个普通类型的 score 变量进行原子的自加操作，最后的结果也和加的次数是一样的，也就是 2000。可以看出，这个类的功能还是非常强大的。下面我们继续看最后两种原子类。Adder 加法器它里面有两种加法器，分别叫作 LongAdder 和 DoubleAdder。Accumulator 积累器最后一种叫 Accumulator 积累器，分别是 LongAccumulator 和 DoubleAccumulator。这两种原子类我们会在后面的课时中展开介绍。以 AtomicInteger 为例，分析在 Java 中如何利用 CAS 实现原子操作？让我们回到标题中的问题，在充分了解了原子类的作用和种类之后，我们来看下 AtomicInteger 是如何通过 CAS 操作实现并发下的累加操作的，以其中一个重要方法 getAndAdd 方法为突破口。getAndAdd方法这个方法的代码在 Java 1.8 中的实现如下：//JDK 1.8实现public final int getAndAdd(int delta) {   return unsafe.getAndAddInt(this, valueOffset, delta);}可以看出，里面使用了 Unsafe 这个类，并且调用了 unsafe.getAndAddInt 方法。所以这里需要简要介绍一下 Unsafe 类。Unsafe 类Unsafe 类主要是用于和操作系统打交道的，因为大部分的 Java 代码自身无法直接操作内存，所以在必要的时候，可以利用 Unsafe 类来和操作系统进行交互，CAS 正是利用到了 Unsafe 类。那么我们就来看一下 AtomicInteger 的一些重要代码，如下所示：public class AtomicInteger extends Number implements java.io.Serializable {   // setup to use Unsafe.compareAndSwapInt for updates   private static final Unsafe unsafe = Unsafe.getUnsafe();   private static final long valueOffset;    static {       try {           valueOffset = unsafe.objectFieldOffset               (AtomicInteger.class.getDeclaredField(&quot;value&quot;));       } catch (Exception ex) { throw new Error(ex); }   }    private volatile int value;   public final int get() {return value;}   ...}可以看出，在数据定义的部分，首先还获取了 Unsafe 实例，并且定义了 valueOffset。我们往下看到 static 代码块，这个代码块会在类加载的时候执行，执行时我们会调用 Unsafe 的 objectFieldOffset 方法，从而得到当前这个原子类的 value 的偏移量，并且赋给 valueOffset 变量，这样一来我们就获取到了 value 的偏移量，它的含义是在内存中的偏移地址，因为 Unsafe 就是根据内存偏移地址获取数据的原值的，这样我们就能通过 Unsafe 来实现 CAS 了。value 是用 volatile 修饰的，它就是我们原子类存储的值的变量，由于它被 volatile 修饰，我们就可以保证在多线程之间看到的 value 是同一份，保证了可见性。接下来继续看 Unsafe 的 getAndAddInt 方法的实现，代码如下：public final int getAndAddInt(Object var1, long var2, int var4) {   int var5;   do {       var5 = this.getIntVolatile(var1, var2);   } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));   return var5;}首先我们看一下结构，它是一个 do-while 循环，所以这是一个死循环，直到满足循环的退出条件时才可以退出。那么我们来看一下 do 后面的这一行代码 var5 = this.getIntVolatile(var1, var2) 是什么意思。这是个 native 方法，作用就是获取在 var1 中的 var2 偏移处的值。那传入的是什么呢？传入的两个参数，第一个就是当前原子类，第二个是我们最开始获取到的 offset，这样一来我们就可以获取到当前内存中偏移量的值，并且保存到 var5 里面。此时 var5 实际上代表当前时刻下的原子类的数值。现在再来看 while 的退出条件，也就是 compareAndSwapInt 这个方法，它一共传入了 4 个参数，这 4 个参数是 var1、var2、var5、var5 + var4，为了方便理解，我们给它们取了新了变量名，分别 object、offset、expectedValue、newValue，具体含义如下： 第一个参数 object 就是将要操作的对象，传入的是 this，也就是 atomicInteger 这个对象本身； 第二个参数是 offset，也就是偏移量，借助它就可以获取到 value 的数值； 第三个参数 expectedValue，代表“期望值”，传入的是刚才获取到的 var5； 而最后一个参数 newValue 是希望修改的数值 ，等于之前取到的数值 var5 再加上 var4，而 var4 就是我们之前所传入的 delta，delta 就是我们希望原子类所改变的数值，比如可以传入 +1，也可以传入 -1。所以 compareAndSwapInt 方法的作用就是，判断如果现在原子类里 value 的值和之前获取到的 var5 相等的话，那么就把计算出来的 var5 + var4 给更新上去，所以说这行代码就实现了 CAS 的过程。一旦 CAS 操作成功，就会退出这个 while 循环，但是也有可能操作失败。如果操作失败就意味着在获取到 var5 之后，并且在 CAS 操作之前，value 的数值已经发生变化了，证明有其他线程修改过这个变量。这样一来，就会再次执行循环体里面的代码，重新获取 var5 的值，也就是获取最新的原子变量的数值，并且再次利用 CAS 去尝试更新，直到更新成功为止，所以这是一个死循环。我们总结一下，Unsafe 的 getAndAddInt 方法是通过循环 + CAS 的方式来实现的，在此过程中，它会通过 compareAndSwapInt 方法来尝试更新 value 的值，如果更新失败就重新获取，然后再次尝试更新，直到更新成功。总结在本课时我们首先介绍了原子类的作用，然后对 6 类原子类进行了介绍，分别是 Atomic* 基本类型原子类、AtomicArray 数组类型原子类、AtomicReference 引用类型原子类、Atomic*FieldUpdater 升级类型原子类、Adder 加法器和 Accumulator 积累器。然后我们对它们逐一进行了展开介绍，了解了它们的基本作用和用法，接下来我们以 AtomicInteger 为例，分析了在 Java 中是如何利用 CAS 实现原子操作的。我们从 getAndAdd 方法出发，逐步深入，最后到了 Unsafe 的 getAndAddInt 方法。所以通过源码分析之后，我们也清楚地看到了，它实现的原理是利用自旋去不停地尝试，直到成功为止。 参考：占小狼https://www.jianshu.com/p/fb6e91b013cc" }, { "title": "选择合适的阻塞队列", "url": "/posts/blocking-queue-5/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-25 15:33:00 +0000", "snippet": "他山之石，可以攻玉。对于如何选择最合适的阻塞队列这个问题，实际上线程池已经率先给我们做了表率。线程池有很多种，不同种类的线程池会根据自己的特点，来选择适合自己的阻塞队列。所以我们就首先来复习一下这些非常经典的线程池是如何挑选阻塞队列的，借鉴它们的经验之后，我们再去总结一套规则，来归纳出自己在选取阻塞队列时可以对哪些点进行考虑。线程池对于阻塞队列的选择下面我们来看线程池的选择要诀。上面表格左侧是线程池，右侧为它们对应的阻塞队列，你可以看到 5 种线程池只对应了 3 种阻塞队列，下面我们对它们进行逐一的介绍。 FixedThreadPool（SingleThreadExecutor 同理）选取的是 LinkedBlockingQueue因为 LinkedBlockingQueue 不同于 ArrayBlockingQueue，ArrayBlockingQueue 的容量是有限的，而 LinkedBlockingQueue 是链表长度默认是可以无限延长的。由于 FixedThreadPool 的线程数是固定的，在任务激增的时候，它无法增加更多的线程来帮忙处理 Task，所以需要像 LinkedBlockingQueue 这样没有容量上限的 Queue 来存储那些还没处理的 Task。如果所有的 corePoolSize 线程都正在忙，那么新任务将会进入阻塞队列等待，由于队列是没有容量上限的，队列永远不会被填满，这样就保证了对于线程池 FixedThreadPool 和 SingleThreadExecutor 而言，不会拒绝新任务的提交，也不会丢失数据。 CachedThreadPool 选取的是 SynchronousQueue对于 CachedThreadPool 而言，为了避免新提交的任务被拒绝，它选择了无限制的 maximumPoolSize（在专栏中，maxPoolSize 等同于 maximumPoolSize），所以既然它的线程的最大数量是无限的，也就意味着它的线程数不会受到限制，那么它就不需要一个额外的空间来存储那些 Task，因为每个任务都可以通过新建线程来处理。SynchronousQueue 会直接把任务交给线程，而不需要另外保存它们，效率更高，所以 CachedThreadPool 使用的 Queue 是 SynchronousQueue。 ScheduledThreadPool（SingleThreadScheduledExecutor同理）选取的是延迟队列对于 ScheduledThreadPool 而言，它使用的是 DelayedWorkQueue。延迟队列的特点是：不是先进先出，而是会按照延迟时间的长短来排序，下一个即将执行的任务会排到队列的最前面。我们来举个例子：例如我们往这个队列中，放一个延迟 10 分钟执行的任务，然后再放一个延迟 10 秒钟执行的任务。通常而言，如果不是延迟队列，那么按照先进先出的排列规则，也就是延迟 10 分钟执行的那个任务是第一个放置的，会放在最前面。但是由于我们此时使用的是阻塞队列，阻塞队列在排放各个任务的位置的时候，会根据延迟时间的长短来排放。所以，我们第二个放置的延迟 10 秒钟执行的那个任务，反而会排在延迟 10 分钟的任务的前面，因为它的执行时间更早。我们选择使用延迟队列的原因是，ScheduledThreadPool 处理的是基于时间而执行的 Task，而延迟队列有能力把 Task 按照执行时间的先后进行排序，这正是我们所需要的功能。ArrayBlockingQueue除了线程池选择的 3 种阻塞队列外，还有一种常用的阻塞队列叫作 ArrayBlockingQueue，它也经常被用于我们手动创建的线程池中。这种阻塞队列内部是用数组实现的，在新建对象的时候要求传入容量值，且后期不能扩容，所以 ArrayBlockingQueue的最大特点就是容量是有限且固定的。这样一来，使用 ArrayBlockingQueue 且设置了合理大小的最大线程数的线程池，在任务队列放满了以后，如果线程数也已经达到了最大值，那么线程池根据规则就会拒绝新提交的任务，而不会无限增加任务或者线程数导致内存不足，可以非常有效地防止资源耗尽的情况发生。归纳下面让我们总结一下经验，通常我们可以从以下 5 个角度考虑，来选择合适的阻塞队列： 功能第 1 个需要考虑的就是功能层面，比如是否需要阻塞队列帮我们排序，如优先级排序、延迟执行等。如果有这个需要，我们就必须选择类似于 PriorityBlockingQueue 之类的有排序能力的阻塞队列。 容量第 2 个需要考虑的是容量，或者说是否有存储的要求，还是只需要“直接传递”。在考虑这一点的时候，我们知道前面介绍的那几种阻塞队列，有的是容量固定的，如 ArrayBlockingQueue；有的默认是容量无限的，如 LinkedBlockingQueue；而有的里面没有任何容量，如 SynchronousQueue；而对于 DelayQueue 而言，它的容量固定就是 Integer.MAX_VALUE。所以不同阻塞队列的容量是千差万别的，我们需要根据任务数量来推算出合适的容量，从而去选取合适的 BlockingQueue。 能否扩容第 3 个需要考虑的是能否扩容。因为有时我们并不能在初始的时候很好的准确估计队列的大小，因为业务可能有高峰期、低谷期。如果一开始就固定一个容量，可能无法应对所有的情况，也是不合适的，有可能需要动态扩容。如果我们需要动态扩容的话，那么就不能选择 ArrayBlockingQueue ，因为它的容量在创建时就确定了，无法扩容。相反，PriorityBlockingQueue 即使在指定了初始容量之后，后续如果有需要，也可以自动扩容。所以我们可以根据是否需要扩容来选取合适的队列。 内存结构第 4 个需要考虑的点就是内存结构。在上一课时我们分析过 ArrayBlockingQueue 的源码，看到了它的内部结构是“数组”的形式。和它不同的是，LinkedBlockingQueue 的内部是用链表实现的，所以这里就需要我们考虑到，ArrayBlockingQueue 没有链表所需要的“节点”，空间利用率更高。所以如果我们对性能有要求可以从内存的结构角度去考虑这个问题。 性能第 5 点就是从性能的角度去考虑。比如 LinkedBlockingQueue 由于拥有两把锁，它的操作粒度更细，在并发程度高的时候，相对于只有一把锁的 ArrayBlockingQueue 性能会更好。另外，SynchronousQueue 性能往往优于其他实现，因为它只需要“直接传递”，而不需要存储的过程。如果我们的场景需要直接传递的话，可以优先考虑 SynchronousQueue。" }, { "title": "阻塞队列和非阻塞队列的并发安全原理", "url": "/posts/blocking-queue-4/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-24 15:33:00 +0000", "snippet": "以 ArrayBlockingQueue 为例，首先分析 BlockingQueue 即阻塞队列的线程安全原理，然后再看看它的兄弟——非阻塞队列的并发安全原理。通过本课时的学习，我们就可以了解到关于并发队列的底层原理了。ArrayBlockingQueue 源码分析ArrayBlockingQueue 的源码，ArrayBlockingQueue 有以下几个重要的属性：final Object[] items;int takeIndex;int putIndex;int count;第一个就是最核心的、用于存储元素的 Object 类型的数组；然后它还会有两个位置变量，分别是 takeIndex 和 putIndex，这两个变量就是用来标明下一次读取和写入位置的；另外还有一个 count 用来计数，它所记录的就是队列中的元素个数。另外，我们再来看下面这三个变量：final ReentrantLock lock;private final Condition notEmpty;private final Condition notFull;这三个变量也非常关键，第一个就是一个 ReentrantLock，而下面两个 Condition 分别是由 ReentrantLock 产生出来的，这三个变量就是我们实现线程安全最核心的工具。ArrayBlockingQueue 正是利用了 ReentrantLock 和它的两个 Condition 实现的并发安全，真正执行在读写操作前，都需要先获取到锁才行。下面，我们来分析一下最重要的 put 方法：public void put(E e) throws InterruptedException {    checkNotNull(e);    final ReentrantLock lock = this.lock;    lock.lockInterruptibly();    try {        while (count == items.length)        notFull.await();        enqueue(e);    } finally {        lock.unlock();    }}在 put 方法中，首先用 checkNotNull 方法去检查插入的元素是不是 null。如果不是 null，我们会用 ReentrantLock 上锁，并且上锁方法是 lock.lockInterruptibly()。这个方法我们在第 23 课时的时候讲过，在获取锁的同时是可以响应中断的，这也正是我们的阻塞队列在调用 put 方法时，在尝试获取锁但还没拿到锁的期间可以响应中断的底层原因。紧接着 ，是一个非常经典的 try finally 代码块，finally 中会去解锁，try 中会有一个 while 循环，它会检查当前队列是不是已经满了，也就是 count 是否等于数组的长度。如果等于就代表已经满了，于是我们便会进行等待，直到有空余的时候，我们才会执行下一步操作，调用 enqueue 方法让元素进入队列，最后用 unlock 方法解锁。你看到这段代码不知道是否眼熟，在第 5 课时我们讲过，用 Condition 实现生产者/消费者模式的时候，写过一个 put 方法，代码如下：public void put(Object o) throws InterruptedException {    lock.lock();    try {        while (queue.size() == max) {        notFull.await();    }    queue.add(o);    notEmpty.signalAll();    } finally {        lock.unlock();    }}可以看出，这两个方法几乎是一模一样的，所以当时在第 5 课时的时候我们就说过，我们自己用 Condition 实现生产者/消费者模式，实际上其本质就是自己实现了简易版的 BlockingQueue。你可以对比一下这两个 put 方法的实现，这样对 Condition 的理解就会更加深刻。和 ArrayBlockingQueue 类似，其他各种阻塞队列如 LinkedBlockingQueue、PriorityBlockingQueue、DelayQueue、DelayedWorkQueue 等一系列 BlockingQueue 的内部也是利用了 ReentrantLock 来保证线程安全，只不过细节有差异，比如 LinkedBlockingQueue 的内部有两把锁，分别锁住队列的头和尾，比共用同一把锁的效率更高，不过总体思想都是类似的。非阻塞队列 ConcurrentLinkedQueue看完阻塞队列之后，我们就来看看非阻塞队列 ConcurrentLinkedQueue。顾名思义，ConcurrentLinkedQueue 是使用链表作为其数据结构的，我们来看一下关键方法 offer 的源码：public boolean offer(E e) {    checkNotNull(e);    final Node&amp;lt;E&amp;gt; newNode = new Node&amp;lt;E&amp;gt;(e);    for (Node&amp;lt;E&amp;gt; t = tail, p = t;;) {        Node&amp;lt;E&amp;gt; q = p.next;        if (q == null) {            // p is last node            if (p.casNext(null, newNode)) {                // Successful CAS is the linearization point                // for e to become an element of this queue,                // and for newNode to become &quot;live&quot;.                if (p != t) // hop two nodes at a time                    casTail(t, newNode);  // Failure is OK.                return true;            }            // Lost CAS race to another thread; re-read next        } else if (p == q)            // We have fallen off list.  If tail is unchanged, it            // will also be off-list, in which case we need to            // jump to head, from which all live nodes are always            // reachable.  Else the new tail is a better bet.            p = (t != (t = tail)) ? t : head;        else            // Check for tail updates after two hops.            p = (p != t &amp;amp;&amp;amp; t != (t = tail)) ? t : q;    }}在这里我们不去一行一行分析具体的内容，而是把目光放到整体的代码结构上，在检查完空判断之后，可以看到它整个是一个大的 for 循环，而且是一个非常明显的死循环。在这个循环中有一个非常亮眼的 p.casNext 方法，这个方法正是利用了 CAS 来操作的，而且这个死循环去配合 CAS 也就是典型的乐观锁的思想。我们就来看一下 p.casNext 方法的具体实现，其方法代码如下：boolean casNext(Node&amp;lt;E&amp;gt; cmp, Node&amp;lt;E&amp;gt; val) {    return UNSAFE.compareAndSwapObject(this, nextOffset, cmp, val);}可以看出这里运用了 UNSAFE.compareAndSwapObject 方法来完成 CAS 操作，而 compareAndSwapObject 是一个 native 方法，最终会利用 CPU 的 CAS 指令保证其不可中断。可以看出，非阻塞队列 ConcurrentLinkedQueue 使用 CAS 非阻塞算法 + 不停重试，来实现线程安全，适合用在不需要阻塞功能，且并发不是特别剧烈的场景。总结中阻塞队列最主要是利用了 ReentrantLock 以及它的 Condition 来实现，而非阻塞队列则是利用 CAS 方法实现线程安全。参考：https://javadoop.com/post/java-concurrent-queue" }, { "title": "常用的阻塞队列", "url": "/posts/blocking-queue-3/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-23 15:33:00 +0000", "snippet": "BlockingQueue 接口的实现类都被放在了 J.U.C 包中，包括 ArrayBlockingQueue LinkedBlockingQueue SynchronousQueue PriorityBlockingQueue DelayQueue。ArrayBlockingQueueArrayBlockingQueue 是最典型的有界队列，其内部是用数组存储元素的，利用 ReentrantLock 实现线程安全。我们在创建它的时候就需要指定它的容量，之后也不可以再扩容了，在构造函数中我们同样可以指定是否是公平的，代码如下：ArrayBlockingQueue(int capacity, boolean fair)第一个参数是容量，第二个参数是是否公平。正如 ReentrantLock 一样，如果 ArrayBlockingQueue 被设置为非公平的，那么就存在插队的可能；如果设置为公平的，那么等待了最长时间的线程会被优先处理，其他线程不允许插队，不过这样的公平策略同时会带来一定的性能损耗，因为非公平的吞吐量通常会高于公平的情况。LinkedBlockingQueue正如名字所示，这是一个内部用链表实现的 BlockingQueue。如果我们不指定它的初始容量，那么它容量默认就为整型的最大值 Integer.MAX_VALUE，由于这个数非常大，我们通常不可能放入这么多的数据，所以 LinkedBlockingQueue 也被称作无界队列，代表它几乎没有界限。SynchronousQueue如图所示，SynchronousQueue 最大的不同之处在于，它的容量为 0，所以没有一个地方来暂存元素，导致每次取数据都要先阻塞，直到有数据被放入；同理，每次放数据的时候也会阻塞，直到有消费者来取。需要注意的是，SynchronousQueue 的容量不是 1 而是 0，因为 SynchronousQueue 不需要去持有元素，它所做的就是直接传递（direct handoff）。由于每当需要传递的时候，SynchronousQueue 会把元素直接从生产者传给消费者，在此期间并不需要做存储，所以如果运用得当，它的效率是很高的。另外，由于它的容量为 0，所以相比于一般的阻塞队列，SynchronousQueue 的很多方法的实现是很有意思的，我们来举几个例子：SynchronousQueue 的 peek 方法永远返回 null，代码如下：public E peek() {    return null;}因为 peek 方法的含义是取出头结点，但是 SynchronousQueue 的容量是 0，所以连头结点都没有，peek 方法也就没有意义，所以始终返回 null。同理，element 始终会抛出 NoSuchElementException 异常。而 SynchronousQueue 的 size 方法始终返回 0，因为它内部并没有容量，代码如下：public int size() {    return 0;}直接 return 0，同理，isEmpty 方法始终返回 true：public boolean isEmpty() {    return true;}因为它始终都是空的。PriorityBlockingQueue前面我们所说的 ArrayBlockingQueue 和 LinkedBlockingQueue 都是采用先进先出的顺序进行排序，可是如果有的时候我们需要自定义排序怎么办呢？这时就需要使用 PriorityBlockingQueue。PriorityBlockingQueue 是一个支持优先级的无界阻塞队列，可以通过自定义类实现 compareTo() 方法来指定元素排序规则，或者初始化时通过构造器参数 Comparator 来指定排序规则。同时，插入队列的对象必须是可比较大小的，也就是 Comparable 的，否则会抛出 ClassCastException 异常。它的 take 方法在队列为空的时候会阻塞，但是正因为它是无界队列，而且会自动扩容，所以它的队列永远不会满，所以它的 put 方法永远不会阻塞，添加操作始终都会成功，也正因为如此，它的成员变量里只有一个 Condition：private final Condition notEmpty;这和之前的 ArrayBlockingQueue 拥有两个 Condition（分别是 notEmpty 和 notFull）形成了鲜明的对比，我们的 PriorityBlockingQueue 不需要 notFull，因为它永远都不会满，真是“有空间就可以任性”。DelayQueueDelayQueue 这个队列比较特殊，具有“延迟”的功能。我们可以设定让队列中的任务延迟多久之后执行，比如 10 秒钟之后执行，这在例如“30 分钟后未付款自动取消订单”等需要延迟执行的场景中被大量使用。它是无界队列，放入的元素必须实现 Delayed 接口，而 Delayed 接口又继承了 Comparable 接口，所以自然就拥有了比较和排序的能力，代码如下：public interface Delayed extends Comparable&amp;lt;Delayed&amp;gt; {    long getDelay(TimeUnit unit);}可以看出这个 Delayed 接口继承自 Comparable，里面有一个需要实现的方法，就是 getDelay。这里的 getDelay 方法返回的是“还剩下多长的延迟时间才会被执行”，如果返回 0 或者负数则代表任务已过期。元素会根据延迟时间的长短被放到队列的不同位置，越靠近队列头代表越早过期。DelayQueue 内部使用了 PriorityQueue 的能力来进行排序，而不是自己从头编写，我们在工作中可以学习这种思想，对已有的功能进行复用，不但可以减少开发量，同时避免了“重复造轮子”，更重要的是，对学到的知识进行合理的运用，让知识变得更灵活，做到触类旁通。总结以上就是本课时的内容，我们对于 ArrayBlockingQueue、LinkedBlockingQueue、SynchronousQueue、PriorityBlockingQueue 以及 DelayQueue 这些常见的和常用的阻塞队列的特点进行了讲解。" }, { "title": "阻塞队列的常用方法以及区别", "url": "/posts/blocking-queue-2/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-22 15:33:00 +0000", "snippet": "在阻塞队列中有很多方法，而且它们都非常相似，所以非常有必要对这些类似的方法进行辨析。把 BlockingQueue 中最常用的和添加、删除相关的 8 个方法列出来，并且把它们分为三组，每组方法都和添加、移除元素相关。这三组方法由于功能很类似，所以比较容易混淆。它们的区别仅在于特殊情况：当队列满了无法添加元素，或者是队列空了无法移除元素时，不同组的方法对于这种特殊情况会有不同的处理方式： 抛出异常：add、remove、element 返回结果但不抛出异常：offer、poll、peek 阻塞：put、take第一组：add、remove、elementadd 方法add 方法是往队列里添加一个元素，如果队列满了，就会抛出异常来提示队列已满。示例代码如下：private static void addTest() {    BlockingQueue&amp;lt;Integer&amp;gt; blockingQueue = new                     ArrayBlockingQueue&amp;lt;Integer&amp;gt;(2);    blockingQueue.add(1);    blockingQueue.add(1);    blockingQueue.add(1);}在这段代码中，创建了一个容量为 2 的 BlockingQueue，并且尝试往里面放 3 个值，超过了容量上限，那么在添加第三个值的时候就会得到异常：Exception in thread &quot;main&quot; java.lang.IllegalStateException:Queue fullremove 方法remove 方法的作用是删除元素，如果我们删除的队列是空的，由于里面什么都没有，所以也无法删除任何元素，那么 remove 方法就会抛出异常。示例代码如下：private static void removeTest() {    ArrayBlockingQueue&amp;lt;Integer&amp;gt; blockingQueue = new     ArrayBlockingQueue&amp;lt;Integer&amp;gt;(2);    blockingQueue.add(1);    blockingQueue.add(1);    blockingQueue.remove();    blockingQueue.remove();    blockingQueue.remove();}在这段代码中，我们往一个容量为 2 的 BlockingQueue 里放入 2 个元素，并且删除 3 个元素。在删除前面两个元素的时候会正常执行，因为里面依然有元素存在，但是在删除第三个元素时，由于队列里面已经空了，所以便会抛出异常：Exception in thread &quot;main&quot; java.util.NoSuchElementExceptionelement 方法element 方法是返回队列的头部节点，但是并不删除。和 remove 方法一样，如果我们用这个方法去操作一个空队列，想获取队列的头结点，可是由于队列是空的，我们什么都获取不到，会抛出和前面 remove 方法一样的异常：NoSuchElementException。示例代码如下：private static void elementTest() {    ArrayBlockingQueue&amp;lt;Integer&amp;gt; blockingQueue = new     ArrayBlockingQueue&amp;lt;Integer&amp;gt;(2);    blockingQueue.element();}我们新建了一个容量为 2 的 ArrayBlockingQueue，直接调用 element 方法，由于之前没有往里面添加元素，默认为空，那么会得到异常：Exception in thread &quot;main&quot; java.util.NoSuchElementException第二组：offer、poll、peek实际上我们通常并不想看到第一组方法抛出的异常，这时我们可以优先采用第二组方法。第二组方法相比于第一组而言要友好一些，当发现队列满了无法添加，或者队列为空无法删除的时候，第二组方法会给一个提示，而不是抛出一个异常。offer 方法offer 方法用来插入一个元素，并用返回值来提示插入是否成功。如果添加成功会返回 true，而如果队列已经满了，此时继续调用 offer 方法的话，它不会抛出异常，只会返回一个错误提示：false。示例代码如下：private static void offerTest() {    ArrayBlockingQueue&amp;lt;Integer&amp;gt; blockingQueue = new ArrayBlockingQueue&amp;lt;Integer&amp;gt;(2);    System.out.println(blockingQueue.offer(1));    System.out.println(blockingQueue.offer(1));    System.out.println(blockingQueue.offer(1));}我们创建了一个容量为 2 的 ArrayBlockingQueue，并且调用了三次 offer方法尝试添加，每次都把返回值打印出来，运行结果如下：truetruefalse可以看出，前面两次添加成功了，但是第三次添加的时候，已经超过了队列的最大容量，所以会返回 false，表明添加失败。poll 方法poll 方法和第一组的 remove 方法是对应的，作用也是移除并返回队列的头节点。但是如果当队列里面是空的，没有任何东西可以移除的时候，便会返回 null 作为提示。正因如此，我们是不允许往队列中插入 null 的，否则我们没有办法区分返回的 null 是一个提示还是一个真正的元素。示例代码如下：private static void pollTest() {    ArrayBlockingQueue&amp;lt;Integer&amp;gt; blockingQueue = new ArrayBlockingQueue&amp;lt;Integer&amp;gt;(3);    blockingQueue.offer(1);    blockingQueue.offer(2);    blockingQueue.offer(3);    System.out.println(blockingQueue.poll());    System.out.println(blockingQueue.poll());    System.out.println(blockingQueue.poll());    System.out.println(blockingQueue.poll());}在这个代码中我们创建了一个容量为 3 的 ArrayBlockingQueue，并且先往里面放入 3 个元素，然后四次调用 poll 方法，运行结果如下：123null前面三次 poll 都运行成功了，并且返回了元素内容 1、2、3，是先进先出的顺序。第四次的 poll 方法返回 null，代表此时已经没有元素可以移除了。peek 方法peek 方法和第一组的 element 方法是对应的，意思是返回队列的头元素但并不删除。如果队列里面是空的，它便会返回 null 作为提示。示例代码如下：private static void peekTest() {    ArrayBlockingQueue&amp;lt;Integer&amp;gt; blockingQueue = new ArrayBlockingQueue&amp;lt;Integer&amp;gt;(2);    System.out.println(blockingQueue.peek());}运行结果：null新建了一个空的 ArrayBlockingQueue，然后直接调用 peek，返回结果 null，代表此时并没有东西可以取出。带超时时间的 offer 和 poll第二组还有一些额外值得讲解的内容，offer 和 poll 都有带超时时间的重载方法。offer(E e, long timeout, TimeUnit unit)它有三个参数，分别是元素、超时时长和时间单位。通常情况下，这个方法会插入成功并返回 true；如果队列满了导致插入不成功，在调用带超时时间重载方法的 offer 的时候，则会等待指定的超时时间，如果时间到了依然没有插入成功，就会返回 false。poll(long timeout, TimeUnit unit)带时间参数的 poll 方法和 offer 类似：如果能够移除，便会立刻返回这个节点的内容；如果队列是空的就会进行等待，等待时间正是我们指定的时间，直到超时时间到了，如果队列里依然没有元素可供移除，便会返回 null 作为提示。第三组：put、take第三组是我们比较熟悉的、阻塞队列最大特色的 put 和 take 方法，我们复习一下 34 课时里对于 put 和 take 方法的讲解。put 方法put 方法的作用是插入元素。通常在队列没满的时候是正常的插入，但是如果队列已满就无法继续插入，这时它既不会立刻返回 false 也不会抛出异常，而是让插入的线程陷入阻塞状态，直到队列里有了空闲空间，此时队列就会让之前的线程解除阻塞状态，并把刚才那个元素添加进去。take 方法take 方法的作用是获取并移除队列的头结点。通常在队列里有数据的时候会正常取出数据并删除；但是如果执行 take 的时候队列里无数据，则阻塞，直到队列里有数据；一旦队列里有数据了，就会立刻解除阻塞状态，并且取到数据。总结" }, { "title": "阻塞队列", "url": "/posts/blocking-queue-1/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-21 15:33:00 +0000", "snippet": "阻塞队列的作用阻塞队列，也就是 BlockingQueue，它是一个接口，如代码所示：public interface BlockingQueue&amp;lt;E&amp;gt; extends Queue&amp;lt;E&amp;gt;{...}BlockingQueue 继承了 Queue 接口，是队列的一种。Queue 和 BlockingQueue 都是在 Java 5 中加入的。BlockingQueue 是线程安全的，我们在很多场景下都可以利用线程安全的队列来优雅地解决我们业务自身的线程安全问题。比如说，使用生产者/消费者模式的时候，我们生产者只需要往队列里添加元素，而消费者只需要从队列里取出它们就可以了，如图所示：在图中，左侧有三个生产者线程，它会把生产出来的结果放到中间的阻塞队列中，而右侧的三个消费者也会从阻塞队列中取出它所需要的内容并进行处理。因为阻塞队列是线程安全的，所以生产者和消费者都可以是多线程的，不会发生线程安全问题。既然队列本身是线程安全的，队列可以安全地从一个线程向另外一个线程传递数据，所以我们的生产者/消费者直接使用线程安全的队列就可以，而不需要自己去考虑更多的线程安全问题。这也就意味着，考虑锁等线程安全问题的重任从“你”转移到了“队列”上，降低了我们开发的难度和工作量。同时，队列它还能起到一个隔离的作用。比如说我们开发一个银行转账的程序，那么生产者线程不需要关心具体的转账逻辑，只需要把转账任务，如账户和金额等信息放到队列中就可以，而不需要去关心银行这个类如何实现具体的转账业务。而作为银行这个类来讲，它会去从队列里取出来将要执行的具体的任务，再去通过自己的各种方法来完成本次转账。这样就实现了具体任务与执行任务类之间的解耦，任务被放在了阻塞队列中，而负责放任务的线程是无法直接访问到我们银行具体实现转账操作的对象的，实现了隔离，提高了安全性。主要并发队列关系图上图展示了 Queue 最主要的实现类，可以看出 Java 提供的线程安全的队列（也称为并发队列）分为阻塞队列和非阻塞队列两大类。阻塞队列的典型例子就是 BlockingQueue 接口的实现类，BlockingQueue 下面有 6 种最主要的实现，分别是 ArrayBlockingQueue、LinkedBlockingQueue、SynchronousQueue、DelayQueue、PriorityBlockingQueue 和 LinkedTransferQueue，它们各自有不同的特点，对于这些常见的阻塞队列的特点，我们会在第 36 课时中展开说明。非阻塞并发队列的典型例子是 ConcurrentLinkedQueue，这个类不会让线程阻塞，利用 CAS 保证了线程安全。我们可以根据需要自由选取阻塞队列或者非阻塞队列来满足业务需求。还有一个和 Queue 关系紧密的 Deque 接口，它继承了 Queue，如代码所示：public interface Deque&amp;lt;E&amp;gt; extends Queue&amp;lt;E&amp;gt; {//...}Deque 的意思是双端队列，音标是 [dek]，是 double-ended-queue 的缩写，它从头和尾都能添加和删除元素；而普通的 Queue 只能从一端进入，另一端出去。这是 Deque 和 Queue 的不同之处，Deque 其他方面的性质都和 Queue 类似。阻塞队列的特点阻塞队列区别于其他类型的队列的最主要的特点就是“阻塞”这两个字，所以下面重点介绍阻塞功能：阻塞功能使得生产者和消费者两端的能力得以平衡，当有任何一端速度过快时，阻塞队列便会把过快的速度给降下来。实现阻塞最重要的两个方法是 take 方法和 put 方法。take 方法take 方法的功能是获取并移除队列的头结点，通常在队列里有数据的时候是可以正常移除的。可是一旦执行 take 方法的时候，队列里无数据，则阻塞，直到队列里有数据。一旦队列里有数据了，就会立刻解除阻塞状态，并且取到数据。过程如图所示：put 方法以上过程中的阻塞和解除阻塞，都是 BlockingQueue 完成的，不需要我们自己处理。是否有界（容量有多大）此外，阻塞队列还有一个非常重要的属性，那就是容量的大小，分为有界和无界两种。无界队列意味着里面可以容纳非常多的元素，例如 LinkedBlockingQueue 的上限是 Integer.MAX_VALUE，约为 2 的 31 次方，是非常大的一个数，可以近似认为是无限容量，因为我们几乎无法把这个容量装满。但是有的阻塞队列是有界的，例如 ArrayBlockingQueue 如果容量满了，也不会扩容，所以一旦满了就无法再往里放数据了。以上就是本课时的全部内容，本课时讲解了什么是阻塞队列，首先我们讲解了阻塞队列的作用；然后看了 Java 8 中的并发队列，分为阻塞队列和非阻塞队列，并且在阻塞队列中有 6 种常见的实现；最后我们看了阻塞队列的特点，包括 take 方法、put 方法和是否有界。 经常看到 java 并发相关的材料里提到“无界”队列，可以理解为从数学上说这些所谓的无界队列都是“有界”的，只是这个界值很大，是Integer.MAX_VALUE,非常大，所以可以认为是无界的，但是如果较真的话其实java里并没有原生提供数学意义上无界的队列？ 如果队列不是线程安全的会怎么样呢？ 会发生线程安全问题，比如HashMap是线程不安全的Map，可能丢失数据。" }, { "title": "CopyOnWriteArrayList 的优点", "url": "/posts/CopyOnWriteArrayList/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-20 15:33:00 +0000", "snippet": "在 CopyOnWriteArrayList 出现之前，已经有了 ArrayList 和 LinkedList 作为 List 的数组和链表的实现，而且也有了线程安全的 Vector 和 Collections.synchronizedList() 可以使用。所以首先就看下线程安全的 Vector 的 size 和 get 方法的代码：public synchronized int size() {    return elementCount;}public synchronized E get(int index) {    if (index &amp;gt;= elementCount)        throw new ArrayIndexOutOfBoundsException(index);    return elementData(index);}可以看出，Vector 内部是使用 synchronized 来保证线程安全的，并且锁的粒度比较大，都是方法级别的锁，在并发量高的时候，很容易发生竞争，并发效率相对比较低。在这一点上，Vector 和 Hashtable 很类似。并且，前面这几种 List 在迭代期间不允许编辑，如果在迭代期间进行添加或删除元素等操作，则会抛出 ConcurrentModificationException 异常，这样的特点也在很多情况下给使用者带来了麻烦。所以从 JDK1.5 开始，Java 并发包里提供了使用 CopyOnWrite 机制实现的并发容器 CopyOnWriteArrayList 作为主要的并发 List，CopyOnWrite 的并发集合还包括 CopyOnWriteArraySet，其底层正是利用 CopyOnWriteArrayList 实现的。所以今天我们以 CopyOnWriteArrayList 为突破口，来看一下 CopyOnWrite 容器的特点。适用场景 读操作可以尽可能的快，而写即使慢一些也没关系在很多应用场景中，读操作可能会远远多于写操作。比如，有些系统级别的信息，往往只需要加载或者修改很少的次数，但是会被系统内所有模块频繁的访问。对于这种场景，我们最希望看到的就是读操作可以尽可能的快，而写即使慢一些也没关系。 读多写少黑名单是最典型的场景，假如我们有一个搜索网站，用户在这个网站的搜索框中，输入关键字搜索内容，但是某些关键字不允许被搜索。这些不能被搜索的关键字会被放在一个黑名单中，黑名单并不需要实时更新，可能每天晚上更新一次就可以了。当用户搜索时，会检查当前关键字在不在黑名单中，如果在，则提示不能搜索。这种读多写少的场景也很适合使用 CopyOnWrite 集合。读写规则 读写锁的规则读写锁的思想是：读读共享、其他都互斥（写写互斥、读写互斥、写读互斥），原因是由于读操作不会修改原有的数据，因此并发读并不会有安全问题；而写操作是危险的，所以当写操作发生时，不允许有读操作加入，也不允许第二个写线程加入。 对读写锁规则的升级CopyOnWriteArrayList 的思想比读写锁的思想又更进一步。为了将读取的性能发挥到极致，CopyOnWriteArrayList 读取是完全不用加锁的，更厉害的是，写入也不会阻塞读取操作，也就是说你可以在写入的同时进行读取，只有写入和写入之间需要进行同步，也就是不允许多个写入同时发生，但是在写入发生时允许读取同时发生。这样一来，读操作的性能就会大幅度提升。特点 CopyOnWrite的含义从 CopyOnWriteArrayList 的名字就能看出它是满足 CopyOnWrite 的 ArrayList，CopyOnWrite 的意思是说，当容器需要被修改的时候，不直接修改当前容器，而是先将当前容器进行 Copy，复制出一个新的容器，然后修改新的容器，完成修改之后，再将原容器的引用指向新的容器。这样就完成了整个修改过程。这样做的好处是，CopyOnWriteArrayList 利用了“不变性”原理，因为容器每次修改都是创建新副本，所以对于旧容器来说，其实是不可变的，也是线程安全的，无需进一步的同步操作。我们可以对 CopyOnWrite 容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素，也不会有修改。CopyOnWriteArrayList 的所有修改操作（add，set等）都是通过创建底层数组的新副本来实现的，所以 CopyOnWrite 容器也是一种读写分离的思想体现，读和写使用不同的容器。 迭代期间允许修改集合内容ArrayList 在迭代期间如果修改集合的内容，会抛出 ConcurrentModificationException 异常。让我们来分析一下 ArrayList 会抛出异常的原因。在 ArrayList 源码里的 ListItr 的 next 方法中有一个 checkForComodification 方法，代码如下：final void checkForComodification() {    if (modCount != expectedModCount)        throw new ConcurrentModificationException();}这里会首先检查 modCount 是否等于 expectedModCount。modCount 是保存修改次数，每次我们调用 add、remove 或 trimToSize 等方法时它会增加，expectedModCount 是迭代器的变量，当我们创建迭代器时会初始化并记录当时的 modCount。后面迭代期间如果发现 modCount 和 expectedModCount 不一致，就说明有人修改了集合的内容，就会抛出异常。和 ArrayList 不同的是，CopyOnWriteArrayList 的迭代器在迭代的时候，如果数组内容被修改了，CopyOnWriteArrayList 不会报 ConcurrentModificationException 的异常，因为迭代器使用的依然是旧数组，只不过迭代的内容可能已经过时了。演示代码如下：/*** 描述： 演示CopyOnWriteArrayList迭代期间可以修改集合的内容*/public class CopyOnWriteArrayListDemo {    public static void main(String[] args) {        CopyOnWriteArrayList&amp;lt;Integer&amp;gt; list = new CopyOnWriteArrayList&amp;lt;&amp;gt;(new Integer[]{1, 2, 3});        System.out.println(list); //[1, 2, 3]        //Get iterator 1        Iterator&amp;lt;Integer&amp;gt; itr1 = list.iterator();         //Add one element and verify list is updated        list.add(4);        System.out.println(list); //[1, 2, 3, 4]         //Get iterator 2        Iterator&amp;lt;Integer&amp;gt; itr2 = list.iterator();        System.out.println(&quot;====Verify Iterator 1 content====&quot;);         itr1.forEachRemaining(System.out::println); //1,2,3        System.out.println(&quot;====Verify Iterator 2 content====&quot;);        itr2.forEachRemaining(System.out::println); //1,2,3,4    }}这段代码会首先创建一个 CopyOnWriteArrayList，并且初始值被赋为 [1, 2, 3]，此时打印出来的结果很明显就是 [1, 2, 3]。然后我们创建一个叫作 itr1 的迭代器，创建之后再添加一个新的元素，利用 list.add() 方法把元素 4 添加进去，此时我们打印出 List 自然是 [1, 2, 3, 4]。我们再创建一个叫作 itr2 的迭代器，在下方把两个迭代器迭代产生的内容打印出来，这段代码的运行结果是：[1, 2, 3][1, 2, 3, 4]====Verify Iterator 1 content====123====Verify Iterator 2 content====1234可以看出，这两个迭代器打印出来的内容是不一样的。第一个迭代器打印出来的是 [1, 2, 3]，而第二个打印出来的是 [1, 2, 3, 4]。虽然它们的打印时机都发生在第四个元素被添加之后，但它们的创建时机是不同的。由于迭代器 1 被创建时的 List 里面只有三个元素，后续无论 List 有什么修改，对它来说都是无感知的。以上这个结果说明了，CopyOnWriteArrayList 的迭代器一旦被建立之后，如果往之前的 CopyOnWriteArrayList 对象中去新增元素，在迭代器中既不会显示出元素的变更情况，同时也不会报错，这一点和 ArrayList 是有很大区别的。缺点这些缺点不仅是针对 CopyOnWriteArrayList，其实同样也适用于其他的 CopyOnWrite 容器： 内存占用问题因为 CopyOnWrite 的写时复制机制，所以在进行写操作的时候，内存里会同时驻扎两个对象的内存，这一点会占用额外的内存空间。 在元素较多或者复杂的情况下，复制的开销很大复制过程不仅会占用双倍内存，还需要消耗 CPU 等资源，会降低整体性能。 数据一致性问题由于 CopyOnWrite 容器的修改是先修改副本，所以这次修改对于其他线程来说，并不是实时能看到的，只有在修改完之后才能体现出来。如果你希望写入的的数据马上能被其他线程看到，CopyOnWrite 容器是不适用的。源码分析 数据结构/** 可重入锁对象 */final transient ReentrantLock lock = new ReentrantLock();/** CopyOnWriteArrayList底层由数组实现，volatile修饰，保证数组的可见性 */private transient volatile Object[] array; /*** 得到数组*/final Object[] getArray() {    return array;} /*** 设置数组*/final void setArray(Object[] a) {    array = a;} /*** 初始化CopyOnWriteArrayList相当于初始化数组*/public CopyOnWriteArrayList() {    setArray(new Object[0]);}在这个类中首先会有一个 ReentrantLock 锁，用来保证修改操作的线程安全。下面被命名为 array 的 Object[] 数组是被 volatile 修饰的，可以保证数组的可见性，这正是存储元素的数组，同样，我们可以从 getArray()、setArray 以及它的构造方法看出，CopyOnWriteArrayList 的底层正是利用数组实现的，这也符合它的名字。 add 方法public boolean add(E e) {     // 加锁    final ReentrantLock lock = this.lock;    lock.lock();    try {        // 得到原数组的长度和元素        Object[] elements = getArray();        int len = elements.length;        // 复制出一个新数组        Object[] newElements = Arrays.copyOf(elements, len + 1);        // 添加时，将新元素添加到新数组中        newElements[len] = e;        // 将volatile Object[] array 的指向替换成新数组        setArray(newElements);        return true;    } finally {        lock.unlock();    }}add 方法的作用是往 CopyOnWriteArrayList 中添加元素，是一种修改操作。首先需要利用 ReentrantLock 的 lock 方法进行加锁，获取锁之后，得到原数组的长度和元素，也就是利用 getArray 方法得到 elements 并且保存 length。之后利用 Arrays.copyOf 方法复制出一个新的数组，得到一个和原数组内容相同的新数组，并且把新元素添加到新数组中。完成添加动作后，需要转换引用所指向的对象，利用 setArray(newElements) 操作就可以把 volatile Object[] array 的指向替换成新数组，最后在 finally 中把锁解除。总结流程：在添加的时候首先上锁，并复制一个新数组，增加操作在新数组上完成，然后将 array 指向到新数组，最后解锁。上面的步骤实现了 CopyOnWrite 的思想：写操作是在原来容器的拷贝上进行的，并且在读取数据的时候不会锁住 list。而且可以看到，如果对容器拷贝操作的过程中有新的读线程进来，那么读到的还是旧的数据，因为在那个时候对象的引用还没有被更改。下面我们来分析一下读操作的代码，也就是和 get 相关的三个方法，分别是 get 方法的两个重载和 getArray 方法，代码如下：public E get(int index) {    return get(getArray(), index);}final Object[] getArray() {    return array;}private E get(Object[] a, int index) {    return (E) a[index];}可以看出，get 相关的操作没有加锁，保证了读取操作的高速。 迭代器 COWIterator 类这个迭代器有两个重要的属性，分别是 Object[] snapshot 和 int cursor。其中 snapshot 代表数组的快照，也就是创建迭代器那个时刻的数组情况，而 cursor 则是迭代器的游标。迭代器的构造方法如下：private COWIterator(Object[] elements, int initialCursor) {    cursor = initialCursor;    snapshot = elements;}可以看出，迭代器在被构建的时候，会把当时的 elements 赋值给 snapshot，而之后的迭代器所有的操作都基于 snapshot 数组进行的，比如：public E next() {    if (! hasNext())        throw new NoSuchElementException();    return (E) snapshot[cursor++];}在 next 方法中可以看到，返回的内容是 snapshot 对象，所以，后续就算原数组被修改，这个 snapshot 既不会感知到，也不会受影响，执行迭代操作不需要加锁，也不会因此抛出异常。迭代器返回的结果，和创建迭代器的时候的内容一致。分别介绍了在它诞生之前的 Vector 和 Collections.synchronizedList() 的特点，CopyOnWriteArrayList 的适用场景、读写规则，还介绍了它的两个特点，分别是写时复制和迭代期间允许修改集合内容。我们还介绍了它的三个缺点，分别是内存占用问题，在元素较多或者复杂的情况下复制的开销大问题，以及数据一致性问题。最后我们对于它的重要源码进行了解析。 CopyOnWriteArrayList是不是只限制写写，不限制读读和读写 Jdk 内的并发列表如何选择？除了CopyOnWriteArrayList还可以怎么用？—— 同学，你好，目前就这个比较好用哦。 这儿的读写锁换成普通的排他锁可以不 —— 也是可以的，但是读写锁效率更高。" }, { "title": "同样是线程安全，ConcurrentHashMap 和 Hashtable 的区别", "url": "/posts/ConcurrentHashMap-Hashtable/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-19 15:33:00 +0000", "snippet": "32 同样是线程安全，ConcurrentHashMap 和 Hashtable 的区别？HashMap 不是线程安全的，而 ConcurrentHashMap 和 Hashtable 它们两个确实都是线程安全的，那它们有哪些不同点呢？从以下四个角度出发，去分析它们的不同点。出现的版本不同Hashtable 在 JDK1.0 的时候就存在了，并在 JDK1.2 版本中实现了 Map 接口，成为了集合框架的一员。而 ConcurrentHashMap 则是在 JDK1.5 中才出现的，也正是因为它们出现的年代不同，而后出现的往往是对前面出现的类的优化，所以它们在实现方式以及性能上，也存在着较大的不同。实现线程安全的方式不同虽然 ConcurrentHashMap 和 Hashtable 它们两个都是线程安全的，但是从原理上分析，Hashtable 实现并发安全的原理是通过 synchronized 关键字，让我们直接看下源码，以 clear() 方法为例，代码如下：public synchronized void clear() {    Entry&amp;lt;?,?&amp;gt; tab[] = table;    modCount++;    for (int index = tab.length; --index &amp;gt;= 0; )        tab[index] = null;    count = 0;}可以看出这个 clear() 方法是被 synchronized 关键字所修饰的，同理其他的方法例如 put、get、size 等，也同样是被 synchronized 关键字修饰的。之所以 Hashtable 是线程安全的，是因为几乎每个方法都被 synchronized 关键字所修饰了，这也就保证了线程安全。Collections.SynchronizedMap(new HashMap()) 的原理和 Hashtable 类似，也是利用 synchronized 实现的。而我们的 ConcurrentHashMap 实现的原理，却有大大的不同，让我们看一下它在 Java 8 中的结构示意图：对于 ConcurrentHashMap 的原理，我们在第 30 课时的时候有过详细的介绍和源码分析，本质上它实现线程安全的原理是利用了 CAS + synchronized + Node 节点的方式，这和 Hashtable 的完全利用 synchronized 的方式有很大的不同。性能不同正因为它们在线程安全的实现方式上的不同，导致它们在性能方面也有很大的不同。当线程数量增加的时候，Hashtable 的性能会急剧下降，因为每一次修改都需要锁住整个对象，而其他线程在此期间是不能操作的。不仅如此，还会带来额外的上下文切换等开销，所以此时它的吞吐量甚至还不如单线程的情况。而在 ConcurrentHashMap 中，就算上锁也仅仅会对一部分上锁而不是全部都上锁，所以多线程中的吞吐量通常都会大于单线程的情况，也就是说，在并发效率上，ConcurrentHashMap 比 Hashtable 提高了很多。迭代时修改的不同Hashtable（包括 HashMap）不允许在迭代期间修改内容，否则会抛出ConcurrentModificationException 异常，其原理是检测 modCount 变量，迭代器的 next() 方法的代码如下：public T next() {    if (modCount != expectedModCount)        throw new ConcurrentModificationException();    return nextElement();}可以看出在这个 next() 方法中，会首先判断 modCount 是否等于 expectedModCount。其中 expectedModCount 是在迭代器生成的时候随之生成的，并且不会改变。它所代表的含义是当前 Hashtable 被修改的次数，而每一次去调用 Hashtable 的包括 addEntry()、remove()、rehash() 等方法中，都会修改 modCount 的值。这样一来，如果我们在迭代的过程中，去对整个 Hashtable 的内容做了修改的话，也就同样会反映到 modCount 中。这样一来，迭代器在进行 next 的时候，也可以感知到，于是它就会发现 modCount 不等于 expectedModCount，就会抛出 ConcurrentModificationException 异常。所以对于 Hashtable 而言，它是不允许在迭代期间对内容进行修改的。相反，ConcurrentHashMap 即便在迭代期间修改内容，也不会抛出ConcurrentModificationException。本文总结了 ConcurrentHashMap 与 Hashtable 的区别，虽然它们都是线程安全的，但是在出现的版本上、实现线程安全的方式上、性能上，以及迭代时是否支持修改等方面都有较大的不同，如果有并发的场景，那么使用 ConcurrentHashMap 是最合适的，相反，Hashtable 已经不再推荐使用。" }, { "title": "为什么 Map 桶中超过 8 个才转为红黑树？", "url": "/posts/map-red-black-tree/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-16 15:33:00 +0000", "snippet": "31 为什么 Map 桶中超过 8 个才转为红黑树？JDK 1.8 的 HashMap 和 ConcurrentHashMap 都有这样一个特点：最开始的 Map 是空的，因为里面没有任何元素，往里放元素时会计算 hash 值，计算之后，第 1 个 value 会首先占用一个桶（也称为槽点）位置，后续如果经过计算发现需要落到同一个桶中，那么便会使用链表的形式往后延长，俗称“拉链法”，如图所示：图中，有的桶是空的， 比如第 4 个；有的只有一个元素，比如 1、3、6；有的就是刚才说的拉链法，比如第 2 和第 5 个桶。当链表长度大于或等于阈值（默认为 8）的时候，如果同时还满足容量大于或等于 MIN_TREEIFY_CAPACITY（默认为 64）的要求，就会把链表转换为红黑树。同样，后续如果由于删除或者其他原因调整了大小，当红黑树的节点小于或等于 6 个以后，又会恢复为链表形态。让我们回顾一下 HashMap 的结构示意图：在图中我们可以看到，有一些槽点是空的，有一些是拉链，有一些是红黑树。更多的时候我们会关注，为何转为红黑树以及红黑树的一些特点，可是，为什么转化的这个阈值要默认设置为 8 呢？要想知道为什么设置为 8，那首先我们就要知道为什么要转换，因为转换是第一步。每次遍历一个链表，平均查找的时间复杂度是 O(n)，n 是链表的长度。红黑树有和链表不一样的查找性能，由于红黑树有自平衡的特点，可以防止不平衡情况的发生，所以可以始终将查找的时间复杂度控制在 O(log(n))。最初链表还不是很长，所以可能 O(n) 和 O(log(n)) 的区别不大，但是如果链表越来越长，那么这种区别便会有所体现。所以为了提升查找性能，需要把链表转化为红黑树的形式。那为什么不一开始就用红黑树，反而要经历一个转换的过程呢？其实在 JDK 的源码注释中已经对这个问题作了解释： Because TreeNodes are about twice the size of regular nodes,use them only when bins contain enough nodes to warrant use(see TREEIFY_THRESHOLD). And when they become too small (due removal or resizing) they are converted back to plain bins.这段话的意思是：单个 TreeNode 需要占用的空间大约是普通 Node 的两倍，所以只有当包含足够多的 Nodes 时才会转成 TreeNodes，而是否足够多就是由 TREEIFY_THRESHOLD 的值决定的。而当桶中节点数由于移除或者 resize 变少后，又会变回普通的链表的形式，以便节省空间。通过查看源码可以发现，默认是链表长度达到 8 就转成红黑树，而当长度降到 6 就转换回去，这体现了时间和空间平衡的思想，最开始使用链表的时候，空间占用是比较少的，而且由于链表短，所以查询时间也没有太大的问题。可是当链表越来越长，需要用红黑树的形式来保证查询的效率。对于何时应该从链表转化为红黑树，需要确定一个阈值，这个阈值默认为 8，并且在源码中也对选择 8 这个数字做了说明，原文如下： In usages with well-distributed user hashCodes, tree bins are rarely used. Ideally, under random hashCodes, the frequency of nodes in bins follows a Poisson distribution (http://en.wikipedia.org/wiki/Poisson_distribution) with a parameter of about 0.5 on average for the default resizing threshold of 0.75, although with a large variance because of resizing granularity. Ignoring variance, the expected occurrences of list size k are (exp(-0.5) * pow(0.5, k) / factorial(k)). The first values are: 0: 0.60653066 1: 0.30326533 2: 0.07581633 3: 0.01263606 4: 0.00157952 5: 0.00015795 6: 0.00001316 7: 0.00000094 8: 0.00000006 more: less than 1 in ten million上面这段话的意思是，如果 hashCode 分布良好，也就是 hash 计算的结果离散好的话，那么红黑树这种形式是很少会被用到的，因为各个值都均匀分布，很少出现链表很长的情况。在理想情况下，链表长度符合泊松分布，各个长度的命中概率依次递减，当长度为 8 的时候，概率仅为 0.00000006。这是一个小于千万分之一的概率，通常我们的 Map 里面是不会存储这么多的数据的，所以通常情况下，并不会发生从链表向红黑树的转换。但是，HashMap 决定某一个元素落到哪一个桶里，是和这个对象的 hashCode 有关的，JDK 并不能阻止我们用户实现自己的哈希算法，如果我们故意把哈希算法变得不均匀，例如：@Overridepublic int hashCode() {    return 1;}这里 hashCode 计算出来的值始终为 1，那么就很容易导致 HashMap 里的链表变得很长。让我们来看下面这段代码：public class HashMapDemo {    public static void main(String[] args) {        HashMap map = new HashMap&amp;lt;HashMapDemo,Integer&amp;gt;(1);        for (int i = 0; i &amp;lt; 1000; i++) {            HashMapDemo hashMapDemo1 = new HashMapDemo();            map.put(hashMapDemo1, null);        }        System.out.println(&quot;运行结束&quot;);    }     @Override    public int hashCode() {        return 1;    }}在这个例子中，我们建了一个 HashMap，并且不停地往里放入值，所放入的 key 的对象，它的 hashCode 是被重写过得，并且始终返回 1。这段代码运行时，如果通过 debug 让程序暂停在 System.out.println(“运行结束”) 这行语句，我们观察 map 内的节点，可以发现已经变成了 TreeNode，而不是通常的 Node，这说明内部已经转为了红黑树。事实上，链表长度超过 8 就转为红黑树的设计，更多的是为了防止用户自己实现了不好的哈希算法时导致链表过长，从而导致查询效率低，而此时转为红黑树更多的是一种保底策略，用来保证极端情况下查询的效率。通常如果 hash 算法正常的话，那么链表的长度也不会很长，那么红黑树也不会带来明显的查询时间上的优势，反而会增加空间负担。所以通常情况下，并没有必要转为红黑树，所以就选择了概率非常小，小于千万分之一概率，也就是长度为 8 的概率，把长度 8 作为转化的默认阈值。所以如果平时开发中发现 HashMap 或是 ConcurrentHashMap 内部出现了红黑树的结构，这个时候往往就说明我们的哈希算法出了问题，需要留意是不是我们实现了效果不好的 hashCode 方法，并对此进行改进，以便减少冲突。" }, { "title": "ConcurrentHashMap 在 Java 7 和 8 有何不整", "url": "/posts/ConcurrentHashMap-java7-java8/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-15 15:33:00 +0000", "snippet": "在 Java 8 中，对于 ConcurrentHashMap 这个常用的工具类进行了很大的升级，对比之前 Java 7 版本在诸多方面都进行了调整和变化。不过，在 Java 7 中的 Segment 的设计思想依然具有参考和学习的价值，所以在很多情况下面试官都会问你：ConcurrentHashMap 在 Java 7 和 Java 8 中的结构分别是什么？它们有什么相同点和不同点？所以本课时就对 ConcurrentHashMap 在这两个版本的特点和性质进行对比和介绍。Java 7 版本的 ConcurrentHashMap从图中我们可以看出，在 ConcurrentHashMap 内部进行了 Segment 分段，Segment 继承了 ReentrantLock，可以理解为一把锁，各个 Segment 之间都是相互独立上锁的，互不影响。相比于之前的 Hashtable 每次操作都需要把整个对象锁住而言，大大提高了并发效率。因为它的锁与锁之间是独立的，而不是整个对象只有一把锁。每个 Segment 的底层数据结构与 HashMap 类似，仍然是数组和链表组成的拉链法结构。默认有 0~15 共 16 个 Segment，所以最多可以同时支持 16 个线程并发操作（操作分别分布在不同的 Segment 上）。16 这个默认值可以在初始化的时候设置为其他值，但是一旦确认初始化以后，是不可以扩容的。Java 8 版本的 ConcurrentHashMap在 Java 8 中，几乎完全重写了 ConcurrentHashMap，代码量从原来 Java 7 中的 1000 多行，变成了现在的 6000 多行，所以也大大提高了源码的阅读难度。而为了方便我们理解，我们还是先从整体的结构示意图出发，看一看总体的设计思路，然后再去深入细节。图中的节点有三种类型。 第一种是最简单的，空着的位置代表当前还没有元素来填充。 第二种就是和 HashMap 非常类似的拉链法结构，在每一个槽中会首先填入第一个节点，但是后续如果计算出相同的 Hash 值，就用链表的形式往后进行延伸。 第三种结构就是红黑树结构，这是 Java 7 的 ConcurrentHashMap 中所没有的结构，在此之前我们可能也很少接触这样的数据结构。当第二种情况的链表长度大于某一个阈值（默认为 8），且同时满足一定的容量要求的时候，ConcurrentHashMap 便会把这个链表从链表的形式转化为红黑树的形式，目的是进一步提高它的查找性能。所以，Java 8 的一个重要变化就是引入了红黑树的设计，由于红黑树并不是一种常见的数据结构，所以我们在此简要介绍一下红黑树的特点。红黑树是每个节点都带有颜色属性的二叉查找树，颜色为红色或黑色，红黑树的本质是对二叉查找树 BST 的一种平衡策略，我们可以理解为是一种平衡二叉查找树，查找效率高，会自动平衡，防止极端不平衡从而影响查找效率的情况发生。由于自平衡的特点，即左右子树高度几乎一致，所以其查找性能近似于二分查找，时间复杂度是 O(log(n)) 级别；反观链表，它的时间复杂度就不一样了，如果发生了最坏的情况，可能需要遍历整个链表才能找到目标元素，时间复杂度为 O(n)，远远大于红黑树的 O(log(n))，尤其是在节点越来越多的情况下，O(log(n)) 体现出的优势会更加明显。红黑树还有一些其他的特点，比如根节点是黑色，并且红色节点不能连续，但是由于红黑树并不是本课程的重点，所以我们只需要知道红黑树善于自平衡就可以了。现在就可以理解为什么 Java 8 的 ConcurrentHashMap 要引入红黑树了。好处就是避免在极端的情况下冲突链表变得很长，在查询的时候，效率会非常慢。而红黑树具有自平衡的特点，所以，即便是极端情况下，也可以保证查询效率在 O(log(n))。分析 Java 8 版本的 ConcurrentHashMap 的重要源码由于 Java 7 版本已经过时了，所以我们把重点放在 Java 8 版本的源码分析上。Node 节点put 方法源码分析get 方法源码分析对比Java7 和Java8 的异同和优缺点数据结构ava 7 采用 Segment 分段锁来实现，而 Java 8 中的 ConcurrentHashMap 使用数组 + 链表 + 红黑树，在这一点上它们的差别非常大。并发度Java 7 中，每个 Segment 独立加锁，最大并发个数就是 Segment 的个数，默认是 16。但是到了 Java 8 中，锁粒度更细，理想情况下 table 数组元素的个数（也就是数组长度）就是其支持并发的最大个数，并发度比之前有提高。保证并发安全的原理Java 7 采用 Segment 分段锁来保证安全，而 Segment 是继承自 ReentrantLock。Java 8 中放弃了 Segment 的设计，采用 Node + CAS + synchronized 保证线程安全。遇到 Hash 碰撞Java 7 在 Hash 冲突时，会使用拉链法，也就是链表的形式。Java 8 先使用拉链法，在链表长度超过一定阈值时，将链表转换为红黑树，来提高查找效率。查询时间复杂度Java 7 遍历链表的时间复杂度是 O(n)，n 为链表长度。Java 8 如果变成遍历红黑树，那么时间复杂度降低为 O(log(n))，n 为树的节点个数。 concurrenthashmap 在putval的过程中假设两个线程放入的key的hash值是相同的，然后concurrenthashmap的index是null的，两个线程都是cas放入，如果有一个失败了这个if(cas())的判断是false，程序会怎么走呢 —— 会再次循环，然后尝试插入，再次插入的时候会发现不是 null，然后根据类型继续判断。 有个疑惑，get 时为什么不考虑红黑树的情况。 —— 考虑了，find方法就是处理红黑树的。" }, { "title": "HashMap 为什么是线程不安全的", "url": "/posts/HashMap-unsafe-thread/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-14 15:33:00 +0000", "snippet": "HashMap 是平时工作和学习中用得非常非常多的一个容器，也是 Map 最主要的实现类之一，但是它自身并不具备线程安全的特点，可以从多种情况中体现出来，具体的分析如下：源码分析第一步，HashMap 中 put 方法的源码：public V put(K key, V value) {    if (key == null)        return putForNullKey(value);    int hash = hash(key.hashCode());    int i = indexFor(hash, table.length);    for (Entry&amp;lt;K,V&amp;gt; e = table[i]; e != null; e = e.next) {        Object k;        if (e.hash == hash &amp;amp;&amp;amp; ((k = e.key) == key || key.equals(k))) {            V oldValue = e.value;            e.value = value;            e.recordAccess(this);            return oldValue;        }    }      //modCount++ 是一个复合操作    modCount++;    addEntry(hash, key, value, i);    return null;}在 HashMap 的 put() 方法中，可以看出里面进行了很多操作，那么在这里，我们把目光聚焦到标记出来的 modCount++ 这一行代码中，相信有经验的小伙伴一定发现了，这相当于是典型的“i++”操作，正是我们在 06 课时讲过的线程不安全的“运行结果错误”的情况。从表面上看 i++ 只是一行代码，但实际上它并不是一个原子操作，它的执行步骤主要分为三步，而且在每步操作之间都有可能被打断。 第一个步骤是读取； 第二个步骤是增加； 第三个步骤是保存。接下来具体看一下如何发生的线程不安全问题。我们根据箭头指向依次看，假设线程 1 首先拿到 i=1 的结果，然后进行 i+1 操作，但此时 i+1 的结果并没有保存下来，线程 1 就被切换走了，于是 CPU 开始执行线程 2，它所做的事情和线程 1 是一样的 i++ 操作，但此时我们想一下，它拿到的 i 是多少？实际上和线程 1 拿到的 i 的结果一样都是 1，为什么呢？因为线程 1 虽然对 i 进行了 +1 操作，但结果没有保存，所以线程 2 看不到修改后的结果。然后假设等线程 2 对 i 进行 +1 操作后，又切换到线程 1，让线程 1 完成未完成的操作，即将 i + 1 的结果 2 保存下来，然后又切换到线程 2 完成 i = 2 的保存操作，虽然两个线程都执行了对 i 进行 +1 的操作，但结果却最终保存了 i = 2 的结果，而不是我们期望的 i = 3，这样就发生了线程安全问题，导致了数据结果错误，这也是最典型的线程安全问题。所以，从源码的角度，或者说从理论上来讲，这完全足以证明 HashMap 是线程非安全的了。因为如果有多个线程同时调用 put() 方法的话，它很有可能会把 modCount 的值计算错（上述的源码分析针对的是 Java 7 版本的源码，而在 Java 8 版本的 HashMap 的 put 方法中会调用 putVal 方法，里面同样有 ++modCount 语句，所以原理是一样的）。实验：扩容期间取出的值不准确刚才我们分析了源码，你可能觉得不过瘾，下面我们就打开代码编辑器，用一个实验来证明 HashMap 是线程不安全的。为什么说 HashMap 不是线程安全的呢？我们先来讲解下原理。HashMap 本身默认的容量不是很大，如果不停地往 map 中添加新的数据，它便会在合适的时机进行扩容。而在扩容期间，它会新建一个新的空数组，并且用旧的项填充到这个新的数组中去。那么，在这个填充的过程中，如果有线程获取值，很可能会取到 null 值，而不是我们所希望的、原来添加的值。所以我们程序就想演示这种情景，我们来看一下这段代码：public class HashMapNotSafe {    public static void main(String[] args) {        final Map&amp;lt;Integer, String&amp;gt; map = new HashMap&amp;lt;&amp;gt;();        final Integer targetKey = 0b1111_1111_1111_1111; // 65 535        final String targetValue = &quot;v&quot;;        map.put(targetKey, targetValue);        new Thread(() -&amp;gt; {            IntStream.range(0, targetKey).forEach(key -&amp;gt; map.put(key, &quot;someValue&quot;));        }).start();        while (true) {            if (null == map.get(targetKey)) {                throw new RuntimeException(&quot;HashMap is not thread safe.&quot;);            }        }    }}代码中首先建立了一个 HashMap，并且定义了 key 和 value， key 的值是一个二进制的 1111_1111_1111_1111，对应的十进制是 65535。之所以选取这样的值，就是为了让它在扩容往回填充数据的时候，尽量不要填充得太快，比便于我们能捕捉到错误的发生。而对应的 value 是无所谓的，我们随意选取了一个非 null 的 “v” 来表示它，并且把这个值放到了 map 中。接下来，我们就用一个新的线程不停地往我们的 map 中去填入新的数据，我们先来看是怎么填入的。首先它用了一个 IntStream，这个 range 是从 0 到之前所讲过的 65535，这个 range 是一个左闭右开的区间，所以会从 0、1、2、3……一直往上加，并且每一次加的时候，这个 0、1、2、3、4 都会作为 key 被放到 map 中去。而它的 value 是统一的，都是 “someValue”，因为 value 不是我们所关心的。然后，我们就会把这个线程启动起来，随后就进入一个 while 循环，这个 while 循环是关键，在 while 循环中我们会不停地检测之前放入的 key 所对应的 value 还是不是我们所期望的字符串 “v”。我们在 while 循环中会不停地从 map 中取 key 对应的值。如果 HashMap 是线程安全的，那么无论怎样它所取到的值都应该是我们最开始放入的字符串 “v”，可是如果取出来是一个 null，就会满足这个 if 条件并且随即抛出一个异常，因为如果取出 null 就证明它所取出来的值和我们一开始放入的值是不一致的，也就证明了它是线程不安全的，所以在此我们要抛出一个 RuntimeException 提示我们。下面就让我们运行这个程序来看一看是否会抛出这个异常。一旦抛出就代表它是线程不安全的，这段代码的运行结果：Exception in thread &quot;main&quot; java.lang.RuntimeException: HashMap is not thread safe.at lesson29.HashMapNotSafe.main(HashMapNotSafe.java:25)很明显，很快这个程序就抛出了我们所希望看到的 RuntimeException，并且我们把它描述为：HashMap is not thread safe，一旦它能进入到这个 if 语句，就已经证明它所取出来的值是 null，而不是我们期望的字符串 “v”。通过以上这个例子，我们也证明了HashMap 是线程非安全的。除了刚才的例子之外，还有很多种线程不安全的情况，例如：同时 put 碰撞导致数据丢失比如，有多个线程同时使用 put 来添加元素，而且恰好两个 put 的 key 是一样的，它们发生了碰撞，也就是根据 hash 值计算出来的 bucket 位置一样，并且两个线程又同时判断该位置是空的，可以写入，所以这两个线程的两个不同的 value 便会添加到数组的同一个位置，这样最终就只会保留一个数据，丢失一个数据。可见性问题无法保证我们再从可见性的角度去考虑一下。可见性也是线程安全的一部分，如果某一个数据结构声称自己是线程安全的，那么它同样需要保证可见性，也就是说，当一个线程操作这个容器的时候，该操作需要对另外的线程都可见，也就是其他线程都能感知到本次操作。可是 HashMap 对此是做不到的，如果线程 1 给某个 key 放入了一个新值，那么线程 2 在获取对应的 key 的值的时候，它的可见性是无法保证的，也就是说线程 2 可能可以看到这一次的更改，但也有可能看不到。所以从可见性的角度出发，HashMap 同样是线程非安全的。死循环造成 CPU 100%下面我们再举一个死循环造成 CPU 100% 的例子。HashMap 有可能会发生死循环并且造成 CPU 100% ，这种情况发生最主要的原因就是在扩容的时候，也就是内部新建新的 HashMap 的时候，扩容的逻辑会反转散列桶中的节点顺序，当有多个线程同时进行扩容的时候，由于 HashMap 并非线程安全的，所以如果两个线程同时反转的话，便可能形成一个循环，并且这种循环是链表的循环，相当于 A 节点指向 B 节点，B 节点又指回到 A 节点，这样一来，在下一次想要获取该 key 所对应的 value 的时候，便会在遍历链表的时候发生永远无法遍历结束的情况，也就发生 CPU 100% 的情况。所以综上所述，HashMap 是线程不安全的，在多线程使用场景中如果需要使用 Map，应该尽量避免使用线程不安全的 HashMap。同时，虽然 Collections.synchronizedMap(new HashMap()) 是线程安全的，但是效率低下，因为内部用了很多的 synchronized，多个线程不能同时操作。推荐使用线程安全同时性能比较好的 ConcurrentHashMap。关于 ConcurrentHashMap 我们会在下一个课时中介绍。 CPU100%的问题主要是在jdk1.8之前的头插法，在jdk1.8已经修改为尾插法，100%问题得到解决了吧？—— 头插法导致的100%问题已经解决，但是还存在其他原因导致CPU100%的，总之并发下不应使用HashMap。" }, { "title": "JVM 对锁的优化", "url": "/posts/jvm-lock/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-13 15:33:00 +0000", "snippet": "相比于 JDK 1.5，在 JDK 1.6 中 HotSopt 虚拟机对 synchronized 内置锁的性能进行了很多优化，包括自适应的自旋、锁消除、锁粗化、偏向锁、轻量级锁等。有了这些优化措施后，synchronized 锁的性能得到了大幅提高，下面我们分别介绍这些具体的优化。自适应的自旋锁首先，我们来看一下自适应的自旋锁。先来复习一下自旋的概念和自旋的缺点。“自旋”就是不释放 CPU，一直循环尝试获取锁，如下面这段代码所示：public final long getAndAddLong(Object var1, long var2, long var4) {    long var6;    do {        var6 = this.getLongVolatile(var1, var2);    } while(!this.compareAndSwapLong(var1, var2, var6, var6 + var4));    return var6;}代码中使用一个 do-while 循环来一直尝试修改 long 的值。自旋的缺点在于如果自旋时间过长，那么性能开销是很大的，浪费了 CPU 资源。在 JDK 1.6 中引入了自适应的自旋锁来解决长时间自旋的问题。自适应意味着自旋的时间不再固定，而是会根据最近自旋尝试的成功率、失败率，以及当前锁的拥有者的状态等多种因素来共同决定。自旋的持续时间是变化的，自旋锁变“聪明”了。比如，如果最近尝试自旋获取某一把锁成功了，那么下一次可能还会继续使用自旋，并且允许自旋更长的时间；但是如果最近自旋获取某一把锁失败了，那么可能会省略掉自旋的过程，以便减少无用的自旋，提高效率。锁消除第二个优化是锁消除。public class Person {    private String name;    private int age;    public Person(String personName, int personAge) {        name = personName;        age = personAge;    }    public Person(Person p) {        this(p.getName(), p.getAge());    }    public String getName() {        return name;    }    public int getAge() {        return age;    }}class Employee {    private Person person;    // makes a defensive copy to protect against modifications by caller    public Person getPerson() {        return new Person(person);    }    public void printEmployeeDetail(Employee emp) {        Person person = emp.getPerson();        // this caller does not modify the object, so defensive copy was unnecessary        System.out.println(&quot;Employee&#39;s name: &quot; + person.getName() + &quot;; age: &quot; + person.getAge());    }}在这段代码中，我们看到下方的 Employee 类中的 getPerson() 方法，这个方法中使用了类里面的 person 对象，并且新建一个和它属性完全相同的新的 person 对象，目的是防止方法调用者修改原来的 person 对象。但是在这个例子中，其实是没有任何必要新建对象的，因为我们的 printEmployeeDetail() 方法没有对这个对象做出任何的修改，仅仅是打印，既然如此，我们其实可以直接打印最开始的 person 对象，而无须新建一个新的。如果编译器可以确定最开始的 person 对象不会被修改的话，它可能会优化并且消除这个新建 person 的过程。根据这样的思想，接下来我们就来举一个锁消除的例子，经过逃逸分析之后，如果发现某些对象不可能被其他线程访问到，那么就可以把它们当成栈上数据，栈上数据由于只有本线程可以访问，自然是线程安全的，也就无需加锁，所以会把这样的锁给自动去除掉。例如，我们的 StringBuffer 的 append 方法如下所示：@Overridepublic synchronized StringBuffer append(Object obj) {    toStringCache = null;    super.append(String.valueOf(obj));    return this;}从代码中可以看出，这个方法是被 synchronized 修饰的同步方法，因为它可能会被多个线程同时使用。但是在大多数情况下，它只会在一个线程内被使用，如果编译器能确定这个 StringBuffer 对象只会在一个线程内被使用，就代表肯定是线程安全的，那么我们的编译器便会做出优化，把对应的 synchronized 给消除，省去加锁和解锁的操作，以便增加整体的效率。锁粗化如果释放了锁，紧接着什么都没做，又重新获取锁，例如下面这段代码所示：public void lockCoarsening() {    synchronized (this) {        //do something    }    synchronized (this) {        //do something    }    synchronized (this) {        //do something    }}那么其实这种释放和重新获取锁是完全没有必要的，如果把同步区域扩大，也就是只在最开始加一次锁，并且在最后直接解锁，那么就可以把中间这些无意义的解锁和加锁的过程消除，相当于是把几个 synchronized 块合并为一个较大的同步块。这样做的好处在于在线程执行这些代码时，就无须频繁申请与释放锁了，这样就减少了性能开销。不过，这样做也有一个副作用，就是会让同步区域变大。如果在循环中也这样做，如代码所示：for (int i = 0; i &amp;lt; 1000; i++) {    synchronized (this) {        //do something    }}也就是我们在第一次循环的开始，就开始扩大同步区域并持有锁，直到最后一次循环结束，才结束同步代码块释放锁的话，这就会导致其他线程长时间无法获得锁。所以，这里的锁粗化不适用于循环的场景，仅适用于非循环的场景。锁粗化功能是默认打开的，用 -XX:-EliminateLocks 可以关闭该功能。偏向锁/轻量级锁/重量级锁这三种锁是特指 synchronized 锁的状态的，通过在对象头中的 mark word 来表明锁的状态。 偏向锁对于偏向锁而言，它的思想是如果自始至终，对于这把锁都不存在竞争，那么其实就没必要上锁，只要打个标记就行了。一个对象在被初始化后，如果还没有任何线程来获取它的锁时，它就是可偏向的，当有第一个线程来访问它尝试获取锁的时候，它就记录下来这个线程，如果后面尝试获取锁的线程正是这个偏向锁的拥有者，就可以直接获取锁，开销很小。 轻量级锁JVM 的开发者发现在很多情况下，synchronized 中的代码块是被多个线程交替执行的，也就是说，并不存在实际的竞争，或者是只有短时间的锁竞争，用 CAS 就可以解决。这种情况下，重量级锁是没必要的。轻量级锁指当锁原来是偏向锁的时候，被另一个线程所访问，说明存在竞争，那么偏向锁就会升级为轻量级锁，线程会通过自旋的方式尝试获取锁，不会阻塞。 重量级锁这种锁利用操作系统的同步机制实现，所以开销比较大。当多个线程直接有实际竞争，并且锁竞争时间比较长的时候，此时偏向锁和轻量级锁都不能满足需求，锁就会膨胀为重量级锁。重量级锁会让其他申请却拿不到锁的线程进入阻塞状态。锁升级的路径从无锁到偏向锁，再到轻量级锁，最后到重量级锁。结合前面我们讲过的知识，偏向锁性能最好，避免了 CAS 操作。而轻量级锁利用自旋和 CAS 避免了重量级锁带来的线程阻塞和唤醒，性能中等。重量级锁则会把获取不到锁的线程阻塞，性能最差。JVM 默认会优先使用偏向锁，如果有必要的话才逐步升级，这大幅提高了锁的性能。 锁粗化的意识是，他会将多个重复的锁，会自动变成一个吗？ - 是的" }, { "title": "自旋锁", "url": "/posts/spin-lock/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-12 15:33:00 +0000", "snippet": "什么是自旋首先，我们了解什么叫自旋？“自旋”可以理解为“自我旋转”，这里的“旋转”指“循环”，比如 while 循环或者 for 循环。“自旋”就是自己在这里不停地循环，直到目标达成。而不像普通的锁那样，如果获取不到锁就进入阻塞。对比自旋和非自旋的获取锁的流程一张流程图来对比一下自旋锁和非自旋锁的获取锁的过程。首先，我们来看自旋锁，它并不会放弃 CPU 时间片，而是通过自旋等待锁的释放，也就是说，它会不停地再次地尝试获取锁，如果失败就再次尝试，直到成功为止。我们再来看下非自旋锁，非自旋锁和自旋锁是完全不一样的，如果它发现此时获取不到锁，它就把自己的线程切换状态，让线程休眠，然后 CPU 就可以在这段时间去做很多其他的事情，直到之前持有这把锁的线程释放了锁，于是 CPU 再把之前的线程恢复回来，让这个线程再去尝试获取这把锁。如果再次失败，就再次让线程休眠，如果成功，一样可以成功获取到同步资源的锁。可以看出，非自旋锁和自旋锁最大的区别，就是如果它遇到拿不到锁的情况，它会把线程阻塞，直到被唤醒。而自旋锁会不停地尝试。那么，自旋锁这样不停尝试的好处是什么呢？自旋锁的好处首先，阻塞和唤醒线程都是需要高昂的开销的，如果同步代码块中的内容不复杂，那么可能转换线程带来的开销比实际业务代码执行的开销还要大。在很多场景下，可能我们的同步代码块的内容并不多，所以需要的执行时间也很短，如果我们仅仅为了这点时间就去切换线程状态，那么其实不如让线程不切换状态，而是让它自旋地尝试获取锁，等待其他线程释放锁，有时我只需要稍等一下，就可以避免上下文切换等开销，提高了效率。用一句话总结自旋锁的好处，那就是自旋锁用循环去不停地尝试获取锁，让线程始终处于 Runnable 状态，节省了线程状态切换带来的开销。AtomicLong 的实现在 Java 1.5 版本及以上的并发包中，也就是 java.util.concurrent 的包中，里面的原子类基本都是自旋锁的实现。比如我们看一个 AtomicLong 的实现，里面有一个 getAndIncrement 方法，源码如下：public final long getAndIncrement() {    return unsafe.getAndAddLong(this, valueOffset, 1L);}可以看到它调用了一个 unsafe.getAndAddLong，所以我们再来看这个方法：public final long getAndAddLong (Object var1,long var2, long var4){    long var6;    do {        var6 = this.getLongVolatile(var1, var2);    } while (!this.compareAndSwapLong(var1, var2, var6, var6 + var4));    return var6;}在这个方法中，它用了一个 do while 循环。这里就很明显了：do {    var6 = this.getLongVolatile(var1, var2);} while (!this.compareAndSwapLong(var1, var2, var6, var6 + var4));这里的 do-while 循环就是一个自旋操作，如果在修改过程中遇到了其他线程竞争导致没修改成功的情况，就会 while 循环里进行死循环，直到修改成功为止。实现一个可重入的自旋锁下是自己实现的一个可重入的自旋锁。代码如下所示：package lesson27;import java.util.concurrent.atomic.AtomicReference;import java.util.concurrent.locks.Lock; /** * 描述：     实现一个可重入的自旋锁 */public class ReentrantSpinLock  {    private AtomicReference&amp;lt;Thread&amp;gt; owner = new AtomicReference&amp;lt;&amp;gt;();     //重入次数    private int count = 0;    public void lock() {        Thread t = Thread.currentThread();        if (t == owner.get()) {            ++count;            return;        }        //自旋获取锁        while (!owner.compareAndSet(null, t)) {            System.out.println(&quot;自旋了&quot;);        }    }    public void unlock() {        Thread t = Thread.currentThread();        //只有持有锁的线程才能解锁        if (t == owner.get()) {            if (count &amp;gt; 0) {                --count;            } else {                //此处无需CAS操作，因为没有竞争，因为只有线程持有者才能解锁                owner.set(null);            }        }    }     public static void main(String[] args) {        ReentrantSpinLock spinLock = new ReentrantSpinLock();        Runnable runnable = new Runnable() {            @Override            public void run() {                System.out.println(Thread.currentThread().getName() + &quot;开始尝试获取自旋锁&quot;);                spinLock.lock();                try {                    System.out.println(Thread.currentThread().getName() + &quot;获取到了自旋锁&quot;);                    Thread.sleep(4000);                } catch (InterruptedException e) {                    e.printStackTrace();                } finally {                    spinLock.unlock();                    System.out.println(Thread.currentThread().getName() + &quot;释放了了自旋锁&quot;);                }            }        };        Thread thread1 = new Thread(runnable);        Thread thread2 = new Thread(runnable);        thread1.start();        thread2.start();    }}这段代码的运行结果是：...自旋了自旋了自旋了自旋了自旋了自旋了自旋了自旋了Thread-0释放了了自旋锁Thread-1获取到了自旋锁前面会打印出很多“自旋了”，说明自旋期间，CPU依然在不停运转。缺点那么自旋锁有没有缺点呢？其实自旋锁是有缺点的。它最大的缺点就在于虽然避免了线程切换的开销，但是它在避免线程切换开销的同时也带来了新的开销，因为它需要不停得去尝试获取锁。如果这把锁一直不能被释放，那么这种尝试只是无用的尝试，会白白浪费处理器资源。也就是说，虽然一开始自旋锁的开销低于线程切换，但是随着时间的增加，这种开销也是水涨船高，后期甚至会超过线程切换的开销，得不偿失。适用场景所以我们就要看一下自旋锁的适用场景。首先，自旋锁适用于并发度不是特别高的场景，以及临界区比较短小的情况，这样我们可以利用避免线程切换来提高效率。可是如果临界区很大，线程一旦拿到锁，很久才会释放的话，那就不合适用自旋锁，因为自旋会一直占用 CPU 却无法拿到锁，白白消耗资源。 本文流程图参考自https://tech.meituan.com/2018/11/15/java-lock.html自旋锁的实现的代码来自https://www.fatalerrors.org/a/java-implementation-of-spin-lock.html 将阻塞状态的线程编程 runnable 是操作系统干的" }, { "title": "读锁的插队吗以及读写锁的升降级", "url": "/posts/ReadWriteLock-Downgrade/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-11 15:33:00 +0000", "snippet": "读锁插队策略ReentrantLock，如果锁被设置为非公平，那么它是可以在前面线程释放锁的瞬间进行插队的，而不需要进行排队。在读写锁这里，策略也是这样的吗？首先，看到 ReentrantReadWriteLock 可以设置为公平或者非公平，代码如下：// 公平锁ReentrantReadWriteLock reentrantReadWriteLock = new ReentrantReadWriteLock(true);// 非公平锁ReentrantReadWriteLock reentrantReadWriteLock = new ReentrantReadWriteLock(false);如果是公平锁，就在构造函数的参数中传入 true；如果是非公平锁，就在构造函数的参数中传入 false。默认是非公平锁。在获取读锁之前，线程会检查 readerShouldBlock() 方法，同样，在获取写锁之前，线程会检查 writerShouldBlock() 方法，来决定是否需要插队或者是去排队。公平锁对于这两个方法的实现：final boolean writerShouldBlock() { return hasQueuedPredecessors();}final boolean readerShouldBlock() { return hasQueuedPredecessors();}很明显，在公平锁的情况下，只要等待队列中有线程在等待，也就是 hasQueuedPredecessors() 返回 true 的时候，那么 writer 和 reader 都会 block，也就是一律不允许插队，都乖乖去排队，这也符合公平锁的思想。非公平锁的实现：final boolean writerShouldBlock() { // writers can always barge return false;}final boolean readerShouldBlock() { return apparentlyFirstQueuedIsExclusive();}在 writerShouldBlock() 这个方法中始终返回 false，可以看出，对于想获取写锁的线程而言，由于返回值是 false，所以它是随时可以插队的，这就和我们的 ReentrantLock 的设计思想是一样的，但是读锁却不一样。这里实现的策略很有意思，先让我们来看下面这种场景：假设线程 2 和线程 4 正在同时读取，线程 3 想要写入，但是由于线程 2 和线程 4 已经持有读锁了，所以线程 3 就进入等待队列进行等待。此时，线程 5 突然跑过来想要插队获取读锁：第一种策略：允许插队由于现在有线程在读，而线程 5 又不会特别增加它们读的负担，因为线程们可以共用这把锁，所以第一种策略就是让线程 5 直接加入到线程 2 和线程 4 一起去读取。这种策略看上去增加了效率，但是有一个严重的问题，那就是如果想要读取的线程不停地增加，比如线程 6，那么线程 6 也可以插队，这就会导致读锁长时间内不会被释放，导致线程 3 长时间内拿不到写锁，也就是那个需要拿到写锁的线程会陷入“饥饿”状态，它将在长时间内得不到执行。第二种策略：不允许插队这种策略认为由于线程 3 已经提前等待了，所以虽然线程 5 如果直接插队成功，可以提高效率，但是我们依然让线程 5 去排队等待：按照这种策略线程 5 会被放入等待队列中，并且排在线程 3 的后面，让线程 3 优先于线程 5 执行，这样可以避免“饥饿”状态，这对于程序的健壮性是很有好处的，直到线程 3 运行完毕，线程 5 才有机会运行，这样谁都不会等待太久的时间。策略选择演示策略的选择取决于具体锁的实现，ReentrantReadWriteLock 的实现选择了策略 2 ，是很明智的。下面我们就用实际的代码来演示一下上面这种场景。策略演示代码如下所示：/** * 描述： 演示读锁不插队 */public class ReadLockJumpQueue { private static final ReentrantReadWriteLock reentrantReadWriteLock = new ReentrantReadWriteLock(); private static final ReentrantReadWriteLock.ReadLock readLock = reentrantReadWriteLock.readLock(); private static final ReentrantReadWriteLock.WriteLock writeLock = reentrantReadWriteLock.writeLock(); private static void read() { readLock.lock(); try { System.out.println(Thread.currentThread().getName() + &quot;得到读锁，正在读取&quot;); Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } finally { System.out.println(Thread.currentThread().getName() + &quot;释放读锁&quot;); readLock.unlock(); } } private static void write() { writeLock.lock(); try { System.out.println(Thread.currentThread().getName() + &quot;得到写锁，正在写入&quot;); Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } finally { System.out.println(Thread.currentThread().getName() + &quot;释放写锁&quot;); writeLock.unlock(); } } public static void main(String[] args) throws InterruptedException { new Thread(() -&amp;gt; read(),&quot;Thread-2&quot;).start(); new Thread(() -&amp;gt; read(),&quot;Thread-4&quot;).start(); new Thread(() -&amp;gt; write(),&quot;Thread-3&quot;).start(); new Thread(() -&amp;gt; read(),&quot;Thread-5&quot;).start(); }}以上代码的运行结果是：Thread-2得到读锁, 正在读取Thread-4得到读锁, 正在读取Thread-2释放读锁Thread-4释放读锁Thread-3得到写锁, 正在写入Thread-3释放写锁Thread-5得到读锁, 正在读取Thread-5释放读锁从这个结果可以看出，ReentrantReadWriteLock 的实现选择了“不允许插队”的策略，这就大大减小了发生“饥饿”的概率。（如果运行结果和课程不一致，可以在每个线程启动后增加 100ms 的睡眠时间，以便保证线程的运行顺序）。锁的升降级读写锁降级功能代码演示锁的升降级，看一下这段代码，这段代码演示了在更新缓存的时候，如何利用锁的降级功能。public class CachedData { Object data; volatile boolean cacheValid; final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); void processCachedData() { rwl.readLock().lock(); if (!cacheValid) { //在获取写锁之前，必须首先释放读锁。 rwl.readLock().unlock(); rwl.writeLock().lock(); try { // 这里需要再次判断数据的有效性； // 因为在释放读锁和获取写锁的空隙之内，可能有其他线程修改了数据。                if (!cacheValid) { data = new Object();                    cacheValid = true; }                // 在不释放写锁的情况下，直接获取读锁，这就是读写锁的降级。 rwl.readLock().lock();            } finally { // 释放了写锁，但是依然持有读锁 rwl.writeLock().unlock();            } }        try { System.out.println(data);        } finally { //释放读锁 rwl.readLock().unlock();        } }}在这段代码中有一个读写锁，最重要的就是中间的 processCachedData 方法，在这个方法中，会首先获取到读锁，也就是 rwl.readLock().lock()，它去判断当前的缓存是否有效，如果有效那么就直接跳过整个 if 语句，如果已经失效，代表需要更新这个缓存了。由于需要更新缓存，所以之前获取到的读锁是不够用的，需要获取写锁。在获取写锁之前，首先释放读锁，然后利用 rwl.writeLock().lock() 来获取到写锁，然后是经典的 try finally 语句，在 try 语句中我们首先判断缓存是否有效，因为在刚才释放读锁和获取写锁的过程中，可能有其他线程抢先修改了数据，所以在此我们需要进行二次判断。如果发现缓存是无效的，就用 new Object() 这样的方式来示意，获取到了新的数据内容，并把缓存的标记位设置为 ture，让缓存变得有效。由于我们后续希望打印出 data 的值，所以不能在此处释放掉所有的锁。我们的选择是在不释放写锁的情况下直接获取读锁，也就是rwl.readLock().lock() 这行语句所做的事情，然后，在持有读锁的情况下释放写锁，最后，在最下面的 try 中把 data 的值打印出来。这就是一个非常典型的利用锁的降级功能的代码。为什么要这么麻烦进行降级呢？我一直持有最高等级的写锁不就可以了吗？这样谁都没办法来影响到我自己的工作，永远是线程安全的。为什么需要锁的降级？如果我们在刚才的方法中，一直使用写锁，最后才释放写锁的话，虽然确实是线程安全的，但是也是没有必要的，因为我们只有一处修改数据的代码：data = new Object();后面我们对于 data 仅仅是读取。如果还一直使用写锁的话，就不能让多个线程同时来读取了，持有写锁是浪费资源的，降低了整体的效率，所以这个时候利用锁的降级是很好的办法，可以提高整体性能。支持锁的降级，不支持升级如果我们运行下面这段代码，在不释放读锁的情况下直接尝试获取写锁，也就是锁的升级，会让线程直接阻塞，程序是无法运行的。final static ReentrantReadWriteLock rwl = new ReentrantReadWriteLock();public static void main(String[] args) {    upgrade();}public static void upgrade() {    rwl.readLock().lock();    System.out.println(&quot;获取到了读锁&quot;);    rwl.writeLock().lock();    System.out.println(&quot;成功升级&quot;);}这段代码会打印出“获取到了读锁”，但是却不会打印出“成功升级”，因为 ReentrantReadWriteLock 不支持读锁升级到写锁。为什么不支持锁的升级？读写锁的特点是如果线程都申请读锁，是可以多个线程同时持有的，可是如果是写锁，只能有一个线程持有，并且不可能存在读锁和写锁同时持有的情况。正是因为不可能有读锁和写锁同时持有的情况，所以升级写锁的过程中，需要等到所有的读锁都释放，此时才能进行升级。假设有 A，B 和 C 三个线程，它们都已持有读锁。假设线程 A 尝试从读锁升级到写锁。那么它必须等待 B 和 C 释放掉已经获取到的读锁。如果随着时间推移，B 和 C 逐渐释放了它们的读锁，此时线程 A 确实是可以成功升级并获取写锁。但是考虑一种特殊情况。假设线程 A 和 B 都想升级到写锁，那么对于线程 A 而言，它需要等待其他所有线程，包括线程 B 在内释放读锁。而线程 B 也需要等待所有的线程，包括线程 A 释放读锁。这就是一种非常典型的死锁的情况。谁都愿不愿意率先释放掉自己手中的锁。但是读写锁的升级并不是不可能的，也有可以实现的方案，如果保证每次只有一个线程可以升级，那么就可以保证线程安全。只不过最常见的 ReentrantReadWriteLock 对此并不支持。总结对于 ReentrantReadWriteLock 而言。 插队策略 公平策略下，只要队列里有线程已经在排队，就不允许插队。 非公平策略下： 如果允许读锁插队，那么由于读锁可以同时被多个线程持有，所以可能造成源源不断的后面的线程一直插队成功，导致读锁一直不能完全释放，从而导致写锁一直等待，为了防止“饥饿”，在等待队列的头结点是尝试获取写锁的线程的时候，不允许读锁插队。 写锁可以随时插队，因为写锁并不容易插队成功，写锁只有在当前没有任何其他线程持有读锁和写锁的时候，才能插队成功，同时写锁一旦插队失败就会进入等待队列，所以很难造成“饥饿”的情况，允许写锁插队是为了提高效率。 升降级策略：只能从写锁降级为读锁，不能从读锁升级为写锁。 为什么获取读锁才能拿到 data —— 读本身是线程安全的，加读锁，主要是为了让写锁感知到，在有人读取的时候，不要同时写入。 为啥要获取读锁，直接释放写锁不香吗？ —— 直接释放写锁会导致其他线程可能抢到该锁，但是有时业务并不想完全释放该锁。 除了读读，其他都互斥。这里有了写锁，然后又申请读锁，不互斥吗？还是说同一线程，不互斥?如果同一线程，不互斥，那为什么 申请了读锁的情况下，又不能申请写锁了？（升级）" }, { "title": "读写锁 ReadWriteLock 获取锁的规则", "url": "/posts/ReadWriteLock-rules/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-10 15:33:00 +0000", "snippet": "在没有读写锁之前，假设使用普通的 ReentrantLock，那么虽然我们保证了线程安全，但是也浪费了一定的资源，因为如果多个读操作同时进行，其实并没有线程安全问题，我们可以允许让多个读操作并行，以便提高程序效率。但是写操作不是线程安全的，如果多个线程同时写，或者在写的同时进行读操作，便会造成线程安全问题。我们的读写锁就解决了这样的问题，它设定了一套规则，既可以保证多个线程同时读的效率，同时又可以保证有写入操作时的线程安全。整体思路是它有两把锁，第 1 把锁是写锁，获得写锁之后，既可以读数据又可以修改数据，而第 2 把锁是读锁，获得读锁之后，只能查看数据，不能修改数据。读锁可以被多个线程同时持有，所以多个线程可以同时查看数据。在读的地方合理使用读锁，在写的地方合理使用写锁，灵活控制，可以提高程序的执行效率。读写锁的获取规则我们在使用读写锁时遵守下面的获取规则： 当一个线程已经占有了读锁，那么其他线程如果想要申请读锁，可以申请成功； 当一个线程已经占有了读锁，而且有其他线程想要申请获取写锁的话，是不能申请成功的，因为读写互斥； 当一个线程已经占有了写锁，那么此时其他线程无论是想申请读锁还是写锁，都无法申请成功。所以我们用一句话总结：要么是一个或多个线程同时有读锁，要么是一个线程有写锁，但是两者不会同时出现。也可以总结为：读读共享、其他都互斥（写写互斥、读写互斥、写读互斥）。使用案例下面我们举个例子来应用读写锁，ReentrantReadWriteLock 是 ReadWriteLock 的实现类，最主要的有两个方法：readLock() 和 writeLock() 用来获取读锁和写锁。代码如下：/** * 描述： 演示读写锁用法 */public class ReadWriteLockDemo { private static final ReentrantReadWriteLock reentrantReadWriteLock = new ReentrantReadWriteLock(false); private static final ReentrantReadWriteLock.ReadLock readLock = reentrantReadWriteLock.readLock(); private static final ReentrantReadWriteLock.WriteLock writeLock = reentrantReadWriteLock.writeLock(); private static void read() { readLock.lock(); try { System.out.println(Thread.currentThread().getName() + &quot;得到读锁，正在读取&quot;); Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } finally { System.out.println(Thread.currentThread().getName() + &quot;释放读锁&quot;); readLock.unlock(); } } private static void write() { writeLock.lock(); try { System.out.println(Thread.currentThread().getName() + &quot;得到写锁，正在写入&quot;); Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } finally { System.out.println(Thread.currentThread().getName() + &quot;释放写锁&quot;); writeLock.unlock(); } } public static void main(String[] args) throws InterruptedException { new Thread(() -&amp;gt; read()).start(); new Thread(() -&amp;gt; read()).start(); new Thread(() -&amp;gt; write()).start(); new Thread(() -&amp;gt; write()).start(); }}程序的运行结果是：Thread-0得到读锁，正在读取Thread-1得到读锁，正在读取Thread-0释放读锁Thread-1释放读锁Thread-2得到写锁，正在写入Thread-2释放写锁Thread-3得到写锁，正在写入Thread-3释放写锁可以看出，读锁可以同时被多个线程获得，而写锁不能。读写锁适用场合最后我们来看下读写锁的适用场合，相比于 ReentrantLock 适用于一般场合，ReadWriteLock 适用于读多写少的情况，合理使用可以进一步提高并发效率。 对于为什么要对读加锁：读本身是线程安全的，加读锁，主要是为了让写锁感知到，在有人读取的时候，不要同时写入。 如果都是”读”这一操作，本身没有线程安全问题；但是既存在对共享变量的读，又存在写操作得话，假如读操作不加锁，读的过程中，共享变量是允许其他线程修改的，那么就可能发生问题，读到的不是原期望值。" }, { "title": "公平锁和非公平锁", "url": "/posts/fair-lock-nonfair-lock/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-09 15:33:00 +0000", "snippet": "什么是公平和非公平公平锁指的是按照线程请求的顺序，来分配锁；非公平锁指的是不完全按照请求的顺序，在一定情况下，可以允许插队。但需要注意这里的非公平并不是指完全的随机，不是说线程可以任意插队，而是仅仅“在合适的时机”插队。那么什么时候是合适的时机呢？假设当前线程在请求获取锁的时候，恰巧前一个持有锁的线程释放了这把锁，那么当前申请锁的线程就可以不顾已经等待的线程而选择立刻插队。但是如果当前线程请求的时候，前一个线程并没有在那一时刻释放锁，那么当前线程还是一样会进入等待队列。为了能够更好的理解公平锁和非公平锁，我们举一个生活中的例子，假设我们还在学校读书，去食堂排队买饭，我排在队列的第二个，我前面还有一位同学，但此时我脑子里想的不是午饭，而是上午的一道数学题并陷入深思，所以当前面的同学打完饭之后轮到我时我走神了，并也没注意到现在轮到我了，此时前面的同学突然又回来插队，说“不好意思，阿姨麻烦给我加个鸡腿”，像这样的行为就可以类比我们的公平锁和非公平锁。看到这里，你可能不解，为什么要设置非公平策略呢，而且非公平还是 ReentrantLock的默认策略，如果我们不加以设置的话默认就是非公平的，难道我的这些排队的时间都白白浪费了吗，为什么别人比我有优先权呢？毕竟公平是一种很好的行为，而非公平是一种不好的行为。考虑一种情况，假设线程 A 持有一把锁，线程 B 请求这把锁，由于线程 A 已经持有这把锁了，所以线程 B 会陷入等待，在等待的时候线程 B 会被挂起，也就是进入阻塞状态，那么当线程 A 释放锁的时候，本该轮到线程 B 苏醒获取锁，但如果此时突然有一个线程 C 插队请求这把锁，那么根据非公平的策略，会把这把锁给线程 C，这是因为唤醒线程 B 是需要很大开销的，很有可能在唤醒之前，线程 C 已经拿到了这把锁并且执行完任务释放了这把锁。相比于等待唤醒线程 B 的漫长过程，插队的行为会让线程 C 本身跳过陷入阻塞的过程，如果在锁代码中执行的内容不多的话，线程 C 就可以很快完成任务，并且在线程 B 被完全唤醒之前，就把这个锁交出去，这样是一个双赢的局面，对于线程 C 而言，不需要等待提高了它的效率，而对于线程 B 而言，它获得锁的时间并没有推迟，因为等它被唤醒的时候，线程 C 早就释放锁了，因为线程 C 的执行速度相比于线程 B 的唤醒速度，是很快的，所以 Java 设计者设计非公平锁，是为了提高整体的运行效率。公平的场景用图示来说明公平和非公平的场景，先来看公平的情况。假设我们创建了一个公平锁，此时有 4 个线程按顺序来请求公平锁，线程 1 在拿到这把锁之后，线程 2、3、4 会在等待队列中开始等待，然后等线程 1 释放锁之后，线程 2、3、4 会依次去获取这把锁，线程 2 先获取到的原因是它等待的时间最长。不公平的场景下面我们再来看看非公平的情况，假设线程 1 在解锁的时候，突然有线程 5 尝试获取这把锁，那么根据我们的非公平策略，线程 5 是可以拿到这把锁的，尽管它没有进入等待队列，而且线程 2、3、4 等待的时间都比线程 5 要长，但是从整体效率考虑，这把锁此时还是会交给线程 5 持有。代码案例：演示公平和非公平的效果下面公平和非公平的实际效果，代码如下：package cn.happymaya.base.lock;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * 描述：演示公平锁，分别展示公平和不公平的情况，非公平锁会让现在持有锁的线程优先再次获取到锁。代码借鉴自Java并发编程实战手册2.7。 */public class FairAndUnfair { public static void main(String args[]) { PrintQueue printQueue = new PrintQueue(); Thread thread[] = new Thread[10]; for (int i = 0; i &amp;lt; 10; i++) { thread[i] = new Thread(new Job(printQueue), &quot;Thread &quot; + i); } for (int i = 0; i &amp;lt; 10; i++) { thread[i].start(); try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } } }}class Job implements Runnable { private PrintQueue printQueue; public Job(PrintQueue printQueue) { this.printQueue = printQueue; } @Override public void run() { System.out.printf(&quot;%s: Going to print a job\\n&quot;, Thread.currentThread().getName()); printQueue.printJob(new Object()); System.out.printf(&quot;%s: The document has been printed\\n&quot;, Thread.currentThread().getName()); }}class PrintQueue { private final Lock queueLock = new ReentrantLock(false); public void printJob(Object document) { queueLock.lock(); try { Long duration = (long) (Math.random() * 10000); System.out.printf(&quot;%s: PrintQueue: Printing a Job during %d seconds\\n&quot;, Thread.currentThread().getName(), (duration / 1000)); Thread.sleep(duration); } catch (InterruptedException e) { e.printStackTrace(); } finally { queueLock.unlock(); } queueLock.lock(); try { Long duration = (long) (Math.random() * 10000); System.out.printf(&quot;%s: PrintQueue: Printing a Job during %d seconds\\n&quot;, Thread.currentThread().getName(), (duration / 1000)); Thread.sleep(duration); } catch (InterruptedException e) { e.printStackTrace(); } finally { queueLock.unlock(); } }}其中，可以通过改变 new ReentrantLock(false) 中的参数来设置公平/非公平锁。以上代码在公平的情况下的输出：Thread 0: Going to print a jobThread 0: PrintQueue: Printing a Job during 6 secondsThread 1: Going to print a jobThread 2: Going to print a jobThread 3: Going to print a jobThread 4: Going to print a jobThread 5: Going to print a jobThread 6: Going to print a jobThread 7: Going to print a jobThread 8: Going to print a jobThread 9: Going to print a jobThread 0: PrintQueue: Printing a Job during 8 secondsThread 0: The document has been printedThread 1: PrintQueue: Printing a Job during 9 secondsThread 1: PrintQueue: Printing a Job during 8 secondsThread 1: The document has been printedThread 2: PrintQueue: Printing a Job during 6 secondsThread 2: PrintQueue: Printing a Job during 4 secondsThread 2: The document has been printedThread 3: PrintQueue: Printing a Job during 9 secondsThread 3: PrintQueue: Printing a Job during 8 secondsThread 3: The document has been printedThread 4: PrintQueue: Printing a Job during 4 secondsThread 4: PrintQueue: Printing a Job during 2 secondsThread 4: The document has been printedThread 5: PrintQueue: Printing a Job during 2 secondsThread 5: PrintQueue: Printing a Job during 5 secondsThread 5: The document has been printedThread 6: PrintQueue: Printing a Job during 2 secondsThread 6: PrintQueue: Printing a Job during 6 secondsThread 6: The document has been printedThread 7: PrintQueue: Printing a Job during 6 secondsThread 7: PrintQueue: Printing a Job during 4 secondsThread 7: The document has been printedThread 8: PrintQueue: Printing a Job during 3 secondsThread 8: PrintQueue: Printing a Job during 6 secondsThread 8: The document has been printedThread 9: PrintQueue: Printing a Job during 3 secondsThread 9: PrintQueue: Printing a Job during 5 secondsThread 9: The document has been printed可以看出，线程直接获取锁的顺序是完全公平的，先到先得。而以上代码在非公平的情况下的输出是这样的：Thread 0: Going to print a jobThread 0: PrintQueue: Printing a Job during 6 secondsThread 1: Going to print a jobThread 2: Going to print a jobThread 3: Going to print a jobThread 4: Going to print a jobThread 5: Going to print a jobThread 6: Going to print a jobThread 7: Going to print a jobThread 8: Going to print a jobThread 9: Going to print a jobThread 0: PrintQueue: Printing a Job during 8 secondsThread 0: The document has been printedThread 1: PrintQueue: Printing a Job during 9 secondsThread 1: PrintQueue: Printing a Job during 8 secondsThread 1: The document has been printedThread 2: PrintQueue: Printing a Job during 6 secondsThread 2: PrintQueue: Printing a Job during 4 secondsThread 2: The document has been printedThread 3: PrintQueue: Printing a Job during 9 secondsThread 3: PrintQueue: Printing a Job during 8 secondsThread 3: The document has been printedThread 4: PrintQueue: Printing a Job during 4 secondsThread 4: PrintQueue: Printing a Job during 2 secondsThread 4: The document has been printedThread 5: PrintQueue: Printing a Job during 2 secondsThread 5: PrintQueue: Printing a Job during 5 secondsThread 5: The document has been printedThread 6: PrintQueue: Printing a Job during 2 secondsThread 6: PrintQueue: Printing a Job during 6 secondsThread 6: The document has been printedThread 7: PrintQueue: Printing a Job during 6 secondsThread 7: PrintQueue: Printing a Job during 4 secondsThread 7: The document has been printedThread 8: PrintQueue: Printing a Job during 3 secondsThread 8: PrintQueue: Printing a Job during 6 secondsThread 8: The document has been printedThread 9: PrintQueue: Printing a Job during 3 secondsThread 9: PrintQueue: Printing a Job during 5 secondsThread 9: The document has been printed可以看出，非公平情况下，存在抢锁“插队”的现象，比如Thread 0 在释放锁后又能优先获取到锁，虽然此时在等待队列中已经有 Thread 1 ~ Thread 9 在排队了。公平和非公平的优缺点   优势 劣势 公平锁 各线程公平平等，每个线程在等待一段时间后，总有执行的机会 更慢，吞吐量更小 不公平锁 更快，吞吐量更大 有可能产生线程在长时间内，始终得不到执行 公平锁的优点在于各个线程公平平等，每个线程等待一段时间后，都有执行的机会，而它的缺点就在于整体执行速度更慢，吞吐量更小，相反非公平锁的优势就在于整体执行速度更快，吞吐量更大，但同时也可能产生线程饥饿问题，也就是说如果一直有线程插队，那么在等待队列中的线程可能长时间得不到运行。源码分析公平和非公平锁的源码，具体看下它们是怎样实现的，看看 ReentrantLock 的源码，代码如下：public class ReentrantLock implements Lock, java.io.Serializable { private static final long serialVersionUID = 7373984872572414699L; /** Synchronizer providing all implementation mechanics */ private final Sync sync;}Sync 类的代码：abstract static class Sync extends AbstractQueuedSynchronizer {...}根据代码可知，非公平锁 NonfairSync 和公平锁 FairSync 继承了Sync：static final class NonfairSync extends Sync {...}static final class FairSync extends Sync {...}公平锁的锁获取源码如下：protected final boolean tryAcquire(int acquires) {    final Thread current = Thread.currentThread();    int c = getState();    if (c == 0) {        if (!hasQueuedPredecessors() &amp;amp;&amp;amp; //这里判断了 hasQueuedPredecessors()                compareAndSetState(0, acquires)) {            setExclusiveOwnerThread(current);            return true;        }    } else if (current == getExclusiveOwnerThread()) {        int nextc = c + acquires;        if (nextc &amp;lt; 0) {            throw new Error(&quot;Maximum lock count exceeded&quot;);        }        setState(nextc);        return true;    }    return false;}非公平锁的锁获取源码如下：final boolean nonfairTryAcquire(int acquires) {    final Thread current = Thread.currentThread();    int c = getState();    if (c == 0) {        if (compareAndSetState(0, acquires)) { //这里没有判断      hasQueuedPredecessors()            setExclusiveOwnerThread(current);            return true;        }    } else if (current == getExclusiveOwnerThread()) {        int nextc = c + acquires;        if (nextc &amp;lt; 0) // overflow       throw new Error(&quot;Maximum lock count exceeded&quot;);        setState(nextc);        return true;    }    return false;}通过对比，我们可以发现这两个方法其实整体思路都是很类似的，而最大的不同点就是在于非公平锁缺少了一个hasQueuedPredecessors的判断。这个方法就是判断在等待队列中是否已经有线程在排队了。这也就是公平锁和非公平锁的核心区别，如果是公平锁，那么一旦已经有线程在排队了，当前线程就不再尝试获取锁；对于非公平锁而言，无论是否已经有线程在排队，都会尝试获取一下锁，获取不到的话，再去排队。这里有一个特例需要我们注意，针对 tryLock() 方法，它不遵守设定的公平原则。例如，当有线程执行 tryLock() 方法的时候，一旦有线程释放了锁，那么这个正在 tryLock 的线程就能获取到锁，即使设置的是公平锁模式，即使在它之前已经有其他正在等待队列中等待的线程，简单地说就是 tryLock 可以插队。看它的源码就会发现：public boolean tryLock() {    return sync.nonfairTryAcquire(1);}这里调用的就是 nonfairTryAcquire()，表明了是不公平的，和锁本身是否是公平锁无关。最后我们做一下总结。非公平锁在特定的情况下可以插队，这虽然破坏了整体的执行顺序，但是却也因此在更宏观的层面上提升了程序的运行效率。 公平锁实例main中启动线程后不加sleep还是会乱序呀 ———— 线程先 start 不代表先执行，需要听从线程调度器的安排，所以需要加 sleep 保证顺序。 非公平锁与公平锁唯一区别看起来就是某个新来的线程去获取锁，是否考虑等待队列;那么如果都获取不到锁，进入等待队列后，持有锁的线程解锁后，是不是无论是什么锁，都随机唤醒呢 - 不是随机，是按队列里的顺序。" }, { "title": "Lock 的常用方法", "url": "/posts/Lock-common-metod/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-05 15:33:00 +0000", "snippet": "简介Lock 接口是 Java 5 引入的，最常见的实现类是 ReentrantLock，可以起到“锁”的作用。Lock 和 synchronized 是两种最常见的锁，锁是一种工具，用于控制对共享资源的访问，而 Lock 和 synchronized 都可以达到线程安全的目的，但是在使用上和功能上又有较大的不同。所以 Lock 并不是用来代替 synchronized 的，而是当使用 synchronized 不合适或不足以满足要求的时候，Lock 可以用来提供更高级功能的。通常情况下，Lock 只允许一个线程来访问这个共享资源。不过有的时候，一些特殊的实现也可允许并发访问，比如 ReadWriteLock 里面的 ReadLock。方法Lock 接口的各个方法，如代码所示。public interface Lock {    void lock();    void lockInterruptibly() throws InterruptedException;    boolean tryLock();    boolean tryLock(long time, TimeUnit unit) throws InterruptedException;    void unlock();    Condition newCondition();}Lock 接口加解锁相关的主要有 5 个方法，这 5 种方法分别是： lock()； tryLock()； tryLock(long time, TimeUnit unit) ； lockInterruptibly()； unlock()lock() 方法在 Lock 接口中声明了 4 种方法来获取锁： lock()； tryLock()； tryLock(long time, TimeUnit unit)； lockInterruptibly()）首先，lock() 是最基础的获取锁的方法。在线程获取锁时如果锁已被其他线程获取，则进行等待，是最初级的获取锁的方法。对于 Lock 接口而言，获取锁和释放锁都是显式的，不像 synchronized 那样是隐式的，所以 Lock 不会像 synchronized 一样在异常时自动释放锁（synchronized 即使不写对应的代码也可以释放），lock 的加锁和释放锁都必须以代码的形式写出来，所以使用 lock() 时必须由我们自己主动去释放锁，因此最佳实践是执行 lock() 后，首先在 try{} 中操作同步资源，如果有必要就用 catch{} 块捕获异常，然后在 finally{} 中释放锁，以保证发生异常时锁一定被释放，示例代码如下所示。Lock lock = ...;lock.lock();try { // 获取到了被本锁保护的资源，处理任务 // 捕获异常} finally { lock.unlock(); // 释放锁}在这段代码中我们创建了一个 Lock，并且用 Lock 方法加锁，然后立刻在 try 代码块中进行相关业务逻辑的处理，如果有需要还可以进行 catch 来捕获异常，但是最重要的是 finally，大家一定不要忘记在 finally 中添加 unlock() 方法，以便保障锁的绝对释放。如果不遵守在 finally 里释放锁的规范，就会让 Lock 变得非常危险，因为不知道未来什么时候由于异常的发生，导致跳过了 unlock() 语句，使得这个锁永远不能被释放了，其他线程也无法再获得这个锁，这就是 Lock 相比于 synchronized 的一个劣势，使用 synchronized 时不需要担心这个问题。与此同时，lock() 方法不能被中断，这会带来很大的隐患：一旦陷入死锁，lock() 就会陷入永久等待，所以一般用 tryLock() 等其他更高级的方法来代替 lock()。tryLock()tryLock() 用来尝试获取锁，如果当前锁没有被其他线程占用，则获取成功，返回 true，否则返回 false，代表获取锁失败。相比于 lock()，这样的方法显然功能更强大，我们可以根据是否能获取到锁来决定后续程序的行为。因为该方法会立即返回，即便在拿不到锁时也不会一直等待，所以通常情况下，用 if 语句判断 tryLock() 的返回结果，根据是否获取到锁来执行不同的业务逻辑，典型使用方法如下。Lock lock = ...;if(lock.tryLock()) {     try {         //处理任务     } finally{         lock.unlock();   //释放锁     } } else {    //如果不能获取锁，则做其他事情}我们创建 lock() 方法之后使用 tryLock() 方法并用 if 语句判断它的结果，如果 if 语句返回 true，就使用 try finally 完成相关业务逻辑的处理，如果 if 语句返回 false 就会进入 else 语句，代表它暂时不能获取到锁，可以先去做一些其他事情，比如等待几秒钟后重试，或者跳过这个任务，有了这个强大的 tryLock() 方法我们便可以解决死锁问题，代码如下所示。public void tryLock(Lock lock1, Lock lock2) throws InterruptedException {        while (true) {            if (lock1.tryLock()) {                try {                    if (lock2.tryLock()) {                        try {                            System.out.println(&quot;获取到了两把锁，完成业务逻辑&quot;);                            return;                        } finally {                            lock2.unlock();                        }                    }                } finally {                    lock1.unlock();                }            } else {                Thread.sleep(new Random().nextInt(1000));            }        }    }如果代码中我们不用 tryLock() 方法，那么便可能会产生死锁，比如有两个线程同时调用这个方法，传入的 lock1 和 lock2 恰好是相反的，那么如果第一个线程获取了 lock1 的同时，第二个线程获取了 lock2，它们接下来便会尝试获取对方持有的那把锁，但是又获取不到，于是便会陷入死锁，但是有了 tryLock() 方法之后，我们便可以避免死锁的发生，首先会检测 lock1 是否能获取到，如果能获取到再尝试获取 lock2，但如果 lock1 获取不到也没有关系，我们会在下面进行随机时间的等待，这个等待的目标是争取让其他的线程在这段时间完成它的任务，以便释放其他线程所持有的锁，以便后续供我们使用，同理如果获取到了 lock1 但没有获取到 lock2，那么也会释放掉 lock1，随即进行随机的等待，只有当它同时获取到 lock1 和 lock2 的时候，才会进入到里面执行业务逻辑，比如在这里我们会打印出“获取到了两把锁，完成业务逻辑”，然后方法便会返回。tryLock(long time, TimeUnit unit)tryLock() 的重载方法是 tryLock(long time, TimeUnit unit)，这个方法和 tryLock() 很类似，区别在于 tryLock(long time, TimeUnit unit) 方法会有一个超时时间，在拿不到锁时会等待一定的时间，如果在时间期限结束后，还获取不到锁，就会返回 false；如果一开始就获取锁或者等待期间内获取到锁，则返回 true。这个方法解决了 lock() 方法容易发生死锁的问题，使用 tryLock(long time, TimeUnit unit) 时，在等待了一段指定的超时时间后，线程会主动放弃这把锁的获取，避免永久等待；在等待的期间，也可以随时中断线程，这就避免了死锁的发生。本方法和下面介绍的 lockInterruptibly() 是非常类似的，让我们来看一下 lockInterruptibly() 方法。lockInterruptibly()这个方法的作用就是去获取锁，如果这个锁当前是可以获得的，那么这个方法会立刻返回，但是如果这个锁当前是不能获得的（被其他线程持有），那么当前线程便会开始等待，除非它等到了这把锁或者是在等待的过程中被中断了，否则这个线程便会一直在这里执行这行代码。一句话总结就是，除非当前线程在获取锁期间被中断，否则便会一直尝试获取直到获取到为止。顾名思义，lockInterruptibly() 是可以响应中断的。相比于不能响应中断的 synchronized 锁，lockInterruptibly() 可以让程序更灵活，可以在获取锁的同时，保持对中断的响应。我们可以把这个方法理解为超时时间是无穷长的 tryLock(long time, TimeUnit unit)，因为 tryLock(long time, TimeUnit unit) 和 lockInterruptibly() 都能响应中断，只不过 lockInterruptibly() 永远不会超时。这个方法本身是会抛出 InterruptedException 的，所以使用的时候，如果不在方法签名声明抛出该异常，那么就要写两个 try 块，如下所示。public void lockInterruptibly() {        try {            lock.lockInterruptibly();            try {                System.out.println(&quot;操作资源&quot;);            } finally {                lock.unlock();            }        } catch (InterruptedException e) {            e.printStackTrace();        } }在这个方法中我们首先执行了 lockInterruptibly 方法，并且对它进行了 try catch 包装，然后同样假设我们能够获取到这把锁，和之前一样，就必须要使用 try finall 来保障锁的绝对释放。unlock()最后要介绍的方法是 unlock() 方法，是用于解锁的，u方法比较简单，对于 ReentrantLock 而言，执行 unlock() 的时候，内部会把锁的“被持有计数器”减 1，直到减到 0 就代表当前这把锁已经完全释放了，如果减 1 后计数器不为 0，说明这把锁之前被“重入”了，那么锁并没有真正释放，仅仅是减少了持有的次数。 能让lockInterruptibly响应的中断有哪些？只能程序异常么 —— 不是异常，是中断，比如调用Thread的interrupt()方法。 如果减 1 后计数器不为 0，说明这把锁之前被“重入”了，那么锁并没有真正释放，仅仅是减少了持有的次数？怎样才能真正释放，如果有重入的情况？ - 当计数器减到0的时候，代表真正释放了。 —— 当计数器减到0的时候，代表真正释放了。 使用了lockInterruptibly就可以不用notify来唤醒等待线程了吧 —— lockInterruptibly和notify属于两组不同的概念。在wait期间，中断或者notify都可以唤醒线程。" }, { "title": "synchronized 与 lock", "url": "/posts/sysnchronzied-vs-lock/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-04 15:33:00 +0000", "snippet": "synchronized 和 Lock 的异同点，以及该如何选择。相同点synchronized 和 Lock 的相同点非常多，我们这里重点讲解 3 个比较大的相同点。 synchronized 和 Lock 都是用来保护资源线程安全的。这一点毋庸置疑，这是它们的基本作用。 都可以保证可见性。对于 synchronized 而言，线程 A 在进入 synchronized 块之前或在 synchronized 块内进行操作，对于后续的获得同一个 monitor 锁的线程 B 是可见的，也就是线程 B 是可以看到线程 A 之前的操作的，这也体现了 happens-before 针对 synchronized 的一个原则。而对于 Lock 而言，它和 synchronized 是一样，都可以保证可见性，如图所示，在解锁之前的所有操作对加锁之后的所有操作都是可见的。如果你之前不了解什么是可见性，此时理解可能会有一定的困难，可以在学习本专栏的 Java 内存模型相关内容后，再复习本课时，就会豁然开朗。 synchronized 和 ReentrantLock 都拥有可重入的特点。这里的 ReentrantLock 是 Lock 接口的一个最主要的实现类，在对比 synchronized 和 Lock 的时候，也会选择 Lock 的主要实现类来进行对比。可重入指的是某个线程如果已经获得了一个锁，现在试图再次请求这个它已经获得的锁，如果它无需提前释放这个锁，而是直接可以继续使用持有的这个锁，那么就是可重入的。如果必须释放锁后才能再次申请这个锁，就是不可重入的。而 synchronized 和 ReentrantLock 都具有可重入的特性。不同点下面我们来看下 synchronized 和 Lock 的区别，和相同点一样，它们之间也有非常多的区别，这里讲解其中比较大的 7 点不同。 用法区别synchronized 关键字可以加在方法上，不需要指定锁对象（此时的锁对象为 this），也可以新建一个同步代码块并且自定义 monitor 锁对象；而 Lock 接口必须显示用 Lock 锁对象开始加锁 lock() 和解锁 unlock()，并且一般会在 finally 块中确保用 unlock() 来解锁，以防发生死锁。与 Lock 显式的加锁和解锁不同的是 synchronized 的加解锁是隐式的，尤其是抛异常的时候也能保证释放锁，但是 Java 代码中并没有相关的体现。 加解锁顺序不同对于 Lock 而言如果有多把 Lock 锁，Lock 可以不完全按照加锁的反序解锁，比如我们可以先获取 Lock1 锁，再获取 Lock2 锁，解锁时则先解锁 Lock1，再解锁 Lock2，加解锁有一定的灵活度，如代码所示。lock1.lock();lock2.lock();···lock1.unlock();lock2.unlock();但是 synchronized 无法做到，synchronized 解锁的顺序和加锁的顺序必须完全相反，例如：synchronized(obj1){ synchronized(obj2){ ... }}那么在这里，顺序就是先对 obj1 加锁，然后对 obj2 加锁，然后对 obj2 解锁，最后解锁 obj1。这是因为 synchronized 加解锁是由 JVM 实现的，在执行完 synchronized 块后会自动解锁，所以会按照 synchronized 的嵌套顺序加解锁，不能自行控制。 synchronized 锁不够灵活一旦 synchronized 锁已经被某个线程获得了，此时其他线程如果还想获得，那它只能被阻塞，直到持有锁的线程运行完毕或者发生异常从而释放这个锁。如果持有锁的线程持有很长时间才释放，那么整个程序的运行效率就会降低，而且如果持有锁的线程永远不释放锁，那么尝试获取锁的线程只能永远等下去。相比之下，Lock 类在等锁的过程中，如果使用的是 lockInterruptibly 方法，那么如果觉得等待的时间太长了不想再继续等待，可以中断退出，也可以用 tryLock() 等方法尝试获取锁，如果获取不到锁也可以做别的事，更加灵活。 synchronized 锁只能同时被一个线程拥有，但是 Lock 锁没有这个限制例如在读写锁中的读锁，是可以同时被多个线程持有的，可是 synchronized 做不到。 原理区别synchronized 是内置锁，由 JVM 实现获取锁和释放锁的原理，还分为偏向锁、轻量级锁、重量级锁。Lock 根据实现不同，有不同的原理，例如 ReentrantLock 内部是通过 AQS 来获取和释放锁的。 是否可以设置公平/非公平公平锁是指多个线程在等待同一个锁时，根据先来后到的原则依次获得锁。ReentrantLock 等 Lock 实现类可以根据自己的需要来设置公平或非公平，synchronized 则不能设置。 性能区别在 Java 5 以及之前，synchronized 的性能比较低，但是到了 Java 6 以后，发生了变化，因为 JDK 对 synchronized 进行了很多优化，比如自适应自旋、锁消除、锁粗化、轻量级锁、偏向锁等，所以后期的 Java 版本里的 synchronized 的性能并不比 Lock 差。如何选择在 《Java 并发编程实战》和 《Java 核心技术》里都认为： 如果能不用最好既不使用 Lock 也不使用 synchronized。因为在许多情况下你可以使用 java.util.concurrent 包中的机制，它会为你处理所有的加锁和解锁操作，也就是推荐优先使用工具类来加解锁。 如果 synchronized 关键字适合你的程序， 那么请尽量使用它，这样可以减少编写代码的数量，减少出错的概率。因为一旦忘记在 finally 里 unlock，代码可能会出很大的问题，而使用 synchronized 更安全。 如果特别需要 Lock 的特殊功能，比如尝试获取锁、可中断、超时功能等，才使用 Lock。" }, { "title": "synchronized 背后的“monitor 锁”", "url": "/posts/sysnchronized-monitor/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-03 06:33:00 +0000", "snippet": "获取和释放 monitor 锁的时机最简单的同步方式就是利用 synchronized 关键字来修饰代码块或者修饰一个方法，那么这部分被保护的代码，在同一时刻就最多只有一个线程可以运行，而 synchronized 的背后正是利用 monitor 锁实现的。所以首先看下获取和释放 monitor 锁的时机，每个 Java 对象都可以用作一个实现同步的锁，这个锁也被称为内置锁或 monitor 锁，获得 monitor 锁的唯一途径就是进入由这个锁保护的同步代码块或同步方法，线程在进入被 synchronized 保护的代码块之前，会自动获取锁，并且无论是正常路径退出，还是通过抛出异常退出，在退出的时候都会自动释放锁。synchronized 修饰方法的代码的例子：public synchronized void method() { method body}我们看到 method() 方法是被 synchronized 修饰的，为了方便理解其背后的原理，我们把上面这段代码改写为下面这种等价形式的伪代码。public void method() { this.intrinsicLock.lock(); try { method body } finally { this.intrinsicLock.unlock(); }}在这种写法中，进入 method 方法后，立刻添加内置锁，并且用 try 代码块把方法保护起来，最后用 finally 释放这把锁，这里的 intrinsicLock 就是 monitor 锁。经过这样的伪代码展开之后，相信你对 synchronized 的理解就更加清晰了。用 javap 命令查看反汇编的结果JVM 实现 synchronized 方法和 synchronized 代码块的细节是不一样的，同步代码块同步代码块的实现，如代码所示。public class SynTest { public void synBlock() { synchronized (this) { System.out.println(&quot;happymaya&quot;); }}在 SynTest 类中的 synBlock 方法，包含一个同步代码块，synchronized 代码块中有一行代码打印了 happymaya 字符串，下面通过命令看下 synchronized 关键字到底做了什么事情： 首先用 cd 命令切换到 SynTest.java 类所在的路径，然后执行 javac SynTest.java，于是就会产生一个名为 SynTest.class 的字节码文件; 然后执行 javap -verbose SynTest.class，就可以看到对应的反汇编内容。关键信息如下：  public void synBlock();    descriptor: ()V    flags: ACC_PUBLIC    Code:      stack=2, locals=3, args_size=1         0: aload_0         1: dup         2: astore_1         3: monitorenter         4: getstatic     #2    // Field java/lang/System.out:Ljava/io/PrintStream;         7: ldc           #3    // String lagou         9: invokevirtual #4    // Method java/io/PrintStream.println:(Ljava/lang/String;)V        12: aload_1        13: monitorexit        14: goto          22        17: astore_2        18: aload_1        19: monitorexit        20: aload_2        21: athrow        22: return同步方法从上面可以看出，同步代码块是使用 monitorenter 和 monitorexit 指令实现的。对于 synchronized 方法，并不是依靠 monitorenter 和 monitorexit 指令实现的，被 javap 反汇编后可以看到，synchronized 方法和普通方法大部分是一样的，不同在于，这个方法会有一个叫作 ACC_SYNCHRONIZED 的 flag 修饰符，来表明它是同步方法。同步方法的代码如下所示：public synchronized void synMethod() {}对应的反汇编指令如下所示：public synchronized void synMethod(); descriptor: ()V flags: ACC_PUBLIC, ACC_SYNCHRONIZED Code: stack=0, locals=1, args_size=1 0: return LineNumberTable: line 16: 0可以看出，被 synchronized 修饰的方法会有一个 ACC_SYNCHRONIZED 标志。当某个线程要访问某个方法的时候，会首先检查方法是否有 ACC_SYNCHRONIZED 标志，如果有则需要先获得 monitor 锁，然后才能开始执行方法，方法执行之后再释放 monitor 锁。其他方面， synchronized 方法和刚才的 synchronized 代码块是很类似的，例如这时如果其他线程来请求执行方法，也会因为无法获得 monitor 锁而被阻塞。 monitor锁是操作系统层面的么 ? Java的线程是映射到操作系统原生线程之上的，如果要阻塞或唤醒一个线程就需要操作系统的帮忙。 “如果线程已经拥有了这个 monitor ，则它将重新进入，并且累加计数。”synchronize是不可重入锁，每次进入前不是需要先释放锁吗 - synchronized是可重入锁 synchronized 修饰方法的时候，使用对象的monitor 锁是当前对象吗？ - 是的" }, { "title": "悲观锁和乐观锁的本质", "url": "/posts/Pessimism-and-optimistic-lock/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-02 15:33:00 +0000", "snippet": "悲观锁和乐观锁是从是否锁住资源的角度进行分类的。悲观锁悲观锁比较悲观，它认为如果不锁住这个资源，别的线程就会来争抢，就会造成数据结果错误，所以悲观锁为了确保结果的正确性，会在每次获取并修改数据时，都把数据锁住，让其他线程无法访问该数据，这样就可以确保数据内容万无一失。这也和我们人类中悲观主义者的性格是一样的，悲观主义者做事情之前总是担惊受怕，所以会严防死守，保证别人不能来碰我的东西，这就是悲观锁名字的含义。我们举个例子，假设线程 A 和 B 使用的都是悲观锁，所以它们在尝试获取同步资源时，必须要先拿到锁。假设线程 A 拿到了锁，并且正在操作同步资源，那么此时线程 B 就必须进行等待。而当线程 A 执行完毕后，CPU 才会唤醒正在等待这把锁的线程 B 再次尝试获取锁。如果线程 B 现在获取到了锁，才可以对同步资源进行自己的操作。这就是悲观锁的操作流程。乐观锁乐观锁比较乐观，认为自己在操作资源的时候不会有其他线程来干扰，所以并不会锁住被操作对象，不会不让别的线程来接触它，同时，为了确保数据正确性，在更新之前，会去对比在我修改数据期间，数据有没有被其他线程修改过：如果没被修改过，就说明真的只有我自己在操作，那我就可以正常的修改数据；如果发现数据和我一开始拿到的不一样了，说明其他线程在这段时间内修改过数据，那说明我迟了一步，所以我会放弃这次修改，并选择报错、重试等策略。这和我们生活中乐天派的人的性格是一样的，乐观的人并不会担忧还没有发生的事情，相反，他会认为未来是美好的，所以他在修改数据之前，并不会把数据给锁住。当然，乐天派也不会盲目行动，如果他发现事情和他预想的不一样，也会有相应的处理办法，他不会坐以待毙，这就是乐观锁的思想。乐观锁的实现一般都是利用 CAS 算法实现的。我们举个例子，假设线程 A 此时运用的是乐观锁。那么它去操作同步资源的时候，不需要提前获取到锁，而是可以直接去读取同步资源，并且在自己的线程内进行计算。当它计算完毕之后、准备更新同步资源之前，会先判断这个资源是否已经被其他线程所修改过。如果这个时候同步资源没有被其他线程修改更新，也就是说此时的数据和线程 A 最开始拿到的数据是一致的话，那么此时线程 A 就会去更新同步资源，完成修改的过程。而假设此时的同步资源已经被其他线程修改更新了，线程 A 会发现此时的数据已经和最开始拿到的数据不一致了，那么线程 A 不会继续修改该数据，而是会根据不同的业务逻辑去选择报错或者重试。悲观锁和乐观锁概念并不是 Java 中独有的，这是一种广义的思想，这种思想可以应用于其他领域，比如说在数据库中，同样也有对悲观锁和乐观锁的应用。典型案例 悲观锁：synchronized 关键字和 Lock 接口Java 中悲观锁的实现包括 synchronized 关键字和 Lock 相关类等，我们以 Lock 接口为例，例如 Lock 的实现类 ReentrantLock，类中的 lock() 等方法就是执行加锁，而 unlock() 方法是执行解锁。处理资源之前必须要先加锁并拿到锁，等到处理完了之后再解开锁，这就是非常典型的悲观锁思想。 乐观锁：原子类乐观锁的典型案例就是原子类，例如 AtomicInteger 在更新数据时，就使用了乐观锁的思想，多个线程可以同时操作同一个原子变量。 大喜大悲：数据库数据库中同时拥有悲观锁和乐观锁的思想。例如，如果在 MySQL 选择 select for update 语句，那就是悲观锁，在提交之前不允许第三方来修改该数据，这当然会造成一定的性能损耗，在高并发的情况下是不可取的。相反，利用一个版本 version 字段在数据库中实现乐观锁。在获取及修改数据时都不需要加锁，但是在获取完数据并计算完毕，准备更新数据时，会检查版本号和获取数据时的版本号是否一致，如果一致就直接更新，如果不一致，说明计算期间已经有其他线程修改过这个数据了，那我就可以选择重新获取数据，重新计算，然后再次尝试更新数据。SQL语句示例如下（假设取出数据的时候 version 为1）：UPDATE student    SET         name = ‘小李’,        version= 2    WHERE   id= 100        AND version= 1“汝之蜜糖,彼之砒霜”有一种说法认为，悲观锁由于它的操作比较重量级，不能多个线程并行执行，而且还会有上下文切换等动作，所以悲观锁的性能不如乐观锁好，应该尽量避免用悲观锁，这种说法是不正确的。因为虽然悲观锁确实会让得不到锁的线程阻塞，但是这种开销是固定的。悲观锁的原始开销确实要高于乐观锁，但是特点是一劳永逸，就算一直拿不到锁，也不会对开销造成额外的影响。反观乐观锁虽然一开始的开销比悲观锁小，但是如果一直拿不到锁，或者并发量大，竞争激烈，导致不停重试，那么消耗的资源也会越来越多，甚至开销会超过悲观锁。所以，同样是悲观锁，在不同的场景下，效果可能完全不同，可能在今天的这种场景下是好的选择，在明天的另外的场景下就是坏的选择，这恰恰是“汝之蜜糖，彼之砒霜”。因此，我们就来看一下两种锁各自的使用场景，把合适的锁用到合适的场景中去，把合理的资源分配到合理的地方去。两种锁各自的使用场景悲观锁适合用于并发写入多、临界区代码复杂、竞争激烈等场景，这种场景下悲观锁可以避免大量的无用的反复尝试等消耗。乐观锁适用于大部分是读取，少部分是修改的场景，也适合虽然读写都很多，但是并发并不激烈的场景。在这些场景下，乐观锁不加锁的特点能让性能大幅提高。" }, { "title": "锁的分类及其特点", "url": "/posts/Classification-of-locks/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-09-01 15:33:00 +0000", "snippet": "锁的 7 大分类需要首先指出的是，这些多种多样的分类，是评价一个事物的多种标准，比如评价一个城市，标准有人口多少、经济发达与否、城市面积大小等。而一个城市可能同时占据多个标准，以北京而言，人口多，经济发达，同时城市面积还很大。同理，对于 Java 中的锁而言，一把锁也有可能同时占有多个标准，符合多种分类，比如 ReentrantLock 既是可中断锁，又是可重入锁。根据分类标准我们把锁分为以下 7 大类别，分别是： 偏向锁/轻量级锁/重量级锁； 可重入锁/非可重入锁； 共享锁/独占锁； 公平锁/非公平锁； 悲观锁/乐观锁； 自旋锁/非自旋锁； 可中断锁/不可中断锁。以上是常见的分类标准。偏向锁/轻量级锁/重量级锁这三种锁特指 synchronized 锁的状态，通过在对象头中的 mark word 来表明锁的状态。偏向锁偏向锁的思想：如果自始至终，对于这把锁都不存在竞争，其实是没必要上锁，只需打个标记就行了。一个对象被初始化后，没有任何线程来获取它的锁时，那么它就是可偏向的，当有第一个线程来访问它,并尝试获取锁的时候，它就将这个线程记录下来，以后如果尝试获取锁的线程正是偏向锁的拥有者，就可以直接获得锁，开销很小，性能最好。轻量级锁很多情况下，synchronized 中的代码是被多个线程交替执行的，而不是同时执行的，也就是说并不存在实际的竞争，或者是只有短时间的锁竞争，用 CAS 就可以解决。这种情况下，用完全互斥的重量级锁是没必要的。轻量级锁是指当锁原来是偏向锁的时候，被另一个线程访问，说明存在竞争，那么偏向锁就会升级为轻量级锁，线程会通过自旋的形式尝试获取锁，而不会陷入阻塞。重量级锁重量级锁是互斥锁，它是利用操作系统的同步机制实现的，所以开销相对比较大。当多个线程直接有实际竞争，且锁竞争时间长的时候，轻量级锁不能满足需求，锁就会膨胀为重量级锁。重量级锁会让其他申请却拿不到锁的线程进入阻塞状态。锁升级的路径：无锁→偏向锁→轻量级锁→重量级锁。综上所述： 偏向锁性能最好，可以避免执行 CAS 操作； 轻量级锁利用自旋和 CAS 避免了重量级锁带来的线程阻塞和唤醒，性能中等； 重量级锁则会把获取不到锁的线程阻塞，性能最差。可重入锁/非可重入锁可重入锁，指的是线程当前已经持有这把锁了，能在不释放这把锁的情况下，再次获取这把锁。同理，不可重入锁指的是虽然线程当前持有了这把锁。如果想再次获取这把锁，也必须要先释放锁后。才能再次尝试获取。对于可重入锁而言，最典型的就是 ReentrantLock 了，如它的名字一样，reentrant 的意思就是可重入，它也是 Lock 接口最主要的一个实现类。共享锁/独占锁共享锁指的是，同一把锁可以被多个线程同时获得；独占锁指的是，这把锁只能同时被一个线程获得。读写锁，就最好地诠释了共享锁和独占锁的理念。读写锁中的读锁，是共享锁，而写锁是独占锁。读锁可以被同时读，可以同时被多个线程持有，而写锁最多只能同时被一个线程持有。公平锁/非公平锁公平锁的“公平”含义在于如果线程现在拿不到这把锁，那么线程就都会进入等待，开始排队，在等待队列里等待时间长的线程会优先拿到这把锁，有先来先得的意思。而非公平锁就不那么“完美”了，它会在一定情况下，忽略掉已经在排队的线程，发生插队现象。悲观锁/乐观锁悲观锁的概念是在获取资源之前，必须先拿到锁，以便达到“独占”的状态，当前线程在操作资源的时候，其他线程由于不能拿到锁，所以其他线程不能来影响我。而乐观锁恰恰相反，它并不要求在获取资源前拿到锁，也不会锁住资源；相反，乐观锁利用 CAS 理念，在不独占资源的情况下，完成了对资源的修改。自旋锁/非自旋锁自旋锁的理念是如果线程现在拿不到锁，并不直接陷入阻塞或者释放 CPU 资源，而是开始利用循环，不停地尝试获取锁，这个循环过程被形象地比喻为“自旋”，就像是线程在“自我旋转”。相反，非自旋锁的理念就是没有自旋的过程，如果拿不到锁就直接放弃，或者进行其他的处理逻辑，例如去排队、陷入阻塞等。可中断锁/不可中断锁在 Java 中，synchronized 关键字修饰的锁代表的是不可中断锁，一旦线程申请了锁，就没有回头路了，只能等到拿到锁以后才能进行其他的逻辑处理。而ReentrantLock 是一种典型的可中断锁，例如使用 lockInterruptibly 方法在获取锁的过程中，突然不想获取了，那么也可以在中断之后去做其他的事情，不需要一直傻等到获取到锁才离开。 Java的线程是映射到操作系统的原生线程之上的，如果要阻塞或者唤醒一个线程，都需要操作系统帮忙完成，这就需要从用户态切换到内核态，而这很耗费CPU时间。如果锁只会被其他线程持有很短的时间，那么挂起和恢复当前线程就显得不值得，此时我们可以让当前线程执行一个忙循环（自旋），这就是所谓的自旋锁 自旋的过程不也做不了相关业务吗？那和阻塞不是类似吗？ - 区别在于自旋不改变线程状态。 自旋锁应该一直占 CPU 的吧 假设有个任务特别耗时 把cpu资源占满 怎么去优化它 - 先要找到主要的消耗资源的节点，然后针对性的去优化。 乐观锁和自旋锁的区别是什么？乐观锁是没有锁住资源来排斥其他线程来不独占资源，而自旋锁是对互斥锁的获得的过程行为，是对获取锁这个目标行为上的强调。这样的理解对吗？ —— 乐观锁是强调不排斥其他线程，不独占资源，而自旋锁是对获取锁这个目标行为上的强调。" }, { "title": "性能优化的过程方法与总结（17）", "url": "/posts/performance-optimization-summary/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-05-17 15:33:35 +0000", "snippet": "即使熟悉了开发中的各项技术和优化技巧，但在真正的性能优化场景下，依旧会很难开展优化任务。其实原因一方面是项目周期紧，另一方面是因为脑海里面的知识杂乱无章，仅能靠回忆完成片面的优化。为了避免这样的事情发生，准备一份详细的提纲，这样在性能优化的时候，能够指明方向，并以完整的思维方式进行思考。性能优化的多方面权衡应用性能低，有很多方面的因素，比如： 业务需求方面 比如：有一个报表业务，查询非常缓慢，有时候甚至会造成内存溢出，经过分析是查询时间跨度范围太大造成的。 由于业务上的限制，将时间跨度缩小至 1 个月之内，查询速度就快了很多。 架构设计方面 硬件/软件层面 比如：有一个定时任务，可以算是 CPU 密集型的，每次都将 CPU 用得满满的。由于系统有架构上的硬伤，无法做到横向扩容。技术经过评估，如果改造成按照数据分片执行的模式，则需要耗费长达 1 个月的工时。 其实在这种情况下，通过增加硬件配置的方式，便能解决性能瓶颈问题，为业务改进赢得更多的时间。 通过上面三个栗子，表明性能优化有很多优化途径。如果这个性能问题可以通过其他方式解决，那就尽量不要采用调整软件代码的方式，尽可地在效果、工时以及手段这三方面之间进行权衡。确定优化目标通常，关注一个硬件资源（比如 CPU），主要关注以下基本要素： 利用率： 一般是瞬时值，属于采样范围，用来判断有没有峰值，比如 CPU 使用率。 饱和度： 一般指资源是否被合理利用，能否用分担更多的工作。比如，饱和度过高，新请求在特定 queue 里排队；再比如，内存利用率过低、CPU 利用率过高，就可以考虑空间换时间。 错误信息： 错误一般发生在问题严重的情况下，需要特别关注。 联想信息： 对引起的原因进行猜测，并用更多的工具验证猜想，猜测影响因素并不一定是准确的，只是帮助我们分析问题，比如系统响应慢很可能是大量使用了 SWAP 导致的。 首先，我们需要找到性能优化的目标，从 CPU、内存、网络、I/O 等层面看一下性能瓶颈可能存在的匿藏之处。 CPU 查看 CPU 使用可以使用 top 命令，尤其注意它的负载（load）和使用率， vmstat 命令也可以看到系统的一些运行状况，这里关注上下文切换和 swap 分区的使用情况。 内存 内存可以使用 free 命令查看，尤其关注剩余内存的大小（free）。对于 Linux 系统来说，启动之后由于各种缓存和缓冲区的原因，系统内存会被迅速占满，所以需要更加关注的是 JVM 的内存。 top 命令的 RES 列，显示的就是进程实际占用的物理内存，这个值通常比 jmap 命令获取的堆内存要大，因为它还包含大量的堆外内存空间。 网络 iotop 可以看到占用网络流量最高的进程； 通过 netstat 命令或者 ss 命令，能够看到当前机器上的网络连接汇总。在一些较底层的优化中，会涉及针对 mtu 的网络优化。 I/O 通过 iostat 命令，可以查看磁盘 I/O 的使用情况，如果利用率过高，就需要从使用源头找原因； 类似 iftop，iotop 可以查看占用 I/O 最多的进程，很容易可以找到优化目标。 通用 lsof 命令可以查看当前进程所关联的所有资源； sysctl 命令可以查看当前系统内核的配置参数； dmesg 命令可以显示系统级别的一些信息，比如被操作系统的 oom-killer 杀掉的进程就可以在这里找到。 整理了一幅脑图，可以参考：常用工具集合为了找到系统的问题，我们会采用类似于神农尝百草的方式，用多个工具、多种手段获取系统的运行状况。1.信息收集nmon 是一个可以输出系统整体性能数据的命令行工具，应用较为广泛。jvisualvm 和 jmc，都是用来获取 Java 应用性能数据的工具。由于它们是 UI 工具，应用需要开启 JMX 端口才能够被远程连接。2.监控像 top 这样的命令，只在问题发生的时候才会有作用。但很多时候，当发生性能问题时，我们并不在电脑旁边，这就需要有一套工具，定期抓取这些性能数据。通过监控系统，能够获取监控指标的历史时序，通过分析指标趋势，可估算性能瓶颈点，从数据上支撑我们的分析。目前最流行的组合是 prometheus + grafana + telegraf，可以搭功能强大的监控平台。3.压测工具有时候，我们需要评估系统在一定并发量下面的性能，这时候就可以通过压测工具给予系统一些压力。wrk 是一个命令行工具，可以对 HTTP 接口进行压测；jmeter 是较为专业的压测工具，可以生成压测报告。压测工具配合监控工具，可以正确评估系统当前的性能。4.性能深挖大多数情况下，仅通过概括性的性能指标，我们无法知晓性能瓶颈的具体细节，这就需要一些比较深入的工具进行追踪。skywalking 可以用来分析分布式环境下的调用链问题，可以详细地看到每一步执行的耗时。但如果你没有这样的环境，就可以使用命令行工具 arthas 对方法进行 trace，最终也能够深挖找到具体的慢逻辑。jvm-profiling-tools，可以生成火焰图，辅助我们分析问题。另外，更加底层的，针对操作系统的性能测评和调优工具，还有perf和SystemTap，感兴趣的同学可以自行研究一下。 关于工具方面的内容，你可以回顾“04 工具实践：如何获取代码性能数据？”和“05｜工具实践：基准测试 JMH，精确测量方法性能”进行回忆复习，我整理了一幅脑图，可供你参考。 基本解决方式找到了具体的性能瓶颈点，就可以针对性地进行优化。1.CPU 问题CPU 是系统的核心资源，如果 CPU 有瓶颈，很多任务和线程就获取不到时间片，便会运行缓慢。如果此时系统的内存充足，就要考虑是否可以空间换时间，通过数据冗余和更优的算法来减少 CPU 的使用。在 Linux 系统上，通过 top-Hp 便能容易地获取占用 CPU 最高的线程，进行针对性的优化。资源的使用要细分，才能够进行专项优化。我曾经碰见一个棘手的性能问题，线程都阻塞在 ForkJoin 线程池上，经过仔细排查才分析出，代码在等待耗时的 I/O 时，采用了并行流（parallelStrea）处理，但是 Java 默认的方式是所有使用并行流的地方，公用了一个通用的线程池，这个线程池的并行度只有 CPU 的两倍。所以请求量一增加，任务就会排队，造成积压。2.内存问题内存问题通常是 OOM 问题，可以参考“19 | 高级进阶：JVM 常见优化参数”进行优化。如果内存资源很紧张，CPU 利用率低，则可以考虑时间换空间的方式。SWAP 分区使用硬盘来扩展可用内存的大小，但它的速度非常慢。一般在高并发的应用中，会把 SWAP 关掉，因为它很容易会引起卡顿。3.I/O 问题我们通常开发的业务系统，磁盘 I/O 负载都比较小，但网络 I/O 都比较繁忙。当遇到磁盘 I/O 占用高的情况，就要考虑是否是日志打印得太多导致的。通过调整日志级别，或者清理无用的日志代码，便可缓解磁盘 I/O 的压力。业务系统还会有大量的网络 I/O 操作，比如通过 RPC 调用一个远程的服务，我们期望使用 NIO 来减少一些无效的等待，或者使用并行来加快信息的获取。还有一种情况，是类似于 ES 这样的数据库应用，数据写入本身，就会造成繁重的磁盘 I/O。这个时候，可以增加硬件的配置，比如换成 SSD 磁盘，或者增加新的磁盘。 数据库服务本身，也会提供非常多的参数，用来调优性能。根据“06 案例分析：缓冲区如何让代码加速”和“07 案例分析：无处不在的缓存，高并发系统的法宝”的描述，这部分的配置参数，主要影响缓冲和缓存的行为。 比如 ES 的 segment 块大小，translog 的刷新速度等，都可以被微调。举个例子，大量日志写入 ES 的时候，就可以通过增大 translog 写盘的间隔，来获得较大的性能提升。4.网络问题数据包在网络上传输，影响的主要因素就是结果集的大小。通过去除无用的信息，启用合理的压缩，可以获得较大的性能提升。值得注意的是，这里的网络传输值得不仅仅是针对浏览器的，在服务间调用中也有着同样的情况。比如，在 SpringBoot 的配置文件中，通过配置下面的参数，就可以开启 gzip。server: compression:   enabled: true   min-response-size: 1024   mime-types: [&quot;text/html&quot;,&quot;application/json&quot;,&quot;application/octet-stream&quot;]但是，这个 SpringBoot 服务，通过 Feign 接口从另外一个服务获取信息，这个结果集并没有被压缩。可以通过替换 Feign 的底层网络工具为 OkHTTP，使用 OkHTTP 的透明压缩（默认开启 gzip），即可完成服务间调用的信息压缩，但很多同学容易忘掉这一环。我曾经调优果一个项目，将返回的数据包从9MB 压缩到300KB 左右，极大地减少了网络传输，节省了大约 500ms 的时间。网络 I/O 的另外一个问题就是频繁的网络交互，通过将结果集合并，使用批量的方式，可以显著增加性能，但这种方式的使用场景有限，比较适合异步的任务处理。使用 netstat 命令，或者 lsof 命令，可以获取进程所关联的，TIME_WAIT 和 CLOSE_WAIT 网络状态的数量，前者可以通过调整内核参数来解决，但后者多是应用程序的 BUG。我整理了一幅脑图，可供你参考。代码层面" }, { "title": "SpringBoot 服务性能优化（16）", "url": "/posts/springboot/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-05-12 15:33:35 +0000", "snippet": "在开始对 SpringBoot 服务进行性能优化之前，需要做一些准备：将 SpringBoot 服务的一些数据暴漏出来。比如： 服务用到了缓存，就需要把缓存命中率这些数据进行收集； 服务用到了数据库连接池，就需要把连接池的参数给暴露出来这里可以采用目前非常流行的监控工具 Prometheus，它是一个时序数据库，能够存储自定义的指标。而 SpringBoot 可以非常方便地接入到 Prometheus 中。在 SpringBoot 中开启监控创建一个 SpringBoot 项目后，首先加入 maven 依赖，如下：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;!-- prometheus 监控相关依赖 --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;io.micrometer&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;micrometer-registry-prometheus&amp;lt;/artifactId&amp;gt; &amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;然后，在 application.yml配置文件中，开放相关的监控接口，如下：# prometheus 监控平台配置management: endpoint: metrics: enabled: true prometheus: enabled: true health: show-details: ALWAYS endpoints: web: exposure: include: &quot;*&quot; exclude: configprops metrics: export: prometheus: enabled: true tags: application: ${server.servlet.context-path}再然后启动之后，就可以通过访问监控接口，效果如下图所示：想要监控业务数据也非常的简单，只需要注入一个 MeterRegistry实例即可，如下代码：package cn.happymaya.springbootoptimization.controller;import io.micrometer.core.instrument.Counter;import io.micrometer.core.instrument.MeterRegistry;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;import org.springframework.web.bind.annotation.RestController;import javax.annotation.PostConstruct;import java.util.Random;import java.util.concurrent.atomic.AtomicInteger;@RestController@RequestMapping(&quot;/promethues/&quot;)public class PrometheusController { @Autowired MeterRegistry registry; /** * 指标类型设置 */ private Counter counterCore; private Counter counterIndex; private AtomicInteger appOnlineCount; /** * 服务启动时创建自定义指标 */ @PostConstruct private void init() { counterCore = registry.counter(&quot;app_requests_method_count&quot;, &quot;method&quot;, &quot;PrometheusController.core&quot;); counterIndex = registry.counter(&quot;app_requests_method_count&quot;, &quot;method&quot;, &quot;PrometheusController.index&quot;); appOnlineCount = registry.gauge(&quot;app_online_count&quot;, new AtomicInteger(0)); } /** * 监控平台是否可用，每调用一次就记录一次，每调用一次 counterIndex 就加一 * * @return 返回监控平台调用次数 */ @GetMapping(&quot;testIsUsable&quot;) public String testIsUsable() { counterIndex.increment(); return counterIndex.count() + &quot; index of springboot-prometheus.&quot;; } /** * 监控平台核心接口请求次数 * * @return 返回监控平台核心接口请求次数 */ @GetMapping(&quot;testIsCore&quot;) public String testIsCore() { counterCore.increment(); return counterCore.count() + &quot; index of springboot-prometheus.&quot;; } /** * 测试实时在线人数，动态数据，每次请求数据可能都不一样 * * @return */ @GetMapping(&quot;/online&quot;) public Object online() { int people = 0; try { people = new Random().nextInt(2000); appOnlineCount.set(people); } catch (Exception e) { return e; } return &quot;current online people: &quot; + people; } @GetMapping(&quot;/test&quot;) @ResponseBody public String test() { registry.counter(&quot;test&quot;, &quot;from&quot;, &quot;127.0.0.1&quot;, &quot;method&quot;, &quot;test&quot; ).increment(); return &quot;ok&quot;; }}从监控连接中，可以看到添加的监控信息，如下：# HELP test_total # TYPE test_total countertest_total{application=&quot;/springboot-optimization&quot;,from=&quot;127.0.0.1&quot;,method=&quot;test&quot;,} 1.0Prometheus 监控体系目前非常流行的 Prometheus ，通过使用【拉】的方式获取监控数据，这个暴露数据的过程可以交给功能更加齐全的 telegraf 组件，如下图所示：如上图，通常使用 Grafana 进行监控数据的展示，并使用 AlertManager 组件进行提前预警。下图就是一张典型的监控图，可以看到 Redis 的缓存命中率等情况，如下：Java 生成火焰图火焰图是用来分析程序运行瓶颈的工具。于是，我们可以使用火焰图分析 Java 应用，可以从 github 上下载 async-profiler 的压缩包进行相关操作。比如，将其加压到 /root/目录，然后以javaagent 的方式来启动 Java 应用，命令如下：java -agentpath:/root/build/libasyncProfiler.so=start,svg,file=profile.svg -jar spring-petclinic-2.3.1.BUILD-SNAPSHOT.jar运行一段时间后，停止进程，可以看到在当前目录下，生成了 profile.svg 文件，这个文件可以通过浏览器打开。如下图所示，其中： 纵向，表示的是调用栈的深度； 横向，表示的是消耗的时间因此可知，格子的宽度越大，越说明它可能是一个瓶颈，然后一层层向下浏览，即可找到需要优化的目标。优化思路对于一个普通的 Web 服务来说，要访问到具体的数据，经历的主要环节如下图所示：Nginx 根据资源的特性，会承担一部分动静分离的功能。其中，动态功能部分，会进入到我们的 SpringBoot 服务。SpringBoot 默认使用内嵌的 tomcat 作为 Web 容器，使用典型的 MVC 模式，最终访问到目标数据。HTTP 优化下面我们举例来看一下，哪些动作能够加快网页的获取。为了描述方便，我们仅讨论 HTTP1.1 协议的。1.使用 CDN 加速文件获取比较大的文件，尽量使用 CDN（Content Delivery Network）分发，甚至是一些常用的前端脚本、样式、图片等，都可以放到 CDN 上。CDN 通常能够加快这些文件的获取，网页加载也更加迅速。2.合理设置 Cache-Control 值浏览器会判断 HTTP 头 Cache-Control 的内容，用来决定是否使用浏览器缓存，这在管理一些静态文件的时候，非常有用，相同作用的头信息还有 Expires。Cache-Control 表示多久之后过期；Expires 则表示什么时候过期。这个参数可以在 Nginx 的配置文件中进行设置。location ~* ^.+\\.(ico|gif|jpg|jpeg|png)$ { # 缓存1年 add_header Cache-Control: no-cache, max-age=31536000;}3.减少单页面请求域名的数量减少每个页面请求的域名数量，尽量保证在 4 个之内。这是因为，浏览器每次访问后端的资源，都需要先查询一次 DNS，然后找到 DNS 对应的 IP 地址，再进行真正的调用。DNS 有多层缓存，比如浏览器会缓存一份、本地主机会缓存、ISP 服务商缓存等。从 DNS 到 IP 地址的转变，通常会花费 20-120ms 的时间。减少域名的数量，可加快资源的获取。4.开启 gzip开启 gzip，可以先把内容压缩后，浏览器再进行解压。由于减少了传输的大小，会减少带宽的使用，提高传输效率。在 nginx 中可以很容易地开启，配置如下：gzip on;gzip_min_length 1k;gzip_buffers 4 16k;gzip_comp_level 6;gzip_http_version 1.1;gzip_types text/plain application/javascript text/css;5.对资源进行压缩对 JavaScript 和 CSS，甚至是 HTML 进行压缩。道理类似，现在流行的前后端分离模式，一般都是对这些资源进行压缩的。6.使用 keepalive由于连接的创建和关闭，都需要耗费资源。用户访问我们的服务后，后续也会有更多的互动，所以保持长连接可以显著减少网络交互，提高性能。nginx 默认开启了对客户端的 keep avlide 支持，你可以通过下面两个参数来调整它的行为。http {    keepalive_timeout  120s 120s;    keepalive_requests 10000;}nginx 与后端 upstream 的长连接，需要手工开启，参考配置如下：location ~ /{ proxy_pass http://backend; proxy_http_version 1.1;proxy_set_header Connection &quot;&quot;;}自定义 Web 容器如果你的项目并发量比较高，想要修改最大线程数、最大连接数等配置信息，可以通过自定义Web 容器的方式，代码如下所示：@SpringBootApplication(proxyBeanMethods = false)public class App implements WebServerFactoryCustomizer&amp;lt;ConfigurableServletWebServerFactory&amp;gt; { public static void main(String[] args) { SpringApplication.run(PetClinicApplication.class, args); } @Override public void customize(ConfigurableServletWebServerFactory factory) { TomcatServletWebServerFactory f = (TomcatServletWebServerFactory) factory;        f.setProtocol(&quot;org.apache.coyote.http11.Http11Nio2Protocol&quot;); f.addConnectorCustomizers(c -&amp;gt; { Http11NioProtocol protocol = (Http11NioProtocol) c.getProtocolHandler(); protocol.setMaxConnections(200); protocol.setMaxThreads(200); protocol.setSelectorTimeout(3000); protocol.setSessionTimeout(3000); protocol.setConnectionTimeout(3000); }); }}注意上面的代码，设置了它的协议为 org.apache.coyote.http11.Http11Nio2Protocol，意思就是开启了 Nio2。这个参数在 Tomcat 8.0之后才有，开启之后会增加一部分性能。对比如下（测试项目代码见 spring-petclinic-main）：默认：[root@localhost wrk2-master]# ./wrk -t2 -c100 -d30s -R2000 http://172.16.1.57:8080/owners?lastName=Running 30s test @ http://172.16.1.57:8080/owners?lastName=  2 threads and 100 connections Thread calibration: mean lat.: 4588.131ms, rate sampling interval: 16277ms Thread calibration: mean lat.: 4647.927ms, rate sampling interval: 16285ms Thread Stats   Avg     Stdev     Max   +/- Stdev   Latency    16.49s     4.98s   27.34s    63.90%   Req/Sec   106.50      1.50   108.00    100.00%  6471 requests in 30.03s, 39.31MB read Socket errors: connect 0, read 0, write 0, timeout 60Requests/sec:    215.51Transfer/sec:      1.31MBNio2：[root@localhost wrk2-master]# ./wrk -t2 -c100 -d30s -R2000 http://172.16.1.57:8080/owners?lastName=Running 30s test @ http://172.16.1.57:8080/owners?lastName=  2 threads and 100 connections Thread calibration: mean lat.: 4358.805ms, rate sampling interval: 15835ms Thread calibration: mean lat.: 4622.087ms, rate sampling interval: 16293ms Thread Stats   Avg     Stdev     Max   +/- Stdev   Latency    17.47s     4.98s   26.90s    57.69%   Req/Sec   125.50      2.50   128.00    100.00%  7469 requests in 30.04s, 45.38MB read Socket errors: connect 0, read 0, write 0, timeout 4Requests/sec:    248.64Transfer/sec:      1.51MB你甚至可以将 tomcat 替换成 undertow。undertow 也是一个 Web 容器，更加轻量级一些，占用的内存更少，启动的守护进程也更少，更改方式如下：&amp;lt;dependency&amp;gt;      &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;      &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;      &amp;lt;exclusions&amp;gt;        &amp;lt;exclusion&amp;gt;          &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;          &amp;lt;artifactId&amp;gt;spring-boot-starter-tomcat&amp;lt;/artifactId&amp;gt;        &amp;lt;/exclusion&amp;gt;      &amp;lt;/exclusions&amp;gt; &amp;lt;/dependency&amp;gt;&amp;lt;dependency&amp;gt;      &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;      &amp;lt;artifactId&amp;gt;spring-boot-starter-undertow&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;其实，对于 tomcat 优化最为有效的，还是 JVM 参数的配置比如，使用下面的参数启动，QPS 由 248 上升到 308。-XX:+UseG1GC -Xmx2048m -Xms2048m -XX:+AlwaysPreTouchSkywalking 对于一个 web 服务来说，最缓慢的地方就在于数据库操作。所以，使用“07 案例分析：无处不在的缓存，高并发系统的法宝”和“08 案例分析：Redis 如何助力秒杀业务”提供的本地缓存和分布式缓存优化，能够获得最大的性能提升。 对于如何定位到复杂分布式环境中的问题，我这里想要分享另外一个工具：Skywalking。Skywalking 是使用探针技术（JavaAgent）来实现的。通过在 Java 的启动参数中，加入 javaagent 的 Jar 包，即可将性能数据和调用链数据封装，并发送到 Skywalking 的服务器。下载相应的安装包（如果使用 ES 存储，需要下载专用的安装包），配置好存储之后，即可一键启动。将 agent 的压缩包，解压到相应的目录。tar xvf skywalking-agent.tar.gz  -C /opt/在业务启动参数中加入 agent 的包。比如，原来的启动命令是：java  -jar /opt/test-service/spring-boot-demo.jar  --spring.profiles.active=dev改造后的启动命令是：java -javaagent:/opt/skywalking-agent/skywalking-agent.jar -Dskywalking.agent.service_name=the-demo-name  -jar /opt/test-service/spring-boot-demo.ja  --spring.profiles.active=dev访问一些服务的链接，打开 Skywalking 的 UI，即可看到下图的界面。这些指标可以类比衡量指标，然后就可以从图中找到响应比较慢 QPS 又比较高的接口，进行专项优化。各个层次的优化方向1.Controller 层controller 层用于接收前端的查询参数，然后构造查询结果。现在很多项目都采用前后端分离的架构，所以 controller 层的方法，一般会使用 @ResponseBody 注解，把查询的结果，解析成 JSON 数据返回（兼顾效率和可读性）。由于 controller 只是充当了一个类似功能组合和路由的角色，所以这部分对性能的影响就主要体现在数据集的大小上。如果结果集合非常大，JSON 解析组件就要花费较多的时间进行解析，大结果集不仅会影响解析时间，还会造成内存浪费。假如结果集在解析成 JSON 之前，占用的内存是 10MB，那么在解析过程中，有可能会使用 20M 或者更多的内存去做这个工作。我见过很多案例，由于返回对象的嵌套层次太深、引用了不该引用的对象（比如非常大的 byte[] 对象），造成了内存使用的飙升。所以，对于一般的服务，保持结果集的精简，是非常有必要的，这也是 DTO（data transfer object）存在的必要。如果你的项目，返回的结果结构比较复杂，对结果集进行一次转换是非常有必要的。2.Service 层service 层用于处理具体的业务，大部分功能需求都是在这里完成的。service 层一般是使用单例模式，很少会保存状态，而且可以被 controller 复用。service 层的代码组织，对代码的可读性、性能影响都比较大。我们常说的设计模式，大多数都是针对 service 层来说的。service 层会频繁使用更底层的资源，通过组合的方式获取我们所需要的数据，大多数可以通过我们前面课时提供的优化思路进行优化。这里要着重提到的一点，就是分布式事务。如上图，四个操作分散在三个不同的资源中。要想达到一致性，需要三个不同的资源 MySQL、MQ、ElasticSearch 进行统一协调。它们底层的协议，以及实现方式，都是不一样的，那就无法通过 Spring 提供的 Transaction 注解来解决，需要借助外部的组件来完成。很多人都体验过，加入了一些保证一致性的代码，一压测，性能掉的惊掉下巴。分布式事务是性能杀手，因为它要使用额外的步骤去保证一致性，常用的方法有：两阶段提交方案、TCC、本地消息表、MQ 事务消息、分布式事务中间件等。如上图，分布式事务要在改造成本、性能、时效等方面进行综合考虑。有一个介于分布式事务和非事务之间的名词，叫作柔性事务。柔性事务的理念是将业务逻辑和互斥操作，从资源层上移至业务层面。关于传统事务和柔性事务，我们来简单比较一下。ACID关系数据库, 最大的特点就是事务处理, 即满足 ACID。 原子性（Atomicity）：事务中的操作要么都做，要么都不做。 一致性（Consistency）：系统必须始终处在强一致状态下。 隔离性（Isolation）：一个事务的执行不能被其他事务所干扰。 持久性（Durability）：一个已提交的事务对数据库中数据的改变是永久性的。BASEBASE 方法通过牺牲一致性和孤立性来提高可用性和系统性能。BASE 为 Basically Available、Soft-state、Eventually consistent 三者的缩写，其中 BASE 分别代表： 基本可用（Basically Available）：系统能够基本运行、一直提供服务。 软状态（Soft-state）：系统不要求一直保持强一致状态。 最终一致性（Eventual consistency）：系统需要在某一时刻后达到一致性要求。互联网业务，推荐使用补偿事务，完成最终一致性。比如，通过一系列的定时任务，完成对数据的修复。3.Dao 层经过合理的数据缓存，我们都会尽量避免请求穿透到 Dao 层。除非你对 ORM 本身提供的缓存特性特别的熟悉；否则，都推荐你使用更加通用的方式去缓存数据。Dao 层，主要在于对 ORM 框架的使用上。比如，在 JPA 中，如果加了一对多或者多对多的映射关系，而又没有开启懒加载，级联查询的时候就容易造成深层次的检索，造成了内存开销大、执行缓慢的后果。在一些数据量比较大的业务中，多采用分库分表的方式。在这些分库分表组件中，很多简单的查询语句，都会被重新解析后分散到各个节点进行运算，最后进行结果合并。举个例子，select count(*) from a 这句简单的 count 语句，就可能将请求路由到十几张表中去运算，最后在协调节点进行统计，执行效率是可想而知的。目前，分库分表中间件，比较有代表性的是驱动层的 ShardingJdbc 和代理层的 MyCat，它们都有这样的问题。这些组件提供给使用者的视图是一致的，但我们在编码的时候，一定要注意这些区别。总结SpringBoot 常见的优化思路： HTTP 自定义 Web 容器 Skwalking 各个层次的优化 Controller Service Dao 三个新的性能分析工具。 一个是监控系统 Prometheus，可以看到一些具体的指标大小； 一个是火焰图，可以看到具体的代码热点； 一个是 Skywalking，可以分析分布式环境中的调用链。SpringBoot 自身的 Web 容器是 Tomcat，就可以通过对 Tomcat 的调优来获取性能提升。当然，对于服务上层的负载均衡 Nginx，也总结提供了一系列的优化思路。最后，在经典的 MVC 架构下，Controller、Service、Dao 的一些优化方向，并着重看了 Service 层的分布式事务问题。SpringBoot 作为一个广泛应用的服务框架，在性能优化方面已经做了很多工作，选用了很多高速组件。比如： 数据库连接池默认使用 hikaricp； Redis 缓存框架默认使用 lettuce； 本地缓存提供 caffeine 等。对于一个普通的数据库交互的 Web 服务来说，缓存是最主要的优化手段。" }, { "title": "Java 代码优化法则（16）", "url": "/posts/Java-code-optimization-rules/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-05-07 12:33:33 +0000", "snippet": "虽然缓冲、缓存、池化对象、大对象复用、并行计算、锁优化、NIO 等优化方法，它们对性能的提升往往是质的飞跃。但是语言本身对性能也是有影响的，比如我所在的公司就因为语言的特性由 Java 切换到 Golang.对于 Java 语言来说，有它的一套优化规则，这些细微的性能差异，经过多次调用和迭代，会产生越来远大的影响。本文总结一些常用的代码优化经验，从而在编码中保持好的习惯，让代码保持最优的状态。代码优化法则 使用局部变量，避免在堆上分配。由于堆资源是多线程共享的，是垃圾回收器的主要区域，过多的对象会造成 GC 压力；不过，可以通过局部变量的方式，将变量在栈上分配；这种方式，变量会随着方法执行的完毕而销毁，就能够减轻 GC 的压力； 减少变量的作用范围。注意变量的作用的范围，尽量减少对象的创建。如下面的代码： public void reduceVariableScope(String str) { final int a = 100; if (StringUtils.isEmpty(str)) { int b = a * a; } } 访问静态变量直接使用类名 习惯对象访问静态变量，这种方式多了一步寻址操作，需要先找到变量对应的类，再找到类对应的变量，代码如下： public class StaticCall { public static final int A = 1; void test() { System.out.println(this.A); System.out.println(StaticCall.A); }} 对应字节码如下： Last modified 2022-4-17; size 446 bytes MD5 checksum a0b8ec52f58996ff5fb0af5d5a1a4780 Compiled from &quot;StaticCall.java&quot;public class cn.happymaya.javarules.StaticCall minor version: 0 major version: 61 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #2.#3 // java/lang/Object.&quot;&amp;lt;init&amp;gt;&quot;:()V #2 = Class #4 // java/lang/Object #3 = NameAndType #5:#6 // &quot;&amp;lt;init&amp;gt;&quot;:()V #4 = Utf8 java/lang/Object #5 = Utf8 &amp;lt;init&amp;gt; #6 = Utf8 ()V #7 = Fieldref #8.#9 // java/lang/System.out:Ljava/io/PrintStream; #8 = Class #10 // java/lang/System #9 = NameAndType #11:#12 // out:Ljava/io/PrintStream; #10 = Utf8 java/lang/System #11 = Utf8 out #12 = Utf8 Ljava/io/PrintStream; #13 = Methodref #14.#15 // java/io/PrintStream.println:(I)V #14 = Class #16 // java/io/PrintStream #15 = NameAndType #17:#18 // println:(I)V #16 = Utf8 java/io/PrintStream #17 = Utf8 println #18 = Utf8 (I)V #19 = Class #20 // cn/happymaya/javarules/StaticCall #20 = Utf8 cn/happymaya/javarules/StaticCall #21 = Utf8 A #22 = Utf8 I #23 = Utf8 ConstantValue #24 = Integer 1 #25 = Utf8 Code #26 = Utf8 LineNumberTable #27 = Utf8 test #28 = Utf8 SourceFile #29 = Utf8 StaticCall.java{ public static final int A; descriptor: I flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL ConstantValue: int 1 public cn.happymaya.javarules.StaticCall(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&amp;lt;init&amp;gt;&quot;:()V 4: return LineNumberTable: line 3: 0 void test(); descriptor: ()V flags: Code: stack=2, locals=1, args_size=1 0: getstatic #7 // Field java/lang/System.out:Ljava/io/PrintStream; 3: aload_0 4: pop 5: iconst_1 6: invokevirtual #13 // Method java/io/PrintStream.println:(I)V 9: getstatic #7 // Field java/lang/System.out:Ljava/io/PrintStream; 12: iconst_1 13: invokevirtual #13 // Method java/io/PrintStream.println:(I)V 16: return LineNumberTable: line 7: 0 line 8: 9 line 9: 16} 可以看到使用 this 的方式多了一个步骤。 字符串拼接使用 StringBuilder 字符串拼接，使用 StringBuilder 或者 StringBuffer，不要使用 + 号。比如下面这段代码，在循环中拼接了字符串。 public String stringConcatenationUsingStringBuilderOrStringBuffer() { String str = &quot;-1&quot;; for (int i = 0; i &amp;lt; 10; i++) { str += i; } return str;} 从下面对应的字节码内容可以看出，它在每个循环里都创建了一个 StringBuilder 对象。所以，在日常的编码中，显示地创建一次就即可。 5: iload_2 6: bipush 10 8: if_icmpge 36 11: new #4 // class java/lang/StringBuilder 14: dup 15: invokespecial #5 // Method java/lang/StringBuilder.&quot;&amp;lt;init&amp;gt;&quot;:()V 18: aload_1 19: invokevirtual #6 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 22: iload_2 23: invokevirtual #7 // Method java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder; 26: invokevirtual #8 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 29: astore_1 30: iinc 2, 1 33: goto 5 重写对象的 HashCode ，不要简单地返回固定值。 在代码 review 的时候，发现有开发重写 HashCode 和 Equals 方法时，会把 HashCode 的值返回固定的 0，而这样做是不恰当的。当这些对象存入 HashMap 时，性能就会非常低，因为 HashMap 是通过 HashCode 定位到 Hash 槽，有冲突的时候，才会使用链表或者红黑树组织节点。固定地返回 0，相当于把 Hash 寻址功能给废除了。 HashMap 等集合在初始化的时候，指定初始值大小 参考 大对象的使用。这样的对象有很多，比如 ArrayList、StringBuilder 等，通过指定初始值大小可减少扩容造成的性能损耗。 遍历 Map 的时候，使用 EntrySet 方法。 使用 EntrySet 方法，可以直接返回 set 对象，直接拿来用即可；而使用 KeySet 方法，获得的是 Key 的集合，还需要再进行一次 get 操作，显然多了一个操作步骤。因此，使用 EntrySet 方式遍历 Map. 不要在多线程下使用同一个 Random Random 类的 seed 会在并发访问的情况下发生竞争，造成性能降低，建议在多线程环境下使用 ThreadLocalRandom 类。在 Linux 上，通过加入 JVM 配置 -Djava.security.egd=file:/dev/./urandom，使用 urandom 随机生成器，在进行随机数获取时，速度会更快。 自增推荐使用 LongAddr自增运算可以通过 synchronized 和 volatile 的组合，或者也可以使用原子类（比如 AtomicLong）。后者的速度比前者要高一些，AtomicLong 使用 CAS 进行比较替换，在线程多的情况下会造成过多无效自旋，所以可以使用 LongAdder 替换 AtomicLong 进行进一步的性能提升。 不要使用异常控制程序流程 异常，是用来了解并解决程序中遇到的各种不正常的情况，它的实现方式比较昂贵，比平常的条件判断语句效率要低很多。这是因为异常在字节码层面，需要生成一个如下所示的异常表（Exception table），多了很多判断步骤。 Exception table: from to target type 7 17 20 any 20 23 20 any 所以，尽量不要使用异常控制程序流程。 不要在循环中使用 try...catch 道理与上面类似，很多文章介绍，不要把异常处理放在循环里，而应该把它放在最外层，但实际测试情况表明这两种方式性能相差并不大。既然性能没什么差别，那么就推荐根据业务的需求进行编码。比如，循环遇到异常时，不允许中断，也就是允许在发生异常的时候能够继续运行下去，那么异常就只能在 for 循环里进行处理。 不要捕捉 RuntimeException Java 异常分为两种，一种是可以通过预检查机制避免的 RuntimeException；另外一种就是普通异常。其中，RuntimeException 不应该通过 catch 语句去捕捉，而应该使用编码手段进行规避。 如下面的代码，list 可能会出现数组越界异常。是否越界是可以通过代码提前判断的，而不是等到发生异常时去捕捉。提前判断这种方式，代码会更优雅，效率也更高。 // BAD public String test1(List&amp;lt;String&amp;gt; list, int index) { try { return list.get(index); } catch (IndexOutOfBoundsException ex) { return null; } } // GOOD public String test2(List&amp;lt;String&amp;gt; list, int index){ if (index &amp;gt;= list.size() || index &amp;lt;=0 ) { return null; } return list.get(index); } 合理使用 PreparedStatement PreparedStatement 使用预编译对 SQL 的执行进行提速，大多数数据库都会努力对这些能够复用的查询语句进行预编译优化，并能够将这些编译结果缓存起来。这样等到下次用到的时候，就可以很快进行执行，也就少了一步对 SQL 的解析动作。 PreparedStatement 还能提高程序的安全性，能够有效防止 SQL 注入。但是如果程序每次 SQL 都会变化，不得不手工拼接一些数据，那么 PreparedStatement 就失去了它的作用，反而使用普通的 Statement 速度会更快一些。 日志打印的注意事项 日常使用 debug 输出一些调式信息，然后在线上将其关掉，代码如下： logger.debug(&quot;superhsc:&quot;+ topic + &quot; is awesome&quot; ); 程序每次运行到这里，都会构造一个字符串，不管你是否把日志级别调试到 INFO 还是 WARN，这样效率就会很低。 可以在每次打印之前都使用 isDebugEnabled 方法判断一下日志级别，代码如下： if(logger.isDebugEnabled()) { logger.debug(&quot;superhsc:&quot;+ topic + &quot; is awesome&quot; );} 使用占位符的方式，也可以达到相同的效果，就不用手动添加 isDebugEnabled 方法了，代码也优雅得多。 logger.debug(&quot;superhsc:{} is awesome&quot; ,topic); 对于业务系统来说，日志对系统的性能影响非常大，不需要的日志，尽量不要打印，避免占用 I/O 资源。 减少事务的作用范围 如果的程序使用了事务，那一定要注意事务的作用范围，尽量以最快的速度完成事务操作。这是因为，事务的隔离性是使用锁实现的，可以类比使用多线程锁的优化 中的多线程锁进行优化。 @Transactional public void test(String id){ String value = rpc.getValue(id); //高耗时 testDao.update(sql,value);} 如上面的代码，由于 rpc 服务耗时高且不稳定，就应该把它移出到事务之外，改造如下： public void test(String id){ String value = rpc.getValue(id); //高耗时 testDao(value);}@Transactional public void testDao(String value){ testDao.update(value);} 这里有一点需要注意的地方，由于 SpringAOP 的原因，@Transactional 注解只能用到 public 方法上，如果用到 private 方法上，将会被忽略。 使用位移操作替代乘除法 计算机是使用二进制表示的，位移操作会极大地提高性能。 « 左移相当于乘以 2； &amp;gt;&amp;gt; 右移相当于除以 2； &amp;gt;&amp;gt;&amp;gt; 无符号右移相当于除以 2，但它会忽略符号位，空位都以 0 补齐 int a = 2;int b = (a++) &amp;lt;&amp;lt; (++a) + (++a);System.out.println(b); 注意：位移操作的优先级非常低，所以上面的代码，输出是 1024。 不要打印大集合或者使用大集合的 toString 方法 有的开发喜欢将集合作为字符串输出到日志文件中，这个习惯是非常不好的。 拿 ArrayList 来说，它需要遍历所有的元素来迭代生成字符串。在集合中元素非常多的情况下，这不仅会占用大量的内存空间，执行效率也非常慢。我曾经就遇到过这种批量打印方式造成系统性能直线下降的实际案例。 下面这段代码，就是 ArrayList 的 toString 方法。它需要生成一个迭代器，然后把所有的元素内容拼接成一个字符串，非常浪费空间。 public String toString() { Iterator&amp;lt;E&amp;gt; it = iterator(); if (! it.hasNext()) return &quot;[]&quot;; StringBuilder sb = new StringBuilder(); sb.append(&#39;[&#39;); for (;;) { E e = it.next(); sb.append(e == this ? &quot;(this Collection)&quot; : e); if (! it.hasNext()) return sb.append(&#39;]&#39;).toString(); sb.append(&#39;,&#39;).append(&#39; &#39;); }} 正则表达式可以预先编译，加快速度。Java 的正则表达式需要先编译再使用。代码如下： Pattern pattern = Pattern.compile({pattern});Matcher pattern = Pattern.matches({content}); Pattern 编译非常耗时，它的 Matcher 方法是线程安全的，每次调用方法这个方法都会生成一个新的 Matcher 对象。所以，一般 Pattern 初始化一次即可，可以作为类的静态成员变量。 在代码中少用反射 反射的功能很强大，但它是通过解析字节码实现的，性能就不是很理想。 现实中有很多对反射的优化方法，比如把反射执行的过程（比如 Method）缓存起来，使用复用来加快反射速度。 Java 7.0 之后，加入了新的包 java.lang.invoke，同时加入了新的 JVM 字节码指令 invokedynamic，用来支持从 JVM 层面，直接通过字符串对目标方法进行调用。 如果对性能有非常苛刻的要求，则使用 invoke 包下的 MethodHandle 对代码进行着重优化,但它的编程不如反射方便，在平常的编码中，反射依然是首选。 下面是一个使用 MethodHandle 编写的代码实现类。它可以完成一些动态语言的特性，通过方法名称和传入的对象主体，进行不同的调用，而 Bike 和 Man 类，可以是没有任何关系的。 import java.lang.invoke.MethodHandle;import java.lang.invoke.MethodHandles;import java.lang.invoke.MethodType;public class MethodHandleDemo { static class Bike { String sound() { return &quot;ding ding&quot;; } } static class Animal { String sound() { return &quot;wow wow&quot;; } } static class Man extends Animal { @Override String sound() { return &quot;hou hou&quot;; } } String sound(Object o) throws Throwable { MethodHandles.Lookup lookup = MethodHandles.lookup(); MethodType methodType = MethodType.methodType(String.class); MethodHandle methodHandle = lookup.findVirtual(o.getClass(), &quot;sound&quot;, methodType); String obj = (String) methodHandle.invoke(o); return obj; } public static void main(String[] args) throws Throwable { String str = new MethodHandleDemo().sound(new Bike()); System.out.println(str); str = new MethodHandleDemo().sound(new Animal()); System.out.println(str); str = new MethodHandleDemo().sound(new Man()); System.out.println(str);   }} 栗子栗子 1：正则表达式和状态机正则表达式的执行效率是非常慢的，尤其是贪婪模式。下面是我在实际工作中对正则的一个优化，使用状态机完成字符串匹配。考虑到下面的一个 SQL 语句，它的语法类似于 NamedParameterJdbcTemplate，但我对它做了增强。SQL 接收两个参数：smallId 和 firstName，当 firstName 为空的时候，处在 ##{} 之间的语句将被抹去。select * from USERSwhere id&amp;gt;:smallId##{ and FIRST_NAME like concat(&#39;%&#39;,:firstName,&#39;%&#39;) }可以看到，使用正则表达式可以很容易地实现这个功能。#\\{(.*?:([a-zA-Z0-9_]+).*?)\\}通过定义上面这样一个正则匹配，使用 Pattern 的 group 功能便能提取到相应的字符串。我们把匹配到的字符串保存下来，最后使用 replace 函数，将它替换成空字符串即可。结果在实际使用的时候，发现正则的解析速度特别慢，尤其是在 SQL 非常大的时候，这种情况下，可以使用状态机去优化。我这里选用的是 ragel，你也可以使用类似 javacc 或者 antlr 之类的工具。它通过语法解析和简单的正则表达式，最终可以生成 Java 语法的代码。生成的代码一般是不可读的，只关注定义文件即可。如下定义文件代码所示，通过定义一批描述符和处理程序，使用一些中间数据结构对结果进行缓存，只需要对 SQL 扫描一遍，即可获取相应的结果。pairStart = &#39;#{&#39;;pairEnd = &#39;}&#39;;namedQueryStringFull = ( &#39;:&#39;alnum+) &amp;gt;buffer %namedQueryStringFull;pairBlock = (pairStart any* namedQueryStringFull any* pairEnd) &amp;gt;pairBlockBegin %pairBlockEnd;main := any* pairBlock any*;把文件定义好之后，即可通过 ragel 命令生成 Java 语法的最终文件。ragel -G2 -J -o P.java P.rl它的性能，从测试结果可以看到，ragel 模式的性能是 regex 模式的 3 倍还多，SQL 越长，效果越明显。Benchmark Mode Cnt Score Error   UnitsRegexVsRagelBenchmark.ragel thrpt 10 691.224 ± 446.217  ops/msRegexVsRagelBenchmark.regex thrpt 10 201.322 ±  47.056  ops/ms栗子 2：HikariCP 的字节码修改HikariCP 对字节码的修改，这个职责是由 JavassistProxyFactory 类来管理的。Javassist 是一个字节码类库，HikariCP 就是用它对字节码进行修改。它通过 generateProxyClass 生成代理类，主要是针对 Connection、Statement、ResultSet、DatabaseMetaData 等 jdbc 的核心接口。运行这个类，可以看到代码生成了一堆 Class 文件。Generating com.zaxxer.hikari.pool.HikariProxyConnectionGenerating com.zaxxer.hikari.pool.HikariProxyStatementGenerating com.zaxxer.hikari.pool.HikariProxyResultSetGenerating com.zaxxer.hikari.pool.HikariProxyDatabaseMetaDataGenerating com.zaxxer.hikari.pool.HikariProxyPreparedStatementGenerating com.zaxxer.hikari.pool.HikariProxyCallableStatementGenerating method bodies for com.zaxxer.hikari.proxy.ProxyFactory对于这一部分的代码组织，使用了设计模式中的委托模式。发现 HikariCP 源码中的代理类，比如 ProxyConnection，都是 abstract 的，它的具体实例就是使用 javassist 生成的 class 文件。反编译这些生成的 class 文件，可以看到它实际上是通过调用父类中的委托对象进行处理的。另外，注意到 ProxyFactory 类中的方法，都是静态方法，而不是通过单例实现的。为什么这么做呢？这就涉及 JVM 底层的两个字节码指令：invokestatic 和 invokevirtual。下面是两种不同类型调用的字节码。 invokevirtual public final java.sql.PreparedStatement prepareStatement(java.lang.String, java.lang.String[]) throws java.sql.SQLException; flags: ACC_PRIVATE, ACC_FINAL Code: stack=5, locals=3, args_size=3 0: getstatic     #59                 // Field PROXY_FACTORY:Lcom/zaxxer/hikari/proxy/ProxyFactory; 3: aload_0 4: aload_0 5: getfield      #3                  // Field delegate:Ljava/sql/Connection; 8: aload_1 9: aload_2 10: invokeinterface #74,  3           // InterfaceMethod java/sql/Connection.prepareStatement:(Ljava/lang/String;[Ljava/lang/String;)Ljava/sql/PreparedStatement; 15: invokevirtual #69                 // Method com/zaxxer/hikari/proxy/ProxyFactory.getProxyPreparedStatement:(Lcom/zaxxer/hikari/proxy/ConnectionProxy;Ljava/sql/PreparedStatement;)Ljava/sql/PreparedStatement; 18: return invokestatic private final java.sql.PreparedStatement prepareStatement(java.lang.String, java.lang.String[]) throws java.sql.SQLException;flags: ACC_PRIVATE, ACC_FINALCode: stack=4, locals=3, args_size=3 0: aload_0 1: aload_0 2: getfield #3 // Field delegate:Ljava/sql/Connection; 5: aload_1 6: aload_2 7: invokeinterface #72, 3 // InterfaceMethod java/sql/Connection.prepareStatement:(Ljava/lang/String;[Ljava/lang/String Ljava/sql/PreparedStatement; 12: invokestatic  #67，// Method com/zaxxer/hikari/proxy/ProxyFactory.getProxyPreparedStatement:(Lcom/zaxxer/hikari/proxy/ConnectionProxy;Ljava/sql/PreparedStatement;)Ljava/sql/PreparedStatement; 15: areturn 大多数普通方法调用，使用的是invokevirtual指令，属于虚方法调用。 很多时候，JVM 需要根据调用者的动态类型，来确定调用的目标方法，这就是动态绑定的过程；相对比，invokestatic指令，就属于静态绑定过程，能够直接识别目标方法，效率会高那么一点点。虽然 HikariCP 的这些优化有点吹毛求疵，但能够从中看到 HikariCP 这些追求性能极致的编码技巧。总结此外，还可以参考《阿里巴巴 Java 开发规范》，里面也有很多不错的建议。" }, { "title": "IO 模型", "url": "/posts/NIO-BIO-AIO/", "categories": "Java, Base", "tags": "IO, BIO, NIO, AIO", "date": "2019-05-01 07:33:22 +0000", "snippet": "Netty 的高性能架构，是基于一个网络编程设计模式 Reactor 进行设计的。目前，大多数与 I/O 相关的组件，都会使用 Reactor 模型，比如： Tomcat Redis Nginx 等可见 Reactor 应用可谓广泛。并且 Reactor 是 NIO 的基础。阻塞 I/O 模型如上图，是典型的 BIO 模型，每当有一个连接到来，经过协调器的处理，就开启一个对应的线程进行接管。如果连接有 1000 条，那就需要 1000 个线程。线程资源是非常昂贵的，除了占用大量的内存，还会占用非常多的 CPU 调度时间，所有 BIO 在连接非常多的情况下，效率会变得非常低。下面的代码是使用 ServerSocket 实现的一个简单 Socket 服务器，监听在 8888 端口。public class BIO { static boolean stop = false; public static void main(String[] args) { int connectionNum = 0; int port = 8888; ExecutorService service = Executors.newCachedThreadPool(); try { ServerSocket serverSocket = new ServerSocket(port); while (!stop) { if (10 == connectionNum) { stop = true; } Socket socket = serverSocket.accept(); service.execute(() -&amp;gt; { try { Scanner scanner = new Scanner(socket.getInputStream()); PrintStream printStream = new PrintStream(socket.getOutputStream()); while (!stop) { String s = scanner.next().trim(); printStream.println(&quot;PONG:&quot; + s); } } catch (IOException e) { e.printStackTrace(); } finally { service.shutdown(); try { serverSocket.close(); } catch (IOException e) { e.printStackTrace(); } } }); connectionNum++; } } catch (Exception e) { e.printStackTrace(); }; }}启动之后，使用 nc 命令 进行连接测试，结果如下：$ nc -v localhost 8888Connection to localhost port 8888 [tcp/ddi-tcp-1] succeeded!helloPONG:hellonicePONG:nice再然后使用获取代码性能数据的工具 ，可以看到有多个线程在运行，和连接数是一一对应的。可以看到，BIO 的读写操作时阻塞的，线程的整个生命周期和连接的生命周期时一样的，而且不能够被复用。对于单个阻塞 I/O 来说，它的效率并不比 NIO 慢。但是当服务的连接增多，考虑到整个服务器的资源调度和资源利用率等因素，NIO 就有了显著的效果，NIO 非常适合高并发场景。非阻塞 I/O 模型其实，在处理 I/O 动作时，有大部分时间是在等待。比如：socket 连接需要花费很长时间进行连接操作，在完成连接的这段时间内，它并没有占用额外的系统资源，但它只能阻塞等待在线程中。这种情况下，系统资源并不能被合理利用。Java 的 NIO，在 Linux 上底层是使用 epoll 实现的，epoll 是一个高性能的多路复用 I/O 工具，改进了 select 和 poll 等工具的一些功能。epoll 的数据结构是直接在内核上进行支持的，通过 epoll_create 和 epoll_ctl 等函数的操作，可以构造描述符（fd）相关的事件组合（event）。这里面有两个重要的概念： fd，每条连接、每个文件，都对应着一个描述符，比如端口号。内核在定位到这些连接的时候，就是通过 fd 进行寻址的。 event，当 fd 对应的资源，有状态或者数据变动，就会更新 epoll_item 结构。在没有事件变更的时候，epoll 就阻塞等待，也不会占用系统资源；一旦有新的事件到来，epoll 就会被激活，将事件通知到应用方。 相对于 select，epoll 有哪些改进？ epoll 不再需要像 select 一样对 fd 集合进行轮询，也不需要在调用时将 fd 集合在用户态和内核态进行交换； 应用程序获得就绪 fd 的事件复杂度，epoll 是 O(1)，select 是 O(n)； select 最大支持约 1024 个 fd，epoll 支持 65535个； select 使用轮询模式检测就绪事件，epoll 采用通知方式，更加高效 还是以 Java 中的 NIO 代码为例，来看一下 NIO 的具体概念：public class NIO { static boolean stop = false; public static void main(String[] args) { int connectionNum = 0; int port = 8888; ExecutorService service = Executors.newCachedThreadPool(); try { ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.socket().bind(new InetSocketAddress(&quot;localhost&quot;, port)); Selector selector = Selector.open(); serverSocketChannel.register(selector, serverSocketChannel.validOps()); while (!stop) { if (10 == connectionNum) { stop = true; } int num = selector.select(); if (num == 0) { continue; } Iterator&amp;lt;SelectionKey&amp;gt; events = selector.selectedKeys().iterator(); while (events.hasNext()) { SelectionKey event = events.next(); if (event.isAcceptable()) { SocketChannel socketChannel = serverSocketChannel.accept(); socketChannel.configureBlocking(false); socketChannel.register(selector, SelectionKey.OP_READ); connectionNum ++ ; } else if (event.isReadable()) { try { SocketChannel socketChannel = (SocketChannel) event.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); int size = socketChannel.read(byteBuffer); if (-1 == size) { socketChannel.close(); } String result = new String(byteBuffer.array()).trim(); ByteBuffer wrap = ByteBuffer.wrap((&quot;PONG: &quot; + result).getBytes(StandardCharsets.UTF_8)); socketChannel.write(wrap); } catch (Exception e) { e.printStackTrace(); } finally { service.shutdown(); serverSocketChannel.close(); } } events.remove(); } } } catch (IOException e) { e.printStackTrace(); } }}上面这段代码比较长，是使用 NIO 实现的和 BIO 相同的功能。从它的 API 设计上，能够看到 epoll 的一些影子。首先，创建了一个服务端 ssc，并开启一个新的事件选择器，监听它的 OP_ACCEPT 事件。ServerSocketChannel ssc = ServerSocketChannel.open();Selector selector = Selector.open();ssc.register(selector, ssc.validOps());共有 4 种事件类型，分别是： 新连接事件（OP_ACCEPT）； 连接就绪事件（OP_CONNECT）； 读就绪事件（OP_READ）； 写就绪事件（OP_WRITE）。任何网络和文件操作，都可以抽象成这四个事件。接下来，在 while 循环里，使用 select 函数，阻塞在主线程里。所谓阻塞，就是操作系统不再分配 CPU 时间片到当前线程中，所以 select 函数是几乎不占用任何系统资源的。int num = selector.select();一旦有新的事件到达，比如有新的连接到来，主线程就能够被调度到，程序就能够向下执行。这时候，就能够根据订阅的事件通知，持续获取订阅的事件。由于注册到 selector 的连接和事件可能会有多个，所以这些事件也会有多个。我们使用安全的迭代器循环进行处理，在处理完毕之后，将它删除。 如果事件不删除的话，或者漏掉了某个事件的处理，会有什么后果？Iterator&amp;lt;SelectionKey&amp;gt; events = selector.selectedKeys().iterator();while (events.hasNext()) { SelectionKey event = events.next(); ... events.remove();}有新的连接到达时，由于订阅了更多的事件。对于数据读取来说，对应的事件就是 OP_READ。和 BIO 编程面向流的方式不同，NIO 操作的对象是抽象的概念 Channel，通过缓冲区进行数据交换。SocketChannel sc = ssc.accept();sc.configureBlocking(false);sc.register(selector, SelectionKey.OP_READ);值得注意的是：服务端和客户端的实现方式，可以是不同的。比如，服务端是 NIO，客户端可以是 BIO，它们并没有什么强制要求。另外一个面试时候经常问到的事件就是 OP_WRITE。我们上面提到过，这个事件是表示写就绪的，当底层的缓冲区有空闲，这个事件就会一直发生，浪费占用 CPU 资源。所以，我们一般是不注册 OP_WRITE 的。这里还有一个细节，在读取数据的时候，并没有像 BIO 的方式一样使用循环来获取数据。如下面的代码，我们创建了一个 1024 字节的缓冲区，用于数据的读取。如果连接中的数据，大于 1024 字节怎么办？SocketChannel sc = (SocketChannel) event.channel();ByteBuffer buf = ByteBuffer.allocate(1024);int size = sc.read(buf);这涉及两种事件的通知机制： 水平触发(level-triggered) 称作 LT 模式。只要缓冲区有数据，事件就会一直发生 边缘触发(edge-triggered) 称作 ET 模式。缓冲区有数据，仅会触发一次。事件想要再次触发，必须先将 fd 中的数据读完才行可以看到，Java 的 NIO 采用的就是水平触发的方式。LT 模式频繁环唤醒线程，效率相比较ET模式低，所以 Netty 使用 JNI 的方式，实现了 ET 模式，效率上更高一些。Reactor 模式了解了 BIO 和 NIO 的一些使用方式，Reactor 模式就呼之欲出了。NIO 是基于事件机制的，有一个叫作 Selector 的选择器，阻塞获取关注的事件列表。获取到事件列表后，可以通过分发器，进行真正的数据操作。你可以回看下我在上文举例的 “Java 中的 NIO 代码”，对比分析一下，你会发现 Reactor模型 里面有四个主要元素： Acceptor处理 client 的连接，并绑定具体的事件处理器； Event具体发生的事件，比如图中s的read、send等； Handler执行具体事件的处理者，比如处理读写事件的具体逻辑； Reactor将具体的事件分配（dispatch）给 Handler。我们可以对上面的模型进行进一步细化，如下图所示，将 Reactor 分为 mainReactor 和 subReactor 两部分。 mainReactor负责监听处理新的连接，然后将后续的事件处理交给 subReactor； subReactor对事件处理的方式，也由阻塞模式变成了多线程处理，引入了任务队列的模式。熟悉 Netty 的同学可以看到，这个 Reactor 模型就是 Netty 设计的基础。在 Netty 中，Boss 线程对应着对连接的处理和分派，相当于 mainReactor；Worker 线程对应着 subReactor，使用多线程负责读写事件的分发和处理。这种模式将每个组件的职责分得更细，耦合度也更低，能有效解决 C10k 问题。AIO关于 NIO 的概念，误解还是比较多的。为什么我在使用 NIO 时，使用 Channel 进行读写，socket 的操作依然是阻塞的？NIO 的作用主要体现在哪里？//这行代码是阻塞的int size = sc.read(buf);这时你可以回答：NIO 只负责对发生在 fd 描述符上的事件进行通知。事件的获取和通知部分是非阻塞的，但收到通知之后的操作，却是阻塞的，即使使用多线程去处理这些事件，它依然是阻塞的。AIO 更近一步，将这些对事件的操作也变成非阻塞的。下面是一段典型的 AIO 代码，它通过注册 CompletionHandler 回调函数进行事件处理。这里的事件是隐藏的，比如 read 函数，它不仅仅代表 Channel 可读了，而且会把数据自动的读取到 ByteBuffer 中。等完成了读取，就会通过回调函数通知你，进行后续的操作。public class AIO { public static void main(String[] args) throws Exception { int port = 8888; AsynchronousServerSocketChannel ssc = AsynchronousServerSocketChannel.open(); ssc.bind(new InetSocketAddress(&quot;localhost&quot;, port)); ssc.accept(null, new CompletionHandler&amp;lt;AsynchronousSocketChannel, Object&amp;gt;() { void job(final AsynchronousSocketChannel sc) { ByteBuffer buffer = ByteBuffer.allocate(1024); sc.read(buffer, buffer, new CompletionHandler&amp;lt;Integer, ByteBuffer&amp;gt;() { @Override public void completed(Integer result, ByteBuffer attachment) { String str = new String(attachment.array()).trim(); ByteBuffer wrap = ByteBuffer.wrap((&quot;PONG:&quot; + str).getBytes()); sc.write(wrap, null, new CompletionHandler&amp;lt;Integer, Object&amp;gt;() { @Override public void completed(Integer result, Object attachment) { job(sc); } @Override public void failed(Throwable exc, Object attachment) { System.out.println(&quot;error&quot;); } }); } @Override public void failed(Throwable exc, ByteBuffer attachment) { System.out.println(&quot;error&quot;); } }); } @Override public void completed(AsynchronousSocketChannel sc, Object attachment) { ssc.accept(null, this); job(sc); } @Override public void failed(Throwable exc, Object attachment) { exc.printStackTrace(); System.out.println(&quot;error&quot;); } }); Thread.sleep(Integer.MAX_VALUE); }AIO 是 Java 1.7 加入的，理论上性能会有提升，但实际测试并不理想。这是因为，AIO主要处理对数据的自动读写操作。这些操作的具体逻辑，假如不放在框架中，也要放在内核中，并没有节省操作步骤，对性能的影响有限。而 Netty 的 NIO 模型加上多线程处理，在这方面已经做得很好，编程模式也比AIO简单。所以，市面上对 AIO 的实践并不多，在采用技术选型的时候，一定要谨慎。响应式编程你可能听说过 Spring 5.0 的 WebFlux，WebFlux 是可以替代 Spring MVC 的一套解决方案，可以编写响应式的应用，两者之间的关系如下图所示：Spring WebFlux 的底层使用的是 Netty，所以操作是异步非阻塞的，类似的组件还有 vert.x、akka、rxjava 等。WebFlux 是运行在 project reactor 之上的一个封装，其根本特性是后者提供的，至于再底层的非阻塞模型，就是由 Netty 保证的了。非阻塞的特性我们可以理解，那响应式又是什么概念呢？响应式编程是一种面向数据流和变化传播的编程范式。这意味着可以在编程语言中很方便地表达静态或动态的数据流，而相关的计算模型会自动将变化的值，通过数据流进行传播。这段话很晦涩，在编程方面，它表达的意思就是：把生产者消费者模式，使用简单的API 表示出来，并自动处理背压（Backpressure）问题。背压，指的是生产者与消费者之间的流量控制，通过将操作全面异步化，来减少无效的等待和资源消耗。Java 的 Lambda 表达式可以让编程模型变得非常简单，Java 9 更是引入了响应式流（Reactive Stream），方便了我们的操作。比如，下面是 Spring Cloud GateWay 的 Fluent API 写法，响应式编程的 API 都是类似的。public RouteLocator customerRouteLocator(RouteLocatorBuilder builder) { return builder.routes().route(r -&amp;gt; r.path(&quot;/market/**&quot;).filters(f -&amp;gt; f.filter(new RequestTimeFilter()).addResponseHeader(&quot;X-Response-Default-Foo&quot;, &quot;Default-Bar&quot;)).uri(&quot;http://localhost:8080/market/list&quot;).order(0).id(&quot;customer_filter_router&quot;)).build();}从传统的开发模式过渡到 Reactor 的开发模式，是有一定成本的，不过它确实能够提高我们应用程序的性能，至于是否采用，这取决于你在编程难度和性能之间的取舍。小结本课时，我们系统地学习了 BIO、NIO、AIO 等概念和基本的编程模型 Reactor，我们了解到： BIO 的线程模型是一个连接对应一个线程的，非常浪费资源； NIO通过对关键事件的监听，通过主动通知的方式完成非阻塞操作，但它对事件本身的处理依然是阻塞的； AIO 完全是异步非阻塞的，但现实中使用很少。使用 Netty 的多 Acceptor 模式和多线程模式，我们能够方便地完成类似 AIO 这样的操作。Netty 的事件触发机制使用了高效的 ET 模式，使得支持的连接更多，性能更高。使用 Netty，能够构建响应式编程的基础，加上类似 Lambda 表达式这样的书写风格，能够完成类似 WebFlux 这样的响应式框架。响应式编程是一个趋势，现在有越来越多的框架和底层的数据库支持响应式编程，我们的应用响应也会更加迅速。" }, { "title": "IO 模型", "url": "/posts/io-model/", "categories": "Java, Base", "tags": "IO, BIO, NIO, AIO", "date": "2019-05-01 02:33:22 +0000", "snippet": "I/O (Input/Output)，输入/输出计算机机构角度根据冯·诺伊曼结构，计算机结构分为五个部分： 运算器 控制器 存储器 输入设备 输出设备如下图所示：输入设备和输出设备都属于外部设备。比如：键盘属于输入设备，显示器属于输出设备，网卡、硬盘这种既可以属于输出设备、也可以属于输入设备。输入设备向计算机输入数据，输出设备接收计算机输出的数据。从计算机结构的角度来看，I/O 描述了计算机系统与外部设备之间通信的过程。应用程序的角度根据操作系统相关的知识，为了保证操作系统的稳定性和安全性。一个进程的地址空间分为： 用户空间（User Space) 内核空间（Kernel Space）像平常运行的应用程序都是运行在用户空间，只有内核空间才能进行系统态级别的资源的有关操作，比如文件管理、进程通信、内存管理等。也就是说，想要进行 I/O 操作，一定是要依赖内核空间的能力。并且，用户空间的程序不能直接访问内核空间。如果想要执行 I/O 操作，必须通过系统调用间接访问内核空间。日常开发中，接触最多的就是磁盘I/O（读写文件）和网络I/O（网络请求和响应）。从应用程序的角度来看，应用程序对操作系统的内核发起了I/O 调用（系统调用），操作系统负责的内核执行具体的 I/O 操作。也就是说，应用程序只是发起了 I/O 操作的调用而已，具体的 IO 执行是由操作系统的内核来完成。应用程序发起 I/O 调用后，会经历两个步骤： 内核等待 I/O 设备准备好数据； 内核将数据从内核空间拷贝到用户空间常见的 I/O 模型在 UNIX 系统下，IO 模型一共有 5 种（经常提到的 5 种 IO 模型），分布式： 同步阻塞 I/O，Synchronous blocking I/O 同步非阻塞 I/O，Synchronous non-blocking I/O I/O 多路复用，I/O multi-road reuse 信号驱动 I/O，Signal driver I/O 异步 I/O，Asynchronous I/OJava 种 3 种常见的 I/O 模型在 Java 种有三种常见的 IO 模型，分别是： BIO (Blocking I/O) NIO (Non-blocking/New I/O) AIO (Asynchronous I/O)BIO (Blocking I/O)BIO 属于同步阻塞 IO 模型。同步阻塞 I/O 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核将数据拷贝到用户空间。在客户端连接数量不高的情况下，是没有问题的。但是，当面对成千上万连接的时候，传统的 BIO 模型是无能为力的，因此，需要一种更加高效的 I/O 处理模型应对更高的并发量。NIO (Non-blocking/New I/O)Java 中的 NIO 是在 Java 1.4 中引入，对应的是 java.nio 包，提供了 Channel，Selector，Buffer 等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它支持面向缓冲，基于通道的 I/O 操作方法。.对于高负载、高并发的（网络）应用，应该使用 NIO ！！！Java 中的 NIO 应该看作是 I/O 多路复用。很多书和博客经同步非阻塞 I/O 模型同步非阻塞 I/O 模型中，应用程序会一直发起 read 调用。在等待数据从内核空间拷贝用户空间的这段时间里，线程依然是阻塞的（直到内核把数据拷贝到用户空间）。相比于同步阻塞 IO 模型，同步非阻塞 IO 模型有了很大的改进：通过轮询操作，避免了一直阻塞！！！但是，同步非阻塞 IO 模型存在的问题是：应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程非常消耗 CPU 资源。此时，I/O 多路复用模型就上场了。I/O 多路复用模型在该模型中，用户线程（空间）首先发起 select 调用，询问内核空间是否将数据准备就绪，等内核将数据准备好了，用户线程（空间）再发起 read 调用。read 调用的过程（数据从内核空间拷贝到用户空间）依旧是阻塞的。 目前支持 IO 多路复用的系统调用，有 select，epoll 等等。select 系统调用，目前几乎在所有的操作系统上都有支持。 select 调用 ：内核提供的系统调用，它支持一次查询多个系统调用的可用状态。几乎所有的操作系统都支持。 epoll 调用 ：linux 2.6 内核，属于 select 调用的增强版本，优化了 IO 的执行效率。 IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。Java 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。AIO (Asynchronous I/O)AIO 也就是 NIO 2。Java 7 中引入了 NIO 的改进版 NIO 2,它是异步 IO 模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。目前来说 AIO 的应用还不是很广泛。Netty 之前也尝试使用过 AIO，不过又放弃了。这是因为，Netty 使用了 AIO 之后，在 Linux 系统上的性能并没有多少提升。最后，来一张图，简单总结一下 Java 中的 BIO、NIO、AIO。参考 《深入拆解 Tomcat &amp;amp; Jetty》 如何完成一次 IO：https://llc687.top/126.html 程序员应该这样理解 IO：https://www.jianshu.com/p/fa7bdc4f3de7 10 分钟看懂， Java NIO 底层原理：https://www.cnblogs.com/crazymakercircle/p/10225159.html IO 模型知多少 理论篇：https://www.cnblogs.com/sheng-jie/p/how-much-you-know-about-io-models.html 《UNIX 网络编程 卷 1；套接字联网 API 》6.2 节 IO 模型" }, { "title": "乐观锁和无锁（14）", "url": "/posts/optimistic-locking-and-pessimistic-locking/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-04-27 12:33:33 +0000", "snippet": "concurrent 下面的 Lock ，是 API 级别，对共享资源进行更细粒度的控制。Lock 是基于 AQS (AbstractQueuedSynchronizer) 实现的，AQS 是用来构建 Lock 或 其它同步组件的基础，它使用了一个 int 成员变量来表示 state (同步状态)，通过内置的 FIFO 队列，来完成资源获取线程的排队。synchronized 的方式加锁，会让线程在BLOCKED 状态和RUNNABLE 状态之间切换，在操作系统上，就会造成用户态和内核态的频繁切换，效率就比较低。与 synchronized 的实现方式不同，AQS 中很多数据结构的变化，都是依赖 CAS 进行操作的，而 CAS 就是乐观锁的一种实现。CASCAS 是 Compare And Swap 的缩写，意思是：比较并替换。CAS 机制当中使用了 3 个基本操作数：内存地址 V、期望值 E、要修改的新值 N。更新一个变量的时候，只有当变量的期望值 E 和 内存地址 V 的真正值相同时，才会将内存地址 V 对应的值修改为 N.如果修改不成功，在很多情况下，它将一直重试，直到修改为期望的值。比如 AtomicInteger 类来说，相关的代码如下：/** * Atomically sets the value to the given updated value * if the current value {@code ==} the expected value. * 如果当前值 {@code ==} 是预期值，则自动将值设置为给定的更新值。 * * @param expect the expected value 期望值 * @param update the new value 新值 * @return {@code true} if successful. False return indicates that * the actual value was not equal to the expected value. * 如果成功。 错误返回表明实际值不等于预期值。 */public final boolean compareAndSet(int expect, int update) { return unsafe.compareAndSwapInt(this, valueOffset, expect, update);}比较和替换是两个动作，CAS 是如何保证这两个操作的原子性呢 ？继续向下追踪，发现是 jdk.internal.misc.Unsafe 类实现，循环重试就是在这里发生的：public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;}追踪到 JVM 内部，在 linux 机器上参照 os_cpu/linux_x86/atomic_linux_x86.hpp。可以看到，最底层的调用，是汇编语言，而最重要的，就是**cmpxchgl**指令。到这里没法再往下找代码了，因为 CAS 的原子性实际上是硬件 CPU 直接保证的。template&amp;lt;&amp;gt;template&amp;lt;typename T&amp;gt;inline T Atomic::PlatformCmpxchg&amp;lt;4&amp;gt;::operator()(T exchange_value, T volatile* dest, T compare_value,atomic_memory_order /* order */) const { STATIC_ASSERT(4 == sizeof(T)); __asm__ volatile (&quot;lock cmpxchgl %1,(%3)&quot; : &quot;=a&quot; (exchange_value) : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest) : &quot;cc&quot;, &quot;memory&quot;); return exchange_value;}CAS 实现的原子类，带来的性能提升有多少 ？ 我开启了 20 个线程，对共享变量进行自增操作。从测试结果得知，针对频频的写操作，原子类的性能是 synchronized 方式的 3 倍。之所以 CAS 原理，在近几年面试中的考察率越来越高，主要是由于乐观锁在读多写少的互联网场景中，使用频率越发频繁。可以看到一些乐观锁的变种，但最根本的思想是一样的，都是基于比较替换并替换的基本操作。关于 Atomic 类，还有一个细节，那就是它的主要变量，使用了 volatile 关键字进行修饰。代码如下:private volatile int value; 使用了 volatile 关键字的变量，每当变量的值有变动的时候，都会将更改立即同步到主内存中，而如果某个线程想要使用这个变量，就先要从主存中刷新到工作内存，这样就确保了变量的可见性。有了这个关键字的修饰，就能保证每次比较的时候，拿到的值总是最新的。乐观锁乐观锁严格来说，并不是一种锁，它提供了一种检测冲突的机制，并在有冲突的时候，采取重试的方法完成某项操作。假如没有重试操作，乐观锁就仅仅是一个判断逻辑而已。从这里可以看出乐观锁与悲观锁的一些区别： 悲观锁，每次操作数据的时候，都会认为别人会修改，所以每次在操作数据的时候，都会加锁，除非别人释放掉锁； 乐观锁，在检测到冲突的时候，会有多次重试操作。因此，乐观锁适合用在读多写少的场景； 在资源冲突比较严重的场景，乐观锁会出现多次失败的情况，造成 CPU 的空转，所以悲观锁在这种场景下，会有更好的性能。为什么读多写少的情况，就适合使用乐观锁呢？悲观锁在读多写少的情况下，也有很少的冲突；其实，问题不在于冲突的频繁性，而在于加锁这个动作上： 悲观锁需要遵循下面三种模式：一锁、二读、三更新，即使在没有冲突的情况下，执行也会非常慢； 如之前所说，乐观锁本质上不是锁，它只是一个判断逻辑，资源冲突少的情况下，它不会产生任何开销。上面谈的** CAS 操作，就是一种典型的乐观锁实现方式**，顺便看一下 CAS 的缺点，也就是乐观锁的一些缺点。 在并发量比较高的情况下，有些线程可能会一直尝试修改某个资源，但由于冲突比较严重，一直更新不成功，这时候，就会给 CPU 带来很大的压力； JDK 1.8 中新增的 LongAdder，通过把原值进行拆分，最后再以 sum 的方式，减少 CAS 操作冲突的概率，性能要比 AtomicLong 高出 10 倍左右； CAS 操作的对象，只能是单个资源，如果想要保证多个资源的原子性，最好使用 synchronized 等经典加锁方式； ABA 问题，意思是指在 CAS 操作时，有其他的线程现将变量的值由 A 变成了 B，然后又改成了 A，当前线程在操作时，发现值仍然是 A，于是进行了交换操作。这种情况在某些场景下可不用过度关注，比如 AtomicInteger，因为没什么影响；但在一些其他操作，比如链表中，会出现问题，必须要避免。可以使用 AtomicStampedReference 给引用标记上一个整型的版本戳，来保证原子性。乐观锁实现余额更新对余额的操作，是交易系统里最常见的操作了。先读出余额的值，进行一番修改之后，再写回这个值。对余额的任何更新，都需要进行加锁。因为读取和写入操作并不是原子性的，如果同一时刻发生了多次与余额的操作，就会产生不一致的情况。举一个比较明显的例子。同时发起了一笔消费 80 元和 5 元的请求，经过操作之后，两个支付都成功了，但最后余额却只减了 5 元。相当于花了 5 块钱买了 85 元的东西。请看下面的时序：请求A：读取余额100请求B：读取余额100请求A：花掉5元，临时余额是95请求B：花掉80元，临时余额是20请求B：写入余额20成功请求A：写入余额95成功我曾经在线上遇到过一个 BUG，用户通过构造请求，频繁发起 100 元的提现和 1 分钱的提现，造成了比较严重的后果。所以，对余额操作加锁，是必须的。 这个过程和多线程的操作是类似的，不过多线程是单机的，而余额的场景是分布式的。对于数据库来说，就可以通过加行锁进行解决，对于 MySQL 来说，MyISAM 是不支持行锁的，只能使用 InnoDB，典型的 SQL 语句如下：select * from user where userid={id} for update使用 select for update 这么一句简单的 SQL，其实在底层就加了三把锁，非常昂贵。一种比较好的办法，就是使用乐观锁。根据乐观锁的定义，就可以抽象两个概念： 检测冲突的机制：先查出本次操作的余额 E，在更新时判断是否与当前数据库的值相同，如果相同则执行更新动作 重试策略：有冲突直接失败，或者重试5次后失败伪代码如下，可以看到这其实就是 CAS。# old_balance获取select balance from user where userid={id}# 更新动作 update user set balance = balance - 20 where userid={id} and balance &amp;gt;= 20 and balance = $old_balance还有一种 CAS 的变种，就是使用版本号机制。通过在表中加一个额外的字段 version，来代替对余额的判断。这种方式不用去关注具体的业务逻辑，可控制多个变量的更新，可扩展性更强，典型的伪代码如下：version,balance = dao.getBalance(userid)balance = balance - costdao.exec(&quot;update user set balance = balance - 20version = version + 1where userid=id and balance &amp;gt;= 20and version = $old_version&quot;)Redis 分布式锁Redis 的分布式锁，是经常使用的方案。是使用 setnx 指令或者带参数的 set 方法来实现的，但 Redis 的分布式锁其实有很多坑。使用分布式锁，可以实现更灵活地控制一些商品校验、订单生成等，它主要依赖SETNX 指令或者带参数的 SET 指令： 锁创建： SETNX [KEY] [VALUE] 原子操作，意思是在指定的 KEY 不存在的时候，创建一个并返回 1，否则返回 0； 通常使用参数更全的set key value [EX seconds] [PX milliseconds] [NX|XX] 命令，同时对 KEY 设置一个超时时间； 锁查询：GET KEY，通过简单地判断 KEY 是否存在即可； 锁删除：DEL KEY，删掉相应的 KEY 即可根据原生的语义，有下面简单的 lock 和 unlock 方法，lock 方法通过不断的重试，来获取到分布式锁，然后通过删除命令销毁分布式锁。public void lock(String key, int timeOutSecond) { for (; ; ) { boolean exist = redisTemplate.opsForValue().setIfAbsent(key, &quot;&quot;, timeOutSecond, TimeUnit.SECONDS); if (exist) { break; } }}public void unlock(String key) { redisTemplate.delete(key);}这段代码中的问题很多，其中一个最严重的问题。在多线程中，执行 unlock() 方法的，只能是当前的线程，但在上面的实现中，由于超时存在的原因，锁被提前释放了。考虑下面 3 个请求的时序： 请求A： 获取了资源 x 的锁，锁的超时时间为 5 秒； 请求A： 由于业务执行时间比较长，业务阻塞等待，超过 5 秒； 请求B： 第 6 秒发起请求，结果发现锁 x 已经失效，于是顺利获得锁； 请求A： 第 7 秒，请求 A 执行完毕，然后执行锁释放动作； 请求C： 请求 C 在锁刚释放的时候发起了请求，结果顺利拿到了锁资源此时，请求 B 和请求 C 都成功地获取了锁 x，分布式锁就失效了，在执行业务逻辑的时候，就容易发生问题。所以，在删除锁的时候，需要判断它的请求方是否正确。首先，获取锁中的当前标识，然后，在删除的时候，判断这个标识是否和解锁请求中的相同。可以看到，读取和判断是两个不同的操作，在这两个操作之间同样会有间隙，高并发下会出现执行错乱问题，而稳妥的方案，是使用 lua 脚本把它们封装成原子操作。改造后的代码如下：public String lock(String key, int timeOutSecond) { public String lock(String key, int timeOutSecond) { for (;;) { String stamp = String.valueOf(System.nanoTime()); boolean exist = redisTemplate.opsForValue().setIfAbsent(key, stamp, timeOutSecond, TimeUnit.SECONDS); if (exist) { return stamp; } } } public void unlock(String key, String stamp) { redisTemplate.execute(script, Arrays.asList(key), stamp); } public void unlock(String key, String stamp) { redisTemplate.execute(script, Arrays.asList(key), stamp); }}相应的 lua 脚本如下：local stamp = ARGV[1]local key = KEYS[1]local current = redis.call(&quot;GET&quot;,key)if stamp == current then redis.call(&quot;DEL&quot;,key) return &quot;OK&quot;end可以看到，reids 实现分布式锁，还是有一定难度的。推荐使用 redlock 的 Java 客户端实现 redisson，它是根据 Redis 官方提出的分布式锁管理方法实现的。这个锁的算法，处理了分布式锁在多 redis 实例场景下，以及一些异常情况的问题，有更高的容错性。比如，我们前面提到的锁超时问题，在 redisson 会通过看门狗机制对锁进行无限续期，来保证业务的正常运行。可以看下 redisson 分布式锁的典型使用代码。String resourceKey = &quot;goodgirl&quot;;RLock lock = redisson.getLock(resourceKey);try { lock.lock(5, TimeUnit.SECONDS); //真正的业务 Thread.sleep(100);} catch (Exception ex) { ex.printStackTrace();} finally { if (lock.isLocked()) { lock.unlock(); }}使用 redis 的 monitor 命令，可以看到具体的执行步骤，这个过程还是比较复杂的。无锁无锁（Lock-Free），指的是在多线程环境下，在访问共享资源的时候，不会阻塞其他线程的执行。在 Java 中，最典型的无锁队列实现，就是 ConcurrentLinkedQueue，但它是无界的，不能够指定它的大小。ConcurrentLinkedQueue 使用 CAS 来处理对数据的并发访问，这是无锁算法得以实现的基础。CAS 指令不会引起上下文切换和线程调度，是非常轻量级的多线程同步机制。它还把入队、出队等对 head 和 tail 节点的一些原子操作，拆分出更细的步骤，进一步缩小了 CAS 控制的范围。ConcurrentLinkedQueue 是一个非阻塞队列，性能很高，但不是很常用。千万不要和阻塞队列 LinkedBlockingQueue（内部基于锁）搞混了。Disruptor 是一个无锁、有界的队列框架，它的性能非常高。它使用 RingBuffer、无锁和缓存行填充等技术，追求性能的极致，在极高并发的场景，可以使用它替换传统的 BlockingQueue。在一些中间件中经常被使用，比如日志、消息等（Storm 使用它实现进程内部通信机制），但它在业务系统上很少用，除非是类似秒杀的场景。因为它的编程模型比较复杂，而且业务的主要瓶颈主要在于缓慢的 I/O 上，而不是慢在队列上。总结乐观锁严格来说，并不是一种锁。它提供了一种检测冲突的机制，并在有冲突的时候，采取重试的方法完成某项操作。假如没有重试操作，乐观锁就仅仅是一个判断逻辑而已。悲观锁每次操作数据的时候，都会认为别人会修改，所以每次在操作数据的时候，都会加锁，除非别人释放掉锁。乐观锁在读多写少的情况下，之所以比悲观锁快，是因为悲观锁需要进行很多额外的操作，并且乐观锁在没有冲突的情况下，也根本不耗费资源。但乐观锁在冲突比较严重的情况下，由于不断地重试，其性能在大多数情况下，是不如悲观锁的。由于乐观锁的这个特性，乐观锁在读多写少的互联网环境中被广泛应用。本课时，我们主要看了在数据库层面的一个乐观锁实现，以及Redis 分布式锁的实现，后者在实现的时候，还是有很多细节需要注意的，建议使用 redisson 的 RLock。当然，乐观锁有它的使用场景。当冲突非常严重的情况下，会进行大量的无效计算；它也只能保护单一的资源，处理多个资源的情况下就捉襟见肘；它还会有 ABA 问题，使用带版本号的乐观锁变种可以解决这个问题。这些经验，我们都可以从 CAS 中进行借鉴。多线程环境和分布式环境有很多相似之处，对于乐观锁来说，我们找到一种检测冲突的机制，就基本上实现了。 一个接口的写操作，大约会花费 5 分钟左右的时间。它在开始写时，会把数据库里的一个字段值更新为 start，写入完成后，更新为 done。有另外一个用户也想写入一些数据，但需要等待状态为 done。于是，在 WEB 端，使用轮询，每隔 5 秒，查询字段值是否为 done，当查询到正确的值，即可开始进行数据写入。这个方法，属于乐观锁吗？有哪些潜在问题？应该如何避免？这个属于乐观锁 潜在的问题是如果查询到 done 到开始写入之间有其他写入也进行了执行那就出现不一致了 根本问题是查询和写入不是一个原子操作 解决办法通过数据库或者redis实现状态位更新做控制 类似cas如果能成功即可写入" }, { "title": "多线程锁的优化（13）", "url": "/posts/optimization-of-multi-threaded-locks/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-04-22 12:33:11 +0000", "snippet": "除了使用 ThreadLocal，来避免 SimpleDateFormat 在并发环境下引起的时间错乱问题。其实还有一种解决方式，就是通过对 parse 方法进行加锁，也能保证日期处理类的正确运行，代码如下图（可见仓库）：public class ThreadSafeDateFormat { SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); public static void main(String[] args) { final ThreadSafeDateFormat threadSafeDateFormat = new ThreadSafeDateFormat(); ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &amp;lt; 1000; i++) { executor.submit(() -&amp;gt; { try { synchronized (threadSafeDateFormat) { System.out.println(threadSafeDateFormat.format.parse(&quot;2020-07-25 08:56:40&quot;)); } } catch (ParseException e) { throw new IllegalStateException(); } }); } executor.shutdown(); }}但是，锁对性能的影响是非常的大。因为对资源加锁以后，资源就被加锁的线程独占，其它线程只能够排队等待这个锁，此时程序就由并行执行，变相地成为了顺序执行，执行速度自然就降低了。Benchmark                                 Mode  Cnt     Score      Error   UnitsSynchronizedNormalBenchmark.sync         thrpt   10  2554.628 ± 5098.059  ops/msSynchronizedNormalBenchmark.threadLocal  thrpt   10  3750.902 ±  103.528  ops/ms========去掉业务影响========Benchmark                                 Mode  Cnt     Score      Error   UnitsSynchronizedNormalBenchmark.sync         thrpt   10    26905.514 ±   1688.600  ops/msSynchronizedNormalBenchmark.threadLocal  thrpt   10  7041876.244 ± 355598.686  ops/ms可以清楚的看到，使用同步锁的方式，性能是比较低的。如果去掉业务本身逻辑的影响（删调执行逻辑），这个差异会更大。代码执行的次数越多，锁的累加影响越大，对锁本身的速度优化，是非常重要的。Java 中有两种加锁的方式：一种是常见的 synchronized 关键字，另外一种，就是使用 concurrent 包里面的 Lock。针对这两种锁，JDK 自身做了很多的优化，它们的实现方式也是不同的。synchroniedsynchronied 关键字给代码或者方法上锁时，都有显示或者隐藏上锁对象。当一个线程试图访问同步代码块时，它首先必须得到锁，而退出或抛出异常时必须释放锁： 给普通方法加锁时，上锁的对象是 this； 给静态方法加锁时，锁的是 class 对象； 给代码块加锁，可以指定一个具体的对象作为锁。1. monitor 原理在面试的时候，有可能会问道，syschronized 在字节码中，是怎么体现的呢 ？参照下面的代码，在命令行执行 javac，然后再执行 javap -v -p ，就可以看到它具体的字节码。可以看到，在字节码的体现上，它只给方法加了一个 flag：ACC_SYSCHRONIZEDsynchronized void syncMethod() { System.out.println(&quot;syncMethod&quot;);}======字节码=====synchronized void syncMethod(); descriptor: ()V flags: ACC_SYNCHRONIZED Code: stack=2, locals=1, args_size=1 0: getstatic #4 3: ldc #5 5: invokevirtual #6 8: return再看一下，同步代码块的字节码。可以看到，字节码是通过 monitorenter 和 monitorexit 两个指令进行控制的。void syncBlock(){ synchronized (Test.class){}}======字节码======void syncBlock(); descriptor: ()V flags: Code: stack=2, locals=3, args_size=1 0: ldc #2 2: dup 3: astore_1 4: monitorenter 5: aload_1 6: monitorexit 7: goto 15 10: astore_2 11: aload_1 12: monitorexit 13: aload_2 14: athrow 15: return Exception table: from to target type 5 7 10 any 10 13 10 any这两者虽然显示的效果不同，但是它们都是通过 monitor 来实现同步的。可以通过下面的一张图，来看一下 monitor 的原理：如上图所示，可以把运行时的对象锁抽象地分为三部分。其中，EntrySet 和 WaitSet 是两个队列，中间虚线部分是当前持有锁的线程。可以想象一下线程的执行过程：当一个线程到来的时候，发现并没有线程持有对象锁，它会直接成为活动线程，进入 RUNNING 状态。紧接着又来了三个线程，要争抢对象锁。此时，这个三个线程发现锁已经被占用了，就先进入 EntrySet 缓存起来，进入 BLOCKED 状态。此时，通过 jstack 命令，可以看到他们展示的信息都是 waiting for monitor entry.&quot;http-nio-8084-exec-120&quot; #143 daemon prio=5 os_prio=31 cpu=122.86ms elapsed=317.88s tid=0x00007fedd8381000 nid=0x1af03 waiting for monitor entry [0x00007000150e1000] java.lang.Thread.State: BLOCKED (on object monitor) at java.io.BufferedInputStream.read(java.base@13.0.1/BufferedInputStream.java:263) - waiting to lock &amp;lt;0x0000000782e1b590&amp;gt; (a java.io.BufferedInputStream) at org.apache.commons.httpclient.HttpParser.readRawLine(HttpParser.java:78) at org.apache.commons.httpclient.HttpParser.readLine(HttpParser.java:106) at org.apache.commons.httpclient.HttpConnection.readLine(HttpConnection.java:1116) at org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1973) at org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1735)处于活动状态的线程，执行完毕退出了；或者由于某种原因执行了 wait 方法，释放了对象锁，进入了 WaitSet 队列，这就是在调用 wait 之前，需要先获得对象锁的原因。就像下面的代码：synchronized (lock){ try { lock.wait(); } catch (InterruptedException e) { e.printStackTrace(); }}此时，jstack 显示的线程状态 WAITING 状态，而原因是 in Object.wait()。&quot;wait-demo&quot; #12 prio=5 os_prio=31 cpu=0.14ms elapsed=12.58s tid=0x00007fb66609e000 nid=0x6103 in Object.wait() [0x000070000f2bd000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(java.base@13.0.1/Native Method) - waiting on &amp;lt;0x0000000787b48300&amp;gt; (a java.lang.Object) at java.lang.Object.wait(java.base@13.0.1/Object.java:326) at WaitDemo.lambda$main$0(WaitDemo.java:7) - locked &amp;lt;0x0000000787b48300&amp;gt; (a java.lang.Object) at WaitDemo$$Lambda$14/0x0000000800b44840.run(Unknown Source) at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)发生了这两种情况，都会造成对象锁的释放，进而导致 EntrySet 里的线程重新争抢对象锁，成功抢到锁的线程成为活动线程，这是一个循环的过程。那 WaitSet 中的线程是如何再次被激活的呢？接下来，在某个地方，执行了锁的 notify 或者 notifyAll 命令，会造成 WaitSet 中的线程，转移到 EntrySet 中，重新进行锁的争夺。如此周而复始，线程就可按顺序排队执行。2. 分级锁在 JDK 1.8 中，synchronized 的速度已经有了显著的提升，它做的主要优化就是 —— 分级锁。JVM 会根据使用情况，对 synchronized 的锁，进而升级，它大体按照下面的路径进行升级：锁只能升级，不能降级。所以一旦升级为重量锁，就只能依靠操作系统进行调度。要想了解锁升级的过程，就需要先了解对象在内存里的结构：对象分为四部分，如下： MarkWord Class Pointer Instance Data Padding和锁升级关系最大的就是 MarkWord，它的长度是 24 位，包含 Thread ID (23 bit)、Age (6 bit)、Biased (1 bit) 以及 Tag (2 bit) 。锁升级就是靠判断 Thread Id、Biased、Tag 等三个变量值来进行的。 偏向锁在只有一个线程使用了锁的情况下，偏向锁能够保证更高的效率。 具体过程是这样的：当第一个线程第一次访问同步块时，会先检测对象头 Mark Word 中的标志位 Tag 是否为 01，以此判断此时对象锁是否处于无锁状态或者偏向锁状态（匿名偏向锁）。 01 是锁默认的状态，线程一旦获取了这把锁，就会把自己的线程 ID 写到 MarkWord 中，在其他线程来获取这把锁之前，锁都处于偏向锁状态。 当下一个线程参与到偏向锁竞争时，会先判断 MarkWord 中保存的线程 ID 是否与这个线程 ID 相等，如果不相等，会立即撤销偏向锁，升级为轻量级锁。 轻量级锁 轻量级锁的获取是怎么进行的呢？它们使用的是自旋方式。 参与竞争的每个线程，会在自己的线程栈中生成一个 LockRecord ( LR )，然后每个线程通过 CAS（自旋）的方式，将锁对象头中的 MarkWord 设置为指向自己的 LR 的指针，哪个线程设置成功，就意味着哪个线程获得锁。 当锁处于轻量级锁的状态时，就不能够再通过简单地对比 Tag 的值进行判断，每次对锁的获取，都需要通过自旋。 当然，自旋也是面向不存在锁竞争的场景，比如一个线程运行完了，另外一个线程去获取这把锁；但如果自旋失败达到一定的次数，锁就会膨胀为重量级锁。 重量级锁重量级锁，即我们对 synchronized 的直观认识，这种情况下，线程会挂起，进入到操作系统内核态，等待操作系统的调度，然后再映射回用户态。系统调用是昂贵的，所以重量级锁的名称由此而来。 如果系统的共享变量竞争非常激烈，锁会迅速膨胀到重量级锁，这些优化就名存实亡。如果并发非常严重，可以通过参数 -XX:-UseBiasedLocking 禁用偏向锁，理论上会有一些性能提升，但实际上并不确定。 Lock在 concurrent 包里，能够发现 ReentrantLock 和 ReentrantReadWriteLock 两个类。Reentrant 就是可重入的意思，它们和 synchronized 关键字一样，都是可重入锁。这里可重入的意思是，一个线程运行时，可以多次获取同一个对象锁，这是因为 Java 的锁是基于线程的，而不是基于调用的。比如下面这段代码，由于方法 a、b、c 锁的都是当前的 this，线程在调用 a 方法的时候，就不需要多次获取对象锁。public synchronized void a(){ b();}public synchronized void b(){ c();}public synchronized void c(){}1. 主要方法Lock 是基于 AQS（AbstractQueuedSynchronizer）实现的，而 AQS 是基于 volitale 和 CAS 实现的。Lock 与 synchronized 的使用方法不同，它需要手动加锁，然后在 finally 中解锁。Lock 接口比 synchronized 灵活性要高，关键方法，如下： Lock： Lock 方法和 synchronized 没什么区别，如果获取不到锁，都会被阻塞； tryLock： 此方法会尝试获取锁，不管能不能获取到锁，都会立即返回，不会阻塞，它是有返回值的，获取到锁就会返回 true； tryLock(long time, TimeUnit unit)： 与 tryLock 类似，但它在拿不到锁的情况下，会等待一段时间，直到超时； LockInterruptibly： 与 Lock 类似，但是可以锁等待，可以被中断，中断后返回 InterruptedException；一般情况下，使用 Lock 方法就可以；但如果业务请求要求响应及时，那使用带超时时间的 tryLock 是更好的选择：我们的业务可以直接返回失败，而不用进行阻塞等待。tryLock 这种优化手段，采用降低请求成功率的方式，来保证服务的可用性，在高并发场景下常被高频采用。2. 读写锁对于有些业务来说，使用 Lock 这种粗粒度的锁还是太慢了。对于一个 HashMap 来说，某个业务是读多写少的常见，此时，如果给读操作，也加上和写操作一样的锁的话，效率就会很慢。ReentranReadWriteLock 是一种读写分离的锁，它允许多个读线程同时进行，但读和写、写和写是互斥的。使用方法如下所示，分别获取读写锁，对写操作加写锁，对读操作加读锁，并在 finally 里释放锁即可。ReentrantReadWriteLock lock = new ReentrantReadWriteLock();Lock readLock = lock.readLock();Lock writeLock = lock.writeLock();public void put(K k, V v) { writeLock.lock(); try { map.put(k, v); } finally { writeLock.unlock(); }}... 除了 ReadWriteLock，能够更快的读写分离模式？ JDK 1.8 加入了哪个 API ?JDK1.8 加了一个 StampedLock，具体的不同在于提供了乐观锁。 获取乐观读锁(会获取一个校验码) 读取一些值 根据步骤1的校验码再次校验，看看是否有被动过 如果没有被动过就结束了4.如果被动过，转为读锁(接下来就和 readwritelock 一致了) 3. 公平锁与非公平锁 非公平锁 平常用到的锁，都是非公平锁。 当持有锁的线程释放锁的时候，EntrySet 里的线程就会争抢这把锁，这个争抢过程，是随机的，也就是说并不知道哪个线程会获取对象锁，谁抢到了就算谁的。 这就一定的概率会发生，某个线程总是抢不到锁的情况。比如，某个线程通过 setPriortity 设置得比较低的优先级，这个抢不到锁的线程，就一直处饥饿状态，这就是线程饥饿的概念。 公平锁公平锁通过把随机变成有序，可以解决这个问题，synchronized 没有这个功能，在 Lock 中可以通过构造参数设置成公平锁，代码如下： public ReentrantReadWriteLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this);} 由于所有的线程都需要排队，需要在多核的场景下维护一个同步队列，在多个线程争抢锁的时候，吞吐量就很低。 下面是 20 个并发之下，锁的 JMH 测试结果，可以看到，非公平锁比公平锁的性能高出两个数量级别。 Benchmark                     Mode Cnt     Score     Error   UnitsFairVSNoFairBenchmark.fair   thrpt   10    186.144 ±   27.462 ops/msFairVSNoFairBenchmark.nofair thrpt   10  35195.649 ± 6503.375 ops/ms 锁优化的总结1. 死锁锁冲突最严重的一种情况：死锁。下面这段代码示例，两线程分别持有对方所需要的锁，并进入了相互等待的状态，那么它们就进入了死锁。在面试中，经常会要求被面试者手写下面的这段代码：public class DeadLockDemo { public static void main(String[] args) { Object object1 = new Object(); Object object2 = new Object(); Thread t1 = new Thread(() -&amp;gt; { synchronized (object1) { try { Thread.sleep(200); } catch (InterruptedException e) { e.printStackTrace(); } synchronized (object2) {} } }, &quot;deadlock-demo-1&quot;); t1.start(); Thread t2 = new Thread(() -&amp;gt; { synchronized (object2) { try { Thread.sleep(200); } catch () { e.printStackTrace(); } synchronized (object1) { } } }, &quot;deadlock-demo-2&quot;); t2.start(); }}代码创建了两把对象锁，线程 1 首先拿到了 object1 的对象锁，200 ms 后尝试获取 object2 的对象锁。但这个时候，object2 的对象锁已经被线程 2 获取了。这两个线程进入了相互等待的状态，产生了死锁。使用前面提到的，带超时时间的 tryLock 方法，有一方超时让步，可以一定程度上避免死锁。2. 优化技巧锁的优化理论非常简单，就是减少锁的冲突。无论是锁的读写分离，还是分段锁，本质上都是为了避免多个线程同时获取同一把锁。所以优化的一般思路就是：减少锁的粒度、减少锁持有的时间、锁分级、锁分离、锁消除、乐观锁以及无锁等。 减少锁粒度 通过减少锁的粒度，可以将冲突分散，从而减少冲突的可能，进而提高并发量。 简而言之，就是把资源进行抽象，针对每类资源使用单独的锁进行保护。 就像下面的代码，由于 list1 和 list2 属于两类资源，就没必要使用同一个对象锁进行处理。 public class LockLessDemo { List&amp;lt;String&amp;gt; list1 = new ArrayList&amp;lt;&amp;gt;(); List&amp;lt;String&amp;gt; list2 = new ArrayList&amp;lt;&amp;gt;(); public synchronized void addList1(String v){ this.list1.add(v); } public synchronized void addList2(String v){ this.list2.add(v); }} 可以创建两个不同的锁，改善情况如下： public class LockLessDemo { List&amp;lt;String&amp;gt; list1 = new ArrayList&amp;lt;&amp;gt;(); List&amp;lt;String&amp;gt; list2 = new ArrayList&amp;lt;&amp;gt;(); final Object lock1 = new Object(); final Object lock2 = new Object(); public void addList1(String v) { synchronized (lock1) { this.list1.add(v); } } public void addList2(String v) { synchronized (lock2) { this.list2.add(v); } }} 减少锁持有时间让锁资源尽快地释放，减少锁持有的时间，其它线程可更迅速地获取锁资源，进行其它业务的处理。 再看下面代码，由于 slowMethod 不在锁的范围内，占用的时间又比较长，可以把它移动到 syschronized 代码块外面，加速锁的释放： public class LockTimeDemo { List&amp;lt;String&amp;gt; list = new ArrayList&amp;lt;&amp;gt;(); final Object lock = new Object(); public void addList(String v) { synchronized (lock) { slowMethod(); this.list.add(v); } } public void slowMethod(){}} 锁分级锁分级，指的是 Synchronied 锁的锁升级，属于 JVM 的内部优化，它从偏向锁开始，逐渐升级为轻量锁、重量锁，这个过程是不可逆的。 锁分离读写锁，就是锁分离技术。这是因为，读操作一般是不会对资源产生影响的，可以并发执行；写操作和其它操作是互斥的，只能排队执行。所以读写锁适合读多写少的场景。 锁消除通过 JIT 编译器，JVM 可以消除某些对象的加锁操作。比如，StringBuffer 和 StringBuilder 都是做字符串拼接的，而且前者是线程安全的。 但其实，如果两个字符串拼接对象用在函数内，JVM 通过逃逸分析这个对象的作用范围就是在很函数中，就会把锁的影响给消除掉。 像下面代码，它和 StringBuilder 的效果是一样的。 String m1(){ StringBuffer sb = new StringBuffer(); sb.append(&quot;&quot;); return sb.toString();} 因此，对于读多写少的互联网场景，最有效的做法，就是用乐观锁、或者无锁。 CopyOnWrite 容器，也算是一种锁的优化（只是一个局部的优化，但是在使用资源，尤其是缓存的资源使用时，可以去掉锁的使用了）。是对于读多写少场景的优化；如果是写多读少，这种优化的效果就是反向的总结Java 中有两种加锁方式：一种是使用 Synchronized 关键字，另外一种是 concurrent 包下面的 Lock，它们对比如下： 类别 Synchronized Lock 实现方式 monitor AQS 底层细节 JVM 优化 Java API 分级锁 是 否 功能特性 单一 丰富 锁分离 无 读写锁 锁超时 无 带超时时间的 tryLock 可中断 否 lockInterruptibly Lock 的功能比 Synchronized 多的，能够对线程行为进行更细粒度的控制。但是如果只是用最简单的锁互斥功能，还是直接使用 Synchronized，有两个原因： Synchronized 的编程模型更加简单，更易于使用； Synchronized 引入了偏向锁，轻量级锁等功能，能够从 JVM 层进行优化，同时 JIT 编译器也会对它执行一些锁消除动作。 Markword 定义文件降级" }, { "title": "并行计算（12）", "url": "/posts/parallel-computing/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-04-17 12:33:33 +0000", "snippet": "现在的电脑，往往都有多核心，即使是一部手机，也往往配备了并且处理器，通过多进程或多线程的手段，就可以让多个 CPU 核同时工作，加快任务的执行。Java 提供了非常丰富的 API，来支持多线程并发。对于 Java 开发者来说，如何将多线程应用到业务场景与注意事项 ？从一个并行数据的栗子，总结知识点。并行获取数据考虑到一种场景。有一个用户数据接口，要求在 50 ms 内返回数据。它的调用逻辑非常复杂，打交道的接口也非常多，需要从 20 多个接口总书记。这些接口，最小的耗时也要 20 ms，哪怕全部都是最优状态，算下来也需要 20 * 20 = 400 ms.解决此问题的方式只有并行，通过多线程同时去获取计算结果，最后再进行结果拼接。但这种编程模型实在是太复杂了，如果使用原生线程 API，或者使用 wait、notify 等函数，代码的复杂度可以想象有多大。但幸运的是，现在 Java 中的大多数并发编程场景，都可以使用 concurrent 包的一些工具来实现。在这种场景中，可以使用 CountDownLatch 完成操作。CountDownLatch 本质上是一个计算器，一般情况下，我们将它初始化为与执行任务相同的数量。当一个任务执行完时，就将计算器的值减 1，直到计数器值达到 0 时，表示完成了所有的任务，在 await 上等待的线程就可以继续执行下去。下面的代码，是俺专门为这个场景封装的一个工具类。它主要传入了两个参数： 一个是要计算的 job 数量 一个是整个大任务超时的毫秒数 public class ParallelFetcher { final long timeout; final CountDownLatch latch; final ThreadPoolExecutor executor = new ThreadPoolExecutor(100, 200, 1, TimeUnit.HOURS, new ArrayBlockingQueue&amp;lt;&amp;gt;(100)); public ParallelFetcher(int jobSize, long timeoutMill) { this.timeout = timeoutMill; this.latch = new CountDownLatch(jobSize); } public void submitJob(Runnable runnable) { executor.execute(() -&amp;gt; { runnable.run(); latch.countDown(); }); } public void await() { try { this.latch.await(timeout, TimeUnit.MILLISECONDS); } catch (InterruptedException e) { throw new IllegalStateException(); } } public void dispose() { this.executor.shutdown(); }} 当 job 运行时间，超过了任务的时间上限，就会被直接终止，这就是 await 函数的功能。下面是使用这段代码的一个示例。SlowInterfaceMock 是一个测试类，用来模拟远程服务的超时动作，会等待 0~60 毫秒，程序运行后，会输出执行结果到 map 集合中。public static void main(String[] args) { final String userId = &quot;123&quot;; final SlowInterfaceMock mock = new SlowInterfaceMock(); ParallelFetcher fetcher = new ParallelFetcher(20, 50); final Map&amp;lt;String, String&amp;gt; result = new HashMap&amp;lt;&amp;gt;(); fetcher.submitJob(() -&amp;gt; result.put(&quot;method0&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method1&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method2&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method3&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method4&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method5&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method6&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method7&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method8&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method9&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method10&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method11&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method12&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method13&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method14&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method15&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method16&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method17&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method18&quot;, mock.method0(userId))); fetcher.submitJob(() -&amp;gt; result.put(&quot;method19&quot;, mock.method0(userId))); fetcher.await(); System.out.println(fetcher.latch); System.out.println(result.size()); System.out.println(result); fetcher.dispose(); }使用这种方式，接口就可以在固定的时间内返回了。concurrent 包里面提供了非常多的类似 CountDownLatch 的工具，在享受便捷性的同时，这段代码需要注意的事情： 首先，latch 的数量加上 map 的 size，总数应该是 20，但运行之后，大概率不是，我们丢失了部分数据。原因就是，main 方法里使用了 HashMap 类，它并不是线程安全的，在并发执行时发生了错乱，造成了错误的结果，将 HashMap 换成 ConcurrentHashMap 即可解决问题。从这个小问题就可以看出：并发编程并不是那么友好，一不小心就会踏进陷阱。如果对集合的使用场景并不是特别在行，直接使用线程安全的类，出错的概率会更少一点。再来看一下线程池的设置，里面有非常多的参数，最大池数量达到了 200 个。那线程数到底设置多少合适呢？按照我们的需求，每次请求需要执行 20 个线程，200 个线程就可以支持 10 个并发量，按照最悲观的 50ms 来算的话，这个接口支持的最小 QPS 就是：1000/50*10=200。这就是说，如果访问量增加，这个线程数还可以调大。在平常的业务中，有计算密集型任务和I/O 密集型任务之分。 I/O 密集型任务对于常见的互联网服务来说，大多数是属于 I/O 密集型的，比如等待数据库的 I/O，等待网络 I/O 等。 在这种情况下，当线程数量等于 I/O 任务的数量时，效果是最好的。虽然线程上下文切换会有一定的性能损耗，但相对于缓慢的 I/O 来说，这点损失是可以接受的。 上面说的情况，是针对同步 I/O 来说的，基本上是一个任务对应一个线程。异步 NIO 会加速这个过程。 计算密集型任务计算密集型的任务却正好相反，比如一些耗时的算法逻辑。 CPU 要想达到最高的利用率，提高吞吐量，最好的方式就是：让它尽量少地在任务之间切换，此时，线程数等于 CPU 数量，是效率最高的。 了解了任务的这些特点，就可以通过调整线程数量增加服务性能。比如，高性能的网络工具包 Netty，EventLoop 默认的线程数量，就是处理器的 2 倍。如果业务 I/O 比较耗时，此时就容易造成任务的阻塞，解决方式有两种： 一种方式是提高 worker 线程池的大小； 另外一种方式是让耗时的操作在另外的线程池里运行。从池化对象原理看线程池线程的资源是比较昂贵的，频繁地创建和销毁同样会影响系统性能。而线程资源是非常适合进行池化的。线程池与其他对象池的设计思路差不多，但它有些细微的差别，下面是线程池参数最全的构造方法：public ThreadPoolExecutor( int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&amp;lt;Runnable&amp;gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler)前几个参数没有什么好说的，相对于普通对象池而言，由于线程资源总是有效，它甚至少了非常多的 Idle 配置参数（与对象池比较），主要是 workQueue 和 handler。关于任务的创建过程，可以说是多线程每次必问的问题了。如下图所示，任务被提交后，首先判断它是否达到了最小线程数（coreSize），如果达到了，就将任务缓存在任务队列里。如果队列也满了，会判断线程数量是否达到了最大线程数（maximumPoolSize），如果也达到了，就会进入任务的拒绝策略（handler）。Executors 工厂类中默认的几个快捷线程池代码。 固定大小线程池 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&amp;lt;Runnable&amp;gt;()); } FixedThreadPool 的最大最小线程数是相等的，其实设置成不等的也不会起什么作用。 主要原因就是它所采用的任务队列 LinkedBlockingQueue 是无界的，代码走不到判断最大线程池的逻辑。keepAliveTime 参数的设置，也没有意义，因为线程池回收的是corePoolSize和maximumPoolSize 之间的线程。 这个线程池的问题是，由于队列是无界的，在任务较多的情况下，会造成内存使用不可控，同时任务也会在队列里长时间等待。 无限大小线程池 public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&amp;lt;Runnable&amp;gt;()); } CachedThreadPool 是另外一个极端，它的最小线程数是 0，线程空闲 1 分钟的都会被回收。 在提交任务时，使用了 SynchronousQueue，不缓存任何任务，直接创建新的线程。这种方式同样会有问题，因为它同样无法控制资源的使用，很容易造成内存溢出和过量的线程创建。 一般在线上，这两种方式都不推荐，需要根据具体的需求，使用 ThreadPoolExecutor 自行构建线程池，这也是阿里开发规范中推荐的方式。 如果任务可以接受一定时间的延迟，那么使用 LinkedBlockingQueue 指定一个队列的上限，缓存一部分任务是合理的； 如果任务对实时性要求很高，比如 RPC 服务，就可以使用 SynchronousQueue 队列对任务进行传递，而不是缓存它们。 拒绝策略（rejection policy） 默认的拒绝策略，就是抛出异常的 AbortPolicy，与之类似的是 DiscardPolicy，它什么都不做，连异常都不抛出，这个非常不推荐。 还有一个叫作 CallerRunsPolicy，当线程池饱和时，它会使用用户的线程执行任务。比如，在Controller 里的线程池满了，会阻塞在 Tomcat 的线程池里对任务进行执行，这很容易会将用户线程占满，造成用户业务长时间等待。具体用不用这种策略，还是要看客户对等待时间的忍受程度。 最后一个策略叫作 DiscardOldestPolicy，它在遇到线程饱和时，会先弹出队列里最旧的任务，然后把当前的任务添加到队列中。 在 SpringBoot 中使用异步SpringBoot 中可以非常容易地实现异步任务。首先，需要在启动类上加上 @EnableAsync 注解，然后在需要异步执行的方法上加上 @Async 注解。一般情况下，我们的任务直接在后台运行就可以，但有些任务需要返回一些数据，这个时候，就可以使用 Future 返回一个代理，供其他的代码使用。关键代码如下：@SpringBootApplication@EnableAsyncpublic class JavaRulesApplication { public static void main(String[] args) { SpringApplication.run(JavaRulesApplication.class, args); }}@Component@Asyncpublic class AsyncJob { public String testJob() { try { Thread.sleep(1000 * 3); System.out.println(Thread.currentThread().getName()); } catch (InterruptedException e) { throw new IllegalStateException(); } return &quot;aaa&quot;; } public Future&amp;lt;String&amp;gt; testJob2() { String result = this.testJob(); return new AsyncResult&amp;lt;&amp;gt;(result); }}默认情况下，Spring 将启动一个默认的线程池供异步任务使用。这个线程池也是无限大的，资源使用不可控，所以强烈建议使用代码设置一个适合自己的。public ThreadPoolTaskExecutor getThreadPoolTaskExecutor() { ThreadPoolTaskExecutor taskExecutor = new ThreadPoolTaskExecutor(); taskExecutor.setCorePoolSize(100); taskExecutor.setMaxPoolSize(200); taskExecutor.setQueueCapacity(100); taskExecutor.setKeepAliveSeconds(60); taskExecutor.setThreadNamePrefix(&quot;test-&quot;); taskExecutor.initialize(); return taskExecutor;}多线程资源总结 线程安全的类HashMap 和 ConcurrentHashMap，后者相对于前者，是线程安全的。多线程的细节非常多，下面就盘点一下，一些常见的线程安全的类。 StringBuilder 对应着 StringBuffer。后者主要是通过 synchronized 关键字实现了线程的同步。值得注意的是，在单个方法区域里，这两者是没有区别的，JIT 的编译优化会去掉 synchronized 关键字的影响。 HashMap 对应着 ConcurrentHashMap。ConcurrentHashMap 的话题很大，这里提醒一下 JDK1.7 和 1.8 之间的实现已经不一样了。1.8 已经去掉了分段锁的概念（锁分离技术），并且使用 synchronized 来代替了 ReentrantLock。 ArrayList 对应着 CopyOnWriteList。后者是写时复制的概念，适合读多写少的场景。 LinkedList 对应着 ArrayBlockingQueue。ArrayBlockingQueue 对默认是不公平锁，可以修改构造参数，将其改成公平阻塞队列，它在 concurrent 包里使用得非常频繁。 HashSet 对应着 CopyOnWriteArraySet。 下面以一个经常发生问题的案例，来说一下线程安全的重要性。 SimpleDateFormat 是经常用到的日期处理类，但它本身不是线程安全的，在多线程运行环境下，会产生很多问题，在以往的工作中，通过 sonar 扫描，发现这种误用的情况特别的多。 public class FaultDateFormat { final static String TIME_FORMAT_YYYY_MM_DD_HH_MM_SS = &quot;yyyy-MM-dd HH:mm:ss&quot;; SimpleDateFormat format = new SimpleDateFormat(TIME_FORMAT_YYYY_MM_DD_HH_MM_SS); public static void main(String[] args) { final FaultDateFormat faultDateFormat = new FaultDateFormat(); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &amp;lt; 1000; i++) { executorService.submit(() -&amp;gt; { try { System.out.println(faultDateFormat.format.parse(&quot;2020-07-25 08:56:40&quot;)); } catch (ParseException e) { throw new IllegalStateException(); } }); } executorService.shutdown(); }} 执行上面的代码，可以看到，时间已经错乱了。 Thu May 01 08:56:40 CST 618104 Thu May 01 08:56:40 CST 618104 Mon Jul 26 08:00:04 CST 1 Tue Jun 30 08:56:00 CST 2020 Thu Oct 01 14:45:20 CST 16 Sun Jul 13 01:55:40 CST 20220200 Wed Dec 25 08:56:40 CST 2019 Sun Jul 13 01:55:40 CST 20220200 解决方式就是使用 ThreadLocal 局部变量，代码如下所示，可以有效地解决线程安全问题。 public class GoodDateFormat { final static String TIME_FORMAT_YYYY_MM_DD_HH_MM_SS = &quot;yyyy-MM-dd HH:mm:ss&quot;; ThreadLocal&amp;lt;SimpleDateFormat&amp;gt; format = new ThreadLocal&amp;lt;SimpleDateFormat&amp;gt;() { @Override protected SimpleDateFormat initialValue() { return new SimpleDateFormat(TIME_FORMAT_YYYY_MM_DD_HH_MM_SS); } }; public static void main(String[] args) { final GoodDateFormat faultDateFormat = new GoodDateFormat(); ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &amp;lt; 1000; i++) { executor.submit(() -&amp;gt; { try { System.out.println(faultDateFormat.format.get().parse(&quot;2020-07-25 08:56:40&quot;)); } catch (ParseException e) { throw new IllegalStateException(); } }); } executor.shutdown(); }} 线程的同步方式 Java 中实现线程同步的方式有很多，大体可以分为以下 8 类。 使用 Object 类中的 wait、notify、notifyAll 等函数。由于这种编程模型非常复杂，现在已经很少用了。这里有一个关键点，那就是对于这些函数的调用，必须放在同步代码块里才能正常运行。 使用 ThreadLocal 线程局部变量的方式，每个线程一个变量，本课时会详细讲解。 使用 synchronized 关键字修饰方法或者代码块。这是 Java 中最常见的方式，有锁升级的概念。 使用 Concurrent 包里的可重入锁 ReentrantLock。使用 CAS 方式实现的可重入锁。 使用 volatile 关键字控制变量的可见性，这个关键字保证了变量的可见性，但不能保证它的原子性。 使用线程安全的阻塞队列完成线程同步。比如，使用 LinkedBlockingQueue 实现一个简单的生产者消费者。 使用原子变量。Atomic* 系列方法，也是使用 CAS 实现的，关于 CAS，我们将在下一课时介绍。 使用 Thread 类的 join 方法，可以让多线程按照指定的顺序执行。 下面的代码，是使用 LinkedBlockingQueue 实现的一个简单生产者和消费者实例，可以看到，我们还使用了一个 volatile 修饰的变量，来决定程序是否继续运行，这也是 volatile 变量的常用场景。 public class ProducerConsumer { private static final int Q_SIZE = 10; private LinkedBlockingQueue&amp;lt;String&amp;gt; queue = new LinkedBlockingQueue&amp;lt;String&amp;gt;(Q_SIZE); private volatile boolean stop = false; Runnable producer = () -&amp;gt; { while (!stop) { try { queue.offer(UUID.randomUUID().toString(), 1, TimeUnit.SECONDS); } catch (InterruptedException e) { //noop } } }; Runnable consumer = () -&amp;gt; { while (!stop) { try { String value = queue.take(); System.out.println(Thread.currentThread().getName() + &quot; | &quot; + value); } catch (InterruptedException e) { // noop } } }; void start() { new Thread(producer, &quot;Thread 1&quot;).start(); new Thread(producer, &quot;Thread 2&quot;).start(); new Thread(consumer, &quot;Thread 3&quot;).start(); new Thread(consumer, &quot;Thread 4&quot;).start(); } public static void main(String[] args) throws Exception { ProducerConsumer pc = new ProducerConsumer(); pc.start(); Thread.sleep(1000 * 10); pc.stop = true; }} FastThreadLocal 在平常的编码中，使用最多的就是 ThreadLocal 类了。拿最常用的 Spring 来说，它事务管理的传播机制，就是使用 ThreadLocal 实现的。因为 ThreadLocal 是线程私有的，所以 Spring 的事务传播机制是不能够跨线程的。在问到 Spring 事务管理是否包含子线程时，要能够想到面试官的真实意图。 /**    * Holder to support the {@code currentTransactionStatus()} method,    * and to support communication between different cooperating advices    * (e.g. before and after advice) if the aspect involves more than a    * single method (as will be the case for around advice). */ private static final ThreadLocal&amp;lt;TransactionInfo&amp;gt; transactionInfoHolder = new NamedThreadLocal&amp;lt;&amp;gt;(&quot;Current aspect-driven transaction&quot;); 既然 Java 中有了 ThreadLocal 类了，为什么 Netty 还自己创建了一个叫作 FastThreadLocal 的结构？ 首先来看一下 ThreadLocal 的实现。 Thread 类中，有一个成员变量 ThreadLocals，存放了与本线程相关的所有自定义信息。对这个变量的定义在 Thread 类，而操作却在 ThreadLocal 类中。 public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); ... } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } 问题就出在 ThreadLocalMap 类上，它虽然叫 Map，但却没有实现 Map 的接口。如下图，ThreadLocalMap 在 rehash 的时候，并没有采用类似 HashMap 的数组+链表+红黑树的做法，它只使用了一个数组，使用开放寻址（遇到冲突，依次查找，直到空闲位置）的方法，这种方式是非常低效的。 由于 Netty 对 ThreadLocal 的使用非常频繁，Netty 对它进行了专项的优化。它之所以快，是因为在底层数据结构上做了文章，使用常量下标对元素进行定位，而不是使用JDK 默认的探测性算法。 伪共享问题吗？底层的 InternalThreadLocalMap对cacheline 也做了相应的优化。 使用多线程中遇到过的问题 通过上面的知识总结，可以看到多线程相关的编程，是属于比较高阶的技能。面试中，面试官会经常问你在多线程使用中遇到的一些问题，以此来判断你实际的应用情况。 先总结一下文中已经给出的示例： 线程池的不正确使用，造成了资源分配的不可控； I/O 密集型场景下，线程池开得过小，造成了请求的频繁失败； 线程池使用了 CallerRunsPolicy 饱和策略，造成了业务线程的阻塞； SimpleDateFormat 造成的时间错乱。 另外，着重一点是，在处理循环的任务时，一定不要忘了捕捉异常。尤其需要说明的是，像 NPE 这样的异常，由于是非捕获型的，IDE 的代码提示往往不起作用。见过很多案例，就是由于忘了处理异常，造成了任务中断，这种问题发生的机率小，是比较难定位的，一定要保持良好的编码习惯。 while(!isInterrupted){ try { ...... } catch(Exception ex){ ...... } } 多线程环境中，异常日志是非常重要的，但线程池的默认行为并不是特别切合实际。参见如下代码，任务执行时，抛出了一个异常，但我们的终端什么都没输出，异常信息丢失了，这对问题排查非常不友好。 ExecutorService executor = Executors.newCachedThreadPool(); executor.submit( ()-&amp;gt; { String s = null; s.substring(0); }); executor.shutdown(); 我们跟踪任务的执行，在 ThreadPoolExecutor 类中可以找到任务发生异常时的方法，它是抛给了 afterExecute 方法进行处理。 可惜的是，ThreadPoolExecutor 中的 afterExecute 方法是没有任何实现的，它是个空方法。 protected void afterExecute(Runnable r, Throwable t) { } 如果通过重写 afterExecute 来改变这个默认行为，但这代价点大。其实，使用 submit 方法提交的任务，会返回一个 Future 对象，只有调用了它的 get 方法，这个异常才会打印。使用 submit 方法提交的任务，代码永远不会走到上图标红的一行，获取异常的方式有且只有这一种。 只有使用 execute 方法提交的任务才会走到这行异常处理代码。如果你想要默认打印异常，推荐使用 execute 方法提交任务，它和 submit 方法的区别，也不仅仅是返回值不一样那么简单。 异步 “异步，并没有减少任务的执行步骤，也没有算法上的改进，那么为什么说异步的速度更快呢？” 其实这是对“异步作用”的错误理解。异步是一种编程模型，它通过将耗时的操作转移到后台线程运行，从而减少对主业务的堵塞，所以我们说异步让速度变快了。但如果你的系统资源使用已经到了极限，异步就不能产生任何效果了，它主要优化的是那些阻塞性的等待。 缓冲、缓存、池化等优化方法，都是用到了异步。它能够起到转移冲突，优化请求响应的作用。由于合理地利用了资源，系统响应确实变快了， 异步还能够对业务进行解耦，如下图所示，它比较像是生产者消费者模型。主线程负责生产任务，并将它存放在待执行列表中；消费线程池负责任务的消费，进行真正的业务逻辑处理。 ParallelFetcher中，如果到了超时时间后，result返回了部分结果，但是剩下的几个线程还是在执行，需不需要把还在执行的线程cancel调？ —— 不需要的。因为任务实际运行的，大多数是阻塞的I/O操作，比如URLConnection。这种情况下，cancel是没什么用的，interrupt函数也没作用。最好是给具体的组件设置一个超时的值。不需要的。因为任务实际运行的，大多数是阻塞的I/O操作，比如URLConnection。这种情况下，cancel是没什么用的，interrupt函数也没作用。最好是给具体的组件设置一个超时的值。—— kafka只能保证partition顺序有效，所以一般都推荐根据partition进行业务划分，只保证分区消息的顺序性，比如把单个用户的所有消息都放在一个partition。想要多个partition全局有效，这个肯定要引入中间协调器，效率会下降的特别严重。像binlog这种有严格顺序的需求特别少，大多数业务需求属于从设计层面去规避的问题，最好不要带到编码里来。 从kafka过来的大量消息数据，要保证消息的顺序，有什么好的优化建议kafka只能保证partition顺序有效，所以一般都推荐根据partition进行业务划分，只保证分区消息的顺序性，比如把单个用户的所有消息都放在一个partition。想要多个partition全局有效，这个肯定要引入中间协调器，效率会下降的特别严重。像binlog这种有严格顺序的需求特别少，大多数业务需求属于从设计层面去规避的问题，最好不要带到编码里来。" }, { "title": "利用设计模式优化性能（11）", "url": "/posts/design-pattern/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-04-12 15:33:00 +0000", "snippet": "代码的结构对应用的整体性能，有着重要的影响。结构优秀的代码，可以避免很多潜在的性能问题，在代码的扩展性上也有巨大的作用；结构清晰、层次分明的代码，也有助于帮找到系统的瓶颈点，进行专项优化。设计模式就是对常用开发技巧进行的总结，它使得程序员之间交流问题，有了更专业、便捷的方式。比如，I/O 模块使用的是装饰器模式，就很容易想到 I/O 模块的代码组织方式。事实上，大多数设计模式并不能增加程序的性能，它只是代码的一种组织方式。本文总结性能相关的几个设计模式，包括代理模式、单例模式、享元模式、原型模式等。如何找到动态代理慢逻辑的原因?Spring 广泛使用了代理模式，它使用 CGLIB 对 Java 的字节码进行了增强。在复杂的项目中，会有非常多的 AOP 代码，比如权限、日志等切面。在方便了编码的同时，AOP 也给不熟悉项目代码的小伙伴带来了很多困扰。使用 arthas 找到动态代理慢逻辑的具体原因，这种方式在复杂项目中，非常有效，不需要熟悉项目的代码，就可以定位到性能瓶颈点。首先，创建一个最简单的 Bean（代码见仓库）。@Component public class ABean { public void method() { System.out.println(&quot;*******************&quot;); } } 然后，使用 Aspect 注解，完成切面的书写，在前置方法里，让线程 sleep 了 1 秒钟。@Aspect @Component public class MyAspect { @Pointcut(&quot;execution(* cn.happymaya.design.pattern.spring.ABean.*(..)))&quot;) public void pointcut() {} @Before(&quot;pointcut()&quot;) public void before() { System.out.println(&quot;before&quot;); try { Thread.sleep(TimeUnit.SECONDS.toMillis(1)); } catch (InterruptedException e) { throw new IllegalStateException(); } } } 创建一个 Controller，当访问 /aop 链接时，将会输出 Bean 的类名称，以及它的耗时。@Controller public class AopController { @Autowired private ABean aBean; @ResponseBody @GetMapping(&quot;/aop&quot;) public String aop() { long begin = System.currentTimeMillis(); aBean.method(); long cost = System.currentTimeMillis() - begin; String cls = aBean.getClass().toString(); return cls + &quot; | &quot; + cost; }}执行结果如下，可以看到 AOP 代理已经生效，内存里的 Bean 对象，已经变成了EnhancerBySpringCGLIB 类型，调用方法 method，耗时达到了1023ms。class com.github.xjjdog.spring.ABean$$EnhancerBySpringCGLIB$$a5d91535 | 1023 下面使用 arthas 分析这个执行过程，找出耗时最高的 AOP 方法。启动 arthas 后，可以从列表中看到我们的应用程序，在这里，输入 2 进入分析界面。在终端输入 trace 命令，然后访问 /aop 接口，终端将打印出一些 debug 信息，可以发现耗时操作就是 Spring 的代理类。trace com.github.xjjdog.spring.ABean method 代理模式代理模式（Proxy）可以通过一个代理类，来控制对一个对象的访问。Java 中实现动态代理主要有两种模式： 一种是使用 JDK； 另外一种是使用 CGLib。其中： JDK 方式是面向接口的，主要的相关类是 InvocationHandler 和 Proxy； CGLib 可以代理普通类，主要的相关类是 MethodInterceptor 和 Enhancer。下面是 JDK 方式和 CGLib 方式代理速度的 JMH 测试结果：Benchmark             Mode Cnt     Score     Error   Units ProxyBenchmark.cglib thrpt   10  78499.580 ± 1771.148 ops/ms ProxyBenchmark.jdk   thrpt   10  88948.858 ±  814.360 ops/ms 我现在用的 JDK 版本是 1.8，可以看到，CGLib 的速度并没有传得那么快（有传言高出 10 倍），相比较而言，它的速度甚至略有下降。再来看下代理的创建速度，结果如下所示。可以看到，在代理类初始化方面，JDK 的吞吐量要高出 CGLib 一倍。Benchmark                   Mode Cnt     Score     Error   Units ProxyCreateBenchmark.cglib thrpt   10   7281.487 ± 1339.779 ops/ms ProxyCreateBenchmark.jdk   thrpt   10 15612.467 ± 268.362 ops/ms 综上所述，JDK 动态代理和 CGLib 代理的创建速度和执行速度，在新版本的 Java 中差别并不是很大，Spring 选用了 CGLib，主要是因为它能够代理普通类的缘故。单例模式Spring 在创建组件的时候，可以通过 scope 注解指定它的作用域，用来标示这是一个prototype（多例）还是 singleton（单例）。当指定为单例时（默认行为），在 Spring 容器中，组件有且只有一份，当你注入相关组件的时候，获取的组件实例也是同一份。如果是普通的单例类，我们通常将单例的构造方法设置成私有的，单例有懒汉加载和饿汉加载模式。了解 JVM 类加载机制的同学都知道，一个类从加载到初始化，要经历 5 个步骤：加载、验证、准备、解析、初始化。其中，static 字段和 static 代码块，是属于类的，在类加载的初始化阶段就已经被执行。它在字节码中对应的是 方法，属于类的（构造方法）。因为类的初始化只有一次，所以它就能够保证这个加载动作是线程安全的。根据以上原理，只要把单例的初始化动作，放在方法里，就能够实现饿汉模式。饿汉模式在代码里用的很少，它会造成资源的浪费，生成很多可能永远不会用到的对象。而对象初始化就不一样了。通常，我们在 new 一个新对象的时候，都会调用它的构造方法，就是，用来初始化对象的属性。由于在同一时刻，多个线程可以同时调用函数，我们就需要使用 synchronized 关键字对生成过程进行同步。目前，公认的兼顾线程安全和效率的单例模式，就是 double check。很多面试官，会要求你手写，并分析 double check 的原理。如上图，是 double check 的关键代码，我们介绍一下四个关键点：第一次检查，当 instance 为 null 的时候，进入对象实例化逻辑，否则直接返回。加同步锁，这里是类锁。第二次检查才是关键。如果不加这次判空动作，可能会有多个线程进入同步代码块，进而生成多个实例。最后一个关键点是 volatile 关键字。在一些低版本的 Java 里，由于指令重排的缘故，可能会导致单例被 new 出来后，还没来得及执行构造函数，就被其他线程使用。 这个关键字，可以阻止字节码指令的重排序，在写 double check 代码时，习惯性会加上 volatile。可以看到，double check 的写法繁杂，注意点很多，它现在其实是一种反模式，已经不推荐使用了，我也不推荐你用在自己的代码里。但它能够考察面试者对并发的理解，所以这个问题经常被问到。推荐使用 enum 实现懒加载的单例，代码片段如下：public class EnumSingleton { private EnumSingleton() {} public static EnumSingleton getInstance() { return Holder.HOLDER.instance; } private enum Holder { HOLDER; private final EnumSingleton instance; Holder() { instance = new EnumSingleton(); } } } 享元模式享元模式（Flyweight）是难得的，专门针对性能优化的设计模式，它通过共享技术，最大限度地复用对象。享元模式一般会使用唯一的标识码进行判断，然后返回对应的对象，使用 HashMap 一类的集合存储非常合适，能看到很多享元模式的身影，比池化对象和对象复用等。设计模式对这平常的编码进行了抽象，从不同的角度去解释设计模式，都会找到设计思想的一些共通点。比如，单例模式就是享元模式的一种特殊情况，它通过共享单个实例，达到对象的复用。值得一提的是，同样的代码，不同的解释，会产生不同的效果。比如下面这段代码：Map&amp;lt;String,Strategy&amp;gt; strategys = new HashMap&amp;lt;&amp;gt;(); strategys.put(&quot;a&quot;,new AStrategy()); strategys.put(&quot;b&quot;,new BStrategy()); 如果从对象复用的角度来说，它就是享元模式； 如果从对象的功能角度来说，它就是策略模式。所以在讨论设计模式的时候，一定要注意上下文语境的这些差别。原型模式原型模式（Prototype）类似复制粘贴的思想，它可以首先创建一个实例，然后通过这个实例进行新对象的创建。在 Java 中，最典型的就是 Object 类的 clone 方法。编码中这个方法很少用，上面在代理模式提到的 prototype，并不是通过 clone 实现的，而是使用了更复杂的反射技术。一个比较重要的原因就是: clone 如果只拷贝当前层次的对象，实现的只是浅拷贝。在现实情况下，对象往往会非常复杂，想要实现深拷贝的话，需要在 clone 方法里做大量的编码，远远不如调用 new 方法方便。实现深拷贝，还有序列化等手段，比如实现 Serializable 接口，或者把对象转化成 JSON。所以，在现实情况下，原型模式变成了一种思想，而不是加快对象创建速度的工具。总结 总结几个与性能相关的设计模式； Java 实现动态代理的两种方式，以及它们的区别，在现版本的 JVM 中，性能差异并不大； 单例模式的三种创建方式，并看了一个 double check 的反例，平常编码中，推荐使用枚举去实现单例； 享元模式和原型模式，它们概念性更强一些，并没有固定的编码模式； arthas 使用 trace 命令，寻找耗时代码块的方法，最终将问题定位到 Spring 的 AOP 功能模块里，而这种场景在复杂项目中经常发生，需要你特别注意。此外，在设计模式中，对性能帮助最大的是生产者消费者模式，比如异步消息、reactor 模型等，" }, { "title": "大对象复用的目标和注意点（10）", "url": "/posts/large-object-reuse/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-04-07 13:33:00 +0000", "snippet": "这里的”大对象“的优化，是一个泛化的概念，它可能存放在 JVM 中，也可能正在网络上传输，也可能存在于数据库中。大对象主要从以下三个方面影响应用性能： 第一，大对象占用的资源多，垃圾回收器要花一部分精力去对它进行回收； 第二，大对象在不同的设备之间交换，会耗费网络流量，以及昂贵的 I/O； 第三，对大对象的解析和处理操作是耗时的，对象职责不聚焦，就会承担额外的性能开销。虽然结合缓存、对象的池化操作，再加上一些中间结果的保存，能够对大对象进行初步的提速。但是这还不够，因为仅仅减少了对象的创建频率，但并没有改变对象”大”这一事实。String 的 substring 方法众所周知，String 在 Java 中是不可变得，如果改动了其中得内容，它就会生成一个新得字符串。假如想用到字符串中得一部分数据，就可以使用 substring()方法。通过上面两个图片，可以看到，当需要一个子字符串得时候，substring 生成了一个新得字符串，这个字符串通过构造函数得 Arrays.copyOfRange 函数进行构造。这个函数在 JDK 7 之后是没有问题得，但在 JDK 6 中，却有着内存泄漏得风险，可以学习一下这个案例，来看一下大对象复用可能会产生得问题。public java.lang.String subString(int beginIndex, int endIndex) { if (beginIndex &amp;lt; 0) { throw new StringIndexOutOfBoundsException(beginIndex); } if (endIndex &amp;gt; count) { throw new StringIndexOutOfBoundsException(endIndex); } if (beginIndex &amp;gt; endIndex) { throw new StringIndexOutOfBoundsException(endIndex - beginIndex); } return (beginIndex == 0) &amp;amp;&amp;amp; (endIndex == count) ? this : new String(offset + beginIndex, endIndex - beginIndex, value);} // Package private constructor which shares value array for speed.String(int offset, int count, char value[]) { this.value = value; this.offset = offset; this.count = count;}上图是 JDK 6 官方的一张截图。可以看到，它在创建子字符串的时候，并不只拷贝所需要的对象，而是把整个 value 引用了起来。如果原字符串比较大，即使不再使用，内存也不会释放。比如，一篇文章内容可能有几兆，我们仅仅是需要其中的摘要信息，也不得不维持整个的大对象。String content = dao.getArticle(id); String summary = content.substring(0,100); articles.put(id,summary);通过 substring() 在 JDK6 和 JDK7 之后的变化，可以借鉴的是：当创建比较大的对象，并基于这个对象生成了一些其他的信息，记得要去掉和这个大对象的引用关系。集合大对象扩容对象扩容，在 Java 中是司空见惯的现象，比如 StringBuilder、StringBuffer、HashMap、ArrayList 等。总的来说，Java 的集合，包括 List、Set、Queue、Map 等，其中的数据都不可控。在容量不足的时候，都会有扩容操作，扩容操作需要重新组织数据，所以都是线程不安全的。其中，StringBuilder 的扩容代码如下： /** * Ensures that the capacity is at least equal to the specified minimum. * If the current capacity is less than the argument, then a new internal * array is allocated with greater capacity. The new capacity is the * larger of: * &amp;lt;ul&amp;gt; * &amp;lt;li&amp;gt;The {@code minimumCapacity} argument. * &amp;lt;li&amp;gt;Twice the old capacity, plus {@code 2}. * &amp;lt;/ul&amp;gt; * If the {@code minimumCapacity} argument is nonpositive, this * method takes no action and simply returns. * Note that subsequent operations on this object can reduce the * actual capacity below that requested here. * * @param minimumCapacity the minimum desired capacity. */ public void ensureCapacity(int minimumCapacity) { if (minimumCapacity &amp;gt; 0) ensureCapacityInternal(minimumCapacity); } /** * For positive values of {@code minimumCapacity}, this method * behaves like {@code ensureCapacity}, however it is never * synchronized. * If {@code minimumCapacity} is non positive due to numeric * overflow, this method throws {@code OutOfMemoryError}. */ private void ensureCapacityInternal(int minimumCapacity) { // overflow-conscious code if (minimumCapacity - value.length &amp;gt; 0) { value = Arrays.copyOf(value, newCapacity(minimumCapacity)); } } /** * Returns a capacity at least as large as the given minimum capacity. * Returns the current capacity increased by the same amount + 2 if * that suffices. * Will not return a capacity greater than {@code MAX_ARRAY_SIZE} * unless the given minimum capacity is greater than that. * * @param minCapacity the desired minimum capacity * @throws OutOfMemoryError if minCapacity is less than zero or * greater than Integer.MAX_VALUE */ private int newCapacity(int minCapacity) { // overflow-conscious code int newCapacity = (value.length &amp;lt;&amp;lt; 1) + 2; if (newCapacity - minCapacity &amp;lt; 0) { newCapacity = minCapacity; } return (newCapacity &amp;lt;= 0 || MAX_ARRAY_SIZE - newCapacity &amp;lt; 0) ? hugeCapacity(minCapacity) : newCapacity; }容量不够的时候，会将内存翻倍，并使用 Array.copyof 复制源数据。下面是 HashMap 的扩容代码，扩容后大小也是翻倍。它的扩容动作就复杂得多，除了有负载因子的影响，它还需要把原来的数据重新进行散列，由于无法使用 native 的 Arrays.copy 方法，速度就会很慢。void addEntry(int hash, K key, V value, int bucketIndex) { if ((size &amp;gt;= threshold) &amp;amp;&amp;amp; (null != table[bucketIndex])) { resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex); } void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1); }保持合适的对象粒度实际案例：有一个并发量非常高的业务系统，需要频繁使用到用户的基本数据。如下图所示，由于用户的基本信息，都是存放在另外一个服务中，所以每次用到用户的基本信息，都需要有一次网络交互。更加让人无法接受的是，即使是只需要用户的性别属性，也需要把所有的用户信息查询，拉取一遍。为了加快数据的查询速度，对数据进行了初步的缓存，放入到了 Redis 中，查询性能有了大的改善，但每次还是要查询很多冗余数据。原始的 redis key 是这样设计的：type: string key: user_${userid} value: json这样的设计有两个问题： 查询其中某个字段的值，需要把所有 json 数据查询出来，并自行解析； 更新其中某个字段的值，需要更新整个 json 串，代价较高。针对这种大粒度 json 信息，就可以采用打散的方式进行优化，使得每次更新和查询，都有聚焦的目标。接下来对 Redis 中的数据进行了以下设计，采用 hash 结构而不是 json 结构：type: hash key: user_${userid} value: {sex:f, id:1223, age:23}这样，就可以使用 hget 命令，或者 hmge 命令，就可以获取到想要的数据，加快信息流转的速度。Bitmap 把对象变小除了以上操作，还能再进一步优化吗？比如，我们系统中就频繁用到了用户的性别数据，用来发放一些礼品，推荐一些异性的好友，定时循环用户做一些清理动作等；或者，存放一些用户的状态信息，比如是否在线，是否签到，最近是否发送信息等，从而统计一下活跃用户等。那么对是、否这两个值的操作，就可以使用 Bitmap 这个结构进行压缩。 Java 的 Boolean 占用的是多少位？在 Java 虚拟机规范里，描述是：将 Boolean 类型映射成的是 1 和 0 两个数字，它占用的空间是和 int 相同的 32 位。即使有的虚拟机实现把 Boolean 映射到了 byte 类型上，它所占用的空间，对于大量的、有规律的 Boolean 值来说，也是太大了。如代码所示，通过判断 int 中的每一位，它可以保存 32 个 Boolean 值！int a = 0b0001_0001_1111_1101_1001_0001_1111_1101;Bitmap 就是使用 Bit 进行记录的数据结构，里面存放的数据不是 0 就是 1。缓存穿透吗？就可以使用 Bitmap 避免，Java 中的相关结构类，就是 java.util.BitSet，BitSet 底层是使用 long 数组实现的，所以它的最小容量是 64。10 亿的 Boolean 值，只需要 128MB 的内存，下面既是一个占用了 256MB 的用户性别的判断逻辑，可以涵盖长度为 10 亿的 ID。static BitSet missSet = new BitSet(010_000_000_000); static BitSet sexSet = new BitSet(010_000_000_000); String getSex(int userId) { boolean notMiss = missSet.get(userId); if (!notMiss) { // lazy fetch String lazySex = dao.getSex(userId); missSet.set(userId, true); sexSet.set(userId, &quot;female&quot;.equals(lazySex)); }    return sexSet.get(userId) ? &quot;female&quot; : &quot;male&quot;; }这些数据，放在堆内内存中，还是过大了。幸运的是，Redis 也支持 Bitmap 结构，如果内存有压力，我们可以把这个结构放到 Redis 中，判断逻辑也是类似的。 给出一个 1GB 内存的机器，提供 60亿 int 数据，如何快速判断有哪些数据是重复的？ 可以类比思考一下。Bitmap 是一个比较底层的结构，在它之上还有一个叫作布隆过滤器的结构（Bloom Filter），布隆过滤器可以判断一个值不存在，或者可能存在。 如图，它相比较 Bitmap，它多了一层 hash 算法。既然是 hash 算法，就会有冲突，所以有可能有多个值落在同一个 bit 上。它不像 HashMap 一样，使用链表或者红黑树来处理冲突，而是直接将这个hash槽重复使用。从这个特性能够看出，布隆过滤器能够明确表示一个值不在集合中，但无法判断一个值确切的在集合中。 Guava 中有一个 BloomFilter 的类，可以方便地实现相关功能。上面这种优化方式，本质上也是把大对象变成小对象的方式，在软件设计中有很多类似的思路。比如： 像一篇新发布的文章，频繁用到的是摘要数据，就不需要把整个文章内容都查询出来； 用户的 feed 信息，也只需要保证可见信息的速度，而把完整信息存放在速度较慢的大型存储里。数据的冷热分离数据除了横向的结构纬度，还有一个纵向的时间维度，对时间维度的优化，最有效的方式就是冷热分离： 所谓热数据，就是靠近用户的，被频繁使用的数据； 而冷数据是那些访问频率非常低，年代非常久远的数据。同一句复杂的 SQL，运行在几千万的数据表上，和运行在几百万的数据表上，前者的效果肯定是很差的。所以，虽然系统刚开始上线时速度很快，但随着时间的推移，数据量的增加，就会渐渐变得很慢。冷热分离是把数据分成两份，如下图，一般都会保持一份全量数据，用来做一些耗时的统计操作。冷热分离在工作中经常遇到（面试也经常提起来）。下面简单介绍三种：1.数据双写把对冷热库的插入、更新、删除操作，全部放在一个统一的事务里面。由于热库（比如 MySQL）和冷库（比如 Hbase）的类型不同，这个事务大概率会是分布式事务。在项目初期，这种方式是可行的，但如果是改造一些遗留系统，分布式事务基本上是改不动的，我通常会把这种方案直接废弃掉。2.写入 MQ 分发通过 MQ 的发布订阅功能，在进行数据操作的时候，先不落库，而是发送到 MQ 中。单独启动消费进程，将 MQ 中的数据分别落到冷库、热库中。使用这种方式改造的业务，逻辑非常清晰，结构也比较优雅。像订单这种结构比较清晰、对顺序性要求较低的系统，就可以采用 MQ 分发的方式。但如果你的数据库实体量非常大，用这种方式就要考虑程序的复杂性了。3.使用 Binlog 同步针对 MySQL，就可以采用 Binlog 的方式进行同步，使用 Canal 组件，可持续获取最新的 Binlog 数据，结合 MQ，可以将数据同步到其他的数据源中。思维发散对于结果集的操作，我们可以再发散一下思维。可以将一个简单冗余的结果集，改造成复杂高效的数据结构。这个复杂的数据结构可以代理我们的请求，有效地转移耗时操作。 比如，常用的数据库索引，就是一种对数据的重新组织、加速。B+ tree 可以有效地减少数据库与磁盘交互的次数，它通过类似 B+ tree 的数据结构，将最常用的数据进行索引，存储在有限的存储空间中。 还有就是，在 RPC 中常用的序列化。有的服务是采用的 SOAP 协议的 WebService，它是基于 XML 的一种协议，内容大传输慢，效率低下。现在的 Web 服务中，大多数是使用 json 数据进行交互的，json 的效率相比 SOAP 就更高一些。另外，大家应该都听过 google 的 protobuf，由于它是二进制协议，而且对数据进行了压缩，性能是非常优越的。protobuf 对数据压缩后，大小只有 json 的 1/10，xml 的 1/20，但是性能却提高了 5-100 倍。 protobuf 的设计是值得借鉴的，它通过 tag leng value 三段对数据进行了非常紧凑的处理，解析和传输速度都特别快。 小结最后总结一内容重点： 比较老的 JDK 版本中，String 为了复用引起的内容泄漏问题，所以我们平常的编码中，一定要注意大对象的回收，及时切断与它的联系； Java 中集合的一些扩容操作，如果你知道确切的集合大小，就可以指定一个初始值，避免耗时的扩容操作； 针对大对象，有结构纬度的优化和时间维度的优化两种方法： 从结构纬度来说，通过把对象切分成合适的粒度，可以把操作集中在小数据结构上，减少时间处理成本；通过把对象进行压缩、转换，或者提取热点数据，就可以避免大对象的存储和传输成本。 从时间纬度来说，就可以通过冷热分离的手段，将常用的数据存放在高速设备中，减少数据处理的集合，加快处理速度。 缓冲、缓存、对象池化、结果缓存池、大对象处理等优化性能的手段，由于它们都加入了额外的中间层，会使得编程模型变得复杂。 关于 redis bimap 的问题 如果作为签到使用,用户 uid 比较稀疏导致 key 比较大 该怎么处理呢? —— 如果只是存“是”和“否”的话，如果特别稀疏，可以作为用户的一项属性存在；一般签到都是统计用户的签到历史的，比如最近一个月的签到情况，这种情况下 bitmap 的 key 就是日期。而不是一个日期一个 bitmap redis 的 hash 结构应该比 string 更占用内存空间吧 ? —— 看怎么设计 string 和 hash 的 key 了。实际上，它们占用内存是相差不大的。但 hash 的好处是可以根据 hashKey 查询处单个值，不必像 string 一样一股脑的查出来，然后在应用端去解析。" }, { "title": "池化对象的应用场景（09）", "url": "/posts/polled-object/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-03-30 15:33:00 +0000", "snippet": "在日常编码中，经常会将一些对象保存起来，这主要考虑的是对象的创建成本。比如： 线程资源 数据库连接资源 TCP 连接等这类对象的初始化花费的时间比较长，如果频繁地申请和销毁，就会消耗大量的系统资源，造成不必要的性能损失。并且这些对象都有一个显著的特征，就是通过轻量级的重置工作，可以循环、重复地使用。这个时候，就可以使用一个虚拟的池子，将这些资源保存起来，当使用的时候，就从池子里快速获取一个就可以了。这种技术就叫做池化技术。在 Java 中，池化技术应用非常广泛，常见的就有：数据库连接池、线程池等。公用池化包 Commons Pool 2.0Java 中有一个公用的池化包 —— Commons Pool 2.0，可以通过它来了解对象池的一般结构。根据业务需求，使用这套 API 能够很容易实现对象的池化管理。GenericObjectPool 是对象池的核心类，通过传入一个对象池的配置和一个对象的工厂，即可快速创建对象池。public GenericObjectPool(final PooledObjectFactory&amp;lt;T&amp;gt; factory, final GenericObjectPoolConfig&amp;lt;T&amp;gt; config)Redis 的常用客户端 Jedis，就是使用 Commons Pool 管理连接池的，可以说是一个最佳实践。下图是 Jedis 使用工厂创建代码的主要代码。对象工厂类最主要的方法是 makeObject ，它的返回值是 PooledObject 类型，可以将对象使用 new DefaultPooledObject&amp;lt;&amp;gt;(obj) 进行简单包装返回。对象的生成过程，如下图，对象在进行获取时，将首先尝试从对象池里拿出一个，如果对象池中没有空闲的对象，就使用工厂类提供的方法，生成一个新的。而后交给一个叫做 LinkdedBlockingDeque 的结构来承担，它是一个双向的队列。下面是 GenericObjectPoolConfig 的主要属性：private int maxTotal = DEFAULT_MAX_TOTAL; private int maxIdle = DEFAULT_MAX_IDLE; private int minIdle = DEFAULT_MIN_IDLE; private boolean lifo = DEFAULT_LIFO; private boolean fairness = DEFAULT_FAIRNESS; private long maxWaitMillis = DEFAULT_MAX_WAIT_MILLIS; private long minEvictableIdleTimeMillis = DEFAULT_MIN_EVICTABLE_IDLE_TIME_MILLIS; private long evictorShutdownTimeoutMillis = DEFAULT_EVICTOR_SHUTDOWN_TIMEOUT_MILLIS; private long softMinEvictableIdleTimeMillis = DEFAULT_SOFT_MIN_EVICTABLE_IDLE_TIME_MILLIS; private int numTestsPerEvictionRun = DEFAULT_NUM_TESTS_PER_EVICTION_RUN; private EvictionPolicy&amp;lt;T&amp;gt; evictionPolicy = null; // Only 2.6.0 applications set this private String evictionPolicyClassName = DEFAULT_EVICTION_POLICY_CLASS_NAME; private boolean testOnCreate = DEFAULT_TEST_ON_CREATE; private boolean testOnBorrow = DEFAULT_TEST_ON_BORROW; private boolean testOnReturn = DEFAULT_TEST_ON_RETURN; private boolean testWhileIdle = DEFAULT_TEST_WHILE_IDLE; private long timeBetweenEvictionRunsMillis = DEFAULT_TIME_BETWEEN_EVICTION_RUNS_MILLIS; private boolean blockWhenExhausted = DEFAULT_BLOCK_WHEN_EXHAUSTED;参数很多，要想了解参数的意义，首先来看一下一个池化对象在整个池子中的生命周期。如下图所示，池子的操作主要有两个：一个是业务线程，一个是检测线程。对象池在进行初始化的时候，需要指定三个主要的参数： maxTotal 对象池管理的对象上限 maxIdle 最大空闲数 minIdle 最小空闲数其中 maxTotal 和业务线程有关，当业务线程想要获取对象时，会首先检查是否会有空闲的对象。如果有，则返回一个；否则进入创建逻辑。此时，如果池中个数已经达到了最大值，就会创建失败，返回空对象。对象在获取的时候，有一个非常重要的参数，就是最大等待时间（maxWaitMillis），这个参数对应用的性能影响非常大。该参数默认为 -1，表示永不超时，直到有对象空闲。如下图，如果对象创建非常缓慢或者使用非常繁忙，业务线程会持续阻塞（blockWhenExhausted 默认为 true），进而导致正常服务也不能运行。我一般都会把最大等待时间，设置成接口可以忍受的最大延迟。比如，一个正常服务响应时间 10ms 左右，达到 1 秒钟就会感觉到卡顿，那么这个参数设置成 500~1000ms 都是可以的。超时之后，会抛出 NoSuchElementException 异常，请求会快速失败，不会影响其他业务线程，这种 Fail Fast 的思想，在互联网应用非常广泛。带有evcit 字样的参数，主要是处理对象逐出的。池化对象除了初始化和销毁的时候比较昂贵，在运行时也会占用系统资源。比如，连接池会占用多条连接，线程池会增加调度开销等。业务在突发流量下，会申请到超出正常情况的对象资源，放在池子中。等这些对象不再被使用，我们就需要把它清理掉。超出 minEvictableIdleTimeMillis 参数指定值的对象，就会被强制回收掉，这个值默认是 30 分钟；softMinEvictableIdleTimeMillis 参数类似，但它只有在当前对象数量大于 minIdle 的时候才会执行移除，所以前者的动作要更暴力一些。还有 4 个 test 参数：testOnCreate、testOnBorrow、testOnReturn、testWhileIdle，分别指定了在创建、获取、归还、空闲检测的时候，是否对池化对象进行有效性检测。开启这些检测，能保证资源的有效性，但它会耗费性能，所以默认为 false。生产环境上，建议只将 testWhileIdle 设置为 true，并通过调整空闲检测时间间隔（timeBetweenEvictionRunsMillis），比如 1 分钟，来保证资源的可用性，同时也保证效率。Jedis JMH 测试使用连接池和不使用连接池，它们之间的性能差距到底有多大呢？下面是一个简单的 JMH 测试例子（见仓库），进行一个简单的 set 操作，为 redis 的 key 设置一个随机值。@Fork(2) @State(Scope.Benchmark) @Warmup(iterations = 5, time = 1) @Measurement(iterations = 5, time = 1) @BenchmarkMode(Mode.Throughput) public class JedisPoolVSJedisBenchmark { JedisPool pool = new JedisPool(&quot;localhost&quot;, 6379); @Benchmark public void testPool() { Jedis jedis = pool.getResource(); jedis.set(&quot;a&quot;, UUID.randomUUID().toString()); jedis.close(); } @Benchmark public void testJedis() { Jedis jedis = new Jedis(&quot;localhost&quot;, 6379); jedis.set(&quot;a&quot;, UUID.randomUUID().toString()); jedis.close(); } ...将测试结果使用 meta-chart 作图，展示结果如下图所示，可以看到使用了连接池的方式，它的吞吐量是未使用连接池方式的 5 倍！数据库连接池 HikariCPHikariCP 源于日语“光”的意思（和光速一样快），它是 SpringBoot 中默认的数据库连接池。数据库是我们工作中经常使用到的组件，针对数据库设计的客户端连接池是非常多的，它的设计原理与我们在本课时开头提到的基本一致，可以有效地减少数据库连接创建、销毁的资源消耗。同是连接池，它们的性能也是有差别的，下图是 HikariCP 官方的一张测试图，可以看到它优异的性能，官方的 JMH 测试代码见 Github，我也已经拷贝了一份到仓库中。HikariCP 为什么快呢？主要有三个方面： 它使用 FastList 替代 ArrayList，通过初始化的默认值，减少了越界检查的操作； 优化并精简了字节码，通过使用 Javassist，减少了动态代理的性能损耗，比如使用 invokestatic 指令代替 invokevirtual 指令； 实现了无锁的 ConcurrentBag，减少了并发场景下的锁竞争。HikariCP 对性能的一些优化操作，是非常值得借鉴的。 数据库连接池同样面临一个最大值（maximumPoolSize）和最小值（minimumIdle）的问题。有时候我会错误的认为，连接池的大小设置得越大越好，有的同学甚至把这个值设置成 1000 以上，这是一种误解。根据经验，数据库连接，只需要 20~50 个就够用了。具体的大小，要根据业务属性进行调整，但大得离谱肯定是不合适的。HikariCP 官方是不推荐设置 minimumIdle 这个值的，它将被默认设置成和 maximumPoolSize 一样的大小。如果你的数据库Server端连接资源空闲较大，不妨也可以去掉连接池的动态调整功能。另外，根据数据库查询和事务类型，一个应用中是可以配置多个数据库连接池的，这个优化技巧很少有人知道，在此简要描述一下。业务类型通常有两种： 一种需要快速的响应时间，把数据尽快返回给用户； 另外一种是可以在后台慢慢执行，耗时比较长，对时效性要求不高。如果这两种业务类型，共用一个数据库连接池，就容易发生资源争抢，进而影响接口响应速度。虽然微服务能够解决这种情况，但大多数服务是没有这种条件的，这时就可以对连接池进行拆分。如图，在同一个业务中，根据业务的属性，分两个连接池，就是来处理这种情况的。HikariCP 还提到了另外一知识点，在 JDBC4 的协议中，通过 Connection isValid() 就可以检测连接的有效性。这样的好处就是不用设置一大堆的 test 参数了，HikariCP 也没有提供这样的参数。结果缓存池发现池（Pool）与缓存（Cache）有许多相似之处。它们之间的一个共同点，就是将对象加工后，存储在相对高速的区域。习惯性将缓存看作是数据对象，而把池中的对象看作是执行对象。缓存中的数据有一个命中率问题，而池中的对象一般都是对等的。有下面一个场景，jsp 提供了网页的动态功能，它可以在执行后，编译成 class 文件，加快执行速度；再或者，一些媒体平台，会将热门文章，定时转化成静态的 html 页面，仅靠 nginx 的负载均衡即可应对高并发请求（动静分离）。这些时候，很难说清楚，这是针对缓存的优化，还是针对对象进行了池化，它们在本质上只是保存了某个执行步骤的结果，使得下次访问时不需要从头再来。这种技术叫作结果缓存池（Result Cache Pool），属于多种优化手段的综合。总结总体来说，当遇到下面的场景，就可以考虑使用池化来增加系统性能： 对象的创建或者销毁，需要耗费较多的系统资源； 对象的创建或者销毁，耗时长，需要繁杂的操作和较长时间的等待； 对象创建后，通过一些状态重置，可被反复使用。平常的编码中，有很多类似的场景。比如 Http 连接池，Okhttp 和 Httpclient 就都提供了连接池的概念。可以类比着去分析一下，关注点也是在连接大小和超时时间上；在底层的中间件，比如 RPC，也通常使用连接池技术加速资源获取，比如 Dubbo 连接池、 Feign 切换成 httppclient 的实现等技术。进而发现，在不同资源层面的池化设计也是类似的。比如线程池，通过队列对任务进行了二层缓冲，提供了多样的拒绝策略等，" }, { "title": "Redis（08）", "url": "/posts/redis-cache/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-03-25 15:33:00 +0000", "snippet": "堆内缓存的特点、注意事项以及使用的场景，对于分布式缓存来说，同样适用。分布式缓存，一种集中管理的思想。假如服务是多节点，堆内缓存在每个节点上都会有一份；而分布式缓存，所有的节点，共用一份缓存，既节约了空间，又减少了管理成本。在分布式缓存领域，使用最多的就是 Redis.Redis 支持非常丰富的数据类型，包括字符串（String）、列表（list）、集合（set）、有序集合（zset）、哈希表（hash）等常用的数据结构。除此之外，它也支持也写其它的比如为图（bitmap） 一类的数据结构。每当提到 Redis，就不得不提一下另一个分布式缓存 Memcached，以下简称 MC。MC 现在已经很少使用了，但它们之间的区别，还是需要了解的：   Redis MC 是否多线程 否 是 数据类型 数据类型丰富 字符串类型 数据保存 数据可持久化到硬盘 断电后数据会丢失 性能表现 存储小数据时性能高 存储大数据（如超 100 kb）性能高 数据划分 使用基于哈希槽（slot）的划分方式 客户端实现的一致性哈希（ConsistencyHashing） SpringBoot 使用 Redis使用 SpringBoot 很容易地对 Redis 进行操作。Java 的 Redis 的客户端，常用的有三个：jedis、redissoon 和 lettuce。Spring 默认使用的是 lettuce。lettuce 是使用 netty 开发的，操作是异步的，性能比常用的 jedis 要高，redisson 也是异步的，但它对常用的业务操作进行了封装，适合有业务含义的代码。通过下面的 jar 包，即可方便地使用 Redis。&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-data-redis&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;上面这种方式，主要是使用 RedisTemplate 这个类。它针对不同的数据类型，抽象了相应的方法组。另外一种方式，是使用 Spring 抽象的缓存包 spring-cache。它使用注解，采用 AOP 的方式，对 Cache 层进行了抽象，可以在各种堆内缓存框架和分布式框架之间进行切换。下面是它的 maven 坐标：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-cache&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt; 与 spring-cache 类似的，还有阿里的 jetcache，都是比较好用的。秒杀业务介绍秒杀，是对正常业务流程的考验。因为会产生突发流量，平常一天的请求，可能集中在几秒内就要完成。比如，某些抢购，可能库存就几百个，但是瞬时进入的流量可能是几十上百万。如果参与秒杀的人，等待很长时间，体验就非常差，想象一下拥堵的高速公路收费站，就能理解秒杀者的心情。同时，被秒杀的资源会成为热点，发生并发争抢的后果。比如 12306 的抢票，如果单纯使用数据库来接受这些请求，就会产生严重的锁冲突，这也是秒杀业务难的地方。此时，秒杀前端需求与数据库之间的速度是严重不匹配的，而且秒杀的资源是热点资源。这种场景下，采用缓存是非常合适的。处理秒杀业务有三个绝招： 选择速度最快的内存作为数据写入； 使用异步处理替代同步请求； 使用分布式横向扩展Lua 脚本完成秒杀秒杀可以分为一下三个阶段： 准备阶段，会提前载入一些必需的数据到缓存中，并提前预热业务数据，用户会不断刷新页面，来查看秒杀是否开始； 抢购阶段，就是我们通常说的秒杀，会产生瞬时的高并发流量，对资源进行集中操作； 结束清算，主要完成数据的一致性，处理一些异常情况和回仓操作。最重要的是秒杀阶段。可以设计一个 Hash 数据结构，来支持库存的扣减：seckill:goods:${goodsId}{ total: 100, start: 0, alloc:0 }这个 Hash 数据结构中，有三个重要部分： total 是一个静态值，表示要秒杀商品的数量，在秒杀开始前，会将这个数值载入到缓存中； start 是一个布尔值。秒杀开始前的值为 0；通过后台或者定时，将这个值改为 1，则表示秒杀开始； 此时，alloc 将会记录已经被秒杀的商品数量，直到它的值达到 total 的上限。 static final String goodsId = &quot;seckill:goods:%s&quot;; String getKey(String id) { return String.format(goodsId, id); } public void prepare(String id, int total) { String key = getKey(id); Map&amp;lt;String, Integer&amp;gt; goods = new HashMap&amp;lt;&amp;gt;(); goods.put(&quot;total&quot;, total); goods.put(&quot;start&quot;, 0); goods.put(&quot;alloc&quot;, 0); redisTemplate.opsForHash().putAll(key, goods); } 秒杀的时候，首先需要判断库存，才能够对库存进行锁定。这两步动作并不是原子的，在分布式环境下，多个节点同时对 Redis 进行操作，就会发生同步问题。为了解决同步问题，一种方式就是使用 Lua 脚本，把这些操作封装起来，这样就能保证原子性。此外一种方式是是使用分布式锁。下面是一个调式好的 Lua 脚本，可以看到一些关键的比较动作，和 HiNCRBY 命令，能够成为一个原子操作。local falseRet = &quot;0&quot; local n = tonumber(ARGV[1]) local key = KEYS[1] local goodsInfo = redis.call(&quot;HMGET&quot;,key,&quot;total&quot;,&quot;alloc&quot;) local total = tonumber(goodsInfo[1]) local alloc = tonumber(goodsInfo[2]) if not total then    return falseRet end if total &amp;gt;= alloc + n  then    local ret = redis.call(&quot;HINCRBY&quot;,key,&quot;alloc&quot;,n)    return tostring(ret) end return falseRet对应的秒杀代码如下，由于使用的是 String 的序列化方式，所以会把库存的扣减数量先转化为字符串，然后再调用 Lua 脚本。public int secKill(String id, int number) {    String key = getKey(id);    Object alloc =  redisTemplate.execute(script, Arrays.asList(key), String.valueOf(number));    return Integer.valueOf(alloc.toString()); }执行仓库里的 testSeckill 方法。启动 1000 个线程对 100 个资源进行模拟秒杀，可以看到生成了 100 条记录，同时其他的线程返回的是 0，表示没有秒杀到。分布式缓存系统带来的问题1. 缓存穿透如果缓存的命中率很低，压力就会集中在数据库持久层。如果找到相关数据，就可以把它缓存起来。但问题是，假如某次请求，在缓存和持久层都没有命中，这种情况叫做缓存的穿透。栗子：假设，有一个登录系统，有外部攻击，一直尝试使用不存在的用户进行登录，这些用户都是虚拟的，不能有效地被缓存起来，每次都会到数据库中查询一次，最后就会造成服务的性能障碍。解决这种问题有两种方案： 第一种，将空对象缓存起来。不是持久层查不到数据？那么久可以把本次请求的结果设置为 null，然后放入到缓存中。通过设置合理的过期时间，就可以保证后端数据库的安全； 缓存空对象会占用额外的缓存空间，还会有数据不一致的时间窗口，所以第二种方法就是针对大数据量的、有规律的键值，使用布隆过滤器进行处理。一条记录存在与不存在，是一个 Bool 值，只需要使用 1 比特就可存储。布隆过滤器就可以把这种是、否操作，压缩到一个数据结构中。比如手机号，用户性别这种数据，就非常适合使用布隆过滤器。2. 缓存击穿缓存击穿，指的也是用户请求落在数据库上的情况，大多数情况，是由于缓存时间批量过期引起的。一般会对缓存中的数据，设置一个过期时间。如果在某个时刻从数据库获取了大量数据，并设置了同样的过期时间，它们将会在同一时刻失效，造成缓存的击穿。对于比较热点的数据，就可以设置它不过期；或者在访问的时候，更新它的过期时间；批量入库的缓存项，也尽量分配一个比较平均的过期时间，避免同一时间失效。3. 缓存雪崩雪崩，这种情况比较严重。缓存是用来对系统加速的，后端的数据库只是数据的备份，而不是作为高可用的备选方案。当缓存系统出现故障，流量会瞬间转移到后端的数据库。没有多长时间，数据库将会被大流量压垮挂掉，这种级联式的服务故障，可以形象地称为雪崩。缓存的高可用建设是非常重要的。Redis 提供了主从和 Cluster 的模式，其中 Cluster 模式使用简单，每个分片也能单独做主从，可以保证极高的可用性。另外，对数据库的性能瓶颈有一个大体的评估。如果缓存系统当掉，那么流向数据库的请求，就可以使用限流组件，将请求拦截在外面。4. 缓存一致性缓存组件，还会带来另一个老大难的问题，就是缓存的一致性。对于一个缓存项，常用的操作有四个：写入、更新、读取、删除。 写入：缓存和数据库是两个不同的组件，只要涉及双写，就存在只有一个写成功的可能性，造成数据不一致； 更新：更新的情况类似，需要更新两个不同的组件； 读取：读取要保证从缓存中读到的信息是更新的，是和数据库中的是一致的； 删除：当删除数据库记录的时候，把缓存中的数据也删掉。由于业务逻辑大多数情况下，是比较复杂的。其中的更新操作，就非常昂贵，比如一个用户的余额，就是通过计算一些系列的资产算出来的一个数。如果这些关联的资产，每个地方改动的时候，都去刷新缓存，那代码结构就会非常混乱，以至于无法维护。解决这种方式，可以使用触发式的缓存一致性方式，使用懒加载的方式，可以让缓存的同步变得非常简单： 当读取缓存的时候，如果缓存里没有相关数据，则执行相关的业务逻辑，构造缓存数据存入到缓存系统； 当与缓存项相关的资源有变动，则先删除相应的缓存项，然后再对资源进行更新，这个时候，即使是资源更新失败，也是没有问题的。这种操作，除了编程模型简单，有一个明显的好处。只有用到这个缓存的时候，才把它加载到缓存系统中。如果每次修改都创建、更新资源，那缓存系统中就会存在非常多的冷数据。上面提到的缓存删除动作，和数据库的更新动作，明显是不在一个事务里的。如果一个请求删除了缓存，同时有另外一个请求到来，此时发现没有相关的缓存项，就从数据库里加载了一份到缓存系统。接下来，数据库的更新操作也完成了，此时数据库的内容和缓存里的内容，就产生了不一致。下面的图，直观地解释了这种不一致的情况，此时，缓存读取 B 操作以及之后的读取操作，都会读到错误的缓存值。可以使用分布式锁来解决这个问题，将缓存操作和数据库删除操作，与其他的缓存读操作，使用锁进行资源隔离即可。一般来说，读操作是步不需要加锁的，它会遇到锁的时候，重试等待，直到超时。 所谓的一致性，就是最后总要有个收口的地方。 双删除操明显不能收口； Redis 的分布式锁性能特别高，能够应付大多数高并发场景 如果 Redis 单机有瓶颈，可以根据锁的内容（比如用户 ID），在近一步 hash 到多台机器上，采用分段的思想解决。这个分段思想不仅仅可以用在锁上，还能用在资源上。比如，把一个红包，先拆分成 100 份，然后每一批人分别对其中进行秒杀。 " }, { "title": "高并发的法宝-无所不在的缓存（07）", "url": "/posts/cache/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-03-25 15:33:00 +0000", "snippet": "和缓冲类似，缓存可能是软件开放中使用最多的优化技术了，比如： 在最核心的 CPU 中，就存在着多级缓存； 为了消除内存和存储之间的差异，各种类似 Redis 的缓存框架更是层出不穷。缓存的优化效果是非常好的，既可以让原本载入非常缓慢的页面，瞬间秒开，也能让本是压力山大的数据库，瞬间清闲下来。缓存，本质上是为了协调两个速度差异非常大的组件，如下图所示，通过加入一个中间层，将常用的数据存放在相对高速的设备中。在平常的应用开发中，根据缓存所处的物理位置，分为进程内缓存和进程外缓存。本文主要聚焦在进程内缓存上，在 Java 中，进程内缓存，就是常说的堆内缓存。Spring 的默认实现里，就包含 **Ehcache、JCache、Caffeine、Guava Cache **等。Guava 的 LoadingCacheGuava 是一个常用的工具包，其中的 LoadingCache（下面简称 LC），是非常好用的堆内缓存工具。通过学习 LC 的结构，即可了解堆内缓存设计的一般思路。缓存一般是比较昂贵的组件，容量是有限制的，设置得过小，或者过大，都会影响缓存性能： 缓存空间过小，就会造成高命中率的元素被频繁移出，失去了缓存的意义； 缓存空间过大，不仅浪费宝贵的缓存资源，还会对垃圾回收产生一定的压力。通过 Maven，即可引入 guava 的 jar 包：&amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;com.google.guava&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;guava&amp;lt;/artifactId&amp;gt;    &amp;lt;version&amp;gt;29.0-jre&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt;LC 的常用操作如下：1. 缓存初始化通过下面的参数设置一下 LC 的大小。一般，只需给缓存提供一个上限： maximumSize 这个参数用来设置缓存池的最大容量，达到此容量将会清理其他元素； initialCapacity 默认值是 16，表示初始化大小； concurrencyLevel 默认值是 4，和初始化大小配合使用，表示会将缓存的内存划分成 4 个 segment，用来支持高并发的存取。2. 缓存操作在缓存中放入数据有两种模式： 使用 put 方法手动处理，比如，我从数据库里查询出一个 User 对象，然后手动调用代码进去； 主动触发（ 这也是 Loading 这个词的由来），通过提供一个 CacheLoader 的实现，就可以在用到这个对象的时候，进行延迟加载。public static void main(String[] args) { LoadingCache&amp;lt;String, String&amp;gt; lc = CacheBuilder .newBuilder() .build(new CacheLoader&amp;lt;String, String&amp;gt;() { @Override public String load(String key) throws Exception { return slowMethod(key); } });} static String slowMethod(String key) throws Exception { Thread.sleep(1000); return key + &quot;.result&quot;; }主动触发的示例代码，你可以使用 get 方法获取缓存的值。比如，当执行lc.get(&quot;a&quot;)时，第一次会比较缓慢，因为它需要到数据源进行获取；第二次就瞬间返回了，也就是缓存命中了。具体时序可以参见下面这张图。除了靠 LC 自带的回收策略，还可以手动删除某一个元素，这就是 invalidate 方法。当然，数据的这些删除操作，也是可以监听到的，只需要设置一个监听器就可以了，代码如下：.removalListener(notification -&amp;gt; System.out.println(notification))3. 回收策略缓存的大小是有限的，这就需要回收策略进行处理，三种回收策略： 第一种回收策略基于容量。这个比较好理解，也就是说如果缓存满了，就会按照 LRU 算法来移除其他元素。 第二种回收策略基于时间。 一种方式是，通过 expireAfterWrite 方法设置数据写入以后在某个时间失效； 另一种是，通过 expireAfterAccess 方法设置最早访问的元素，并优先将其删除。 第三种回收策略基于 JVM 的垃圾回收。对象的引用有强、软、弱、虚等四个级别，通过 weakKeys 等函数即可设置相应的引用级别。当 JVM 垃圾回收的时候，会主动清理这些数据。 关于第三种回收策略，有一个知识点：如果你同时设置了 weakKeys 和 weakValues函数，LC 会有什么反应？答案：如果同时设置了这两个函数，它代表的意思是，当没有任何强引用，与 key 或者 value 有关系时，就删掉整个缓存项。这两个函数经常被误解。4. 缓存造成内存故障LC 可以通过 recordStats 函数，对缓存加载和命中率等情况进行监控。值得注意的是：LC 是基于数据条数而不是基于缓存物理大小的，所以如果你缓存的对象特别大，就会造成不可预料的内存占用。围绕这点，一个由于不正确使用缓存导致的常见内存故障的经历： 大多数堆内缓存，都会将对象的引用设置成弱引用或软引用，这样内存不足时，可以优先释放缓存占用的空间，给其他对象腾出地方。这种做法的初衷是好的，但容易出现问题。当缓存使用非常频繁，数据量又比较大的情况下，缓存会占用大量内存，如果此时发生了垃圾回收（GC），缓存空间会被释放掉，但又被迅速占满，从而会再次触发垃圾回收。如此往返，GC 线程会耗费大量的 CPU 资源，缓存也就失去了它的意义。所以在这种情况下，把缓存设置的小一些，减轻 JVM 的负担，是一个很好的方法。缓存算法1. 算法介绍堆内缓存最常用的有 FIFO、LRU、LFU 这三种算法。 FIFO。这是一种先进先出的模式。如果缓存容量满了，将会移除最先加入的元素。这种缓存实现方式简单，但符合先进先出的队列模式场景的功能不多，应用场景较少； LRU。LRU 是最近最少使用的意思，当缓存容量达到上限，它会优先移除那些最久未被使用的数据，LRU是目前最常用的缓存算法，稍后我们会使用 Java 的 API 简单实现一个； LFU。LFU 是最近最不常用的意思。相对于 LRU 的时间维度，LFU 增加了访问次数的维度。如果缓存满的时候，将优先移除访问次数最少的元素；而当有多个访问次数相同的元素时，则优先移除最久未被使用的元素。2. 实现一个 LRU 算法Java 里面实现 LRU 算法可以有多种方式，其中最常用的就是 LinkedHashMap，这也是一个需要你注意的面试高频考点。首先， LinkedHashMap 的构造方法，如下：public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder)accessOrder 参数是实现 LRU 的关键。当 accessOrder 的值为 true 时，将按照对象的访问顺序排序；当 accessOrder 的值为 false 时，将按照对象的插入顺序排序。我们上面提到过，按照访问顺序排序，其实就是 LRU。如上图，按照缓存的一般设计方式，和 LC 类似，当你向 LinkedHashMap 中添加新对象的时候，就会调用 removeEldestEntry 方法。这个方法默认返回 false，表示永不过期。我们只需要覆盖这个方法，当超出容量的时候返回 true，触发移除动作就可以了。关键代码如下：public class LRU extends LinkedHashMap { int capacity; public LRU(int capacity) { super(16, 0.75f, true); this.capacity = capacity; } @Override protected boolean removeEldestEntry(Map.Entry eldest) { return size() &amp;gt; capacity; } }相比较 LC，这段代码实现的功能是比较简陋的，它甚至不是线程安全的，但它体现了缓存设计的一般思路，是 Java 中最简单的 LRU 实现方式。进一步加速在 Linux 系统中，通过 free 命令，能够看到系统内存的使用状态。其中，有一块叫作 cached 的区域，占用了大量的内存空间。这个区域，其实就是存放了操作系统的文件缓存，当应用再次用到它的时候，就不用再到磁盘里走一圈，能够从内存里快速载入。在文件读取的缓存方面，操作系统做得更多。由于磁盘擅长顺序读写，在随机读写的时候，效率很低，所以，操作系统使用了智能的预读算法（readahead），将数据从硬盘中加载到缓存中。预读算法有三个关键点： 预测性，能够根据应用的使用数据，提前预测应用后续的操作目标； 提前，能够将这些数据提前加载到缓存中，保证命中率； 批量，将小块的、频繁的读取操作，合并成顺序的批量读取，提高性能。预读技术一般都是比较智能的，能够覆盖大多数后续的读取操作。举个极端的例子，如果我们的数据集合比较小，访问频率又非常高，就可以使用完全载入的方式，来替换懒加载的方式。在系统启动的时候，将数据加载到缓存中。缓存优化的一般思路一般，缓存针对的主要是读操作。当你的功能遇到下面的场景时，就可以选择使用缓存组件进行性能优化： 存在数据热点，缓存的数据能够被频繁使用； 读操作明显比写操作要多； 下游功能存在着比较悬殊的性能差异，下游服务能力有限； 加入缓存以后，不会影响程序的正确性，或者引入不可预料的复杂性。缓存组件和缓冲类似，也是在两个组件速度严重不匹配的时候，引入的一个中间层，但它们服务的目标是不同的： 缓冲，数据一般只使用一次，等待缓冲区满了，就执行 flush 操作； 缓存，数据被载入之后，可以多次使用，数据将会共享多次。缓存最重要的指标就是命中率，有以下几个因素会影响命中率。 缓存容量。缓存的容量总是有限制的，所以就存在一些冷数据的逐出问题。但缓存也不是越大越好，它不能明显挤占业务的内存； 数据集类型。如果缓存的数据是非热点数据，或者是操作几次就不再使用的冷数据，那命中率肯定会低，缓存也会失去了它的作用； 缓存失效策略。缓存算法也会影响命中率和性能，目前效率最高的算法是 Caffeine 使用的 W-TinyLFU 算法，它的命中率非常高，内存占用也更小。新版本的 spring-cache，已经默认支持 Caffeine。（从官网的 github 仓库就可以找到 JMH 的测试代码）推荐使用 Guava Cache 或者 Caffeine 作为堆内缓存解决方案，然后通过它们提供的一系列监控指标，来调整缓存的大小和内容，一般来说： 缓存命中率达到 50% 以上，作用就开始变得显著； 缓存命中率低于 10%，那就需要考虑缓存组件的必要性了。引入缓存组件，能够显著提升系统性能，但也会引入新的问题。其中，最典型的是：如何保证缓存与源数据的同步？总结 以 Guava 的 LoadingCache 为例，总结堆内缓存设计的一些思路； 因为缓存不合理利用所造成的内存故障，这些都是面试中的高频问题； 三个常用的缓存算法 LRU、LFU、FIFO； 以 LinkedHashMap 为基础，实现了一个最简单的 LRU 缓存。 使用预读或者提前载入等方式，来进一步加速应用的方法，readahead 技术，在操作系统、数据库中使用非常多，性能提升也比较显著； 通过利用缓存框架的一些监控数据，来调整缓存的命中率，要达到 50% 的命中率才算有较好的效果。接下来，两个缓存应用的经典例子： 第一个是 HTTP 304 状态码，它是 Not Modified 的意思。浏览器客户端会发送一个条件性的请求，服务端可以通过 If-Modified-Since 头信息判断缓冲的文件是否是最新的。如果是，那么客户端就直接使用缓存，不用进行再读取了。 另一个是关于 CDN，这是一种变相的缓存。用户会从离它最近最快的节点，读取文件内容。如果这个节点没有缓存这个文件，那么 CDN 节点就会从源站拉取一份，下次有相同的读取请求时，就可以快速返回。缓存的应用非常广泛，在平常的工作中，也可以尝试进行总结、类比。" }, { "title": "利用缓冲区让代码加速（06）", "url": "/posts/buffer/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-03-20 15:33:00 +0000", "snippet": "缓冲的本质缓冲（Buffer）通过对数据进行暂存，然后批量进行传输或者操作，多采用顺序方式，来缓解不同设备之间次数频繁但速度缓慢的随机读写。我们可以把缓冲区，想象成一个蓄水池。放水的水龙头一直开着，如果池子里有水，它就以恒定的速度流淌，不需要暂停；供水的水龙头速度却不确定，有时候会快一些，有时候会特别慢。它通过判断水池里水的状态，就可以自由控制进水的速度。或者再想象一下包饺子的过程，包馅的需要等着擀皮的。如果擀皮的每擀一个就交给包馅的，速度就会很慢；但如果中间放一个盆子，擀皮的只管往里扔，包馅的只管从盆里取，这个过程就快得多。许多工厂流水线也经常使用这种方法，可见“缓冲”这个理念的普及性和实用性。从宏观上来说，JVM 的堆就是一个大的缓冲区，代码不停地在堆空间中生产对象，而垃圾回收器进程则在背后默默地进行垃圾回收。通过上述比喻和释意，可以发现缓冲区的好处： 缓冲双方能各自保持自己的操作节奏，操作处理顺序也不会打乱，可以 one by one 顺序进行； 以批量的方式处理，减少网络交互和繁重的 I/O 操作，从而减少性能损耗； 优化用户体验，比如常见的音频/视频缓冲加载，通过提前缓冲数据，达到流畅的播放效果。缓冲在 Java 语言中被广泛应用，在 IDEA 中搜索 Buffer，可以看到长长的类列表，其中最典型的就是文件读取和写入字符流。文件读写流Java 的 I/O 流设计，采用的是装饰器模式，当需要给类添加新的功能时，就可以将被装饰者通过参数传递到装饰者，封装成新的功能方法。下图是装饰器模式的典型示意图，就增加功能来说，装饰模式比生成子类更为灵活。在读取和写入流的 API 中，BufferedInputStream 和 BufferedReader 可以加快读取字符的速度，BufferedOutputStream 和 BufferedWriter 可以加快写入的速度。下面是直接读取文件的代码实现：int result = 0; try (Reader reader = new FileReader(FILE_PATH)) { int value; while ((value = reader.read()) != -1) { result += value; } } return result;要使用缓冲方式读取，只需要将 FileReader 装饰一下即可：int result = 0; try (Reader reader = new BufferedReader(new FileReader(FILE_PATH))) { int value; while ((value = reader.read()) != -1) { result += value; } }return result;先看一下与之类似的，BufferedInputStream 类的具体实现方法：// 代码来自 JDK public synchronized int read() throws IOException { if (pos &amp;gt;= count) { fill(); if (pos &amp;gt;= count) return -1; } return getBufIfOpen()[pos++] &amp;amp; 0xff; }当缓冲区的内容读取完毕，将尝试使用 fill 函数把输入流读入缓冲区：//代码来自 JDK private void fill() throws IOException { byte[] buffer = getBufIfOpen(); if (markpos &amp;lt; 0) pos = 0; /* no mark: throw away the buffer */ else if (pos &amp;gt;= buffer.length) /* no room left in buffer */ if (markpos &amp;gt; 0) { /* can throw away early part of the buffer */ int sz = pos - markpos; System.arraycopy(buffer, markpos, buffer, 0, sz); pos = sz; markpos = 0; } else if (buffer.length &amp;gt;= marklimit) { markpos = -1; /* buffer got too big, invalidate mark */ pos = 0; /* drop buffer contents */ } else if (buffer.length &amp;gt;= MAX_BUFFER_SIZE) { throw new OutOfMemoryError(&quot;Required array size too large&quot;); } else { /* grow buffer */ int nsz = (pos &amp;lt;= MAX_BUFFER_SIZE - pos) ? pos * 2 : MAX_BUFFER_SIZE; if (nsz &amp;gt; marklimit) nsz = marklimit; byte nbuf[] = new byte[nsz]; System.arraycopy(buffer, 0, nbuf, 0, pos); if (!bufUpdater.compareAndSet(this, buffer, nbuf)) { // Can&#39;t replace buf if there was an async close. // Note: This would need to be changed if fill() // is ever made accessible to multiple threads. // But for now, the only way CAS can fail is via close. // assert buf == null; throw new IOException(&quot;Stream closed&quot;); } buffer = nbuf; } count = pos; int n = getInIfOpen().read(buffer, pos, buffer.length - pos); if (n &amp;gt; 0) count = n + pos; }程序会调整一些读取的位置，并对缓冲区进行位置更新，然后使用被装饰的 InputStream 进行数据读取：int n = getInIfOpen().read(buffer, pos, buffer.length - pos);那么为什么要这么做呢？直接读写不行吗？这是因为：字符流操作的对象，一般是文件或者 Socket，要从这些缓慢的设备中，通过频繁的交互获取数据，效率非常慢；而缓冲区的数据是保存在内存中的，能够显著地提升读写速度。既然好处那么多，为什么不把所有的数据全部读到缓冲区呢？这就是一个权衡的问题，缓冲区开得太大，会增加单次读写的时间，同时内存价格很高，不能无限制使用，缓冲流的默认缓冲区大小是 8192 字节，也就是 8KB，算是一个比较折中的值。这好比搬砖，如果一块一块搬，时间便都耗费在往返路上了；但若给你一个小推车，往返的次数便会大大降低，效率自然会有所提升。就像 FileReader 和 BufferedReader 读取文件的 JMH 对比（相关代码见仓库），可以看到，使用了缓冲，读取效率有了很大的提升（暂未考虑系统文件缓存）。日志缓冲日志是程序员们最常打交道的地方。在高并发应用中，即使对日志进行了采样，日志数量依旧惊人，所以选择高速的日志组件至关重要。SLF4J 是 Java 里标准的日志记录库，它是一个允许你使用任何 Java 日志记录库的抽象适配层，最常用的实现是 Logback，支持修改后自动 reload，它比 Java 自带的 JUL 还要流行。Logback 性能很高，其中一个原因就是异步日志，它在记录日志时，使用了一个缓冲队列，当缓冲的内容达到一定的阈值时，才会把缓冲区的内容写到文件里。使用异步日志有两个考虑： 同步日志的写入，会阻塞业务，导致服务接口的耗时增加； 日志写入磁盘的代价是昂贵的，如果每产生一条日志就写入一次，CPU 会花很多时间在磁盘 I/O 上。Logback 的异步日志也比较好配置，我们需要在正常配置的基础上，包装一层异步输出的逻辑（详见仓库）。&amp;lt;appender name =&quot;ASYNC&quot; class= &quot;ch.qos.logback.classic.AsyncAppender&quot;&amp;gt;        &amp;lt;discardingThreshold &amp;gt;0&amp;lt;/discardingThreshold&amp;gt;        &amp;lt;queueSize&amp;gt;512&amp;lt;/queueSize&amp;gt;        &amp;lt;!--这里指定了一个已有的Appender--&amp;gt;        &amp;lt;appender-ref ref =&quot;FILE&quot;/&amp;gt; &amp;lt;/appender&amp;gt;如上图，异步日志输出之后，日志信息将暂存在 ArrayBlockingQueue 列表中，后台会有一个 Worker 线程不断地获取缓冲区内容，然后写入磁盘中。上图中有三个关键参数： queueSize，代表了队列的大小，默认是256。如果这个值设置的太大，大日志量下突然断电，会丢掉缓冲区的内容； maxFlushTime，关闭日志上下文后，继续执行写任务的时间，这是通过调用 Thread 类的 join 方法来实现的（worker.join(maxFlushTime))； discardingThreshold，当 queueSize 快达到上限时，可以通过配置，丢弃一些级别比较低的日志，这个值默认是队列长度的 80%；如果担心可能会丢失业务日志，则可以将这个值设置成 0，表示所有的日志都要打印。缓冲区优化思路毫无疑问，缓冲区是可以提高性能的，但它通常会引入一个异步的问题，使得编程模型变复杂。通过文件读写流和 Logback 两个例子，可以了解对于缓冲区设计的一些常规操作。如下图所示，资源 A 读取或写入一些操作到资源 B，这本是一个正常的操作流程，但由于中间插入了一个额外的存储层，所以这个流程被生生截断了，这时就需要手动处理被截断两方的资源协调问题。根据资源的不同，对正常业务进行截断后的操作，分为同步操作和异步操作。同步操作同步操作的编程模型相对简单，在一个线程中就可完成，只需要控制缓冲区的大小，并把握处理的时机。比如，缓冲区大小达到阈值，或者缓冲区的元素在缓冲区的停留时间超时，这时就会触发批量操作。由于所有的操作又都在单线程，或者同步方法块中完成，再加上资源 B 的处理能力有限，那么很多操作就会阻塞并等待在调用线程上。比如写文件时，需要等待前面的数据写入完毕，才能处理后面的请求。异步操作异步操作就复杂很多。缓冲区的生产者一般是同步调用，但也可以采用异步方式进行填充，一旦采用异步操作，就涉及缓冲区满了以后，生产者的一些响应策略。此时，应该将这些策略抽象出来，根据业务的属性选择，比如直接抛弃、抛出异常，或者直接在用户的线程进行等待。你会发现它与线程池的饱和策略是类似的，这部分的详细概念将在 12 课时讲解。许多应用系统还会有更复杂的策略，比如在用户线程等待，设置一个超时时间，以及成功进入缓冲区之后的回调函数等。对缓冲区的消费，一般采用开启线程的方式，如果有多个线程消费缓冲区，还会存在信息同步和顺序问题。Kafka 缓冲区栗子Kafka 有一个知识点是：Kafka 的生产者，有可能会丢失数据？只需要先了解 Kafka 对生产者的一些封装，其中有一个对性能影响非常大的点，就是缓冲。生产者会把发送到同一个 partition 的多条消息，封装在一个 batch（缓冲区）中。当 batch 满了（参数 batch.size），或者消息达到了超时时间（参数 linger.ms），缓冲区中的消息就会被发送到 broker 上。这个缓冲区默认是 16KB，如果生产者的业务突然断电，这 16KB 数据是没有机会发送出去的。此时，就造成了消息丢失。解决的办法有两种： 把缓冲区设置得非常小，此时消息会退化成单条发送，这会严重影响性能； 消息发送前记录一条日志，消息发送成功后，通过回调再记录一条日志，通过扫描生成的日志，就可以判断哪些消息丢失了。另一个知识点：Kafka 生产者会影响业务的高可用吗？同样和生产者的缓冲区有关。缓冲区大小毕竟是有限制的，如果消息产生得过快，或者生产者与 broker 节点之间有网络问题，缓冲区就会一直处于 full 的状态。此时，有新的消息到达，会如何处理呢？通过配置生产者的超时参数和重试次数，可以让新的消息一直阻塞在业务方。一般来说，这个超时值设置成 1 秒就已经够大了，有的应用在线上把超时参数配置得非常大，比如 1 分钟，就造成了用户的线程迅速占满，整个业务不能再接受新的请求。其它使用缓冲区来提升性能的做法非常多，如下： StringBuilder 和 StringBuffer，通过将要处理的字符串缓冲起来，最后完成拼接，提高字符串拼接的性能； 操作系统在写入磁盘，或者网络 I/O 时，会开启特定的缓冲区，来提升信息流转的效率。通常可使用 flush 函数强制刷新数据，比如通过调整 Socket 的参数 SO_SNDBUF 和 SO_RCVBUF 提高网络传输性能； ySQL 的 InnoDB 引擎，通过配置合理的 innodb_buffer_pool_size，减少换页，增加数据库的性能； 在一些比较底层的工具中，也会变相地用到缓冲。比如常见的 ID 生成器，使用方通过缓冲一部分 ID 段，就可以避免频繁、耗时的交互。注意点虽然缓冲区可以大大地提高应用程序的性能，但同时它也有不少问题，在设计时，要注意这些异常情况。其中，比较严重就是缓冲区内容的丢失。即使使用 addShutdownHook 做了优雅关闭，有些情形依旧难以防范避免，比如机器突然间断电，应用程序进程突然死亡等。这时，缓冲区内未处理完的信息便会丢失，尤其金融信息，电商订单信息的丢失都是比较严重的。所以，内容写入缓冲区之前，需要先预写日志，故障后重启时，就会根据这些日志进行数据恢复。在数据库领域，文件缓冲的场景非常多，一般都是采用** WAL 日志（Write-Ahead Logging）**解决。对数据完整性比较严格的系统，甚至会通过电池或者 UPS 来保证缓冲区的落地。这就是性能优化带来的新问题，必须要解决。总结缓冲区优化是对正常的业务流程进行截断，然后加入缓冲组件的一个操作，它分为同步和异步方式，其中异步方式的实现难度相对更高。大多数组件，从操作系统到数据库，从 Java 的 API 到一些中间件，都可以通过设置一些参数，来控制缓冲区大小，从而取得较大的性能提升。但需要注意的是，某些极端场景（断电、异常退出、kill -9等）可能会造成数据丢失，若业务对此容忍度较低，那么就需要花更多精力来应对这些异常。" }, { "title": "基准测试 JMH（05）", "url": "/posts/Benchmarks-JMH/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-03-15 15:33:00 +0000", "snippet": "当测量某段具体代码的性能情况，会经常写些统计执行时间的代码，这些代码穿插在逻辑中，进行一些简单计时运算。比如下面几行代码：long start = System.currentTimeMillis(); //logic long cost = System.currentTimeMillis() - start; System.out.println(&quot;Logic cost : &quot; + cost);但是，这段代码的统计结果，并不一定准确。举例子来说，JVM 在执行时，会对一些代码块，或一些频繁执行逻辑，进行JIT 编译和内联优化。在得到一个稳定的测试结果之前，需要先循环上万次进行预热。预热前和预热后的性能差别非常大。评估性能，有很多指标，如果这些指标数据，每次都要手工去算的话，肯定时枯燥乏味且低效。JMH - 基准测试工具JMH（The Java Microbenchmark Harness）就是这样一个能做基准测试的工具。如果通过上文介绍的一系列外部工具，定位到了热点代码，要测试它的性能数据，评估改善情况，就可以交给 JMH。它的测量精度非常高，可达纳秒级别。JMH 已经在 JDK 12 中被包含，其他版本的需要自行引入 maven ，坐标如下：&amp;lt;dependencies&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.openjdk.jmh&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;jmh-core&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.23&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.openjdk.jmh&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;jmh-generator-annprocess&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.23&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt;JMH 是一个 Jar 包，它和单元测试框架 JUnit 非常像，通过注解进行一些配置。这部分配置有很多是可以通过 main 方法的 OptionsBuilder 进行设置的。一段简单的 JMH 代码如下所示：@BenchmarkMode(Mode.Throughput) @OutputTimeUnit(TimeUnit.MILLISECONDS) @State(Scope.Thread) @Warmup(iterations = 3, time = 1, timeUnit = TimeUnit.SECONDS) @Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS) @Fork(1) @Threads(2)public class BenchmarkTest { @Benchmark public long shift() { long t = 455565655225562L; long a = 0; for (int i = 0; i &amp;lt; 1000; i++) { a = t &amp;gt;&amp;gt; 30; } return a; } @Benchmark public long div() { long t = 455565655225562L; long a = 0; for (int i = 0; i &amp;lt; 1000; i++) { a = t / 1024 / 1024 / 1024; } return a; } public static void main(String[] args) throws Exception { Options opts = new OptionsBuilder() .include(BenchmarkTest.class.getSimpleName()) .resultFormat(ResultFormatType.JSON) .build(); new Runner(opts).run(); } }关键注解注解 @Warmup样例如下：// 对代码预热总计 5 秒（迭代 5 次，每次一秒）@Warmup( iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)warmup 这个注解，可以用在类或方法上，进行预热配置，可以看到，它有以下几个配置参数： timeUnit：时间的单位，默认单位是秒 iterations：预热阶段的迭代数 time：每次预热的时间 batchSize：批处理大小，指定了每次操作调用几次方法预热过程的测试数据，是不记录测量结果的。它的执行结果，如下：# Warmup: 3 iterations, 1 s each # Warmup Iteration   1: 0.281 ops/ns # Warmup Iteration   2: 0.376 ops/ns # Warmup Iteration   3: 0.483 ops/ns一般来说，基准测试都是针对比较小的、执行速度相对较快的代码块，这些代码有很大可能性被 JIT 编译、内联，所以在编码时保持方法的精简，是一个非常好的习惯。提高预热，就不得不提一下分布式环境下的服务预热。在对服务节点进行发布的时候，通常也会有预热过程，逐步放量到相应的服务节点，直到服务达到最优状态。如下图所示，负载均衡负责这个放量过程，一般是根据百分比进行放量。注解 @Measurement样例如下：@Measurement( iterations = 5,time = 1,timeUnit = TimeUnit.SECONDS)Measurement 和 Warmup 的参数是一样的，不同于预热，它指的是真正的迭代次数。可以从日志中看到这个执行过程：# Measurement: 5 iterations, 1 s each Iteration 1: 1646.000 ns/op Iteration 2: 1243.000 ns/op Iteration 3: 1273.000 ns/op Iteration 4: 1395.000 ns/op Iteration 5: 1423.000 ns/op虽然经过预热之后，代码都能表现出它的最优状态，但一般和实际应用场景还是有些出入。如果测试机器性能很高，或者测试机资源利用已经达到极限，都会影响测试结果的数值。所以，通常情况下，我都会在测试中，给机器充足的资源，保持一个稳定的环境。在分析结果时，也会更加关注不同代码实现方式下的性能差异，而不是测试数据本身。注解 @BenchmarkMode此注解用来指定基准测试类型，对应 Mode 选项，用来修饰类和方法都可以。这里的 value ，是一个数组，可以配置多个统计维度。比如：@BenchmarkMode({Throughput,Mode.AverageTime}) // 统计的就是吞吐量和平均执行时间两个指标所谓的模式，就是性能衡量的指标，在 JMH 中，分为以下几种： Throughput AverageTime SampleTime SingleShotTime All以平均时间，大体的执行结果如下：Result &quot;com.imaya.BenchmarkTest.shift&quot;:  2.068 ±(99.9%) 0.038 ns/op [Average] (min, avg, max) = (2.059, 2.068, 2.083), stdev = 0.010  CI (99.9%): [2.030, 2.106] (assumes normal distribution)由于声明的时间单位是纳秒，本次 shift 方法的平均响应时间就是 2.068 纳秒。也可以看到最终的耗时时间：Benchmark Mode Cnt Score Error Units BenchmarkTest.div avgt 5 2.072±0.053 ns/op BenchmarkTest.shift avgt 5 2.068±0.038 ns/op由于是平均数，这里的 Error 值的是误差（或者波动）的意思。可以看到，在衡量这些指标的时，都有一个时间维度，它是通过 **@OutputTimeUnit** 注解进行配置的。这个就比较简单，它指明基准测试结果的时间类型。可用于类或者方法上，一般选择秒、毫秒、微秒，纳秒那是针对的速度非常快的方法。举个例子：**@BenchmarkMode(Mode.Throughput) 和 @OutputTimeUnit(TimeUnit.MILLISECONDS)** 进行组合，代表的就是每毫秒的吞吐量。如下面的关于吞吐量的结果，就是以毫秒计算的：Benchmark Mode Cnt Score       Error   Units BenchmarkTest.div thrpt 5   482999.685 ±  6415.832  ops/ms BenchmarkTest.shift thrpt 5   480599.263 ± 20752.609  ops/msOutputTimeUnit 注解同样可以修饰类或者方法，通过更改时间级别，可以获取更加易读的结果。注解 @Forkfork 的值一般设置成 1，表示只使用一个进程进行测试，如果这个数字大于 1，表示会启用新的进程进行测试但如果设置成 0，程序依然会运行，不过是这样是在用户 JVM 进程上运行的，可以看下下面的提示，但不推荐这么做。# Fork: N/A, test runs in the host VM # *** WARNING: Non-forked runs may silently omit JVM options, mess up profilers, disable compiler hints, etc. *** # *** WARNING: Use non-forked runs only for debugging purposes, not for actual performance runs. ***那么 fork 到底是在进程还是线程环境里运行呢？通过追踪一下 JMH 的源码，发现每个 fork 进程是单独运行在 Proccess 进程里，这样就可以做完全的环境隔离，避免交叉影响。它的输入输出流，通过 Socket 连接的模式，发送到执行终端。 一个小技巧。其实 fork 注解有一个参数叫作 jvmArgsAppend，可以通过它传递一些 JVM 的参数。@Fork(value = 3, jvmArgsAppend = {&quot;-Xmx2048m&quot;, &quot;-server&quot;, &quot;-XX:+AggressiveOpts&quot;})// 在平常的测试中，可以适当增加 fork 数，来减少测试的误差。注解 @Threadsfork 是面向进程的，而 Threads 是面向线程的。指定了这个注解以后，将会开启并行测试。如果配置了 Threads.MAX，则使用和处理机器核数相同的线程数。这个与平常编码中的习惯也是相同的，并不是说开的线程越多越好。线程多了，操作系统就需要耗费更多的时间在上下文切换上，造成了整体性能的下降。注解 @Group@Group 注解只能加在方法上，用来把测试方法进行归类。如果你单个测试文件中方法比较多，或者需要将其归类，则可以使用这个注解。与之关联的 @GroupThreads 注解，会在这个归类的基础上，再进行一些线程方面的设置。这两个注解都很少使用，除非是非常大的性能测试案例。注解 @State@State 指定了在类中变量的作用范围，用于声明某个类是一个“状态”，可以用 Scope 参数用来表示该状态的共享范围。这个注解必须加在类上，否则提示无法运行。Scope 有如下三种值。 Benchmark ：表示变量的作用范围是某个基准测试类。 Thread ：每个线程一份副本，如果配置了 Threads 注解，则每个 Thread 都拥有一份变量，它们互不影响。 Group ：联系上面的 @Group 注解，在同一个 Group 里，将会共享同一个变量实例。在 JMHSample04DefaultState 测试文件中，演示了变量 x 的默认作用范围是 Thread，关键代码如下：@State(Scope.Thread) public class JMHSample_04_DefaultState { double x = Math.PI; @Benchmark public void measure() { x++; }}注解 @Setup 和 注解 @TearDown和单元测试框架 JUnit 类似，@Setup 用于基准测试前的初始化动作，@TearDown 用于基准测试后的动作，来做一些全局的配置。这两个注解，同样有一个 Level 值，标明了方法运行的时机，它有三个取值。 Trial ：默认的级别，也就是 Benchmark 级别。 Iteration ：每次迭代都会运行。 Invocation ：每次方法调用都会运行，这个是粒度最细的。如果你的初始化操作，是和方法相关的，那最好使用 Invocation 级别。但大多数场景是一些全局的资源，比如一个 Spring 的 DAO，那么就使用默认的 Trial，只初始化一次就可以。注解 @Param@Param 注解只能修饰字段，用来测试不同的参数，对程序性能的影响。配合 @State注解，可以同时制定这些参数的执行范围。代码样例如下：@BenchmarkMode(Mode.AverageTime) @OutputTimeUnit(TimeUnit.NANOSECONDS) @Warmup(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS) @Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS) @Fork(1) @State(Scope.Benchmark) public class JMHSample_27_Params { @Param({&quot;1&quot;, &quot;31&quot;, &quot;65&quot;, &quot;101&quot;, &quot;103&quot;}) public int arg; @Param({&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;4&quot;, &quot;8&quot;, &quot;16&quot;, &quot;32&quot;}) public int certainty; @Benchmark public boolean bench() { return BigInteger.valueOf(arg).isProbablePrime(certainty); } public static void main(String[] args) throws RunnerException { Options opt = new OptionsBuilder() .include(JMHSample_27_Params.class.getSimpleName()) // .param(&quot;arg&quot;, &quot;41&quot;, &quot;42&quot;) // Use this to selectively constrain/override parameters .build(); new Runner(opt).run(); }}值得注意的是，如果设置了常多的参数，这些参数将执行多次，通常会运行很长时间。比如： 参数 1 M 个，参数 2 N 个，那么总共要执行 M*N 次。注解 @CompilerControl这可以说是一个非常有用的功能了。Java 中方法调用的开销是比较大的，尤其是在调用量非常大的情况下。拿简单的getter/setter 方法来说，这种方法在 Java 代码中大量存在。我们在访问的时候，就需要创建相应的栈帧，访问到需要的字段后，再弹出栈帧，恢复原程序的执行。如果能够把这些对象的访问和操作，纳入目标方法的调用范围之内，就少了一次方法调用，速度就能得到提升，这就是方法内联的概念。如下图所示，代码经过 JIT 编译之后，效率会有大的提升。这个注解可以用在类或者方法上，能够控制方法的编译行为，常用的有 3 种模式：强制使用内联（INLINE），禁止使用内联（DONT_INLINE），甚至是禁止方法编译（EXCLUDE）等。将结果图形化使用 JMH 测试的结果，可以二次加工，进行图形化展示。结合图表数据，更加直观。通过运行时，指定输出的格式文件，即可获得相应格式的性能测试结果。比如下面这行代码，就是指定输出 JSON 格式的数据：Options opt = new OptionsBuilder() .resultFormat(ResultFormatType.JSON) .build();一般来说，导出成 CSV 文件，直接在 Excel 中操作，生成相应的图形就可以了。结果图形化制图工具 JMH Visualizer** **一个开源的项目，通过导出 json 文件，上传至 JMH Visualizer，可得到简单的统计结果。由于很多操作需要鼠标悬浮在上面进行操作，所以个人认为它的展示方式并不是很好。 JMH Visual Chart** **相对 JMH Visualizer 而言更直观一些。 meta-chart** **一个通用的 在线图表生成器，导出 CSV 文件后，做适当处理，即可导出精美图像。像 Jenkins 等一些持续集成工具，也提供了相应的插件，用来直接显示这些测试结果。官方的 JMH 有非常丰富的示例，比如伪共享（FalseSharing）的影响等高级话题。JMH 这个工具非常好用，它可以使用确切的测试数据，来支持我们的分析结果。一般情况下，如果定位到热点代码，就需要使用基准测试工具进行专项优化，直到性能有了显著的提升。 并不仅仅是热点代码，执行的非常慢的接口也可以进行着重优化。这就是监控平台和 APM 平台的重要性。目前比较主流的做法有类似 Skywalking 的探针，也有大公司使用 Spark Stream 做实时的流式分析，实时输出方法调用的耗时和性能指标。如果没有这些工具，就只能靠业务感觉了。 对于 http，单个请求可以使用curl工具。 “time curl $url&amp;gt;/dev/null” 统计类的可以用wrk或者jmeter 对于java调用，arthas或者skywalking之类的探针 apm 都可去做。" }, { "title": "获取代码性能数据的工具总结（04）", "url": "/posts/tools-to-get-code-performance/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-03-10 15:33:00 +0000", "snippet": "为什么 Kafka 操作磁盘，吞吐量还能那么高？这是因为，磁盘之所以慢，主要就是慢在寻道的操作上面。Kafka 官方测试表明，这个寻道时间长达 10ms。磁盘的顺序写和随机写的速度比，可以达到 6 千倍，Kafka 就是采用的顺序写的方式。如果深入排查，需要收集较详细的性能数据，包括操作系统性能数据、JVM 的性能数据、应用的性能数据等。在平常工作中，我经常使用以下五款性能测试性能工具nmon：获取系统性能数据除了 top、free 等命令，还有一些将资源整合在一起的监控工具。nmon 是一个老牌的 Linux 性能监控工具，它不仅有漂亮的监控界面（如下图所示），还能产出细致的监控报表。在对应用做性能评估时，通常会加上 nmon 的报告，这会让测试结果更加有说服力。操作系统性能指标，都可从 nmon 中获取。它的监控范围很广，包括 CPU、内存、网络、磁盘、文件系统、NFS、系统资源等信息。nmon 在 sourceforge 发布，它的使用非常简单，比如 CentOS 7 系统，选择对应的版本即可执行。./nmon_x86_64_centos7 按 C 键可加入 CPU 面板； 按 M 键可加入内存面板； 按 N 键可加入网络； 按 D 键可加入磁盘等。通过下面的命令，表示每 5 秒采集一次数据，共采集 12 次，它会把这一段时间之内的数据记录下来。比如本次生成了 localhost_200623_1633.nmon 这个文件，把它从服务器上下载下来./nmon_x86_64_centos7  -f -s 5 -c 12 -m  -m .注意：执行命令之后，可以通过 ps 命令找到这个进程。[root@localhost nmon16m_helpsystems]# ps -ef| grep nmonroot      2228     1  0 16:33 pts/0    00:00:00 ./nmon_x86_64_centos7 -f -s 5 -c 12 -m .此外，使用 nmonchart 工具，即可生成 html 文件。jvisualvm：获取 JVM 性能数据jvisualvm 原是随着 JDK 发布的一个工具，Java 9 之后开始单独发布。通过它，可以了解应用在运行中的内部情况。可以连接本地或者远程的服务器，监控大量的性能数据。通过插件功能，jvisualvm 能获得更强大的扩展（建议把所有的插件下载下来进行体验）要想监控远程的应用，需要在被监控的 App 上加入 jmx 参数。-Dcom.sun.management.jmxremote.port=14000-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false# 注意：jmx 参数是加在java命令后面的上述配置的意义是开启 JMX 连接端口 14000，同时配置不需要 SSL 安全认证方式连接。对于性能优化来说，主要用到它的采样器。注意，由于抽样分析过程对程序运行性能有较大的影响，一般只在测试环境中使用此功能。对于一个 Java 应用来说，除了要关注 CPU 指标，垃圾回收方面也是不容忽视的性能点，主要关注以下三点。 CPU 分析：统计方法的执行次数和执行耗时，这些数据可用于分析哪个方法执行时间过长，成为热点等 内存分析：可以通过内存监视和内存快照等方式进行分析，进而检测内存泄漏问题，优化内存使用情况 线程分析：可以查看线程的状态变化，以及一些死锁情况JMC：获取 Java 应用详细性能数据对于常用的 HotSpot 来说，更更强大的工具 JMC（Java Mission Control）。JMC 集成了一个非常好用的功能：JFR（Java Flight Recorder）。Flight Recorder 源自飞机的黑盒子，是用来录制信息然后事后分析的。在 Java 11 中，它可以通过 jcmd 命令进行录制，主要包括 configure、check、start、dump、stop 这五个命令，其执行顺序为，start — dump — stop，例如：jcmd &amp;lt;pid&amp;gt; JFR.startjcmd &amp;lt;pid&amp;gt; JFR.dump filename=recording.jfrjcmd &amp;lt;pid&amp;gt; JFR.stopJFR 功能是建在 JVM 内部的，不需要额外依赖，可以直接使用，它能够监测大量数据。比如，常提到的锁竞争、延迟、阻塞等；甚至在 JVM 内部，比如 SafePoint、JIT 编译等，也能去分析。JMC 集成了 JFR 的功能，参考JMC 使用教程Arthas：获取单个请求的调用链耗时Arthas 是一个 Java 诊断工具，可以排查内存溢出、CPU 飙升、负载高等内容，可以说是一个 jstack、jmap 等命令的大集合。详细使用方法详见官方文档wrk 获取 Web 接口的性能数据wrk 是一款 HTTP 压测工具，和 ab 命令类似，是一个命令行工具。它的执行结果一般如下：Running 30s test @ http://127.0.0.1:8080/index.html12 threads and 400 connectionsThread Stats   Avg     Stdev     Max   +/- StdevLatency   635.91us    0.89ms  12.92ms   93.69%Req/Sec    56.20k     8.07k   62.00k    86.54%22464657 requests in 30.00s, 17.76GB readRequests/sec: 748868.53Transfer/sec:    606.33MB可以看到，wrk 统计了常见的性能指标，对 Web 服务性能测试非常有用。同时，wrk 支持 Lua 脚本，用来控制 setup、init、delay、request、response 等函数，可以更好地模拟用户请求。总结 nmon 获取系统性能数据； jvisualvm 获取 JVM 性能数据； jmc 获取 Java 应用详细性能数据； arthas 获取单个请求的调用链耗时； wrk 获取 Web 接口的性能数据。这些工具有偏低层的、有偏应用的、有偏统计的、有偏细节的，在定位性能问题时，灵活使用这些工具，既从全貌上掌握应用的属性，也从细节上找到性能的瓶颈，对应用性能进行全方位的掌控。一般在服务器上使用图形化工具的不多，要真使用的话，还需要开启 JMX 端口。性能优化不仅仅是面对线上环境的，像很多公司已经具备了全链路压测的功能，在开发测试或者预发布环境，都可以进行性能优化。在工具中，nmon 和 arthas都是命令行的" }, { "title": "容易成为瓶颈的资源(03)", "url": "/posts/resource-bottleneck/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-03-09 13:24:11 +0000", "snippet": "CPU、内存以及 I/O 三个资源是最容易成为瓶颈的。CPU 通过 top 命令，观测 CPU 性能； 通过负载，评估 CPU 任务执行的排队情况； 通过 vmstat，看 CPU 的繁忙程度。top 命令 —— CPU 性能当进入 top 命令后，按 1 键即可看到每核 CPU 的运行指标和详细性能。CPU 的使用有多个维度的指标，分别如下： us： 用户态所占用的 CPU 百分比，即引用程序所耗费的 CPU； sy： 内核态所占用的 CPU 百分比，需要配合 vmstat 命令，查看上下文切换是否频繁； ni： 高优先级应用所占用的 CPU 百分比； wa： 等待 I/O 设备所占用的 CPU 百分比，经常使用它来判断 I/O 问题，过高输入输出设备可能存在非常明显的瓶颈； hi： 硬中断所占用的 CPU 百分比； si： 软中断所占用的 CPU 百分比； st： 在平常的服务器上这个值很少发生变动，因为它测量的是宿主机对虚拟机的影响，即虚拟机等待宿主机 CPU 的时间占比，这在一些超卖的云服务器上，经常发生； id： 空闲 CPU 百分比。一般地，应该比较关注空闲 CPU 的百分比，它可以从整体上体现 CPU 的利用情况。 正常业务 cpu 应在 90% 以下；load 超过了cpu数，则负载过高（需要迁移扩容），wa 过高（10%之上），可初步判断 io 问题。sy，si，hi，st，任何一个超过5%，都有问题。负载 —— CPU 任务排队情况如果评估 CPU 任务执行的排队情况，那么需要通过负载（load）来完成。除了 top 命令，使用 uptime 命令也能够查看负载情况，load 的效果是一样的，分别显示了最近 1min、5min、15min 的数值。以单核操作系统为例，将 CPU 资源抽象成一条单向行驶的马路，则会发生以下三种情况： 马路上的车只有 4 辆，车辆畅通无阻，load 大约是 0.5； 马路上的车有 8 辆，正好能首尾相接安全通过，此时 load 大约为 1； 马路上的车有 12 辆，除了在马路上的 8 辆车，还有 4 辆等在马路外面，需要排队，此时 load 大约为 1.5。那 load 为 1 代表的是啥？针对这个问题，误解还是比较多的。 这里需要注意的是，当 load 为 1 时，对于单核的硬件上就认为系统负载已经到了极限。这没有问题，但在多核硬件上，这种描述就不完全正确，它还与 CPU 的个数有关。例如： 单核的负载达到 1，总 load 的值约为 1； 双核的每核负载都达到 1，总 load 约为 2； 四核的每核负载都达到 1，总 load 约为 4。所以，对于一个 load 到了 10，却是 16 核的机器，系统还远没有达到负载极限。 vmstat —— CPU 繁忙程度要看 CPU 的繁忙程度，可以通过 vmstat 命令。比较关注的有下面几列： b: 如果系统有负载问题，就可以看一下 b 列（Uninterruptible Sleep），它的意思是等待 I/O，可能是读盘或者写盘动作比较多； si/so： 显示了交换分区的一些使用情况，交换分区对性能的影响比较大，需要格外关注； cs: 每秒钟上下文切换（Context Switch）的数量，如果上下文切换过于频繁，就需要考虑是否是进程或者线程数开的过多。每个进程上下文切换的具体数量，可以通过查看内存映射文件获取，如下代码所示：[root@localhost ~]# cat /proc/2788/status...voluntary_ctxt_switches: 93950nonvoluntary_ctxt_switches: 171204内存要想了解内存对性能的影响，则需要从操作系统层面来看一下内存的分布。如下图：![]{http://assets.processon.com/chart_image/626edef9e401fd1b2465852f.png}在平常写完代码后，比如写了一个 C++ 程序，去查看它的汇编，如果看到其中的内存地址，并不是实际的物理内存地址，那么应用程序所使用的，就是逻辑内存。学过计算机组成结构的同学应该都有了解。逻辑地址可以映射到两个内存段上：物理内存和虚拟内存，那么整个系统可用的内存就是两者之和。比如你的物理内存是 4GB，分配了 8GB 的 SWAP 分区，那么应用可用的总内存就是 12GB。 共享内存：可以被多个进程共享的内存。比如加载到内存的 libjvm.so，可以被多个虚拟机使用，不用每个加载一份； 虚拟内存（SWAP 分区）：划一块磁盘充当内存，格式就是 SWAP。它能当内存用但并不是内存； 物理内存：机器的配置，比如 4 GB； 机器的可用内存=物理内存+虚拟内存 ； 逻辑内存：和程序运行有关，和内存占用无关。就是使用硬件，将二进制中的逻辑内存（一份exe的逻辑地址都是固定的），翻译成实际的可用内存。 top 命令从 top 命令可以内存的几个参数，可以注意方块框起来的三个区域，解释如下： VIRT： 这里是指虚拟内存，一般比较大，不用做过多关注； RES： 我们平常关注的是这一列的数值，它代表了进程实际占用的内存，平常在做监控时，主要监控的也是这个数值； SHR： 指的是共享内存，比如可以复用的一些 so 文件等。CPU 缓存由于 CPU 和内存之间的速度差异非常大，解决方式就是加入高速缓存。实际上，这些高速缓存往往会有多层，如下图所示： 高速缓存指的是 CPU 的 L1，L2，L3 甚至 Ln 层缓存。CPU 缓存离 CPU 最近，但由于材质（SRAM）比较贵，所以容量很小，比如一级缓存可能只有32KB。在 Linux 下，可以通过查看 /sys/devices/system/cpu/cpu0/cache 下面的文件，查看高速缓存的配置规格。Java 有大部分知识点是围绕多线程的，那是因为，如果一个线程的时间片跨越了多个 CPU，那么就会存在同步问题。在 Java 中，和 CPU 缓存相关的最典型的知识点，就是在并发编程中，针对 Cache line 的伪共享（False Sharing）问题。伪共享指的是在这些高速缓存中，以缓存行为单位进行存储，哪怕你修改了缓存行中一个很小很小的数据，它都会整个刷新。所以，当多线程修改一些变量的值时，如果这些变量都在同一个缓存行里，就会造成频繁刷新，无意中影响彼此的性能。CPU 的每个核，基本是相同的，拿 CPU0 来说，可以通过以下的命令查看它的缓存行大小，这个值一般是 64。cat /sys/devices/system/cpu/cpu0/cache/index0/coherency_line_sizecat /sys/devices/system/cpu/cpu0/cache/index1/coherency_line_sizecat /sys/devices/system/cpu/cpu0/cache/index2/coherency_line_sizecat /sys/devices/system/cpu/cpu0/cache/index3/coherency_line_size当然，通过 cpuinfo 也能得到一样的结果：# cat /proc/cpuinfo | grep cachecache size : 20480 KBcache_alignment : 64cache size : 20480 KBcache_alignment : 64cache size : 20480 KBcache_alignment : 64cache size : 20480 KBcache_alignment : 64在 JDK 8 以上的版本，通过开启参数 -XX:-RestrictContended，就可以使用注解 @sun.misc.Contended 进行补齐，来避免伪共享的问题。HugePage![]{http://assets.processon.com/chart_image/626edef9e401fd1b2465852f.png}再回顾一下上文提到的这张图，上面有一个 TLB 组件，它的速度很快，但容量有限，在普通的 PC 机上没有什么瓶颈。但如果机器配置比较高，物理内存比较大，那就会产生非常多的映射表，CPU 的检索效率也会随之降低。传统的页大小是 4 KB，在大内存时代这个值偏小了，解决的办法就是增加页的尺寸，比如将其增加到 2 MB，这样，就可以使用较少的映射表来管理大内存。而这种将页增大的技术，就是 Huge Page。同时，HugePage 也伴随着一些副作用，比如竞争加剧，但在一些大内存的机器上，开启后在一定程度上会增加性能。预先加载另外，一些程序的默认行为也会对性能有所影响，比如 JVM 的 -XX:+AlwaysPreTouch 参数。默认情况下，JVM 虽然配置了 Xmx、Xms 等参数，指定堆的初始化大小和最大大小，但它的内存在真正用到时，才会分配；但如果加上 AlwaysPreTouch 这个参数，JVM 会在启动的时候，就把所有的内存预先分配。这样，启动时虽然慢了些，但运行时的性能会增加。I/OI/O 设备是计算机里速度最慢的组件，它指的不仅仅是硬盘，还包括外围的所有设备。在无视不同设备的实现细节的情况下，直接看它的写入速度：缓冲区是解决速度差异的唯一工具。单在一些极端的情况下，比如断电时，就产生了太多的不确定性，这时的缓存区，都容易丢！！！isotat最能体现 I/O 繁忙程度的，就是 top 命令和 vmstat 命令的 wa%.当应用写入大量的日志，I/O wait 就可能非常高。最便捷好用查看磁盘 I/O 的工具，就是 iostat 命令 。而 iostat 命令通过 sysstat 包进行安装。主要的指标如下： %util：需要非常关注这个数值，通常情况下，这个数字超过 80%，就证明 I/O 的负荷已经非常严重了； Device：表示是哪块硬盘，如果有多块磁盘，则会显示多行； avgqu-sz：平均请求队列的长度，这和十字路口排队的汽车也非常类似。显然，这个值越小越好； awai：响应时间包含了队列时间和服务时间。它有一个经验值，通常情况下应该是小于 5ms 的，如果这个值超过了 10ms，则证明等待的时间过长了； svctm：表示操作 I/O 的平均服务时间。在这里就是 AVG 的意思。svctm 和 await 是强相关的，如果它们比较接近，则表示 I/O 几乎没有等待，设备的性能很好；但如果 await 比 svctm 的值高出很多，则证明 I/O 的队列等待时间太长，进而系统上运行的应用程序将变慢。零拷贝硬盘上的数据，在发往网络之前，需要经过多次缓冲区的拷贝，以及用户空间和内核空间的多次切换。如果能减少一些拷贝的过程，效率就能提升，所以零拷贝应运而生。零拷贝是一种非常重要的性能优化手段，比如常见的 **Kafka、Nginx **等，就使用了这种技术。有无零拷贝之间的区别，如下：(1) 没有采取零拷贝手段传统方式中要想将一个文件的内容通过 Socket 发送出去，则需要经过以下步骤： 将文件内容拷贝到内核空间； 将内核空间内存的内容，拷贝到用户空间内存，比如 Java 应用读取 zip 文件； 用户空间将内容写入到内核空间的缓存中； Socket 读取内核缓存中的内容，发送出去。(2) 采取了零拷贝手段零拷贝有多种模式，比如 sendfile 。如下图所示，在内核的支持下，零拷贝少了一个步骤，那就是内核缓存向用户空间的拷贝，这样既节省了内存，也节省了 CPU 的调度时间，让效率更高。在内核的支持下，零拷贝少了一个步骤，那就是内核缓存向用户空间的拷贝。底层实现，可以参考 sendfile 函数。典型的应用如 netty 的 zero copy，kafka 的文件传输，nginx 的大文件传输等。只要数据不需要经过二次加工发送出去，都可以使用零拷贝，非常适合下载这种场景。 kafka 对磁盘操作是顺序读写，顺序读写操作要比随机读写速度快很多，再加上使用零拷贝技术所以其吞吐量非常高，主要有 5 点： Cache/Filesystem Cache PageCache缓存 顺序读写 由于现代的操作系统提供了预读和写技术，磁盘的顺序写大多数情况下比随机写内存还要快。 Zero-copy 零拷⻉，少了一次内存交换。 Batching of Messages 批量量处理。合并小的请求，然后以流的方式进行交互，直顶网络上限。 Pull 拉模式 使用拉模式进行消息的获取消费，与消费端处理能力相符。 总结通过使用一些性能命令观测 CPU、内存以及 I/O，可以帮助我们大体猜测性能问题发生的地方。但是它们对于性能问题，只能起到辅助作用，不能精准地定位到真正的性能瓶颈，还需要做更多深入的排查工作，收集更多信息。" }, { "title": "性能优化类别(02)", "url": "/posts/Entry-point-for-performance-optimization/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-03-08 15:33:00 +0000", "snippet": "性能优化根据优化的类别，分为业务优化和技术优化。业务优化产生的效果也是非常大的，但它属于产品和管理的范畴。同作为程序员，在平常工作中，我们面对的优化方式，主要是通过一系列的技术手段，来完成对既定的优化目标。这一系列的技术手段，大体归纳为如图以下 7 类：1 复用优化在写代码的时候，经常会发现有很多重复的代码可以提取出来，做成公共方法。这样，在下次用时，就不用在费劲写一遍。这种思想就是复用。上面描述的是编码逻辑上的优化，对于数据存取来说，有同样复用情况。在软件系统中，谈到数据复用，首先想到就是缓冲和缓存。这两个的意义是完全不同的： 缓冲（Buffer） 常见于对数据的暂存，然后批量传输或者写入 多使用顺序方式，用来缓解设备之间频繁、缓慢地随机写 主要针对的是写操作 缓存（Cache） 常见于对已读数据的复用 通常将它们缓存在相对高速的区域 主要针对的是读操作 类似的，池化是对对象而言，比如数据库连接池、线程池等。在 Java 中使用得非常频繁。由于这些对象的创建和销毁成本比较大，在使用之后，将这部分对象暂时存储，下次使用时就不用再走一遍耗时的初始化操作。2 结果集优化有一个比较直观的例子，都知道 XML 的表现形式是非常好的，那么为什么还有 JSON 呢？出了书写简单一些，一个重要的原因就是它的体积变小了，传输效率和解析效率变高了，像 Google 的 Protobuf，体积就更小了一些。虽然可读性降低了，但在一些高并发场景下（RPC），能够显著提高效率。这就是典型的对结果集优化。由于目前的 Web 服务，都是 C/S 模式。数据从服务器传输到客户端，需要分发多分，这个数据量是急剧膨胀的，每减少一小部分存储，都会有比较大的传属性和成本提升。 像 Nginx ，一般都会开启 GZIP 压缩，使得传输的内容保持紧凑。客户端只需要一小部分计算能力，就可以方便解压，由于这个操作是分散的，因此性能损失是固定的了解上面的道理，就能得出对结果集优化的一般思路： 尽量保持返回数据的精简。一些客户端不需要的字段，那就在代码中，或 SQL 查询中，直接去掉。 对于时效性要求不高，但对处理能力有高要求的业务，吸取缓冲区的经验，尽量减少网络连接的交互，采用批处理的方式，增加处理速度 结果集合很可能会有二次使用，可能会把它加入缓存中，但依然在速度上有所欠缺，此时，就需要对数据集合进行处理优化，采用索引或 Bitmap 位图等方式，加快数据访问速度。3 高效实现在平时编码中，尽量使用设计理念良好、性能优越的组件。比如 有了 Netty ，就不用比较老的 Mina 组件 设计系统时，从性能因素考虑，不要选 SOAP 这样比较耗时的协议 一个好的语法分析器（如 JavaCC），其效率就会比正则表达式高很多总之，如果通过测试分析，找到了系统的瓶颈点，就要把关键的组件，使用更加高效的组件进行替换。在这种情况下，适配器模式是非常重要的。这也是为什么很多公司喜欢在现有的组件之上，再抽象一层自己的（当在底层组件进行切换的时候，上层的应用并无感知）。4 算法优化算法能够显著提高复杂业务性能，但在实际的业务中，往往都是变种。由于存储越来越便宜，在 CPU 非常紧张的业务中，往往采用空间换取时间的方式，来加快处理速度。算法属于代码调优，代码调优涉及很多编码技巧，需要对所使用语言的 API 非常熟悉。对算法、数据结构的灵活使用，是代码优化的一个重要内容。比如，常用的降低时间复杂度的方式，就有递归、二分、排序、动态规划等。一个优秀的实现，对系统的影响是非常大的。比如： 作为 List 的实现，LinkedList 和 ArrayList 在随机访问的性能上，差了好几个数量级； CopyOnWriteList 采用写时复制的方式，可以显著降低读多写少场景下锁冲突。什么时候使用同步，什么时候是线程安全的，对编码能力有较高的要求这部分的知识，就需要在平常的工作中注意积累。5 计算优化5.1 并行执行目前 CPU 发展速度非常快，绝大多数硬件，都是多核。加快某个任务的执行，最快最优解的解决方式，就是并行执行。并行执行有以下三种模式： 多机；采用负载均衡，将流量或者大的计算拆分成多个部分，同时进行处理。比如，Hadoop 通过 MapReduce 的方式，将任务打散，多机同时进行计算； 多进程；比如 Nginx ，采用 NIO 编程模型，Master 统一管理 Worker 进程，然后由 Worker 进程进行真正的请求代理； 多线程；这是 Java 程序员接触最多的。比如 Netty ，采用 Reactor 编程模型，同样使用 NIO ，但它是基于线程的。Boss 线程用来接收请求，然后调度给相应的 Worker 线程进行真正的业务计算像 Golang 这样的语言，有更加轻量级的协程（Coroutine），协程是一种比线程更加轻量级的存在。但目前在 Java 中还不太成熟，但本质上也是对于多核的应用，使得任务并行执行。5.2 变同步为异步再一种对计算的优化，就是变同步为异步，这通常设计编程模型的改变。同步请求，请求会一直阻塞，直到有成功，或者失败结果的返回。虽然它的编程模型简单，但应对突发的、时间段倾斜的浏览，问题就特别大，请求很容易失败。异步操作可以方便地支持横向扩容，也可以缓解瞬时压力，使请求变得平滑。同步请求，就像拳头打在钢板上；异步请求，就像拳头打在海绵上，富有弹性的，体验更友好。5.3 惰性加载最后一种，就是使用常见的设计模式来优化业务，提高体验，比如单例模式、代理模式等。6 资源冲突优化在平常的开发中，会涉及很多共享资源。这些共享资源： 有的是单机的，比如一个 HashMap 有的是外部存储，比如一个数据库行 有的是单个资源，比如 Redis 某个 key 的Setnx 有的是多个资源的协调，比如事务、分布式事务等。现实中的性能问题，和锁相关的问题是非常多的。大多数会想到： 数据库的行锁、表锁、Java 中的各种锁等 在更底层，比如 CPU 命令级别的锁、JVM 指令级别的锁、操作系统内部锁等，可以说无处不在。只有并发，才能产生资源冲突。也就是在同一时刻，只能有一个处理请求能够获取到共享资源。解决资源冲突的方式，就是加锁。再比如事务，在本质上也是一种锁。按照锁级别，锁可分为： 乐观锁 悲观锁，乐观锁在效率上肯定是更高一些；按照锁类型，锁又分为： 公平锁 非公平锁 二者在对任务的调度上，有一些细微的差别。对资源的争用，会造成严重的性能问题，所以会有一些针对无锁队列之类的研究，对性能的提升也是巨大的。7 JVM 优化Java 是运行在 JVM 虚拟机之上，它的诸多特性，就要受到 JVM 的制约。对 JVM 虚拟机进行优化，也能在一定程度上能够提升 JAVA 程序的性能。如果参数配置不当，甚至会造成 OOM 等比较严重的后果。目前被广泛使用的垃圾回收器是 G1，通过很少的参数配置，内存即可高效回收。CMS 垃圾回收器已经在 Java 14 中被移除，由于它的 GC 时间不可控，有条件应该尽量避免使用。JVM 性能调优涉及方方面面的取舍，是牵一发而动全身，需要全盘考虑各方面的影响。了解 JVM 内部的一些运行原理是特别重要的，有益于我们加深对代码更深层次的理解，帮助我们书写出更高效的代码。" }, { "title": "衡量的指标和注意的点(01)", "url": "/posts/Performance-Optimization-Metrics-and-Points-to-Watch/", "categories": "Java, Performance Optimization", "tags": "性能优化, Performance Optimization", "date": "2019-03-02 15:33:00 +0000", "snippet": "所谓性能，就是使用有限的资源在有限的时间内完成工作。在做性能优化时，能够帮助决策的衡量指标有以下五点，分别是： 性能指标 吞吐量，QPS、TPS、HPS 响应速度， Time 响应时间 平均响应时间 AVG 百分位数 Top Percentile 并发量 秒开率 正确性性能指标吞吐量和响应速度分布式的高并发应用不能把单次请求作为判断依据，因为它往往是一个统计结果。最常用的衡量指标就是吞吐量和响应速度，这两者是在考虑性能时非常重要的概念，理解这两个指标的意义，可以类比为现实交通环境的十字路口。 在交通非常繁忙的情况下，十字路口是典型的瓶颈点，当红绿灯放行时间非常长时，后面往往会排起长队。我们开车开始排队，到车经过红绿灯，这个过程所花费的时间，就是响应时间。当然，我们可以适当地调低红绿灯的间隔时间，这样对于某些车辆来说，通过时间可能会短一些。但是，如果信号灯频繁切换，反而会导致单位时间内通过的车辆减少，换一个角度，我们也可以认为这个十字路口的车辆吞吐量减少了。在日常的开发中，经常提到 QPS、TPS 以及 HPS 等这些单词，这些都是常用、并与吞吐量相关的量化指标，其中： QPS 代表每秒查询数量 TPS 代表每秒事务的数量 HPS 代表每秒的 HTTP 请求数量。在做性能优化时候，首先要搞清楚优化的目标，到底是吞吐量还是响应速度。有时，虽然响应速度比较慢，但整个吞吐量却非常的高，比如一些数据库的批量操作、一些缓冲区的合并等。虽然信息的延迟增加了，但如果目标就是吞吐量，那么这显然也可以算是比较大的性能提升。一般情况下认为： 响应速度是串行只有优化，通过优化执行步骤来解决问题 吞吐量是并行执行的优化，通过合理利用计算资源达到目标而平常的优化主要侧重于响应速度，因为一旦响应速度提升了，那么整个吞吐量自然也会跟着提升。但对于高并发的互联网应用，响应速度和吞吐量都很重要，也都需要（因为都会标榜为高吞吐、高并发的场景）用户对系统的延迟忍耐度是非常差的，需要在有限的硬件资源中，找到一个平衡点。响应时间的衡量响应时间（速度）的衡量方法有平均响应时间和百分位数、平均响应时间和百分位数在高稳定性系统中，目标是干掉严重影响系统的长尾请求。这部分接口性能数据的收集，需要采用更加详细的日志记录方式，而不是仅仅依靠指标。比如：将某个接口，耗时超过 1s 的入参及执行步骤，详细地输出在日志系统中。并发量系统同时能处理的请求数量，反映了系统的负载能力。在高并发应用中，仅仅高吞吐是不够的，它还必须同时能为多个用户提供服务。并发高时，会导致很严重的共享资源竞争的问题，因此要做的就是减少资源冲突、以及长时间占用资源的行为。针对响应时间进行设计，一般来说是万能的。因为响应时间减少，同一时间能够处理的请求必然会增加。不过需要注意的是，即使是一个秒杀系统，经过层层过滤处理，最终到到达某个节点的并发数，大概是五六十左右。在平常的设计中，除非并发量特别低，否则都不需要太过度关注这个指标。秒开率秒开是一种极佳的用户体验。如果能在 1s 内加载完成，用户可以获得流畅的体验，并且不会产生更多的焦虑感。通常而言，可以根据业务情况设定不同页面的打开标准，比如低于 1s 内数据占比是秒开率。正确性有次进行测试的时候，发现接口响应非常流畅，把并发数增加到 20 以后，应用接口响应依旧非常迅速。但等应用真正上线时，却发生了重大事故，这是因为接口返回的都是无法使用的数据。其问题原因也比较好定位，就是项目中使用了熔断。在压测的时候，接口直接超出服务能力，触发熔断了，但是压测并没有对接口响应的正确性做判断，造成了非常低级的错误。所以在进行性能评估的时候，不要忘记正确性这一关键要素。方法论性能优化有很多理论方法，比如： 木桶理论 最常用 系统的整体性能，就取决于系统中最慢的组件 基础测试（Benchmark）、预热 最常用 不是简单的性能测试，而是用来测试某个程序的最佳性能 应用接口在刚启动都有短暂的超时，在测试之前，需要对应该进行预热，消除 JIT编译器等因素的影响 Java 里的一个组件 —— JMH ，可以消除这些差异 Amdahl 定律等注意点" }, { "title": "线程池实现“线程复用”的原理", "url": "/posts/thread-pool-reuse/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-18 15:33:00 +0000", "snippet": "线程复用原理线程池会使用固定数量或可变数量的线程来执行任务，但无论是固定数量或可变数量的线程，其线程数量都远远小于任务数量，面对这种情况线程池可以通过线程复用让同一个线程去执行不同的任务，那么线程复用背后的原理是什么呢？线程池可以把线程和任务进行解耦，线程归线程，任务归任务，摆脱了之前通过 Thread 创建线程时的一个线程必须对应一个任务的限制。在线程池中，同一个线程可以从 BlockingQueue 中不断提取新任务来执行，其核心原理在于线程池对 Thread 进行了封装，并不是每次执行任务都会调用 Thread.start() 来创建新线程，而是让每个线程去执行一个“循环任务”，在这个“循环任务”中，不停地检查是否还有任务等待被执行，如果有则直接去执行这个任务，也就是调用任务的 run 方法，把 run 方法当作和普通方法一样的地位去调用，相当于把每个任务的 run() 方法串联了起来，所以线程数量并不增加。线程池创建新线程的时机和规则：如流程图所示，当提交任务后，线程池首先会检查当前线程数，如果此时线程数小于核心线程数，比如最开始线程数量为 0，则新建线程并执行任务，随着任务的不断增加，线程数会逐渐增加并达到核心线程数，此时如果仍有任务被不断提交，就会被放入 workQueue 任务队列中，等待核心线程执行完当前任务后重新从 workQueue 中提取正在等待被执行的任务。此时，假设我们的任务特别的多，已经达到了 workQueue 的容量上限，这时线程池就会启动后备力量，也就是 maxPoolSize 最大线程数，线程池会在 corePoolSize 核心线程数的基础上继续创建线程来执行任务，假设任务被不断提交，线程池会持续创建线程直到线程数达到 maxPoolSize 最大线程数，如果依然有任务被提交，这就超过了线程池的最大处理能力，这个时候线程池就会拒绝这些任务，我们可以看到实际上任务进来之后，线程池会逐一判断 corePoolSize 、workQueue 、maxPoolSize ，如果依然不能满足需求，则会拒绝任务。具体实现的代码，从 execute 方法开始分析，源码如下所示。public void execute(Runnable command) {     if (command == null)         throw new NullPointerException();    int c = ctl.get();    if (workerCountOf(c) &amp;lt; corePoolSize) {         if (addWorker(command, true))             return;        c = ctl.get();    }     if (isRunning(c) &amp;amp;&amp;amp; workQueue.offer(command)) {         int recheck = ctl.get();        if (! isRunning(recheck) &amp;amp;&amp;amp; remove(command))             reject(command);        else if (workerCountOf(recheck) == 0)             addWorker(null, false);    }     else if (!addWorker(command, false))         reject(command);}线程复用源码解析这段代码短小精悍，内容丰富，首先看下前几行：//如果传入的Runnable的空，就抛出异常if (command == null)     throw new NullPointerException();execute 方法中通过 if 语句判断 command ，也就是 Runnable 任务是否等于 null，如果为 null 就抛出异常。接下来判断当前线程数是否小于核心线程数，如果小于核心线程数就调用 addWorker() 方法增加一个 Worker，这里的 Worker 就可以理解为一个线程：if (workerCountOf(c) &amp;lt; corePoolSize) {     if (addWorker(command, true))         return;        c = ctl.get();}addWorker 方法的主要作用是在线程池中创建一个线程并执行第一个参数传入的任务，它的第二个参数是个布尔值，如果布尔值传入 true 代表增加线程时判断当前线程是否少于 corePoolSize，小于则增加新线程，大于等于则不增加；同理，如果传入 false 代表增加线程时判断当前线程是否少于 maxPoolSize，小于则增加新线程，大于等于则不增加，所以这里的布尔值的含义是以核心线程数为界限还是以最大线程数为界限进行是否新增线程的判断。addWorker() 方法如果返回 true 代表添加成功，如果返回 false 代表添加失败。我们接下来看下一部分代码：if (isRunning(c) &amp;amp;&amp;amp; workQueue.offer(command)) {     int recheck = ctl.get();    if (! isRunning(recheck) &amp;amp;&amp;amp; remove(command))         reject(command);    else if (workerCountOf(recheck) == 0)         addWorker(null, false);}如果代码执行到这里，说明当前线程数大于或等于核心线程数或者 addWorker 失败了，那么就需要通过 if (isRunning(c) &amp;amp;&amp;amp; workQueue.offer(command)) 检查线程池状态是否为 Running，如果线程池状态是 Running 就把任务放入任务队列中，也就是 workQueue.offer(command)。如果线程池已经不处于 Running 状态，说明线程池被关闭，那么就移除刚刚添加到任务队列中的任务，并执行拒绝策略，代码如下所示：if (! isRunning(recheck) &amp;amp;&amp;amp; remove(command))     reject(command);下面我们再来看后一个 else 分支：else if (workerCountOf(recheck) == 0)     addWorker(null, false);能进入这个 else 说明前面判断到线程池状态为 Running，那么当任务被添加进来之后就需要防止没有可执行线程的情况发生（比如之前的线程被回收了或意外终止了），所以此时如果检查当前线程数为 0，也就是 workerCountOf(recheck**) == 0，那就执行 addWorker() 方法新建线程。最后一部分代码：else if (!addWorker(command, false))     reject(command);执行到这里，说明线程池不是 Running 状态或线程数大于或等于核心线程数并且任务队列已经满了，根据规则，此时需要添加新线程，直到线程数达到“最大线程数”，所以此时就会再次调用 addWorker 方法并将第二个参数传入 false，传入 false 代表增加线程时判断当前线程数是否少于 maxPoolSize，小于则增加新线程，大于等于则不增加，也就是以 maxPoolSize 为上限创建新的 worker；addWorker 方法如果返回 true 代表添加成功，如果返回 false 代表任务添加失败，说明当前线程数已经达到 maxPoolSize，然后执行拒绝策略 reject 方法。如果执行到这里线程池的状态不是 Running，那么 addWorker 会失败并返回 false，所以也会执行拒绝策略 reject 方法。可以看出，在 execute 方法中，多次调用 addWorker 方法把任务传入，addWorker 方法会添加并启动一个 Worker，这里的 Worker 可以理解为是对 Thread 的包装，Worker 内部有一个 Thread 对象，它正是最终真正执行任务的线程，所以一个 Worker 就对应线程池中的一个线程，addWorker 就代表增加线程。线程复用的逻辑实现主要在 Worker 类中的 run 方法里执行的 runWorker 方法中，简化后的 runWorker 方法代码如下所示。runWorker(Worker w) {    Runnable task = w.firstTask;    while (task != null || (task = getTask()) != null) {        try {            task.run();        } finally {            task = null;        }    }}可以看出，实现线程复用的逻辑主要在一个不停循环的 while 循环体中。 通过取 Worker 的 firstTask 或者通过 getTask 方法从 workQueue 中获取待执行的任务。 直接调用 task 的 run 方法来执行具体的任务（而不是新建线程）。在这里，找到了最终的实现，通过取 Worker 的 firstTask 或者 getTask方法从 workQueue 中取出了新任务，并直接调用 Runnable 的 run 方法来执行任务，也就是如之前所说的，每个线程都始终在一个大循环中，反复获取任务，然后执行任务，从而实现了线程的复用。 为什么外面已经放入队列了，还要再检查一次呢？if (! isRunning(recheck) —— 如果任务可以成功排队，那么仍然需要仔细检查是否应该添加一个线程（因为现有线程可能自上次检查以来已死亡），或者在进入此方法后，线程池可能关闭。因此，需要重新检查状态。 线程池中的多余的线程是怎么回收的？ —— 在processWorkerExit(Worker w, boolean completedAbruptly)方法里会调用tryTerminate()，向任意空闲线程发出中断信号。所有被阻塞的线程，最终都会被一个个唤醒，回收。 一般情况我们的Ruanable任务是包在Thread中随着线程start，任务开始run的；那么在线程池中，虽然Worker封装了任务和线程，但是好像任务并未包在线程中，那么这个任务到底是什么时候触发执行的？ —— 任务是在“提交任务”的时候被传入的，然后工作线程便可以拿到任务并执行。 核心线程数为16，最大线程数为30，队列大小为20，那么我这个线程池最大可以接受多少线程数量，添加多少个线程会执行拒绝策略 —— 就是当线程数达到30，同时队列满了，再来任务就会拒绝了。 为什么这里的task要重新赋值呢？重新赋值的条件是什么呢？ while (task != null   (task = getTask()) != null) { - 如果w.firstTask为空，那么就要调用getTask()去获取任务，也就是重新赋值。 最大线程满了后，核心线程、最大线程 处理完了任务 是在队列中获取任务吗？- 是这样的" }, { "title": "正确关闭线程池的方式", "url": "/posts/thread-pool-closeure/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-17 15:33:00 +0000", "snippet": "首先，创建一个线程数固定为 10 的线程池，并且往线程池中提交 100 个任务，如代码所示。ExecutorService service = Executors.newFixedThreadPool(10);for (int i = 0; i &amp;lt; 100; i++) {    service.execute(new Task());}那么如果想关闭该线程池该如何做呢？ 有 5 种在 ThreadPoolExecutor 中涉及关闭线程池的方法，如下所示。 void shutdown; boolean isShutdown; boolean isTerminated; boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; List shutdownNow;shutdown()第一种方法叫作 shutdown()，它可以安全地关闭一个线程池，调用 shutdown() 方法之后线程池并不是立刻就被关闭，因为这时线程池中可能还有很多任务正在被执行，或是任务队列中有大量正在等待被执行的任务，调用 shutdown() 方法后线程池会在执行完正在执行的任务和队列中等待的任务后才彻底关闭。但这并不代表 shutdown() 操作是没有任何效果的，调用 shutdown() 方法后如果还有新的任务被提交，线程池则会根据拒绝策略直接拒绝后续新提交的任务。isShutdown()第二个方法叫作 isShutdown()，它可以返回 true 或者 false 来判断线程池是否已经开始了关闭工作，也就是是否执行了 shutdown 或者 shutdownNow 方法。这里需要注意，如果调用 isShutdown() 方法的返回的结果为 true 并不代表线程池此时已经彻底关闭了，这仅仅代表线程池开始了关闭的流程，也就是说，此时可能线程池中依然有线程在执行任务，队列里也可能有等待被执行的任务。isTerminated()第三种方法叫作 isTerminated()，这个方法可以检测线程池是否真正“终结”了，这不仅代表线程池已关闭，同时代表线程池中的所有任务都已经都执行完毕了，因为我们刚才说过，调用 shutdown 方法之后，线程池会继续执行里面未完成的任务，不仅包括线程正在执行的任务，还包括正在任务队列中等待的任务。比如此时已经调用了 shutdown 方法，但是有一个线程依然在执行任务，那么此时调用 isShutdown 方法返回的是 true ，而调用 isTerminated 方法返回的便是 false ，因为线程池中还有任务正在在被执行，线程池并没有真正“终结”。直到所有任务都执行完毕了，调用 isTerminated() 方法才会返回 true，这表示线程池已关闭并且线程池内部是空的，所有剩余的任务都执行完毕了。awaitTermination()第四个方法叫作 awaitTermination()，它本身并不是用来关闭线程池的，而是主要用来判断线程池状态的。比如我们给 awaitTermination 方法传入的参数是 10 秒，那么它就会陷入 10 秒钟的等待，直到发生以下三种情况之一： 等待期间（包括进入等待状态之前）线程池已关闭并且所有已提交的任务（包括正在执行的和队列中等待的）都执行完毕，相当于线程池已经“终结”了，方法便会返回 true； 等待超时时间到后，第一种线程池“终结”的情况始终未发生，方法返回 false； 等待期间线程被中断，方法会抛出 InterruptedException 异常。也就是说，调用 awaitTermination 方法后当前线程会尝试等待一段指定的时间，如果在等待时间内，线程池已关闭并且内部的任务都执行完毕了，也就是说线程池真正“终结”了，那么方法就返回 true，否则超时返回 fasle。我们则可以根据 awaitTermination() 返回的布尔值来判断下一步应该执行的操作。shutdownNow()最后一个方法是 shutdownNow()，也是 5 种方法里功能最强大的，它与第一种 shutdown 方法不同之处在于名字中多了一个单词 Now，也就是表示立刻关闭的意思。在执行 shutdownNow 方法之后，首先会给所有线程池中的线程发送 interrupt 中断信号，尝试中断这些任务的执行，然后会将任务队列中正在等待的所有任务转移到一个 List 中并返回，我们可以根据返回的任务 List 来进行一些补救的操作，例如记录在案并在后期重试。shutdownNow() 的源码如下所示。public List&amp;lt;Runnable&amp;gt; shutdownNow() {     List&amp;lt;Runnable&amp;gt; tasks;    final ReentrantLock mainLock = this.mainLock;    mainLock.lock();    try {         checkShutdownAccess();        advanceRunState(STOP);        interruptWorkers();        tasks = drainQueue();    } finally {         mainLock.unlock();    }     tryTerminate();    return tasks; }你可以看到源码中有一行 interruptWorkers() 代码，这行代码会让每一个已经启动的线程都中断，这样线程就可以在执行任务期间检测到中断信号并进行相应的处理，提前结束任务。这里需要注意的是，由于 Java 中不推荐强行停止线程的机制的限制，即便我们调用了 shutdownNow 方法，如果被中断的线程对于中断信号不理不睬，那么依然有可能导致任务不会停止。可见我们在开发中落地最佳实践是很重要的，我们自己编写的线程应当具有响应中断信号的能力，正确停止线程的方法在第 2 讲有讲过，应当利用中断信号来协同工作。在掌握了这 5 种关闭线程池相关的方法之后，我们就可以根据自己的业务需要，选择合适的方法来停止线程池，比如通常我们可以用 shutdown() 方法来关闭，这样可以让已提交的任务都执行完毕，但是如果情况紧急，那我们就可以用 shutdownNow 方法来加快线程池“终结”的速度。 如果只是想中断线程池某个任务呢？这个要怎么操作 ？ - 可以用Future的cancel方法。 在实际项目中,模块全局单例线程池可以不关闭吧?关闭之后再次调用,任务提交后并不执行呀,只执行第一次调用时提交的任务啊! —— 可以不关闭。 什么情况下需要关闭,线程池不是一直存在的吗 —— 线程池不关闭的话，里面的线程会一直存在。在使用完线程池后，可以选择关闭。 想问下”&amp;gt;isShutdownisTerminated 这2个方法只是获取状态 还是由关闭的功能？？ 看源码好像并没有关闭功能 只是做了一些判断 —— 是的，没有关闭，只是判断。" }, { "title": "根据实际需要自定义线程池", "url": "/posts/custom-thread-pool/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-16 15:33:00 +0000", "snippet": "核心线程数第一个需要设置的参数往往是 corePoolSize 核心线程数，在上一课时我们讲过，合理的线程数量和任务类型，以及 CPU 核心数都有关系，基本结论是线程的平均工作时间所占比例越高，就需要越少的线程；线程的平均等待时间所占比例越高，就需要越多的线程。而对于最大线程数而言，如果我们执行的任务类型不是固定的，比如可能一段时间是 CPU 密集型，另一段时间是 IO 密集型，或是同时有两种任务相互混搭。那么在这种情况下，我们可以把最大线程数设置成核心线程数的几倍，以便应对任务突发情况。当然更好的办法是用不同的线程池执行不同类型的任务，让任务按照类型区分开，而不是混杂在一起，这样就可以按照上一课时估算的线程数或经过压测得到的结果来设置合理的线程数了，达到更好的性能。阻塞队列对于阻塞队列这个参数而言，我们可以选择之前介绍过的 LinkedBlockingQueue 或者 SynchronousQueue 或者 DelayedWorkQueue，不过还有一种常用的阻塞队列叫 ArrayBlockingQueue，它也经常被用于线程池中，这种阻塞队列内部是用数组实现的，在新建对象的时候要求传入容量值，且后期不能扩容，所以 ArrayBlockingQueue 的最大的特点就是容量是有限的。这样一来，如果任务队列放满了任务，而且线程数也已经达到了最大值，线程池根据规则就会拒绝新提交的任务，这样一来就可能会产生一定的数据丢失。但相比于无限增加任务或者线程数导致内存不足，进而导致程序崩溃，数据丢失还是要更好一些的，如果我们使用了 ArrayBlockingQueue 这种阻塞队列，再加上我们限制了最大线程数量，就可以非常有效地防止资源耗尽的情况发生。此时的队列容量大小和 maxPoolSize 是一个 trade-off，如果我们使用容量更大的队列和更小的最大线程数，就可以减少上下文切换带来的开销，但也可能因此降低整体的吞吐量；如果我们的任务是 IO 密集型，则可以选择稍小容量的队列和更大的最大线程数，这样整体的效率就会更高，不过也会带来更多的上下文切换。线程工厂对于线程工厂 threadFactory 这个参数，我们可以使用默认的 defaultThreadFactory，也可以传入自定义的有额外能力的线程工厂，因为我们可能有多个线程池，而不同的线程池之间有必要通过不同的名字来进行区分，所以可以传入能根据业务信息进行命名的线程工厂，以便后续可以根据线程名区分不同的业务进而快速定位问题代码。比如可以通过com.google.common.util.concurrent.ThreadFactoryBuilder 来实现，如代码所示。ThreadFactoryBuilder builder = new ThreadFactoryBuilder();ThreadFactory rpcFactory = builder.setNameFormat(&quot;rpc-pool-%d&quot;).build();最后一个参数是拒绝策略，我们可以根据业务需要，选择第 11 讲里的四种拒绝策略之一来使用：AbortPolicy，DiscardPolicy，DiscardOldestPolicy 或者 CallerRunsPolicy。除此之外，我们还可以通过实现 RejectedExecutionHandler 接口来实现自己的拒绝策略，在接口中我们需要实现 rejectedExecution 方法，在 rejectedExecution 方法中，执行例如打印日志、暂存任务、重新执行等自定义的拒绝策略，以便满足业务需求。如代码所示。拒绝策略最后一个参数是拒绝策略，我们可以根据业务需要，选择第 11 讲里的四种拒绝策略之一来使用：AbortPolicy，DiscardPolicy，DiscardOldestPolicy 或者 CallerRunsPolicy。除此之外，我们还可以通过实现 RejectedExecutionHandler 接口来实现自己的拒绝策略，在接口中我们需要实现 rejectedExecution 方法，在 rejectedExecution 方法中，执行例如打印日志、暂存任务、重新执行等自定义的拒绝策略，以便满足业务需求。如代码所示。private static class CustomRejectionHandler implements RejectedExecutionHandler {     @Override    public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) {         //打印日志、暂存任务、重新执行等拒绝策略    } }总结所以定制自己的线程池和我们的业务是强相关的，首先我们需要掌握每个参数的含义，以及常见的选项，然后根据实际需要，比如说并发量、内存大小、是否接受任务被拒绝等一系列因素去定制一个非常适合自己业务的线程池，这样既不会导致内存不足，同时又可以用合适数量的线程来保障任务执行的效率，并在拒绝任务时有所记录方便日后进行追溯。 假如一个项目有3个不同线程池，那每个线程池的核心数怎么估算 —— 这种较难平衡，要看各线程池的使用频率，也需要根据实际情况不断调整，很难提前给出准确的预判。 现在系统有A和B两个业务场景在同一个服务中，A业务并发量较高，B类较低且非核心业务，两种业务都需要使用线程池，这种可以使用一个线程池吗？如果使用两个不同线程池核心线程数需不需要按并发量进行调整估算？ —— 最好不要用同一个线程池，应该按业务拆分。使用 JMeter 等都可以。" }, { "title": "合适的线程数量以及 CPU 核心数与线程数的关系", "url": "/posts/threadno-cpu/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-15 15:33:00 +0000", "snippet": "调整线程池中的线程数量的最主要的目的是：为了充分并合理地使用 CPU 和内存等资源，从而最大限度地提高程序的性能。在实际工作中，需要根据任务类型的不同选择对应的策略。CPU 密集型任务CPU 密集型任务，比如加密、解密、压缩、计算等一系列需要大量耗费 CPU 资源的任务。对于这样的任务最佳的线程数为 CPU 核心数的 1~2 倍，如果设置过多的线程数，并不会起到很好的效果。假设设置的线程数量是 CPU 核心数的 2 倍以上，因为计算任务非常重，会占用大量的 CPU 资源，所以这时 CPU 的每个核心工作基本都是满负荷的，而我们又设置了过多的线程，每个线程都想去利用 CPU 资源来执行自己的任务，这就会造成不必要的上下文切换，此时线程数的增多并没有让性能提升，反而由于线程数量过多会导致性能下降。针对这种情况，最好同时考虑在同一台机器上还有哪些其他会占用过多 CPU 资源的程序在运行，然后对资源使用做整体的平衡。耗时 IO 型任务*耗时 IO 型，比如数据库、文件的读写，网络通信等任务，这种任务的特点是并不会特别消耗 CPU 资源，但是 IO 操作很耗时，总体会占用比较多的时间。对于这种任务，最大线程数一般会大于 CPU 核心数很多倍，因为 IO 读写速度相比于 CPU 的速度而言是比较慢的。如果设置过少的线程数，就可能导致 CPU 资源的浪费。如果设置更多的线程数，那么当一部分线程正在等待 IO 的时候，它们此时并不需要 CPU 来计算，那么另外的线程便可以利用 CPU 去执行其他的任务，互不影响，这样的话在任务队列中等待的任务就会减少，可以更好地利用资源。计算方法《Java并发编程实战》的作者 Brain Goetz 推荐的计算方法：\\(线程数 = CPU 核心数 * （1 + 平均等待时间/平均工作时间）\\) 如何得出平均工作时间和平均等待时间呢 —— 上线后，可以通过写代码等办法统计到各部分语句的运行时长。 可以使用 JDK 自带的工具 VisualVM 来查看线程等待时间和线程工作时间通过这个公式，可以计算出一个合理的线程数量，如果任务的平均等待时间长，线程数就随之增加，而如果平均工作时间长，也就是 CPU 密集型任务，线程数就随之减少。 合适的线程数量，corePoolSize 和 maxPoolSize 都涉及到，不过主要指 corePoolSize。太少的线程数会使得程序整体性能降低，而过多的线程也会消耗内存等其他资源，所以如果想要更准确的话，可以进行压测，监控 JVM 的线程情况以及 CPU 的负载情况，根据实际情况衡量应该创建的线程数，合理并充分利用资源。结论综上所述可以得出一个结论： 线程的平均工作时间所占比例越高，就需要越少的线程； 线程的平均等待时间所占比例越高，就需要越多的线程； 针对不同的程序，进行对应的实际测试就可以得到最合适的选择。" }, { "title": "自动创建线程池的缺点", "url": "/posts/thread-pools-self-built-fault/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-14 15:33:00 +0000", "snippet": "所谓的自动创建线程池就是直接调用 Executors 的各种方法来生成前面学过的常见的线程池，例如 Executors.newCachedThreadPool()。但这样做是有一定风险的。FixedThreadPool看第一种线程池 FixedThreadPool， 它是线程数量固定的线程池，如源码所示，newFixedThreadPool 内部实际还是调用了 ThreadPoolExecutor 构造函数。public static ExecutorService newFixedThreadPool(int nThreads) {     return new ThreadPoolExecutor(nThreads, nThreads,0L, TimeUnit.MILLISECONDS,new LinkedBlockingQueue&amp;lt;Runnable&amp;gt;());}通过往构造函数中传参，创建了一个核心线程数和最大线程数相等的线程池，它们的数量也就是我们传入的参数，这里的重点是使用的队列是容量没有上限的 LinkedBlockingQueue，如果我们对任务的处理速度比较慢，那么随着请求的增多，队列中堆积的任务也会越来越多，最终大量堆积的任务会占用大量内存，并发生 OOM ，也就是OutOfMemoryError，这几乎会影响到整个程序，会造成很严重的后果。SingleThreadExecutor第二种线程池是 SingleThreadExecutor，分析下创建它的源码。public static ExecutorService newSingleThreadExecutor() {     return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1,0L, TimeUnit.MILLISECONDS,new LinkedBlockingQueue&amp;lt;Runnable&amp;gt;()));}可以看出，newSingleThreadExecutor 和 newFixedThreadPool 的原理是一样的，只不过把核心线程数和最大线程数都直接设置成了 1，但是任务队列仍是无界的 LinkedBlockingQueue，所以也会导致同样的问题，也就是当任务堆积时，可能会占用大量的内存并导致 OOM。CachedThreadPool第三种线程池是 CachedThreadPool，创建它的源码下所示。public static ExecutorService newCachedThreadPool() {     return new ThreadPoolExecutor(0, Integer.MAX_VALUE,60L, TimeUnit.SECONDS,new SynchronousQueue&amp;lt;Runnable&amp;gt;());}这里的 CachedThreadPool 和前面两种线程池不一样的地方在于任务队列使用的是 SynchronousQueue，SynchronousQueue 本身并不存储任务，而是对任务直接进行转发，这本身是没有问题的，但你会发现构造函数的第二个参数被设置成了 Integer.MAX_VALUE，这个参数的含义是最大线程数，所以由于 CachedThreadPool 并不限制线程的数量，当任务数量特别多的时候，就可能会导致创建非常多的线程，最终超过了操作系统的上限而无法创建新线程，或者导致内存不足。ScheduledThreadPool 和 SingleThreadScheduledExecutor第四种线程池 ScheduledThreadPool 和第五种线程池 SingleThreadScheduledExecutor 的原理是一样的，创建 ScheduledThreadPool 的源码如下所示。public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) {     return new ScheduledThreadPoolExecutor(corePoolSize);}而这里的 ScheduledThreadPoolExecutor 是 ThreadPoolExecutor 的子类，调用的它的构造方法如下所示。public ScheduledThreadPoolExecutor(int corePoolSize) {     super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS,new DelayedWorkQueue());}我们通过源码可以看出，它采用的任务队列是 DelayedWorkQueue，这是一个延迟队列，同时也是一个无界队列，所以和 LinkedBlockingQueue 一样，如果队列中存放过多的任务，就可能导致 OOM。你可以看到，这几种自动创建的线程池都存在风险，相比较而言，我们自己手动创建会更好，因为我们可以更加明确线程池的运行规则，不仅可以选择适合自己的线程数量，更可以在必要的时候拒绝新任务的提交，避免资源耗尽的风险。 其实第一次接触线程池的时候就很不明白这个问题，如果不适用，那么为什么会存在这些线程池呢？风险肯定有，但是按需使用不就好了吗？不是很多技术都是“鱼和熊掌”的问题吗？为什么这里要刻意强调自己手动创建？手动创建根本好处在于每个参数都可以自定义吗？可以鱼和熊掌尽收吗？真正开发的时定制有这么严苛吗？—— 手动创建根本好处在于每个参数都可以自定义，这样至少在定义参数的时候，能根据业务情况进行选择，避免了“压根不知道这里有风险” Executors特别好用……是挺好用的，就是不算十分安全。" }, { "title": "线程池常用的阻塞队列", "url": "/posts/thread-pool-blocking/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-13 15:33:00 +0000", "snippet": "线程池内部结构线程池的内部结构主要由四部分组成，如图所示。 第一部分是线程池管理器，它主要负责管理线程池的创建、销毁、添加任务等管理操作，它是整个线程池的管家。 第二部分是工作线程，也就是图中的线程 t0~t9，这些线程勤勤恳恳地从任务队列中获取任务并执行。 第三部分是任务队列，作为一种缓冲机制，线程池会把当下没有处理的任务放入任务队列中，由于多线程同时从任务队列中获取任务是并发场景，此时就需要任务队列满足线程安全的要求，所以线程池中任务队列采用 BlockingQueue 来保障线程安全。 第四部分是任务，任务要求实现统一的接口，以便工作线程可以处理和执行。阻塞队列 FixedThreadPool LinkedBlockingQueue SingleThreadExecutor LinkedBlockingQueue CachedThreadPool SynchronousQueue ScheduledThreadPool DelayedWorkQueue SingleThreadScheduledExector DelayedWorkQueue 线程池中的这四个主要组成部分最值得我们关注的就是阻塞队列了，如表格所示，不同的线程池会选用不同的阻塞队列。表格左侧是线程池，右侧为它们对应的阻塞队列，你可以看到 5 种线程池对应了 3 种阻塞队列，我们接下来对它们进行逐一的介绍。LinkedBlockingQueue对于 FixedThreadPool 和 SingleThreadExector 而言，它们使用的阻塞队列是容量为 Integer.MAX_VALUE 的 LinkedBlockingQueue，可以认为是无界队列。由于 FixedThreadPool 线程池的线程数是固定的，所以没有办法增加特别多的线程来处理任务，这时就需要 LinkedBlockingQueue 这样一个没有容量限制的阻塞队列来存放任务。这里需要注意，由于线程池的任务队列永远不会放满，所以线程池只会创建核心线程数量的线程，所以此时的最大线程数对线程池来说没有意义，因为并不会触发生成多于核心线程数的线程。LinkedBlockingQueue对于 FixedThreadPool 和 SingleThreadExector 而言，它们使用的阻塞队列是容量为 Integer.MAX_VALUE 的 LinkedBlockingQueue，可以认为是无界队列。由于 FixedThreadPool 线程池的线程数是固定的，所以没有办法增加特别多的线程来处理任务，这时就需要 LinkedBlockingQueue 这样一个没有容量限制的阻塞队列来存放任务。这里需要注意，由于线程池的任务队列永远不会放满，所以线程池只会创建核心线程数量的线程，所以此时的最大线程数对线程池来说没有意义，因为并不会触发生成多于核心线程数的线程。SynchronousQueue第二种阻塞队列是 SynchronousQueue，对应的线程池是 CachedThreadPool。线程池 CachedThreadPool 的最大线程数是 Integer 的最大值，可以理解为线程数是可以无限扩展的。CachedThreadPool 和上一种线程池 FixedThreadPool 的情况恰恰相反，FixedThreadPool 的情况是阻塞队列的容量是无限的，而这里 CachedThreadPool 是线程数可以无限扩展，所以 CachedThreadPool 线程池并不需要一个任务队列来存储任务，因为一旦有任务被提交就直接转发给线程或者创建新线程来执行，而不需要另外保存它们。 自己创建使用 SynchronousQueue 的线程池时，如果不希望任务被拒绝，那么就需要注意设置最大线程数要尽可能大一些，以免发生任务数大于最大线程数时，没办法把任务放到队列中也没有足够线程来执行任务的情况DelayedWorkQueue第三种阻塞队列是DelayedWorkQueue，它对应的线程池分别是 ScheduledThreadPool 和 SingleThreadScheduledExecutor，这两种线程池的最大特点就是可以延迟执行任务，比如说一定时间后执行任务或是每隔一定的时间执行一次任务。DelayedWorkQueue 的特点是内部元素并不是按照放入的时间排序，而是会按照延迟的时间长短对任务进行排序，内部采用的是“堆”的数据结构。之所以线程池 ScheduledThreadPool 和 SingleThreadScheduledExecutor 选择 DelayedWorkQueue，是因为它们本身正是基于时间执行任务的，而延迟队列正好可以把任务按时间进行排序，方便任务的执行。 ForkJoinPool用的是哪一种？它自己及fork出来的子线程的队列都没有上限吗？ 用的是 ForkJoinPoo 的内部类 WorkQueue，有上限，上限在源码中规定了： static final int MAXIMUM_QUEUE_CAPACITY = 1 « 26; // 64M SynchronousQueue队列的长队始终为0，那么队列既然始终为零，为啥还要这个队列存在呢？ 直接把任务递交，效率较高。" }, { "title": "六种常见的线程池", "url": "/posts/thread-pool-common/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-12 15:33:00 +0000", "snippet": "常见的线程池如下： FixedThreadPool CachedThreadPool ScheduledThreadPool SingleThreadExecutor SingleThreadScheduledExecutor ForkJoinPoolFixedThreadPool 核心线程数和最大线程数一样，可以把它看作是固定线程数的线程池。特点： 线程池中的线程数除了初始阶段需要从 0 开始增加外，之后的线程数量就是固定的； 就算任务数超过线程数，线程池也不会再创建更多的线程来处理任务，而是会把超出线程处理能力的任务放到任务队列中进行等待； 就算任务队列满了，到了本该继续增加线程数的时候，由于它的最大线程数和核心线程数是一样的，所以也无法再增加新的线程了。如图所示，线程池有 t0 ~ t9，10 个线程，它们会不停地执行任务，如果某个线程任务执行完了，就会从任务队列中获取新的任务继续执行，期间线程数量不会增加也不会减少，始终保持在 10 个。CachedThreadPool这个线程池可以称作可缓存线程池，特点： 线程数是几乎可以无限增加的（实际最大可以达到 Integer.MAX_VALUE，为 2^31-1，这个数非常大，所以基本不可能达到）； 线程闲置时还可以对线程进行回收，也就是说该线程池的线程数量是一直改变的； 它也有一个用于存储提交任务的队列，但这个队列是 SynchronousQueue，队列的容量为0，实际不存储任何任务，只负责对任务进行中转和传递，所以效率比较高。当提交一个任务后，线程池会判断已创建的线程中是否有空闲线程，如果有空闲线程则将任务直接指派给空闲线程，如果没有空闲线程，则新建线程去执行任务，这样就做到了动态地新增线程。如下方代码所示。ExecutorService service = Executors.newCachedThreadPool();    for (int i = 0; i &amp;lt; 1000; i++) {         service.execute(new Task() {     }); }使用 for 循环提交 1000 个任务给 CachedThreadPool，假设这些任务处理的时间非常长，会发生什么情况呢？因为 for 循环提交任务的操作是非常快的，但执行任务却比较耗时，就可能导致 1000 个任务都提交完了但第一个任务还没有被执行完，所以此时 CachedThreadPool 就可以动态的伸缩线程数量，随着任务的提交，不停地创建 1000 个线程来执行任务，而当任务执行完之后，假设没有新的任务了，那么大量的闲置线程又会造成内存资源的浪费，这时线程池就会检测线程在 60 秒内有没有可执行任务，如果没有就会被销毁，最终线程数量会减为 0。ScheduledThreadPool支持定时或周期性执行任务。比如每隔 10 秒钟执行一次任务，而实现这种功能的方法主要有 3 种，如下代码所示：ScheduledExecutorService service = Executors.newScheduledThreadPool(10);service.schedule(new Task(), 10, TimeUnit.SECONDS);service.scheduleAtFixedRate(new Task(), 10, 10, TimeUnit.SECONDS);service.scheduleWithFixedDelay(new Task(), 10, 10, TimeUnit.SECONDS);这三种方法的区别如下： 第一种方法， schedule 比较简单，表示延迟指定时间后执行一次任务，如果代码中设置参数为 10 秒，也就是 10 秒后执行一次任务后就结束。 第二种方法， scheduleAtFixedRate 表示以固定的频率执行任务，它的第二个参数 initialDelay 表示第一次延时时间，第三个参数 period 表示周期，也就是第一次延时后每次延时多长时间执行一次任务。 第三种方法， scheduleWithFixedDelay 与第二种方法类似，也是周期执行任务，区别在于对周期的定义，之前的 scheduleAtFixedRate 是以任务开始的时间为时间起点开始计时，时间到就开始执行第二次任务，而不管任务需要花多久执行；而 scheduleWithFixedDelay 方法以任务结束的时间为下一次循环的时间起点开始计时。举个例子，假设某个同学正在熬夜写代码，需要喝咖啡来提神，假设每次喝咖啡都需要花10分钟的时间，如果此时采用第 2 种方法 scheduleAtFixedRate，时间间隔设置为 1 小时，那么他将会在每个整点喝一杯咖啡，以下是时间表： 00:00 - 开始喝咖啡 00:10 - 喝完了 01:00 - 开始喝咖啡 01:10 - 喝完了 02:00 - 开始喝咖啡 02:10 - 喝完了但是假设他采用第3种方法 scheduleWithFixedDelay，时间间隔同样设置为 1 小时，那么由于每次喝咖啡需要10分钟，而 scheduleWithFixedDelay 是以任务完成的时间为时间起点开始计时的，所以第2次喝咖啡的时间将会在1:10，而不是1:00整，以下是时间表： 00:00: 开始喝咖啡 00:10: 喝完了 01:10: 开始喝咖啡 01:20: 喝完了 02:20: 开始喝咖啡 02:30: 喝完了SingleThreadExecutor该线程池会使用唯一的线程去执行任务，原理和 FixedThreadPool 是一样的，只不过这里线程只有一个，如果线程在执行任务的过程中发生异常，线程池也会重新创建一个线程来执行后续的任务。这种线程池由于只有一个线程，所以非常适合用于所有任务都需要按被提交的顺序依次执行的场景，而前几种线程池不一定能够保障任务的执行顺序等于被提交的顺序，因为它们是多线程并行执行的。ingleThreadScheduledExecutor第五个线程池是 SingleThreadScheduledExecutor，它和第三种 ScheduledThreadPool 线程池非常相似，是 ScheduledThreadPool 的一个特例，内部只有一个线程，如源码所示：new ScheduledThreadPoolExecutor(1)它只是将 ScheduledThreadPool 的核心线程数设置为了 1。 参数 FixedThreadPool CachedThreadPool ScheduledThreadPool SingleThreadExceutor SingleThreadScheduledExecutor corePoolSize 构造函数传入 0 构造函数传入 1 1 maxPoolSize 同 corePoolSize Integer.MAX_VALUE Integer.MAX_VALUE 1 Integer.MAX_VALUE keepAliveTime 0 60 秒 0 0 0 总结上述的五种线程池，以核心线程数 corePoolSize、最大线程数 maxPoolSize，以及线程存活时间 keepAliveTime 三个维度进行对比，如表格所示。 第一个线程池 FixedThreadPool，它的核心线程数和最大线程数都是由构造函数直接传参的，而且它们的值是相等的，所以最大线程数不会超过核心线程数，也就不需要考虑线程回收的问题，如果没有任务可执行，线程仍会在线程池中存活并等待任务。 第二个线程池 CachedThreadPool 的核心线程数是 0，而它的最大线程数是 Integer 的最大值，线程数一般是达不到这么多的，所以如果任务特别多且耗时的话，CachedThreadPool 就会创建非常多的线程来应对。第六种：ForkJoinPool第六种线程池 ForkJoinPool，这个线程池是在 JDK 7 加入的，它的名字 ForkJoin 也描述了它的执行机制，主要用法和之前的线程池是相同的，也是把任务交给线程池去执行，线程池中也有任务队列来存放任务。但是 ForkJoinPool 线程池和之前的线程池有两点非常大的不同之处。第一点它非常适合执行可以产生子任务的任务。如图所示，我们有一个 Task，这个 Task 可以产生三个子任务，三个子任务并行执行完毕后将结果汇总给 Result，比如说主任务需要执行非常繁重的计算任务，我们就可以把计算拆分成三个部分，这三个部分是互不影响相互独立的，这样就可以利用 CPU 的多核优势，并行计算，然后将结果进行汇总。这里面主要涉及两个步骤，第一步是拆分也就是 Fork，第二步是汇总也就是 Join，到这里你应该已经了解到 ForkJoinPool 线程池名字的由来了。举个例子，比如面试中经常考到的菲波那切数列，你一定非常熟悉，这个数列的特点就是后一项的结果等于前两项的和，第 0 项是 0，第 1 项是 1，那么第 2 项就是 0+1=1，以此类推。在写代码时应该首选效率更高的迭代形式或者更高级的乘方或者矩阵公式法等写法，不过假设写成了最初版本的递归形式，伪代码如下所示：if (n &amp;lt;= 1) {    return n; } else {    Fib f1 = new Fib(n - 1);    Fib f2 = new Fib(n - 2);    f1.solve();    f2.solve();    number = f1.number + f2.number;    return number; }可以看到如果 n&amp;lt;=1 则直接返回 n，如果 n&amp;gt;1 ，先将前一项 f1 的值计算出来，然后往前推两项求出 f2 的值，然后将两值相加得到结果，所以我们看到在求和运算中产生了两个子任务。计算 f(4) 的流程如下图所示：在计算 f(4) 时需要首先计算出 f(2) 和 f(3)，而同理，计算 f(3) 时又需要计算 f(1) 和 f(2)，以此类推。这是典型的递归问题，对应到我们的 ForkJoin 模式，如图所示，子任务同样会产生子子任务，最后再逐层汇总，得到最终的结果。ForkJoinPool 线程池有多种方法可以实现任务的分裂和汇总，其中一种用法如下方代码所示：class Fibonacci extends RecursiveTask&amp;lt;Integer&amp;gt; { int n; public Fibonacci(int n) { this.n = n; } @Override public Integer compute() { if (n &amp;lt;= 1) { return n; } Fibonacci f1 = new Fibonacci(n - 1); f1.fork(); Fibonacci f2 = new Fibonacci(n - 2); f2.fork(); return f1.join() + f2.join(); }} 首先继承了 RecursiveTask，RecursiveTask 类是对 ForkJoinTask 的一个简单的包装，这时重写 compute() 方法，当 n&amp;lt;=1 时直接返回，当 n&amp;gt;1 就创建递归任务，也就是 f1 和 f2; 用 fork() 方法分裂任务并分别执行； 最后在 return 的时候，使用 join() 方法把结果汇总，这样就实现了任务的分裂和汇总。public static void main(String[] args) throws ExecutionException, InterruptedException { ForkJoinPool forkJoinPool = new ForkJoinPool(); for (int i = 0; i &amp;lt; 10; i++) { ForkJoinTask task = forkJoinPool.submit(new Fibonacci(i)); System.out.println(task.get()); }}上面这段代码将会打印出斐波那契数列的第 0 到 9 项的值：0112358132134这就是 ForkJoinPool 线程池和其他线程池的第一点不同。第二点不同之处在于内部结构，之前的线程池所有的线程共用一个队列，但 ForkJoinPool 线程池中每个线程都有自己独立的任务队列，如图所示。ForkJoinPool 线程池内部除了有一个共用的任务队列之外，每个线程还有一个对应的双端队列 deque，这时一旦线程中的任务被 Fork 分裂了，分裂出来的子任务放入线程自己的 deque 里，而不是放入公共的任务队列中。如果此时有三个子任务放入线程 t1 的 deque 队列中，对于线程 t1 而言获取任务的成本就降低了，可以直接在自己的任务队列中获取而不必去公共队列中争抢也不会发生阻塞（除了后面会讲到的 steal 情况外），减少了线程间的竞争和切换，是非常高效的。我们再考虑一种情况，此时线程有多个，而线程 t1 的任务特别繁重，分裂了数十个子任务，但是 t0 此时却无事可做，它自己的 deque 队列为空，这时为了提高效率，t0 就会想办法帮助 t1 执行任务，这就是“work-stealing”的含义。双端队列 deque 中，线程 t1 获取任务的逻辑是后进先出，也就是LIFO（Last In Frist Out），而线程 t0 在“steal”偷线程 t1 的 deque 中的任务的逻辑是先进先出，也就是FIFO（Fast In Frist Out），如图所示，图中很好的描述了两个线程使用双端队列分别获取任务的情景。你可以看到，使用 “work-stealing” 算法和双端队列很好地平衡了各线程的负载。最后，我们用一张全景图来描述 ForkJoinPool 线程池的内部结构，你可以看到 ForkJoinPool 线程池和其他线程池很多地方都是一样的，但重点区别在于它每个线程都有一个自己的双端队列来存储分裂出来的子任务。ForkJoinPool 非常适合用于递归的场景，例如树的遍历、最优路径搜索等场景。 ScheduledThreadPool 执行周期性任务,假设某次出现了异常,周期任务还会继续吗 ? —— 查看 ScheduledExecutorService 接口里的 scheduleWithFixedDelay 方法的jdk文档，有描述如下： If any execution of the task encounters an exception, subsequent executions are suppressed. 如果在任务的执行中遇到异常，后续执行被取消。 ScheduledThreadPoolExecutor 的最优实践：将所有执行代码用try-catch包裹。 “线程 t1 的任务特别繁重，分裂了数十个子任务，但是 t0 此时却无事可做，它自己的 deque 队列为空”作者你好，请问一下，为什么会出现这种情况？ —— 每个线程被分到的任务可以是不一样的，有的任务重，有的任务轻。 SingleThreadScheduledExecutor的maxPoolSize没有设置成1或者同corePoolSize,意味着多个任务时也会创建超过一个线程吗？—— 不会的，因为它的队列是无界队列，不会满，不会轮到创建最大线程数量个线程的步骤。 看了上面得表格,maxPoolSize 值为 1、corePoolSize 或 Integer.MAX_VALUE。那什么场景下会出现其他值呢 —— 自己定义线程池时，可以对该值进行其他值的设置。" }, { "title": "线程池的 4 种拒绝策略", "url": "/posts/thread-pool-rejection-policy/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-11 15:33:00 +0000", "snippet": "拒绝时机首先，新建线程池可以指定它的任务拒绝策略，例如：newThreadPoolExecutor(5, 10, 5, TimeUnit.SECONDS, new LinkedBlockingQueue&amp;lt;&amp;gt;(), new ThreadPoolExecutor.DiscardOldestPolicy());以便在必要的时候按照自己的策略来拒绝任务，那么拒绝任务的时机是什么呢？ 线程池会在以下两种情况下会拒绝新提交的任务： 当调用 shutdown 等方法关闭线程池后，即便此时可能线程池内部依然有没执行完的任务正在执行，但是由于线程池已经关闭，此时如果再向线程池内提交任务，就会遭到拒绝； 当线程池没有能力继续处理新提交的任务，也就是工作已经非常饱和的时候。比如：新建一个线程池，使用容量上限为 10 的 ArrayBlockingQueue 作为任务队列，并且指定线程池的核心线程数为 5，最大线程数为 10。假设此时有 20 个耗时任务被提交，在这种情况下： 线程池会首先创建核心数量的线程，也就是 5个线程来执行任务； 然后往队列里去放任务，队列的 10 个容量被放满了之后，会继续创建新线程，直到达到最大线程数 10； 此时线程池中一共有 20 个任务，其中 10 个任务正在被 10 个线程执行，还有 10 个任务在任务队列中等待，而且由于线程池的最大线程数量就是 10，所以已经不能再增加更多的线程来帮忙处理任务了，这就意味着此时线程池工作饱和，这个时候再提交新任务时就会被拒绝如上图： 首先看右侧上方的队列部分，可以看到目前队列已经满了； 而图中队列下方的每个线程都在工作，且线程数已经达到最大值 10； 如果此时再有新的任务提交，线程池由于没有能力继续处理新提交的任务，所以就会拒绝。如何正确地选择拒绝策略呢？Java 在 ThreadPoolExecutor 类中为提供了 4 种默认的拒绝策略来应对不同的场景，都实现了 RejectedExecutionHandler 接口，如图所示：拒绝策略AbortPolicy这种拒绝策略在拒绝任务的时候，会直接抛出一个类型为 RejecetExecutionException 的 RuntimeException ，从而感知到任务被拒绝了，于是就可以根据业务逻辑选择重试或者放弃提交等策略。DiscardPolicy这种拒绝策略如名字一样，当新任务被提交后直接被丢弃掉，也不会给任何的通知，这相对而言存在一定的风险，因为提交的时候根本不知道这个任务会被丢弃，从而有可能造成数据的丢失。DiscardOldestPolicy如果线程池没被关闭，且没有能力执行，则会丢弃任务队列中的头结点，通常是存活时间最长的任务。这种策略与第二种不同之处在于它丢弃的不是最新提交的，而是队列中存活时间最长的，这样就可以腾出空间给新提交的任务，同理它也存在一定的数据丢失风险。CallerRunsPolicy相对其它拒绝策略，它就比较完善了，当有新任务提交后，如果线程池没被关闭且没有能力执行，则把这个任务交于提交任务线程执行，也就是谁提交任务，谁就负责执行任务（提交任务的线程是不固定的，取决于具体是哪个线程执行 submit 等方法）。这样做主要有两点好处： 新提交的任务不会被丢弃，也就不会造成业务损失； 由于谁提交任务谁就要负责执行任务，这样提交任务的线程就得负责执行任务，而执行任务又是比较耗时的，在这段期间，提交任务的线程被占用，也就不会再提交新的任务，减缓了任务提交的速度，相当于是一个负反馈。在此期间，线程池中的线程也可以充分利用这段时间来执行掉一部分任务，腾出一定的空间，相当于是给了线程池一定的缓冲期。" }, { "title": "线程池的各个参数的含义", "url": "/posts/thread-pool-params/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-10 15:33:00 +0000", "snippet": "线程池的参数 参数名 含义 corePoolSize 核心线程数 maxPoolSize 最大线程数 keepAliveTime + 时间单位 空闲线程的存活时间 ThreadFactory 线程工厂、用来创建新线程 workQueue 用于存放任务的队列 Handler 处理被拒绝的任务 线程池中各个参数的含义，如表所示线程池主要有 6 个参数，其中第 3 个参数由 keepAliveTime + 时间单位组成。各自的含义。corePoolSize 与 maximumPoolSizecorePoolSize 是核心线程数，也就是常驻线程池的线程数量，与它对应的是 maxPoolSize，表示线程池最大线程数量，当任务特别多而 corePoolSize 核心线程数无法满足需求的时候，就会向线程池中增加线程，以便应对任务突增的情况。线程创建的时机如上图所示，当提交任务后，线程池首先会检查当前线程数，如果此时线程数小于核心线程数，比如最开始线程数量为 0，则新建线程并执行任务，随着任务的不断增加，线程数会逐渐增加并达到核心线程数，此时如果仍有任务被不断提交，就会被放入 workQueue 任务队列中，等待核心线程执行完当前任务后重新从 workQueue 中提取正在等待被执行的任务。此时，假设任务特别的多，已经达到了 workQueue 的容量上限，这时线程池就会启动后备力量，也就是 maxPoolSize 最大线程数，线程池会在 corePoolSize 核心线程数的基础上继续创建线程来执行任务，假设任务被不断提交，线程池会持续创建线程直到线程数达到 maximumPoolSize 最大线程数，如果依然有任务被提交，这就超过了线程池的最大处理能力，这个时候线程池就会拒绝这些任务，可以看到实际上任务进来之后，线程池会逐一判断 corePoolSize、workQueue、maxiPoolSize，如果依然不能满足需求，则会拒绝任务。通过上面的流程图，corePoolSize 和 maximumPoolSize 的具体含义如下： corePoolSize 指的是核心线程数，线程池初始化时线程数默认为 0，当有新的任务提交后，会创建新线程执行任务，如果不做特殊设置，此后线程数通常不会再小于 corePoolSize ，因为它们是核心线程，即便未来可能没有可执行的任务也不会被销毁； 随着任务量的增加，在任务队列满了之后，线程池会进一步创建新线程，最多可以达到 maximumPoolSize 来应对任务多的场景，如果未来线程有空闲，大于 corePoolSize 的线程会被合理回收。所以正常情况下，线程池中的线程数量会处在 corePoolSize 与 maximumPoolSize 的闭区间内。【长工】与【临时工】可以把 corePoolSize 与 maximumPoolSize 比喻成长工与临时工，通常古代一个大户人家会有几个固定的长工，负责日常的工作，而大户人家起初肯定也是从零开始雇佣长工的。假如长工数量被设定为 5 人，也就对应了 corePoolSize，不管这 5 个长工是忙碌还是空闲，都会一直在大户人家待着，可到了农忙或春节，长工的人手显然就不够用了，这时就需要雇佣更多的临时工，这些临时工就相当于在 corePoolSize 的基础上继续创建新线程，但临时工也是有上限的，也就对应了 maximumPoolSize，随着农忙或春节结束，老爷考虑到人工成本便会解约掉这些临时工，家里工人数量便会从 maximumPoolSize 降到 corePoolSize，所以老爷家的工人数量会一致保持在 corePoolSize 和 maximumPoolSize 的区间。在这里用一个动画把整个线程池变化过程生动地描述出来，比如线程池的 corePoolSize 为 5，maximumPoolSize 为 10，任务队列容量为 100，随着任务被提交，我们的线程数量会从 0 慢慢增长到 5，然后就不再增长了，新的任务会被放入队列中，直到队列被塞满，然后在 corePoolSize 的基础上继续创建新线程来执行队列中的任务，线程会逐渐增加到 maximumPoolSize， 然后线程数不再增加，如果此时仍有任务被不断提交，线程池就会拒绝任务。随着队列中任务被执行完，被创建的 10 个线程现在无事可做了，这时线程池会根据 keepAliveTime 参数来销毁线程，已达到减少内存占用的目的。​ 通过对流程图的理解和动画演示，线程池的几个特点。 线程池希望保持较少的线程数，并且只有在负载变得很大时才增加线程。 线程池只有在任务队列填满时才创建多于 corePoolSize 的线程，如果使用的是无界队列（例如 LinkedBlockingQueue），那么由于队列不会满，所以线程数不会超过 corePoolSize。 通过设置 corePoolSize 和 maximumPoolSize 为相同的值，就可以创建固定大小的线程池。 通过设置 maximumPoolSize 为很高的值，例如 Integer.MAX_VALUE，就可以允许线程池创建任意多的线程。keepAliveTime + 时间单位第三个参数是 keepAliveTime + 时间单位，当线程池中线程数量多于核心线程数时，而此时又没有任务可做，线程池就会检测线程的 keepAliveTime，如果超过规定的时间，无事可做的线程就会被销毁，以便减少内存的占用和资源消耗。如果后期任务又多了起来，线程池也会根据规则重新创建线程，所以这是一个可伸缩的过程，比较灵活，我们也可以用 setKeepAliveTime 方法动态改变 keepAliveTime 的参数值。ThreadFactory第四个参数是 ThreadFactory，ThreadFactory 实际上是一个线程工厂，它的作用是生产线程以便执行任务。我们可以选择使用默认的线程工厂，创建的线程都会在同一个线程组，并拥有一样的优先级，且都不是守护线程，我们也可以选择自己定制线程工厂，以方便给线程自定义命名，不同的线程池内的线程通常会根据具体业务来定制不同的线程名。workQueue 和 Handler最后两个参数是 workQueue 和 Handler，它们分别对应阻塞队列和任务拒绝策略，在后面的课时会对它们进行详细展开讲解。 keepAliveTime + 时间单位默认不会，不过可以配置成连核心线程也销毁。 假如核心线程数10个，目前已创建好了5个线程，其中有俩个线程是空闲的，然后这个时候又进来一个任务，是会创建新的核心线程还是使用空闲的核心线程？ - 创建新的，因为线程池希望快速把线程数扩展到核心线程数。 初始化线程池中线程数是0，有任务时创建线程来执行，是一下子创建n(核心线程数)个线程还是从0到n？ 默认是根据任务的到来，慢慢创建线程数直到核心线程数。 如果一个线程执行任务的时间足够久，超过了KeepAliveTime，那还会被销毁吗？- 行任务的时间不算，只计算空闲时的。 已经达到核心线程数，队列已满，这样会创建新线程执行任务，请问最新的这个任务是马上用新线程执行吗？还是取队列头的一个任务用新建立的线程执行，把这个最新的任务放到队列尾部？ 需要排队 keepAliveTime 参数来销毁线程，目前是用来销毁“临时工”的，也能用来销毁“长工”吗 如何配置 Java 核心线程池的回收由allowCoreThreadTimeOut参数控制，默认为false，若开启为true，则此时线程池中不论核心线程还是非核心线程，只要其空闲时间达到keepAliveTime都会被回收。但如果这样就违背了线程池的初衷（减少线程创建和开销），所以默认该参数为false。 假设，核心线程已满，工作队列已满，新进来任务，这时候会创建非核心线程，线程总数量刚好到达maximumPoolSize，这时候，核心线程和非核心线程的执行顺序，有没有优先级，如果这个时候，非核心线程还有空闲，但核心线程占用，工作队列里面的任务会通过非核心线程执行，还是等待核心线程有空闲线程的时候，进入到核心线程。 会通过非核心线程执行 线程池的拒绝策略是 线程数达到最大线程数并且任务队列已经满了 这时候再增加任务就会被拒绝是吧 有两种拒绝时机，你说的是第一种，第二种是线程池已经被关闭。 如果核心线程数等于最大线程数，当没有任务切超过设定的时间，核心线程数也会被回收吗？ 不会的 假设workQueue已满，此时有新任务到来，扩容到maxSize的过程中，新到的这个任务是进入queue等待还是直接可以执行 需要排队 核心线程数是启动线程池的时候创建等待任务,还是等第一个任务到来,一下子创建核心线程数数量的线程,还是根据任务数慢慢创建线程数直到核心线程数 默认是根据任务慢慢创建线程数直到核心线程数，但是可以通过参数调节具体的策略。 对于使用了无界队列 那么最大线程数是不是就不会用到了 - 通常情况下，如果使用了无界队列，那么队列不会满，所以用不到最大线程数。 线程池是怎么自动回收超时的线程的呢？ - 在runWorker方法中，会调用processWorkerExit方法，里面会移除worker。" }, { "title": "线程池比手动创建线程的优点", "url": "/posts/thread-pool/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-09 15:33:00 +0000", "snippet": "为什么要使用线程池在 Java 诞生之初是没有线程池的概念的，而是先有线程，随着线程数的不断增加，人们发现需要一个专门的类来管理它们，于是才诞生了线程池。没有线程池的时候，每发布一个任务就需要创建一个新的线程，这样在任务少时是没有问题的，如代码所示。/** * 描述： 单个任务的时候，新建线程来执行 */ public class OneTask { public static void main(String[] args) { Thread thread0 = new Thread(new Task()); thread0.start(); } static class Task implements Runnable { public void run() { System.out.println(&quot;Thread Name: &quot; + Thread.currentThread().getName()); } }}在这段代码中，发布了一个新的任务并放入子线程中，然后启动子线程执行任务，这时的任务也非常简单，只是打印出当前线程的名字，这种情况下，打印结果显示 Thread Name: Thread-0，即当前子线程的默认名字。任务执行流程如图所示，主线程调用 start() 方法，启动了一个 t0 的子线程。这是在一个任务的场景下，随着任务增多，比如现在有 10 个任务了，那么就可以使用 for 循环新建 10 个子线程，如代码所示：/** * 描述： for 循环新建 10 个线程 */ public class TenTask { public static void main(String[] args) { for (int i = 0; i &amp;lt; 10; i++) { Thread thread = new Thread(new Task()); thread.start(); } } static class Task implements Runnable { public void run() { System.out.println(&quot;Thread Name: &quot; + Thread.currentThread().getName()); }  } }执行结果：Thread Name: Thread-1Thread Name: Thread-4Thread Name: Thread-3Thread Name: Thread-2Thread Name: Thread-0Thread Name: Thread-5Thread Name: Thread-6Thread Name: Thread-7Thread Name: Thread-8Thread Name: Thread-9这里会发现，打印出来的顺序是错乱的，比如 Thread-4 打印在了 Thread-3 之前，这是因为，虽然 Thread-3 比 Thread-4 先执行 start 方法，但是这并不代表 Thread-3 就会先运行，运行的顺序取决于线程调度器，有很大的随机性，这是需要我们注意的地方。再看来下线程的执行流程，如图所示，主线程通过 for 循环创建了 t0~t9 这 10 个子线程，它们都可以正常的执行任务，但如果此时我们的任务量突然飙升到 10000 会怎么样？我们先来看看依然用 for 循环的实现方式：for (int i = 0; i &amp;lt; 10000; i++) {     Thread thread = new Thread(new Task());    thread.start();}如图所示，创建了 10000 个子线程，而 Java 程序中的线程与操作系统中的线程是一一对应的，此时假设线程中的任务需要一定的耗时才能够完成，便会产生很大的系统开销与资源浪费。创建线程时会产生系统开销，并且每个线程还会占用一定的内存等资源，更重要的是我们创建如此多的线程也会给稳定性带来危害，因为每个系统中，可创建线程的数量是有一个上限的，不可能无限的创建。线程执行完需要被回收，大量的线程又会给垃圾回收带来压力。但我们的任务确实非常多，如果都在主线程串行执行，那效率也太低了，那应该怎么办呢？于是便诞生了线程池来平衡线程与系统资源之间的关系。如果每个任务都创建一个线程会带来下面两点问题： 每一次创建线程和销毁线程都会带来一定的开销，如果任务比较简单，那么就有可能导致创建和销毁线程消耗的资源比线程执行任务本身消耗的资源还要大； 每一个线程都需要占用一定的内存空间，不仅如此，线程切换的时候还有上下文切换的开销。线程池解决问题思路针对性的去解前面提到的这两个问题，一个一个来看： 体现在创建线程和销毁线程所带来的开销，因此我们最主要的方案就是避免线程的重复创建，那么我们就可以去提前创建好一定数量的线程，然后反复去使用它们； 主要是由于线程太多，因此解决思路就是控制线程的总量不至于过多。如何使用线程池线程池就好比一个池塘，池塘里的水是有限且可控的，比如选择线程数固定数量的线程池，假设线程池有 5 个线程，但此时的任务大于 5 个，线程池会让余下的任务进行排队，而不是无限制的扩张线程数量，保障资源不会被过度消耗。如代码所示，往 5 个线程的线程池中放入 10000 个任务并打印当前线程名字，结果会是怎么样呢？/** * 描述：用固定线程数的线程池执行10000个任务 */ public class ThreadPoolDemo { public static void main(String[] args) { ExecutorService service = Executors.newFixedThreadPool(5); for (int i = 0; i &amp;lt; 10000; i++) { service.execute(new Task()); } System.out.println(Thread.currentThread().getName()); } static class Task implements Runnable { public void run() { System.out.println(&quot;Thread Name: &quot; + Thread.currentThread().getName()); }  } }执行效果：Thread Name: pool-1-thread-1Thread Name: pool-1-thread-2Thread Name: pool-1-thread-3Thread Name: pool-1-thread-4Thread Name: pool-1-thread-5Thread Name: pool-1-thread-5Thread Name: pool-1-thread-5Thread Name: pool-1-thread-5Thread Name: pool-1-thread-5Thread Name: pool-1-thread-2Thread Name: pool-1-thread-1Thread Name: pool-1-thread-5Thread Name: pool-1-thread-3Thread Name: pool-1-thread-5...如打印结果所示，打印的线程名始终在 Thread Name: pool-1-thread-1~5 之间变化，并没有超过这个范围，也就证明了线程池不会无限制地扩张线程的数量，始终是这5个线程在工作。执行流程如图所示，首先创建了一个线程池，线程池中有 5 个线程，然后线程池将 10000 个任务分配给这 5 个线程，这 5 个线程反复领取任务并执行，直到所有任务执行完毕，这就是线程池的思想。使用线程池的好处使用线程池比手动创建线程主要有三点好处。 第一点，提高响应速度，线程池可以解决线程生命周期的系统开销问题，同时还可以加快响应速度。因为线程池中的线程是可以复用的，我们只用少量的线程去执行大量的任务，这就大大减小了线程生命周期的开销。而且线程通常不是等接到任务后再临时创建，而是已经创建好时刻准备执行任务，这样就消除了线程创建所带来的延迟，提升了响应速度，增强了用户体验。 第二点，降低资源消耗，线程池可以统筹内存和 CPU 的使用，避免资源使用不当。线程池会根据配置和任务数量灵活地控制线程数量，不够的时候就创建，太多的时候就回收，避免线程过多导致内存溢出，或线程太少导致 CPU 资源浪费，达到了一个完美的平衡。 第三点，提高线程的可管理性，线程池可以统一管理资源。比如线程池可以统一管理任务队列和线程，可以统一开始或结束任务，比单个线程逐一处理任务要更方便、更易于管理，同时也有利于数据统计，比如我们可以很方便地统计出已经执行过的任务的数量。 用线程池是不是要注意关闭，要不然请求多了是不是会溢出：请求多了不会溢出，会在队列里积压，等到线程有空闲的时候再会去处理的。如果线程池确定不再使用，那么可以关闭。 线程解决的问题：避免频繁创建线程和销毁线程造成的额外的系统开销；统一管理内存和CPU等资源，避免消耗过多的资源，导致系统异常；方便任务管理。" }, { "title": "多线程会带来性能问题", "url": "/posts/thread-safe-performance/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-08 15:33:00 +0000", "snippet": "什么是性能问题对于多线程而言，它不仅可能会带来线程安全问题，还有可能会带来性能问题。也许会奇怪，使用多线程的最大目的不就是为了提高性能吗？让多个线程同时工作，加快程序运行速度，为什么反而会带来性能问题呢？这是因为单线程程序是独立工作的，不需要与其他线程进行交互，但多线程之间则需要调度以及合作，调度与合作就会带来性能开销从而产生性能问题。首先，究竟什么是性能问题？其实性能问题有许多的表现形式，比如：服务器的响应慢、吞吐量低、内存占用过多就属于性能问题。我们设计优秀的系统架构、购置更多的 CDN 服务器、购买更大的带宽等都是为了提高性能，提高用户体验，虽然运行速度慢不会带来严重的后果，通常只需要多等几秒就可以，但这会严重影响用户的体验。有研究表明，页面每多响应 1 秒，就会流失至少 7% 的用户，而超过 8 秒无法返回结果的话，几乎所有用户都不会选择继续等待。引入多线程的一大重要原因就是想提高程序性能，所以不能本末倒置，不能因为引入了多线程反而程序运行得更慢了，所以必须要解决多线程带来的性能问题。为什么多线程会带来性能问题什么情况下多线程编程会带来性能问题呢？主要有两个方面： 一方面是线程调度； 另一个方面是线程协作。调度开销上下文切换在实际开发中，线程数往往是大于 CPU 核心数的，比如 CPU 核心数可能是 8 核、16 核，等等。但线程数可能达到成百上千个。这种情况下，操作系统就会按照一定的调度算法，给每个线程分配时间片，让每个线程都有机会得到运行。而在进行调度时就会引起上下文切换，上下文切换会挂起当前正在执行的线程并保存当前的状态，然后寻找下一处即将恢复执行的代码，唤醒下一个线程，以此类推，反复执行。但上下文切换带来的开销是比较大的，假设我们的任务内容非常短，比如只进行简单的计算，那么就有可能发生上下文切换带来的性能开销比执行线程本身内容带来的开销还要大的情况。缓存失效不仅上下文切换会带来性能问题，缓存失效也有可能带来性能问题。由于程序有很大概率会再次访问刚才访问过的数据，所以为了加速整个程序的运行，会使用缓存，这样在使用相同数据时就可以很快地获取数据。可一旦进行了线程调度，切换到其他线程，CPU就会去执行不同的代码，原有的缓存就很可能失效了，需要重新缓存新的数据，这也会造成一定的开销，所以线程调度器为了避免频繁地发生上下文切换，通常会给被调度到的线程设置最小的执行时间，也就是只有执行完这段时间之后，才可能进行下一次的调度，由此减少上下文切换的次数。那么什么情况会导致密集的上下文切换呢？如果程序频繁地竞争锁，或者由于 IO 读写等原因导致频繁阻塞，那么这个程序就可能需要更多的上下文切换，这也就导致了更大的开销，我们应该尽量避免这种情况的发生。协作开销除了线程调度之外，线程协作同样也有可能带来性能问题。因为线程之间如果有共享数据，为了避免数据错乱，为了保证线程安全，就有可能禁止编译器和 CPU 对其进行重排序等优化，也可能出于同步的目的，反复把线程工作内存的数据 flush 到主存中，然后再从主内存 refresh 到其他线程的工作内存中，等等。这些问题在单线程中并不存在，但在多线程中为了确保数据的正确性，就不得不采取上述方法，因为线程安全的优先级要比性能优先级更高，这也间接降低了我们的性能。 这里说的缓存失效中的缓存是CPU高速缓存。" }, { "title": "需要额外注意线程安全问题的场景", "url": "/posts/thread-safe-notice/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-07 15:33:00 +0000", "snippet": "需要额外注意线程安全问题有四种场景。第一种场景：访问共享变量或资源第一种场景是访问共享变量或共享资源的时候，典型的场景有： 访问共享对象的属性； 访问 static 静态变量； 访问共享的缓存等等。因为这些信息不仅会被一个线程访问到，还有可能被多个线程同时访问，那么就有可能在并发读写的情况下发生线程安全问题。比如多线程同时 i++ 的例子：/** * 描述： 共享的变量或资源带来的线程安全问题 */public class ThreadNotSafe1 { static int i; public static void main(String[] args) throws InterruptedException { Runnable r = new Runnable() { @Override public void run() { for (int j = 0; j &amp;lt; 10000; j++) { i++; } } }; Thread thread1 = new Thread(r); Thread thread2 = new Thread(r); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(i);    }}如代码所示：两个线程同时对 i 进行 i++ 操作，最后的输出可能是 15875 等小于20000的数，而不是我们期待的20000，这便是非常典型的共享变量带来的线程安全问题。第二种场景：依赖时序的操作第二个需要注意的场景是依赖时序的操作。如果操作的正确性是依赖时序的，而在多线程的情况下又不能保障执行的顺序和我们预想的一致，这个时候就会发生线程安全问题，如下面的代码所示：if (map.containsKey(key)) { map.remove(obj);}代码中，首先检查 map 中有没有 key 对应的元素，如果有则继续执行 remove 操作。此时，这个组合操作就是危险的，因为它是先检查后操作，而执行过程中可能会被打断。如果此时有两个线程同时进入 if() 语句，然后它们都检查到存在 key 对应的元素，于是都希望执行下面的 remove 操作，随后一个线程率先把 obj 给删除了，而另外一个线程它刚已经检查过存在 key 对应的元素，if 条件成立，所以它也会继续执行删除 obj 的操作，但实际上，集合中的 obj 已经被前面的线程删除了，这种情况下就可能导致线程安全问题。类似的情况还有很多，比如我们先检查 x=1，如果 x=1 就修改 x 的值，代码如下所示：if (x == 1) {    x = 7 * x;}这样类似的场景都是同样的道理，检查与执行并非原子性操作，在中间可能被打断，而检查之后的结果也可能在执行时已经过期、无效，换句话说，获得正确结果取决于幸运的时序。这种情况下，就需要对它进行加锁等保护措施来保障操作的原子性。第三种场景：不同数据之间存在绑定关系第三种需要注意的线程安全场景是：不同数据之间存在相互绑定关系的情况。有时候，不同数据之间是成组出现的，存在着相互对应或绑定的关系，最典型的就是 IP 和端口号。有时候更换了 IP，往往需要同时更换端口号，如果没有把这两个操作绑定在一起，就有可能出现单独更换了 IP 或端口号的情况，而此时信息如果已经对外发布，信息获取方就有可能获取一个错误的 IP 与端口绑定情况，这时就发生了线程安全问题。在这种情况下，也同样需要保障操作的原子性。第四种场景：对方没有声明自己是线程安全的第四种值得注意的场景是：在使用其他类时，如果对方没有声明自己是线程安全的，那么这种情况下对其他类进行多线程的并发操作，就有可能会发生线程安全问题。举个例子，比如说定义了 ArrayList，它本身并不是线程安全的，如果此时多个线程同时对 ArrayList 进行并发读/写，那么就有可能会产生线程安全问题，造成数据出错，而这个责任并不在 ArrayList，因为它本身并不是并发安全的，正如源码注释所写的：Note that this implementation is not synchronized. If multiple threadsaccess an ArrayList instance concurrently, and at least one of the threadsmodifies the list structurally, it must be synchronized externally.这段话的意思是说，如果把 ArrayList 用在了多线程的场景，需要在外部手动用 synchronized 等方式保证并发安全。所以 ArrayList 默认不适合并发读写，是错误地使用了它，导致了线程安全问题。所以，在使用其他类时如果会涉及并发场景，那么一定要首先确认清楚，对方是否支持并发操作。以上就是四种需要额外注意线程安全问题的场景，分别是： 访问共享变量或资源； 依赖时序的操作； 不同数据之间存在绑定关系； 以及对方没有声明自己是线程安全的。 如果采用的数据结构本身不支持并发的话，但又因为特殊需要必须使用特定数据结构，怎么去设计方案优化并发呢？- 可以使用加锁等其他方法辅助。" }, { "title": "三类线程安全问题", "url": "/posts/thread-safe-three/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-06 15:33:00 +0000", "snippet": "什么是线程安全要想弄清楚有哪 3 类线程安全问题，首先需要了解什么是线程安全，线程安全经常在工作中被提到，比如： 你的对象不是线程安全的； 你的线程发生了安全错误。虽然线程安全经常被提到，但对线程安全并没有一个明确的定义。《Java Concurrency In Practice》的作者 Brian Goetz 对线程安全是这样理解的：当多个线程访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行问题，也不需要进行额外的同步，而调用这个对象的行为都可以获得正确的结果，那这个对象便是线程安全的。事实上，Brian Goetz 想表达的意思是:** 如果某个对象是线程安全的，那么对于使用者而言，在使用时就不需要考虑方法间的协调问题，比如不需要考虑不能同时写入或读写不能并行的问题，也不需要考虑任何额外的同步问题(不需要额外自己加 synchronized 锁)，那么它才是线程安全的，可以看出对线程安全的定义还是非常苛刻的。**在实际开发中经常会遇到线程不安全的情况，一共有以下三种线程安全问题: 运行结果错误； 发布和初始化导致线程安全问题； 活跃性问题。运行结果错误首先，来看多线程同时操作一个变量导致的运行结果错误。public class WrongResult {   volatile static int i;   public static void main(String[] args) throws InterruptedException {       Runnable r = new Runnable() {           @Override           public void run() {               for (int j = 0; j &amp;lt; 10000; j++) {                   i++;               }           }       };       Thread thread1 = new Thread(r);       thread1.start();       Thread thread2 = new Thread(r);       thread2.start();       thread1.join();       thread2.join();       System.out.println(i);    }}如代码所示： 首先定义了一个 int 类型的静态变量 i； 然后启动两个线程，分别对变量 i 进行 10000 次 i++ 操作。理论上得到的结果应该是 20000，但实际结果却远小于理论结果，比如可能是12996，也可能是13323，每次的结果都还不一样，这是为什么呢？是因为在多线程下，CPU 的调度是以时间片为单位进行分配的，每个线程都可以得到一定量的时间片。但如果线程拥有的时间片耗尽，它将会被暂停执行并让出 CPU 资源给其他线程，这样就有可能发生线程安全问题。比如 i++ 操作，表面上看只是一行代码，但实际上它并不是一个原子操作，它的执行步骤主要分为三步，而且在每步操作之间都有可能被打断。 第一个步骤是读取； 第二个步骤是增加； 第三个步骤是保存。如何发生的线程不安全问题:根据箭头指向依次看，线程 1 首先拿到 i=1 的结果，然后进行 i+1 操作，但此时 i+1 的结果并没有保存下来，线程 1 就被切换走了，于是 CPU 开始执行线程 2，它所做的事情和线程 1 是一样的 i++ 操作，但此时我们想一下，它拿到的 i 是多少？实际上和线程 1 拿到的 i 的结果一样都是 1，为什么呢？因为线程 1 虽然对 i 进行了 +1 操作，但结果没有保存，所以线程 2 看不到修改后的结果。然后假设等线程 2 对 i 进行 +1 操作后，又切换到线程 1，让线程 1 完成未完成的操作，即将 i+1 的结果 2 保存下来，然后又切换到线程 2 完成 i=2 的保存操作，虽然两个线程都执行了对 i 进行 +1 的操作，但结果却最终保存了 i=2 的结果，而不是我们期望的 i=3，这样就发生了线程安全问题，导致了数据结果错误，这也是最典型的线程安全问题。发布和初始化导致线程安全问题第二种是对象发布和初始化时导致的线程安全问题，我们创建对象并进行发布和初始化供其他类或对象使用是常见的操作，但如果我们操作的时间或地点不对，就可能导致线程安全问题。如代码所示。public class WrongInit {    private Map&amp;lt;Integer, String&amp;gt; students;    public WrongInit() {        new Thread(new Runnable() {            @Override            public void run() {                students = new HashMap&amp;lt;&amp;gt;();                students.put(1, &quot;王小美&quot;);                students.put(2, &quot;钱二宝&quot;);                students.put(3, &quot;周三&quot;);                students.put(4, &quot;赵四&quot;);            }        }).start();     }    public Map&amp;lt;Integer, String&amp;gt; getStudents() {        return students;    }    public static void main(String[] args) throws InterruptedException {        WrongInit multiThreadsError6 = new WrongInit();        System.out.println(multiThreadsError6.getStudents().get(1));    }}在类中，定义一个类型为 Map 的成员变量 students，Integer 是学号，String 是姓名。然后在构造函数中启动一个新线程，并在线程中为 students 赋值。 学号：1，姓名：王小美； 学号：2，姓名：钱二宝； 学号：3，姓名：周三； 学号：4，姓名：赵四。只有当线程运行完 run() 方法中的全部赋值操作后，4 名同学的全部信息才算是初始化完毕，可是我们看在主函数 mian() 中，初始化 WrongInit 类之后并没有进行任何休息就直接打印 1 号同学的信息，试想这个时候程序会出现什么情况？实际上会发生空指针异常。Exception in thread &quot;main&quot; java.lang.NullPointerExceptionat lesson6.WrongInit.main(WrongInit.java:32)这又是为什么呢？因为 students 这个成员变量是在构造函数中新建的线程中进行的初始化和赋值操作，而线程的启动需要一定的时间，但是 main 函数并没有进行等待，就直接获取数据，导致 getStudents 获取的结果为 null，这就是在错误的时间或地点发布或初始化造成的线程安全问题。活跃性问题第三种线程安全问题统称为活跃性问题，最典型的有三种，分别为死锁、活锁和饥饿。什么是活跃性问题呢，活跃性问题就是程序始终得不到运行的最终结果，相比于前面两种线程安全问题带来的数据错误或报错，活跃性问题带来的后果可能更严重，比如发生死锁会导致程序完全卡死，无法向下运行。死锁最常见的活跃性问题是死锁，死锁是指两个线程之间相互等待对方资源，但同时又互不相让，都想自己先执行，如代码所示。 public class MayDeadLock {    Object o1 = new Object();    Object o2 = new Object();    public void thread1() throws InterruptedException {        synchronized (o1) {            Thread.sleep(500);            synchronized (o2) {                System.out.println(&quot;线程1成功拿到两把锁&quot;);           }        }    }    public void thread2() throws InterruptedException {        synchronized (o2) {            Thread.sleep(500);            synchronized (o1) {                System.out.println(&quot;线程2成功拿到两把锁&quot;);            }        }    }    public static void main(String[] args) {        MayDeadLock mayDeadLock = new MayDeadLock();        new Thread(new Runnable() {            @Override            public void run() {                try {                    mayDeadLock.thread1();                } catch (InterruptedException e) {                    e.printStackTrace();                }            }        }).start();        new Thread(new Runnable() {            @Override            public void run() {                try {                    mayDeadLock.thread2()                } catch (InterruptedException e) {                    e.printStackTrace();                }            }        }).start();    }}首先，代码中创建了两个 Object 作为 synchronized 锁的对象，线程 1 先获取 o1 锁，sleep(500) 之后，获取 o2 锁；线程 2 与线程 1 执行顺序相反，先获取 o2 锁，sleep(500) 之后，获取 o1 锁。假设两个线程几乎同时进入休息，休息完后，线程 1 想获取 o2 锁，线程 2 想获取 o1 锁，这时便发生了死锁，两个线程不主动调和，也不主动退出，就这样死死地等待对方先释放资源，导致程序得不到任何结果也不能停止运行。活锁第二种活跃性问题是活锁，活锁与死锁非常相似，也是程序一直等不到结果，但对比于死锁，活锁是活的，什么意思呢？因为正在运行的线程并没有阻塞，它始终在运行中，却一直得不到结果。举一个例子，假设有一个消息队列，队列里放着各种各样需要被处理的消息，而某个消息由于自身被写错了导致不能被正确处理，执行时会报错，可是队列的重试机制会重新把它放在队列头进行优先重试处理，但这个消息本身无论被执行多少次，都无法被正确处理，每次报错后又会被放到队列头进行重试，周而复始，最终导致线程一直处于忙碌状态，但程序始终得不到结果，便发生了活锁问题。饥饿第三个典型的活跃性问题是饥饿，饥饿是指线程需要某些资源时始终得不到，尤其是CPU 资源，就会导致线程一直不能运行而产生的问题。在 Java 中有线程优先级的概念，Java 中优先级分为 1 到 10，1 最低，10 最高。如果我们把某个线程的优先级设置为 1，这是最低的优先级，在这种情况下，这个线程就有可能始终分配不到 CPU 资源，而导致长时间无法运行。或者是某个线程始终持有某个文件的锁，而其他线程想要修改文件就必须先获取锁，这样想要修改文件的线程就会陷入饥饿，长时间不能运行。总结线程安全问题主要有 3 种： 第一种，i++ 等情况导致的运行结果错误，通常是因为并发读写导致的； 第二种，对象没有在正确的时间、地点被发布或初始化； 第三种线程安全问题就是活跃性问题，包括死锁、活锁和饥饿。 上面的那个例子不是已经使用关键字volatile，让它们符合happens-before原则了吗。怎么还是线程不安全了虽然用了volatile，但是 i++ 不具备原子性，这样操作是并发不安全的。 活跃锁可以理解为程序业务错误，又重复无限制的执行" }, { "title": "实现【生产者-消费者模式】的几种方法", "url": "/posts/thread-producer-consumer/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-05 15:33:00 +0000", "snippet": "如何用 wait/notify/Condition/BlockingQueue 实现生产者消费者模式 ？生产者-消费者模式我们先来看看什么是生产者消费者模式，生产者消费者模式是程序设计中非常常见的一种设计模式，被广泛运用在解耦、消息队列等场景。在现实世界中，我们把生产商品的一方称为生产者，把消费商品的一方称为消费者，有时生产者的生产速度特别快，但消费者的消费速度跟不上，俗称“产能过剩”，又或是多个生产者对应多个消费者时，大家可能会手忙脚乱。如何才能让大家更好地配合呢？这时在生产者和消费者之间就需要一个中介来进行调度，于是便诞生了生产者消费者模式。使用生产者消费者模式通常需要在两者之间增加一个阻塞队列作为媒介，有了媒介之后就相当于有了一个缓冲，平衡了两者的能力，整体的设计如图所示，最上面是阻塞队列，右侧的 1 是生产者线程，生产者在生产数据后将数据存放在阻塞队列中，左侧的 2 是消费者线程，消费者获取阻塞队列中的数据。而中间的 3 和 4 分别代表生产者消费者之间互相通信的过程，因为无论阻塞队列是满还是空都可能会产生阻塞，阻塞之后就需要在合适的时机去唤醒被阻塞的线程。那么什么时候阻塞线程需要被唤醒呢？有两种情况。第一种情况是当消费者看到阻塞队列为空时，开始进入等待，这时生产者一旦往队列中放入数据，就会通知所有的消费者，唤醒阻塞的消费者线程。另一种情况是如果生产者发现队列已经满了，也会被阻塞，而一旦消费者获取数据之后就相当于队列空了一个位置，这时消费者就会通知所有正在阻塞的生产者进行生产，这便是对生产者消费者模式的简单介绍。用 BlockingQueue 实现生产者消费者模式public static void main(String[] args) { BlockingQueue&amp;lt;Object&amp;gt; queue = new ArrayBlockingQueue&amp;lt;&amp;gt;(10); Runnable producer = () -&amp;gt; { while (true) { queue.put(new Object()); } }; new Thread(producer).start(); new Thread(producer).start(); Runnable consumer = () -&amp;gt; { while (true) { queue.take(); } }; new Thread(consumer).start(); new Thread(consumer).start();}如代码所示： 首先，创建了一个 ArrayBlockingQueue 类型的 BlockingQueue，命名为 queue，并将它的容量设置为 10； 其次，创建一个简单的生产者，while(true) 循环体中的queue.put() 负责往队列添加数据； 然后，创建两个生产者线程并启动；同样消费者也非常简单，while(true) 循环体中的 queue.take() 负责消费数据，同时创建两个消费者线程并启动。为了代码简洁并突出设计思想，代码里省略了 try/catch 检测，暂时不纠结这些语法细节。以上便是利用 BlockingQueue 实现生产者消费者模式的代码。虽然代码非常简单，但实际上 ArrayBlockingQueue 已经在背后完成了很多工作，比如队列满了就去阻塞生产者线程，队列有空就去唤醒生产者线程等。如何用 Condition 实现生产者消费者模式BlockingQueue 实现生产者消费者模式看似简单，背后却暗藏玄机。在掌握这种方法的基础上仍需要掌握更复杂的实现方法。接下来看如何在掌握了 BlockingQueue 的基础上，利用 Condition 实现生产者消费者模式，它们背后的实现原理非常相似，相当于自己实现一个简易版的 BlockingQueue：public class MyBlockingQueueForCondition { private Queue queue; private int max = 16; private ReentrantLock lock = new ReentrantLock(); private Condition notEmpty = lock.newCondition(); private Condition notFull = lock.newCondition(); public MyBlockingQueueForCondition(int size) { this.max = size; queue = new LinkedList(); } public void put(Object o) throws InterruptedException { lock.lock(); try { while (queue.size() == max) { notFull.await(); } queue.add(o); notEmpty.signalAll(); } finally { lock.unlock(); } } public Object take() throws InterruptedException {}}如代码所示： 首先，定义了一个队列变量 queue 并设置最大容量为 16； 其次，定义了一个 ReentrantLock 类型的 Lock 锁，并在 Lock 锁的基础上创建两个 Condition，一个是 notEmpty，另一个是 notFull，分别代表队列没有空和没有满的条件； 最后，声明了 put 和 take 这两个核心方法。因为生产者消费者模式通常是面对多线程的场景，需要一定的同步措施保障线程安全，所以在 put 方法中先将 Lock 锁上，然后，在 while 的条件里检测 queue 是不是已经满了，如果已经满了，则调用 notFull 的 await() 阻塞生产者线程并释放 Lock，如果没有满，则往队列放入数据并利用 notEmpty.signalAll() 通知正在等待的所有消费者并唤醒它们。最后在 finally 中利用 lock.unlock() 方法解锁，把 unlock 方法放在 finally 中是一个基本原则，否则可能会产生无法释放锁的情况。下面再来看 take 方法，take 方法实际上是与 put 方法相互对应的，同样是通过 while 检查队列是否为空，如果为空，消费者开始等待，如果不为空则从队列中获取数据并通知生产者队列有空余位置，最后在 finally 中解锁。这里需要注意，在 take() 方法中使用 while( queue.size() == 0 ) 检查队列状态，而不能用 if( queue.size() == 0 )。为什么呢？大家思考这样一种情况，因为生产者消费者往往是多线程的，我们假设有两个消费者，第一个消费者线程获取数据时，发现队列为空，便进入等待状态；因为第一个线程在等待时会释放 Lock 锁，所以第二个消费者可以进入并执行 if( queue.size() == 0 )，也发现队列为空，于是第二个线程也进入等待；而此时，如果生产者生产了一个数据，便会唤醒两个消费者线程，而两个线程中只有一个线程可以拿到锁，并执行 queue.remove 操作，另外一个线程因为没有拿到锁而卡在被唤醒的地方，而第一个线程执行完操作后会在 finally 中通过 unlock 解锁，而此时第二个线程便可以拿到被第一个线程释放的锁，继续执行操作，也会去调用 queue.remove 操作，然而这个时候队列已经为空了，所以会抛出 NoSuchElementException 异常，这不符合我们的逻辑。而如果用 while 做检查，当第一个消费者被唤醒得到锁并移除数据之后，第二个线程在执行 remove 前仍会进行 while 检查，发现此时依然满足 queue.size() == 0 的条件，就会继续执行 await 方法，避免了获取的数据为 null 或抛出异常的情况。用 wait/notify 实现生产者消费者模式再来看看使用 wait/notify 实现生产者消费者模式的方法，实际上实现原理和 Condition 是非常类似的，它们是兄弟关系：class MyBlockingQueue { private int maxSize; private LinkedList&amp;lt;Object&amp;gt; storage; public MyBlockingQueue(int size) { this.maxSize = size; storage = new LinkedList&amp;lt;&amp;gt;(); } public synchronized void put() throws InterruptedException { while (storage.size() == maxSize) { wait(); } storage.add(new Object()); notifyAll(); } public synchronized void take() throws InterruptedException { while (storage.size() == 0) { wait(); } System.out.println(storage.remove()); notifyAll(); }}如代码所示，最主要的部分仍是 take 与 put 方法： 先来看 put 方法，put 方法被 synchronized 保护，while 检查队列是否为满，如果不满就往里放入数据并通过 notifyAll() 唤醒其他线程； 同样，take 方法也被 synchronized 修饰，while 检查队列是否为空，如果不为空就获取数据并唤醒其他线程。使用这个 MyBlockingQueue 实现的生产者消费者代码如下：/*** 描述：wait形式实现生产者消费者模式*/public class WaitStyle { public static void main(String[] args) { MyBlockingQueue myBlockingQueue = new MyBlockingQueue(10); Producer producer = new Producer(myBlockingQueue); Consumer consumer = new Consumer(myBlockingQueue); new Thread(producer).start(); new Thread(consumer).start(); }}class Producer implements Runnable { private MyBlockingQueue storage; public Producer(MyBlockingQueue storage) { this.storage = storage; } @Override public void run() { for (int i = 0; i &amp;lt; 100; i++) { try { storage.put(); } catch (InterruptedException e) { e.printStackTrace(); } } }}class Consumer implements Runnable { private MyBlockingQueue storage; public Consumer(MyBlockingQueue storage) { this.storage = storage; } @Override public void run() { for (int i = 0; i &amp;lt; 100; i++) { try { storage.take(); } catch (InterruptedException e) { e.printStackTrace(); } } }}以上就是三种实现生产者消费者模式的讲解，其中： 第一种 BlockingQueue 模式实现比较简单，但其背后的实现原理在第二种、第三种实现方法中得以体现； 第二种、第三种实现方法本质上是自己实现了 BlockingQueue 的一些核心逻辑，供生产者与消费者使用。" }, { "title": "使用 wait/notify/notifyAll 方法的注意事项", "url": "/posts/waitnotify-notifyAll/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-04 15:33:00 +0000", "snippet": "从三个问题入手： 为什么 wait 方法必须在 synchronized 保护的同步代码中使用？ 为什么 wait/notify/notifyAll 被定义在 Object 类中，而 sleep 定义在 Thread 类中？ wait/notify 和 sleep 方法的异同？为什么 wait 必须在 synchronized 保护的同步代码中使用？wait 方法的源码注释是怎么写的。 /** * Causes the current thread to wait until another thread invokes the * {@link java.lang.Object#notify()} method or the * {@link java.lang.Object#notifyAll()} method for this object. * In other words, this method behaves exactly as if it simply * performs the call {@code wait(0)}. * &amp;lt;p&amp;gt; * The current thread must own this object&#39;s monitor. The thread * releases ownership of this monitor and waits until another thread * notifies threads waiting on this object&#39;s monitor to wake up * either through a call to the {@code notify} method or the * {@code notifyAll} method. The thread then waits until it can * re-obtain ownership of the monitor and resumes execution. * &amp;lt;p&amp;gt; * As in the one argument version, interrupts and spurious wakeups are * possible, and this method should always be used in a loop: * &amp;lt;pre&amp;gt; * synchronized (obj) { * while (&amp;amp;lt;condition does not hold&amp;amp;gt;) * obj.wait(); * ... // Perform action appropriate to condition * } * &amp;lt;/pre&amp;gt; * This method should only be called by a thread that is the owner * of this object&#39;s monitor. See the {@code notify} method for a * description of the ways in which a thread can become the owner of * a monitor. * * @throws IllegalMonitorStateException if the current thread is not * the owner of the object&#39;s monitor. * @throws InterruptedException if any thread interrupted the * current thread before or while the current thread * was waiting for a notification. The &amp;lt;i&amp;gt;interrupted * status&amp;lt;/i&amp;gt; of the current thread is cleared when * this exception is thrown. * @see java.lang.Object#notify() * @see java.lang.Object#notifyAll() */ public final void wait() throws InterruptedException { wait(0); }其中，wait method should always be used in a loop...This method should only be called by a thread that is the owner of this object&#39;s monitor.的意思是说，在使用 wait 方法时，必须把 wait 方法写在 synchronized 保护的 while 代码块中，并始终判断执行条件是否满足，如果满足就往下继续执行，如果不满足就执行 wait 方法，而在执行 wait 方法之前，必须先持有对象的 monitor 锁，也就是通常所说的 synchronized 锁。那么设计成这样有什么好处呢？逆向思考这个问题，如果不要求 wait 方法放在 synchronized 保护的同步代码中使用，而是可以随意调用，那么就有可能写出如下的代码：public class BlockingQueue { Queue&amp;lt;String&amp;gt; buffer = new LinkedList&amp;lt;String&amp;gt;(); public void give(String data) { buffer.add(data); notify(); // Since someone may be waiting in take } public String take() throws InterruptedException { while (buffer.isEmpty()) { wait(); } return buffer.remove(); }}在代码中可以看到有两个方法，give 方法负责往 buffer 中添加数据，添加完之后执行 notify 方法来唤醒之前等待的线程，而 take 方法负责检查整个 buffer 是否为空，如果为空就进入等待，如果不为空就取出一个数据，这是典型的生产者消费者的思想。但是这段代码并没有受 synchronized 保护，于是就有可能发生以下场景： 首先，消费者线程调用 take 方法并判断 buffer.isEmpty 方法是否返回 true，若为 true 代表 buffer 是空的，则线程希望进入等待，但是在线程调用 wait 方法之前，就被调度器暂停了，所以此时还没来得及执行 wait 方法。 此时生产者开始运行，执行了整个 give 方法，它往 buffer 中添加了数据，并执行了 notify 方法，但 notify 并没有任何效果，因为消费者线程的 wait 方法没来得及执行，所以没有线程在等待被唤醒。 此时，刚才被调度器暂停的消费者线程回来继续执行 wait 方法并进入了等待。虽然刚才消费者判断了 buffer.isEmpty 条件，但真正执行 wait 方法时，之前的 buffer.isEmpty 的结果已经过期了，不再符合最新的场景了，因为这里的“判断-执行”不是一个原子操作，它在中间被打断了，是线程不安全的。假设这时没有更多的生产者进行生产，消费者便有可能陷入无穷无尽的等待，因为它错过了刚才 give 方法内的 notify 的唤醒。看到正是因为 wait 方法所在的 take 方法没有被 synchronized 保护，所以它的 while 判断和 wait 方法无法构成原子操作，那么此时整个程序就很容易出错。 在没有被synchronized保护的场景，take方法，在调用 wait()方法之前被调度器暂停了，没有调用 wait( )方法的情况下执行的，如果没有被调度器暂停是可能正常运行一段时间的。于是把代码改写成源码注释所要求的被 synchronized 保护的同步代码块的形式，代码如下。public class BlockingQueue { Queue&amp;lt;String&amp;gt; buffer = new LinkedList&amp;lt;String&amp;gt;(); public void give(String data) { buffer.add(data); notify(); // Since someone may be waiting in take } public String take() throws InterruptedException { while (buffer.isEmpty()) { wait(); } return buffer.remove(); } public String takeSync() throws InterruptedException { synchronized (this) { while (buffer.isEmpty()) { wait(); } return buffer.remove(); } }}这样就可以确保 notify 方法永远不会在 buffer.isEmpty 和 wait 方法之间被调用，提升了程序的安全性。另外，wait 方法会释放 monitor 锁，这也要求我们必须首先进入到 synchronized 内持有这把锁。 这里还存在一个“虚假唤醒”（spurious wakeup，虚假唤醒特指没有实际唤醒动作，但是却被唤醒的情况）的问题，线程可能在既没有被notify/notifyAll，也没有被中断或者超时的情况下被唤醒，这种唤醒是不希望看到的。虽然在实际生产中，虚假唤醒发生的概率很小，但是程序依然需要保证在发生虚假唤醒的时候的正确性，所以就需要采用 while 循环结构。 while (condition does not hold) obj.wait(); 这样即便被虚假唤醒了，也会再次检查while里面的条件，如果不满足条件，就会继续wait，也就消除了虚假唤醒的风险。为什么 wait/notify/notifyAll 被定义在 Object 类中，而 sleep 定义在 Thread 类中？第二个问题，为什么 wait/notify/notifyAll 方法被定义在 Object 类中？而 sleep 方法定义在 Thread 类中？主要有两点原因： 因为 Java 中每个对象都有一把称之为 monitor 监视器的锁，由于每个对象都可以上锁，这就要求在对象头中有一个用来保存锁信息的位置。这个锁是对象级别的，而非线程级别的，wait/notify/notifyAll 也都是锁级别的操作，它们的锁属于对象，所以把它们定义在 Object 类中是最合适，因为 Object 类是所有对象的父类。 如果把 wait/notify/notifyAll 方法定义在 Thread 类中，会带来很大的局限性，比如一个线程可能持有多把锁，以便实现相互配合的复杂逻辑，假设此时 wait 方法定义在 Thread 类中，如何实现让一个线程持有多把锁呢？又如何明确线程等待的是哪把锁呢？既然我们是让当前线程去等待某个对象的锁，自然应该通过操作对象来实现，而不是操作线程。 wait/notify 和 sleep 方法的异同？主要对比 wait 和 sleep 方法，相同点： 它们都可以让线程阻塞。 它们都可以响应 interrupt 中断：在等待的过程中如果收到中断信号，都可以进行响应，并抛出 InterruptedException 异常。但是它们也有很多的不同点： wait 方法必须在 synchronized 保护的代码中使用，而 sleep 方法并没有这个要求。 在同步代码中执行 sleep 方法时，并不会释放 monitor 锁，但执行 wait 方法时会主动释放 monitor 锁。 sleep 方法中会要求必须定义一个时间，时间到期后会主动恢复，而对于没有参数的 wait 方法而言，意味着永久等待，直到被中断或被唤醒才能恢复，它并不会主动恢复。 wait/notify 是 Object 类的方法，而 sleep 是 Thread 类的方法。 调用了 sleep()方法的线程，可以使用中断interrupt来提前唤醒线程。 原子性: 描述动作要么被完整执行 要么都不执行的，完整执行并不意味这一口气做完，仍旧受线程调度机制影响原子操作: 不会受线程调度机制而中断，也就是不会出现线程上下文切换。即便是单核处理器，synchronized 执行期间也是可以进行线程切换的，受到线程调度机制的控制。 “但是在线程调用 wait 方法之前，就被调度器暂停了” 这个调度器是线程调度，由操作系统负责" }, { "title": "线程的六个状态", "url": "/posts/thread-status/", "categories": "Java, Concurrent", "tags": "java, thread", "date": "2019-02-04 15:33:00 +0000", "snippet": "就像生物从出生到长大、最终死亡的过程一样，线程也有自己的生命周期，在 Java 中线程的生命周期中一共有 6 种状态。 New（新创建） Runnable（可运行） Block（阻塞） Blocked（被阻塞） Waiting（等待） Timed Waiting（计时等待） Terminated（被终止）确定线程当前的状态，可以通过 getState() 方法，并且线程在任何时刻只可能处于一种状态。New 新创建New 表示线程被创建但尚未启动的状态当用 new Thread() 新建一个线程时，如果线程没有开始运行 start() 方法，所以也没有开始执行 run() 方法里面的代码，那么此时它的状态就是 New。而一旦线程调用了 start()，它的状态就会从 New 变成 Runnable，也就是状态转换成上图中中间大方框里的内容。Runnable 可运行Java 中的 Runable 状态对应操作系统线程状态中的两种状态，分别是 Running 和 Ready，也就是说，Java 中处于 Runnable 状态的线程有可能正在执行，也有可能没有正在执行，正在等待被分配 CPU 资源。所以，如果一个正在运行的线程是 Runnable 状态，当它运行到任务的一半时，执行该线程的 CPU 被调度去做其他事情，导致该线程暂时不运行，它的状态依然不变，还是 Runnable，因为它有可能随时被调度回来继续执行任务。阻塞状态Runnable 下面的三个方框，它们统称为阻塞状态，在 Java 中阻塞状态通常不仅仅是 Blocked，实际上它包括三种状态，分别是 Blocked(被阻塞）、Waiting(等待）、Timed Waiting(计时等待），这三种状态统称为阻塞状态。这三种状态具体是什么含义。Blocked 被阻塞最简单的是 Blocked，从箭头的流转方向，可以看出，从 Runnable 状态进入 Blocked 状态只有一种可能：就是进入 synchronized 保护的代码，但没有抢到 monitor 锁，无论是进入 synchronized 代码块，还是 synchronized 方法，都是一样的。再往右看，当处于 Blocked 的线程抢到 monitor 锁，就会从 Blocked 状态回到Runnable 状态。Waiting 等待线程进入 Waiting 状态有三种可能性。 没有设置 Timeout 参数的 Object.wait() 方法； 没有设置 Timeout 参数的 Thread.join() 方法； LockSupport.park() 方法。刚才强调过，Blocked 仅仅针对 synchronized monitor 锁，可是在 Java 中还有很多其他的锁，比如 ReentrantLock，如果线程在获取这种锁时没有抢到该锁，就会进入 Waiting 状态，因为本质上它执行了 LockSupport.park() 方法，所以会进入 Waiting 状态。同样，Object.wait() 和 Thread.join() 也会让线程进入 Waiting 状态。Blocked 与 Waiting 的区别是: Blocked 在等待其他线程释放 monitor 锁，而 Waiting 则是在等待某个条件，比如 join 的线程执行完毕，或者是 notify()/notifyAll() 。Timed Waiting 限期等待在 Waiting 上面是 Timed Waiting 状态，这两个状态是非常相似的，区别仅在于有没有时间限制，Timed Waiting 会等待超时，由系统自动唤醒，或者在超时前被唤醒信号唤醒。以下情况会让线程进入 Timed Waiting 状态。 设置了时间参数的 Thread.sleep(long millis) 方法； 设置了时间参数的 Object.wait(long timeout) 方法； 设置了时间参数的 Thread.join(long millis) 方法； 设置了时间参数的 LockSupport.parkNanos(long nanos) 方法和 LockSupport.parkUntil(long deadline) 方法。三种状态阻塞状态如何流转到下一个状态Blocked -&amp;gt; Next Status从 Blocked 状态进入 Runnable 状态，要求线程获取 monitor 锁；Waiting -&amp;gt; Next Status从 Waiting 状态流转到其他状态则比较特殊，由于 Waiting 是不限时的，也就是无论过了多长时间它都不会主动恢复，只有当执行了 LockSupport.unpark() 或者 join 的线程运行结束，或者被中断时才可以进入 Runnable 状态。如果其他线程调用 notify() 或 notifyAll() 来唤醒它，它会直接进入 Blocked 状态，这是因为唤醒 Waiting 线程的线程如果调用 notify() 或 notifyAll()，要求必须首先持有该 monitor 锁，所以处于 Waiting 状态的线程被唤醒时拿不到该锁，就会进入 Blocked 状态，直到执行了 notify()/notifyAll() 的唤醒它的线程执行完毕并释放 monitor 锁，才可能轮到它去抢夺这把锁，如果它能抢到，就会从 Blocked 状态回到 Runnable 状态。同样在 Timed Waiting 中执行 notify() 和 notifyAll() 也是一样的道理，它们会先进入 Blocked 状态，然后抢夺锁成功后，再回到 Runnable 状态。当然对于 Timed Waiting 而言，如果它的超时时间到了且能直接获取到锁/join的线程运行结束/被中断/调用了LockSupport.unpark()，会直接恢复到 Runnable 状态，而无需经历 Blocked 状态。Terminated 终止再来看看最后一种状态，Terminated 终止状态，要想进入这个状态有两种可能。 run() 方法执行完毕，线程正常退出。 出现一个没有捕获的异常，终止了 run() 方法，最终导致意外终止。注意点线程转换的两个注意点。 线程的状态是需要按照箭头方向来走的，比如线程从 New 状态是不可以直接进入 Blocked 状态的，它需要先经历 Runnable 状态。 线程生命周期不可逆：一旦进入 Runnable 状态就不能回到 New 状态；一旦被终止就不可能再有任何状态的变化。所以一个线程只能有一次 New 和 Terminated 状态，只有处于中间状态才可以相互转换。" }, { "title": "正确停止线程", "url": "/posts/tread-stop/", "categories": "Java, Concurrent", "tags": "java, thread", "date": "2019-02-02 15:33:00 +0000", "snippet": "启动一个线程非常简单，只需要两步： 在 run() 方法中定义需要执行的任务 调用 Thread 类的 start() 方法但是，正确停止线程就没那么容了。原理通常情况下，我们不会手动停止一个线程，而是允许线程运行到结束，然后让它自然停止。但是依然会有许多特殊的情况需要提前停止线程，比如： 用户突然关闭程序 程序运行出错重启等。在这种情况下，即将停止的线程在很多业务场景下仍然很有价值。尤其是想写一个健壮性很好，能够安全应对各种场景的程序时，正确停止线程就显得格外重要。不过 Java 没有提供简单易用，能够直接安全停止线程的能力。Java 没有安全停止线程能力的原因对于 Java 而言，最正确停止线程的方式是使用 interrupt。但是 interrupt 仅仅起到通知被停止线程的作用。而对于被停止的线程而言，它拥有完全的自主权，既可以选择立即停止，也可以选择一段时间后停止。事实上，Java 希望程序间能够相互通知、相互协作地管理线程，因为如果不了解对方（线程）正在做的工作，贸然强制停止(线程）可能会造成一些安全的问题，为了避免造成问题就需要给对方一定的时间来整理收尾工作。比如：有一个线程正在写入一个文件，此时收到终止信号，它就需要根据自身业务判断，是选择立即停止，还是将整个文件写入成功后停止，假如选择立即停止，就可能造成数据不完整，不管是中断命令的发起者，还是接收者都不希望数据出现问题。如何用 interrupt 停止线程while (!Thread.currentThread().isInterrupted() &amp;amp;&amp;amp; more work to do) { do more work}一旦调用某个线程的 interrupt() 之后，这个线程的中断标记就会被设置成 true，每个线程都有这样的标记位。当线程执行时，应该定期检查这个标记位，如果标记位被设置成 true，就说明有程序想终止该线程。上面的代码，可以看到在 while 循环体判断语句中： 首先通过 Thread.currentThread().isInterrupt() 判断线程是否被中断； 随后检查是否还有工作要做其中，&amp;amp;&amp;amp; 逻辑表示只有当两个判断条件同时满足的情况下，才会去执行下面的工作。具体栗子：public class ThreadStop implements Runnable { @Override public void run() { int count = 0; // 线程会在每次循环开始之前，检查是否被中断了 while (!Thread.currentThread().isInterrupted() &amp;amp;&amp;amp; count &amp;lt; 1000) { // 先判断线程是否被中断，而后判断 count 值是否小于 1000 System.out.println(&quot;count = &quot; + count++); // 打印 0~999 的数字，每打印一个数字 count 值加 1 } } public static void main(String[] args) throws InterruptedException { System.out.println(&quot;1. 创建线程..................&quot;); Thread thread = new Thread(new ThreadStop()); System.out.println(&quot;2. 启动线程..................，&quot; + thread.getName()); thread.start(); System.out.println(&quot;3. 线程休眠 5 毫秒..................&quot;); Thread.sleep(5); System.out.println(&quot;4. 线程中断..................，&quot; + thread.getName()); thread.interrupt(); }}在 ThreadStop 类的 run() 方法中，首先判断线程是否被中断，然后判断 count 值是否小于 1000这个线程的工作内容很简单，就是打印 0~999 的数字，每打印一个数字 count 值加 1，可以看到，线程会在每次循环开始之前，检查是否被中断了。接下来在 main 函数中会启动该线程，然后休眠 5 毫秒后立刻中断线程，该线程会检测到中断信号，于是在还没打印完 1000 个数的时候就会停下来，这种就属于通过 interrupt 正确停止线程的情况。sleep 期间能否感受到中断public class ThreadStop implements Runnable { @Override public void run() { int count = 0; // 线程会在每次循环开始之前，检查是否被中断了 while (!Thread.currentThread().isInterrupted() &amp;amp;&amp;amp; count &amp;lt; 1000) { // 先判断线程是否被中断，而后判断 count 值是否小于 1000 System.out.println(&quot;count = &quot; + count++); // 打印 0~999 的数字，每打印一个数字 count 值加 1 } } public static void main(String[] args) throws InterruptedException {// System.out.println(&quot;1. 创建线程..................&quot;);// Thread thread = new Thread(new ThreadStop());// System.out.println(&quot;2. 启动线程..................，&quot; + thread.getName());// thread.start();// System.out.println(&quot;3. 线程休眠 5 毫秒..................&quot;);// Thread.sleep(5);// System.out.println(&quot;4. 线程中断..................，&quot; + thread.getName());// thread.interrupt(); Runnable runnable = () -&amp;gt; { int num = 0; try { while (!Thread.currentThread().isInterrupted() &amp;amp;&amp;amp; num &amp;lt;= 1000) { System.out.println(num); num++; Thread.sleep(1000000); } } catch (InterruptedException e) { e.printStackTrace(); } }; Thread thread = new Thread(runnable); thread.start(); Thread.sleep(5); thread.interrupt(); }}考虑一种特殊情况，改写上面的代码，如果线程在执行任务期间有休眠需求，也就是每打印一个数字，就进入一次 sleep ，如果将 Thread.sleep() 的休眠时间设置为 1000 秒钟。主线程休眠 5 毫秒后，通知子线程中断，此时子线程仍在执行 sleep 语句，处于休眠中。那么就需要考虑一点，在休眠中的线程是否能够感受到中断通知呢？是否需要等到休眠结束后才能中断线程呢？如果是这样，就会带来严重的问题，因为响应中断太不及时了。正因为如此，Java 设计者在设计之初就考虑到了这一点。如果 sleep、wait 等可以让线程进入阻塞的方法使线程休眠了，而处于休眠中的线程被中断，那么线程是可以感受到中断信号的，并且会抛出一个 InterruptedException 异常，同时清除中断信号，将中断标记位设置成 false。这样一来就不用担心长时间休眠中线程感受不到中断了，因为即便线程还在休眠，仍然能够响应中断通知，并抛出异常。java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at cn.happymaya.base.ThreadStop.lambda$main$0(ThreadStop.java:29) at java.lang.Thread.run(Thread.java:748)进程已结束,退出代码0两种最佳处理方式在实际开发中肯定是团队协作的，不同的人负责编写不同的方法，然后相互调用来实现整个业务的逻辑。如果我们负责编写的方法需要被别人调用，同时我们的方法内调用了 sleep 或者 wait 等能响应中断的方法时，仅仅 catch 异常是不够的。void subTask1() { try { Thread.sleep(1000); } catch (InterruptedException e) { // 在这里不处理该异常是非常不好的 }}可以在方法中使用 try/catch 或在方法签名中声明 throws InterruptedException。方法签名抛异常，run() 强制 try/catch如上面的代码所示，catch 语句块里代码是空的，它并没有进行任何处理。假设线程执行到这个方法，并且正在 sleep，此时有线程发送 interrupt 通知试图中断线程，就会立即抛出异常，并清除中断信号。抛出的异常被 catch 语句块捕捉。但是，捕捉到异常的 catch 没有进行任何处理逻辑，相当于把中断信号给隐藏了，这样做是非常不合理的，那么究竟应该怎么处理呢？ 首先，可以选择在方法签名中抛出异常。/** * 在方法签名中抛出异常 * @throws InterruptedException */void subTask2() throws InterruptedException { Thread.sleep(1000);}正如代码所示，要求每一个方法的调用方有义务去处理异常。调用方要不使用 try/catch 并在 catch 中正确处理异常，要不将异常声明到方法签名中。如果每层逻辑都遵守规范，便可以将中断信号层层传递到顶层，最终让 run() 方法可以捕获到异常。而对于 run() 方法而言，它本身没有抛出 checkedException 的能力，只能通过 try/catch 来处理异常。层层传递异常的逻辑保障了异常不会被遗漏，而对 run() 方法而言，就可以根据不同的业务逻辑来进行相应的处理。再次中断除了刚才推荐的将异常声明到方法签名中的方式外，还可以在 catch 语句中再次中断线程。如下代码所示，需要在 catch 语句块中调用 Thread.currentThread().interrupt() 函数。/** * 在 catch 里面再次中断线程 */void reInterrupt() { try { Thread.sleep(2000); } catch (InterruptedException e) { Thread.currentThread().interrupt(); e.printStackTrace(); }}如果线程在休眠期间被中断，就会自动清除中断信号。假设此时手动添加中断信号，中断信号依然可以被捕捉到。这样后续执行的方法依然可以检测到这里发生过中断，可以做出相应的处理，整个线程可以正常退出。 在实际开发中不能盲目吞掉中断，如果不在方法签名中声明，也不在 catch 语句块中再次恢复中断，而是在 catch 中不作处理，这种行为是“屏蔽了中断请求”。如果盲目屏蔽了中断请求，会导致中断信号被完全忽略，最终导致线程无法正确停止。用 volatile 标记位的停止方法是错误的错误的停止方法几种停止线程的错误方法。比如 stop()，suspend() 和 resume()，这些方法已经被 Java 直接标记为 @Deprecated。如果再调用这些方法，IDE 会友好地提示，不应该再使用它们了。为什么它们不能使用，原因如下： stop() 会直接把线程停止，这样就没有给线程足够的时间来处理想要在停止前保存数据的逻辑，任务戛然而止，会导致出现数据完整性等问题； suspend() 和 resume() 的问题在于，如果线程调用 suspend()，它并不会释放锁，就开始进入休眠，如果此时仍持有锁，很容易导致死锁问题，因为这把锁在线程被 resume() 之前，是不会被释放的。 假设线程 A 调用了 suspend() 方法让线程 B 挂起，线程 B 进入休眠，而线程 B 又刚好持有一把锁，此时假设线程 A 想访问线程 B 持有的锁，但由于线程 B 并没有释放锁就进入休眠了，所以对于线程 A 而言，此时拿不到锁，也会陷入阻塞，那么线程 A 和线程 B 就都无法继续向下执行。正是因为有这样的风险，所以 suspend() 和 resume() 组合使用的方法也被废弃了。volatile 修饰标记位适用的场景public class VolatileCanStop implements Runnable{ private volatile boolean cancled = false; @Override public void run() { int num = 0; while (!cancled &amp;amp;&amp;amp; num &amp;lt;= 1000000) { if (num % 10 == 0) { System.out.println(num + &quot; 是 10 的倍数。&quot;); } num++; try { Thread.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } } } public static void main(String[] args) throws InterruptedException { VolatileCanStop r = new VolatileCanStop(); Thread thread = new Thread(r); thread.start(); Thread.sleep(3000); r.cancled = true; }}什么场景下 volatile 修饰标记位可以让线程正常停止呢？如上面代码所示，声明了一个叫作 VolatileStopThread 的类： 首先，它实现了 Runnable 接口； 然后，在 run() 中进行 while 循环； 在循环体中又进行了两层判断， 首先判断 canceled 变量的值，canceled 变量是一个被 volatile 修饰的初始值为 false 的布尔值，当该值变为 true 时，while 跳出循环； 第二个判断条件是 num 值小于 1000000（一百万），在while 循环体里，只要是 10 的倍数就打印出来，然后 num++。 接下来，首先启动线程，然后经过 3 秒钟的时间，把用 volatile 修饰的布尔值的标记位设置成 true，这样，正在运行的线程就会在下一次 while 循环中判断出 canceled 的值已经变成 true 了，这样就不再满足 while 的判断条件，跳出整个 while 循环，线程就停止了。这种情况是演示 volatile 修饰的标记位可以正常工作的情况，但是如果说某个方法是正确的，那么它应该不仅仅是在一种情况下适用，而在其他情况下也应该是适用的。volatile 修饰标记位不适用的场景接下来就用一个生产者/消费者模式的案例，来演示为什么说 volatile 标记位的停止方法是不完美的。public class Producer implements Runnable { public volatile boolean canceled = false; BlockingQueue storage; public Producer(BlockingQueue storage) { this.storage = storage; } @Override public void run() { int num = 0; try { while (num &amp;lt;= 100000 &amp;amp;&amp;amp; !canceled) { if (num % 50 == 0) { storage.put(num); System.out.println(num + &quot;是 50 的倍数，被放到仓库中了&quot;); } num++; } } catch (InterruptedException e) { e.printStackTrace(); } finally { System.out.println(&quot;生产者结束运行&quot;); } }} 首先，声明了一个生产者 Producer，通过 volatile 标记的初始值为 false 的布尔值 canceled 来停止线程； 在 run() 方法中，while 的判断语句是 num 是否小于 100000 及 canceled 是否被标记； 在 while 循环体中判断 num 如果是 50 的倍数就放到 storage 仓库中，storage 是生产者与消费者之间进行通信的存储器，当 num 大于 100000 或被通知停止时，会跳出 while 循环并执行 finally 语句块，告诉大家“生产者结束运行”。public class Consumer { BlockingQueue storage; public Consumer(BlockingQueue storage){ this.storage = storage; } public boolean needMoreNum(){ if (Math.random() &amp;gt; 0.97) { return false; } return true; }} 对于消费者 Consumer，它与生产者共用同一个仓库 storage； 并且在方法内通过 needMoreNums() 方法判断是否需要继续使用更多的数字， 生产者生产了一些 50 的倍数供消费者使用，消费者是否继续使用数字的判断条件是产生一个随机数并与 0.97 进行比较，大于 0.97 就不再继续使用数字。public class Demo { public static void main(String[] args) throws InterruptedException { ArrayBlockingQueue storage = new ArrayBlockingQueue(8); Producer producer = new Producer(storage); Thread producerThread = new Thread(producer); producerThread.start(); Thread.sleep(500); Consumer consumer = new Consumer(storage); while (consumer.needMoreNum()) { System.out.println(consumer.storage.take() + &quot;被消费了&quot;); Thread.sleep(100); } System.out.println(&quot;消费者不需要更多数据了....&quot;); // 一旦消费不需要更多数据了，应该让生产者也停下来，但是实际情况却停不下来 producer.canceled = true; System.out.println(producer.canceled); }}main 函数中： 首先创建了生产者/消费者共用的仓库 BlockingQueue storage，仓库容量是 8； 并且建立生产者将生产者放入线程后启动线程，启动后进行 500 毫秒的休眠，休眠时间保障生产者有足够的时间把仓库塞满，而仓库达到容量后就不会再继续往里塞，这时生产者会阻塞，500 毫秒后消费者也被创建出来，并判断是否需要使用更多的数字，然后每次消费后休眠 100 毫秒，这样的业务逻辑是有可能出现在实际生产中的； 当消费者不再需要数据，就会将 canceled 的标记位设置为 true，理论上此时生产者会跳出 while 循环，并打印输出“生产者运行结束”。 然而结果却不是我们想象的那样，尽管已经把 canceled 设置成 true，但生产者仍然没有停止，这是因为在这种情况下，生产者在执行 storage.put(num) 时发生阻塞，在它被叫醒之前是没有办法进入下一次循环判断 canceled 的值的，所以在这种情况下用 volatile 是没有办法让生产者停下来的，相反如果用 interrupt 语句来中断，即使生产者处于阻塞状态，仍然能够感受到中断信号，并做响应处理。总结 如何正确停止线程： 从原理上讲应该用 interrupt 来请求中断，而不是强制停止，因为这样可以避免数据错乱，也可以让线程有时间结束收尾工作。 如果是子方法的编写，遇到了 interruptedException，这样处理： 异常声明在方法中，以便顶层方法可以感知，捕获到异常， 在 catch 中再次声明中断，这样下次循环也可以感知中断。 要想正确停止线程，就要求停止方，被停止方，子方法的编写者相互配合，大家都按照一定的规范来编写代码，就可以正确地停止线程了。 哪些方法是不够好的，比如说已经被舍弃的 stop()、suspend() 和 resume()，它们由于有很大的安全风险，比如死锁风险而被舍弃，而 volatile 这种方法在某些特殊的情况下，比如线程被长时间阻塞的情况，就无法及时感受中断，所以 volatile 是不够全面的停止线程的方法。" }, { "title": "线程的实现方式 - 本质上只有一种", "url": "/posts/tread-Implementation/", "categories": "Java, Concurrent", "tags": "thread", "date": "2019-02-01 15:33:00 +0000", "snippet": "方式一：实现 Runbale 接口class RunnableThread implements Runnable { @Override public void run() { System.out.println(&quot;使用实现 Runnable 接口实现线程&quot;); }}步骤： 通过 RunnableThread 类实现 Runnable 接口； 重写 run() 方法； 将实现了 run() 方法的实例传到 Thread 类中就可以实现多线程。方式二：继承 Threadclass ExtendsThread extends Thread { @Override public void run() { System.out.println(&quot;使用继承 Thread 接口实现线程&quot;); }}步骤： 与第一种方式不同的是没有实现接口，而是继承 Thread 类； 重写了其中的 run() 方法。上面两种方式，在日常工作中经常使用。方式三：通过线程池创建线程线程池确实实现了多线程，比如：给线程池的线程数量设置成为 10 ，则会有 10 个线程来工作。对线程池而言，本质上是通过线程工厂创建线程的，默认采用 DefaultThreadFactory.DefaultThreadFactory 会给线程池创建的线程设置一些默认值，比如：线程的名字、是否为守护线程、线程的优先级 等，但是无论怎么设置这些熟悉，最终它（DefaultThreadFactory）还是通过 new Thread() 创建线程的，不同的是构造函数传入的参数要多一些。DefaultThreadFactory 的源码如下（jdk 8）：/** * The default thread factory */static class DefaultThreadFactory implements ThreadFactory { private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; DefaultThreadFactory() { SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = &quot;pool-&quot; + poolNumber.getAndIncrement() + &quot;-thread-&quot;; } public Thread newThread(Runnable r) { Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; }}方式四：有返回值的 Callable 创建线程Runnable 创建线程是无返回值的，而 Callable 和与之相关的 Future、FutureTask，可以把线程执行的结果最为返回值返回下面的栗子，实现了 Callable 接口，并且将它的泛型设置成 Integer ，然后它会返回一个随机数。static class CallableTask implements Callable&amp;lt;Integer&amp;gt; { public Integer call() throws Exception { return new Random().nextInt(); }}无论是 Callable 还是 FutureTask ，它们首先和 Runnable 一样，都是一个任务，是需要执行的。而不是说它们本身就是线程，它们可以放到线程池中执行。如下代码，submit() 方法把任务放到线程池中，并由线程池创建线程，最终都是靠线程来执行的，而子线程的创建方式仍然离不开实现 Runnable 和继承 Thread 类 */void createThread() { // 创建线程池 ExecutorService e = Executors.newFixedThreadPool(10); // 提交任务，并用 Future 提交返回结果 Future&amp;lt;Integer&amp;gt; future = e.submit(new CallableTask());}方式五：其他定时器 Timer定时器也可以实现线程。如果新建一个 Timer，令其每个 10 秒或设置两个小时后，执行一些任务，那么这时它确实也创建了线程并执行了任务，但深入了解定时器的源码，本质上它还是一个继承自Thread 的类 TimerThread，因此定时器创建线程又绕回到最基本的两种方式。TimerThread 源码如下（jdk 8）：/** * This &quot;helper class&quot; implements the timer&#39;s task execution thread, which * waits for tasks on the timer queue, executions them when they fire, * reschedules repeating tasks, and removes cancelled tasks and spent * non-repeating tasks from the queue. */class TimerThread extends Thread { // 具体实现}其他方法匿名内部类/*** 匿名内部类创建线程* 用一个匿名内部类将需要传入的 new Runnable 给实例出来*/void innerClass() { new Thread(new Runnable() { public void run() { System.out.println(Thread.currentThread().getName()); } }).start();}lamdba 表达式/** * lambda 表达式创建线程 */void lambdaThread() { new Thread(() -&amp;gt; System.out.println(Thread.currentThread().getName())).start();}像匿名内部类或 lambda 表达式这些创建线程，都仅仅是在语法层面上，不能将它们归结于实现多线程的方式。总结实现线程只有一种方式其他的创建方式，比如线程池或是定时器，仅仅是在 new Thread() 外做了一层封装，如果把这些都叫作一种新的方式，那么创建线程的方式便会千变万化、层出不穷。比如 JDK 更新了，它可能会多出几个类，会把 new Thread() 重新封装，表面上看又会是一种新的实现线程的方式，透过现象看本质，打开封装后，会发现它们最终都是基于 Runnable 接口或继承 Thread 类实现的。基于 Runnable 接口和继承 Thread 类实现的 是一样的理由是： 启动线程需要调用 start() 方法，而 start() 方法最终还会调用 run() 方法； 基于 Runable 接口 run() 方法的实现如下： /** * If this thread was constructed using a separate * &amp;lt;code&amp;gt;Runnable&amp;lt;/code&amp;gt; run object, then that * &amp;lt;code&amp;gt;Runnable&amp;lt;/code&amp;gt; object&#39;s &amp;lt;code&amp;gt;run&amp;lt;/code&amp;gt; method is called; * otherwise, this method does nothing and returns. * &amp;lt;p&amp;gt; * Subclasses of &amp;lt;code&amp;gt;Thread&amp;lt;/code&amp;gt; should override this method. * * @see #start() * @see #stop() * @see #Thread(ThreadGroup, Runnable, String) */ @Override public void run() { if (target != null) { // 判断 target 是否等于 null target.run(); // 如果不等于 null，执行 target.run() } } run() 方法中的 target 实际上就是一个 Runable，，即使用 Runnable 接口实现线程并传给 Thread 类的对象。 继承 Thread 类实现。继承 Thread 类之后，会把上述的 run() 方法重写，重写后 run() 方法里直接就是所需要执行的任务，但它最终还是需要调用 thread.start() 方法来启动线程，而 start() 方法最终也会调用这个已经被重写的 run() 方法来执行它的任务由此，创建线程只有一种方式，就是构造一个 Thread 类，这是创建线程的唯一方式。可以得出结论：本质上，实现线程只有一种方式，而要想实现线程执行的内容，却有两种方式， 实现 Runnable 接口的方式或是继承 Thread 类重写 run() 方法的方式，把想要执行的代码传入，让线程去执行，在此基础上，如果还想有更多实现线程的方式，比如线程池和 Timer 定时器只需要在此基础上进行封装即可。实现 Runnable 接口比继承 Thread 类实现线程要好处有三点: 从代码的架构考虑，Runnable 里只有一个 run() 方法，它定义了需要执行的内容，在这种情况下，实现了 Runnable 与 Thread 类的解耦，Thread 类负责线程启动和属性设置等内容，权责分明； 某些情况下可以提高性能 如果使用继承 Thread 类方式，每次执行一次任务，都需要新建一个独立的线程，执行完任务后线程走到生命周期的尽头被销毁，如果还想执行这个任务，就必须再新建一个继承了 Thread 类的类，如果此时执行的内容比较少，那么它所带来的开销并不大，相比于整个线程从开始创建到执行完毕被销毁，这一系列的操作比 run() 方法打印文字本身带来的开销要大得多，相当于捡了芝麻丢了西瓜，得不偿失； 如果使用实现 Runnable 接口的方式，就可以把任务直接传入线程池，使用一些固定的线程来完成任务，不需要每次新建销毁线程，大大降低了性能开销。 Java 语言不支持双继承，如果类继承了 Thread 类，那么后续就没有办法再继承其他的类，这样一来，如果未来这个类需要继承其他类实现一些功能上的拓展，它就没有办法做到了，相当于限制了代码未来的可拓展性。 shi由此，应该优先选择通过实现 Runnable 接口的方式来创建线程（实现 runnable 可以说明使用了组合的方式要比继承的方式要好）。" }, { "title": "启用 Google Analytics 收集页面的浏览量", "url": "/posts/enable-google-pv/", "categories": "Blog, Tutorial", "tags": "google analytics, pageviews, 浏览量, 谷歌分析", "date": "2019-01-03 23:32:00 +0000", "snippet": "This post is to enable Page Views on the Chirpy theme based blog that you just built. This requires technical knowledge and it’s recommended to keep the google_analytics.pv.* empty unless you have a good reason. If your website has low traffic, the page views count would discourage you to write more blogs. With that said, let’s start with the setup.Set up Google AnalyticsCreate GA account and propertyFirst, you need to set up your account on Google analytics. While you create your account, you must create your first Property as well. Head to https://analytics.google.com/ and click on Start Measuring Enter your desired Account Name and choose the desired checkboxes Enter your desired Property Name. This is the name of the tracker project that appears on your Google Analytics dashboard Enter the required information About your business Hit Create and accept any license popup to set up your Google Analytics account and create your propertyCreate Data StreamWith your property created, you now need to set up Data Stream to track your blog traffic. After you signup, the prompt should automatically take you to create your first Data Stream. If not, follow these steps: Go to Admin on the left column Select the desired property from the drop-down on the second column Click on Data Streams Add a stream and click on Web Enter your blog’s URLIt should look like this:Now, click on the new data stream and grab the Measurement ID. It should look something like G-V6XXXXXXXX. Copy this to your _config.yml file:google_analytics: id: &#39;G-V6XXXXXXX&#39; # fill in your Google Analytics ID # Google Analytics pageviews report settings pv: proxy_endpoint: # fill in the Google Analytics superProxy endpoint of Google App Engine cache_path: # the local PV cache data, friendly to visitors from GFW regionWhen you push these changes to your blog, you should start seeing the traffic on your Google Analytics. Play around with the Google Analytics dashboard to get familiar with the options available as it takes like 5 mins to pick up your changes. You should now be able to monitor your traffic in real time.Setup Page ViewsThere is a detailed tutorial available to set up Google Analytics superProxy. But, if you are interested to just quickly get your Chirpy-based blog display page views, follow along. These steps were tested on a Linux machine. If you are running Windows, you can use the Git bash terminal to run Unix-like commands.Setup Google App Engine Visit https://console.cloud.google.com/appengine Click on Create Application Click on Create Project Enter the name and choose the data center close to you Select Python language and Standard environment Enable billing account. Yeah, you have to link your credit card. But, you won’t be billed unless you exceed your free quota. For a simple blog, the free quota is more than sufficient. Go to your App Engine dashboard on your browser and select API &amp;amp; Services from the left navigation menu Click on Enable APIs and Services button on the top Enable the following APIs: Google Analytics API On the left, Click on OAuth Consent Screen and accept Configure Consent Screen. Select External since your blog is probably hosted for the public. Click on Publish under Publishing Status Click on Credentials on the left and create a new OAuth Client IDs credential. Make sure to add an entry under Authorized redirect URIs that matches: https://&amp;lt;project-id&amp;gt;.&amp;lt;region&amp;gt;.r.appspot.com/admin/auth Note down the Your Client ID and Your Client Secret. You’ll need this in the next section. Download and install the cloud SDK for your platform: https://cloud.google.com/sdk/docs/quickstart Run the following commands: [root@bc96abf71ef8 /]# gcloud init~snip~Go to the following link in your browser: https://accounts.google.com/o/oauth2/auth?response_type=code&amp;amp;client_id=XYZ.apps.googleusercontent.com&amp;amp;redirect_uri=ABCDEFGEnter verification code: &amp;lt;VERIFICATION CODE THAT YOU GET AFTER YOU VISIT AND AUTHENTICATE FROM THE ABOVE LINK&amp;gt;You are logged in as: [blah_blah@gmail.com].Pick cloud project to use:[1] chirpy-test-300716[2] Create a new projectPlease enter numeric choice or text value (must exactly match listitem): 1[root@bc96abf71ef8 /]# gcloud info# Your selected project info should be displayed here Setup Google Analytics superProxy Clone the Google Analytics superProxy project on Github: https://github.com/googleanalytics/google-analytics-super-proxy to your local. Remove the first 2 lines in the src/app.yaml file: - application: your-project-id- version: 1 In src/config.py, add the OAUTH_CLIENT_ID and OAUTH_CLIENT_SECRET that you gathered from your App Engine Dashboard. Enter any random key for XSRF_KEY, your config.py should look similar to this #!/usr/bin/python2.7__author__ = &#39;pete.frisella@gmail.com (Pete Frisella)&#39;# OAuth 2.0 Client SettingsAUTH_CONFIG = { &#39;OAUTH_CLIENT_ID&#39;: &#39;YOUR_CLIENT_ID&#39;, &#39;OAUTH_CLIENT_SECRET&#39;: &#39;YOUR_CLIENT_SECRET&#39;, &#39;OAUTH_REDIRECT_URI&#39;: &#39;%s%s&#39; % ( &#39;https://chirpy-test-XXXXXX.ue.r.appspot.com&#39;, &#39;/admin/auth&#39; )}# XSRF SettingsXSRF_KEY = &#39;OnceUponATimeThereLivedALegend&#39; You can configure a custom domain instead of https://PROJECT_ID.REGION_ID.r.appspot.com.But, for the sake of keeping it simple, we will be using the Google provided default URL. From inside the src/ directory, deploy the app [root@bc96abf71ef8 src]# gcloud app deployServices to deploy:descriptor: [/tmp/google-analytics-super-proxy/src/app.yaml]source: [/tmp/google-analytics-super-proxy/src]target project: [chirpy-test-XXXX]target service: [default]target version: [VESRION_NUM]target url: [https://chirpy-test-XXXX.ue.r.appspot.com]Do you want to continue (Y/n)? YBeginning deployment of service [default]...╔════════════════════════════════════════════════════════════╗╠═ Uploading 1 file to Google Cloud Storage ═╣╚════════════════════════════════════════════════════════════╝File upload done.Updating service [default]...done.Setting traffic split for service [default]...done.Deployed service [default] to [https://chirpy-test-XXXX.ue.r.appspot.com]You can stream logs from the command line by running:$ gcloud app logs tail -s defaultTo view your application in the web browser run:$ gcloud app browse Visit the deployed service. Add a /admin to the end of the URL. Click on Authorize Users and make sure to add yourself as a managed user. If you get any errors, please Google it. The errors are self-explanatory and should be easy to fix. If everything went good, you’ll get this screen:Create Google Analytics QueryHead to https://PROJECT_ID.REGION_ID.r.appspot.com/admin and create a query after verifying the account. GA Core Reporting API query request can be created in Query Explorer.The query parameters are as follows: start-date: fill in the first day of blog posting end-date: fill in today (this is a parameter supported by GA Report, which means that it will always end according to the current query date) metrics: select ga:pageviews dimensions: select ga:pagePathIn order to reduce the returned results and reduce the network bandwidth, we add custom filtering rules 1: filters: fill in ga:pagePath=~^/posts/.*/$;ga:pagePath!@=. Among them, ; means using logical AND to concatenate two rules. If the site.baseurl is specified, change the first filtering rule to ga:pagePath=~^/BASE_URL/posts/.*/$, where BASE_URL is the value of site.baseurl. After Run Query, copy the generated contents of API Query URI at the bottom of the page and fill in the Encoded URI for the query of SuperProxy on GAE.After the query is saved on GAE, a Public Endpoint (public access address) will be generated, and we will get the query result in JSON format when accessing it. Finally, click Enable Endpoint in Public Request Endpoint to make the query effective, and click Start Scheduling in Scheduling to start the scheduled task.Configure Chirpy to Display Page ViewOnce all the hard part is done, it is very easy to enable the Page View on Chirpy theme. Your superProxy dashboard should look something like below and you can grab the required values.Update the _config.yml file of Chirpy project with the values from your dashboard, to look similar to the following:google_analytics: id: &#39;G-V6XXXXXXX&#39; # fill in your Google Analytics ID pv: proxy_endpoint: &#39;https://PROJECT_ID.REGION_ID.r.appspot.com/query?id=&amp;lt;ID FROM SUPER PROXY&amp;gt;&#39; cache_path: # the local PV cache data, friendly to visitors from GFW regionNow, you should see the Page View enabled on your blog.Reference Google Analytics Core Reporting API: Filters &amp;#8617; " }, { "title": "类加载机制", "url": "/posts/classloader-base/", "categories": "Java, Base", "tags": "reflect, 反射", "date": "2018-11-04 03:34:00 +0000", "snippet": "类加载器（ClassLoader）就是加载其他类的类，负责将字节码文件加载到内存，创建 Class 对象。与反射、注解和动态代理一样，在大部分的应用编程中，需要自己实现 ClassLoader。不过，理解类加载的机制和过程，有助于更好地理解反射、注解和动态代理。比如在反射中的 Class 的静态方法 Class.forName。ClassLoader，一般是系统提供的，不需要自己实现，不过，通过创建自定义的 ClassLoader，可以实现一些强大灵活的功能，比如： 热部署。在不重启 Java 程序的情况下，动态替换类的实现，比如 Java Web 开发中的 JSP 技术就利用自定义的 ClassLoader 实现修改 JSP 代码即生效，OSGI(Open Service Gateway Initiative) 框架使用自定义ClassLoader 实现动态更新； 应用的模块化和相互隔离。不同的 ClassLoader 可以加载相同的类但互相隔离、互不影响。Web 应用服务器如 Tomcati 利用这一点在一个程序中管理多个 Web 应用程序，每个 Web 应用使用自己的 ClassLoader，这些 Web 应用互不干扰。OSGI 和J ava9 利用这一点实现了一个动态模块化架构，每个模块有自己的ClassLoader，不同模块可以互不干扰。 从不同地方灵活加载。系统默认的 ClassLoader 一般从本地的 .class 文件或 jar 文件中加载字节码文件，通过自定义的ClassLoader,我们可以从共享的Web服务器、数据库、缓存服务器等其他地方加载字节码文件。理解自定义 ClassLoader 有助于理解这些系统程序和框架，如 Tomat、JSP、OSGI，在业务需要的时候，也可以借助自定义 ClassLoader 实现动态灵活的功能。类加载机制和过程运行 Java 程序，就是执行 java 这个命令，指定包含 main 方法的完整类名，以及一个 classpath，即类路径。类路径可以有多个： 对于直接的 class 文件，路径是 class 文件的根目录； 对于jar包，路径是 jar 包的完整名称（包括路径和jar包名）。Java 运行时，会根据类的完全限定名寻找并加载类，寻找的方式基本就是在系统类和指定的类路径中寻找，如果是 class 文件的根目录，则直接查看是否有对应的子目录及文件；如果是 jar 文件，则首先在内存中解压文件，然后再查看是否有对应的类。负责加载类的类就是类加载器，它的输入是完全限定的类名，输出是 Class 对象。类加载器不是只有一个，一般程序运行时，都会有三个（适用于Java9 之前，Java9 引入了模块化，基本概念是类似的，但有一些变化）。 启动类加载器（Bootstrap ClassLoader）：该加载器是 Java 虚拟机实现的一部分，不是 Java 语言实现的，一般是 C++ 实现的，它负责加载 Java 的基础类，主要是 &amp;lt;JAVA_HOME-/lib/rt.jar，日常用的 Java 类库比如String、ArrayList 等都位于该包内。 扩展类加载器（Extension ClassLoader）：这个加载器的实现类是sun.misc.Launcher$ExtClassLoader，它负责加载 Java 的一些扩展类，一般是&amp;lt;JAVA_HOME&amp;gt;Iib/ext目录中的jar包。 应用程序类加载器(Application ClassLoader)：这个加载器的实现类是sun.misc.LauncherSAppClassLoader，它负责加载应用程序的类，包括自己写的和引入的第三方法类库，即所有在类路径中指定的类。 这三个类加载器有一定的关系，可认为是父子关系，Application ClassLoader 的父亲是 Extension ClassLoader，Extension 的父亲是 Bootstrap ClassLoader.。注意不是父子继承关系，而是父子委派关系。子 ClassLoader 有一个变量 parent 指向父 ClassLoader，在子 ClassLoader 加载类时，一般会首先通过父 ClassLoader 加载，具体来说，在加载一个类时，基本过程是： 判断是否已经加载过了，加载过了，直接返回 Class 对象，一个类只会被一个 ClassLoader 加载一次； 如果没有被加载，先让父 ClassLoader 去加载，如果加载成功，返回得到的 Class 对象； 在父 ClassLoader 没有加载成功的前提下，自己尝试加载类。这个过程一般被称为”双亲委派“模型，即优先让父 ClassLoader 去加载。为什么要先让父 ClassLoader 去加载呢？因为这样可以避免 Java 类库被覆盖的问题。比如，自己自定义了一个类 java.lang.String，通过双亲委派，java.lang.String 只会被 Bootstrap ClassLoader 加载，避免自定义的 String 覆盖 Java 类库的定义。不过需要注意的是，“双亲委派” 只是一般模型，但也有一些例外，比如： 自定义的加载顺序：尽管不被建议，自定义的 ClassLoader 可以不遵从“双亲委派这个约定，不过，即使不遵从，以 java 开头的类也不能被自定义类加载器加载，这是由 Java 的安全机制保证的，以避免混乱； 网状加载顺序：在 OSGI 框架和 Java9 模块化系统中，类加载器之间的关系是一个网，每个模块有一个类加载器，不同模块之间可能有依赖关系，在一个模块加载一个类时，可能是从自己模块加载，也可能是委派给其他模块的类加载器加载。 父加载器委派给子加载器加载：典型的例子有 JNDI 服务（Java Naming and Directory Interface）,它是 Java 企业级应用中的一项服务。一个程序运行时，会创建一个 Application ClassLoader，在程序中用到 ClassLoader 的地方，如果没有指定，一般用的都是这个 ClassLoader，所以，这个 ClassLoader 也被称为系统类加载器（System ClassLoader）。ClassLoader类 ClassLoader 是一个抽象类。Application ClassLoader 的具体实现类是：sun.misc.Launcher$AppClassLoader；Extension ClassLoader 的具体实现类是：sun.misc.Launcher$ExtClassLoader;Bootstrap ClassLoader 不是由 Java 实现的，没有对应的类。每个 Class 对象都有一个方法，可以获取实际加载它的 ClassLoader，方法是：public ClassLoader getClassLoader() { ClassLoader cl = getClassLoader0(); if (cl == null) { return null; } SecurityManager sm = System.getSecurityManager(); if (sm != null) { ClassLoader.checkClassLoaderPermission(cl, Reflection.getCallerClass()); } return cl;}有一个方法，可以获取它的父 ClassLoader：public final ClassLoader getParent()如果 ClassLoader 是 Bootstrap ClassLoader，返回值为 null，比如：public class ClassLoaderDemo { public static void main(String[] args) { ClassLoader cl = ClassLoaderDemo.class.getClassLoader(); while (cl != null) { System.out.println(cl.getClass().getName()); cl = cl.getParent(); } System.out.println(String.class.getClassLoader()); }}输出结果为：sun.misc.Launcher$AppClassLoadersun.misc.Launcher$ExtClassLoadernullClassLoader 有一个静态方法，可以获取默认的系统类加载器：public static ClassLoader getSystemClassLoader()有一个主要方法，用于加载类：public final ClassLoader getClassLoader()比如：public class ClassLoaderDemo { public static void main(String[] args) { try { Class&amp;lt;?&amp;gt; cls = cl.loadClass(&quot;java.util.ArrayList&quot;); ClassLoader actualLoader = cls.getClassLoader(); System.out.println(actualLoader); } catch (ClassNotFoundException e) { e.printStackTrace(); } }}// 输出结果为：null由于委派机制，Class 的 getClassLoader 方法返回的不一定是调用 load-Class 的 ClassLoader，比如，上面代码中，java.util.ArrayList 实际由 BootStrap ClassLoader加载，所以返回值就是 null。在反射中，Class 的两个静态方法 forName：public static Class&amp;lt;?&amp;gt; forName(String className);public static Class&amp;lt;?&amp;gt; forName(String name, boolean initialize, ClassLoader loader) 第一个方法，使用系统类加载器加载， 第二个方法，指定 ClassLoader，参数 initialize 表示加载后是否执行类的初始化代码（如static语句块），没有指定默认为true。ClassLoader 的 loadClass 方法 与 Class 的 forName方法 都可以加载类，它们基本是一样的，不过，ClassLoader 的 loadClass 不会执行类的初始化代码，例子：public class CLInitDemo { public static class Hello { static { System.out.println(&quot;hello&quot;); } } public static void main(String[] args) { ClassLoader cl = ClassLoader.getSystemClassLoader(); String className = CLInitDemo.class.getName() + &quot;$Hello&quot;; try { Class&amp;lt;?&amp;gt; cls = cl.loadClass(className);// Class&amp;lt;?&amp;gt; cla = Class.forName(className); } catch (ClassNotFoundException e) { e.printStackTrace(); } }}使用 ClassLoader 加载静态内部类 Hello，Hello 有一个 static 语句快，输出 ”hello“，运行该程序，类被加载了，但没有任何输出，也就是 static 语句块没有被执行。如果将 loadClass 的语句换为：Class&amp;lt;?&amp;gt; cla = Class.forName(className);则 static 语句块会被执行，屏幕将输出 ”hello“。ClassLoader 的 loadClass 代码是：public Class&amp;lt;?&amp;gt; loadClass(String name) throws ClassNotFoundException { return loadClass(name, false);}它调用了另一个 loadClass 方法，主要代码为：protected Class&amp;lt;?&amp;gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // First, check if the class has already been loaded // 首先，检查类是否已经被加载了 Class&amp;lt;?&amp;gt; c = findLoadedClass(name); if (c == null) { // 没有加载，先委派父 ClassLoader 或 BootStrap ClassLoader 去加载 long t0 = System.nanoTime(); try { if (parent != null) { // 委派父 ClassLoader，resolve 参数固定为 false c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found // from the non-null parent class loader // 没有找到，捕获异常，以便尝试自己加载 } if (c == null) { // If still not found, then invoke findClass in order // to find the class. // 自己加载，findClass 才是当前 ClassLoader 的真正加载方法 long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { // 链接，执行 static 语句块 resolveClass(c); } return c; } }参数 resolve 类似 Class.forName 中的参数 initialize： 其默认值为 false，即使通过自定义 ClassLoader 重写 loadClass； 设置 resolve 为 true，它调用父 ClassLoader 的时候，传递的也是固定的 false。findClass 是一个 protected 方法，类 ClassLoader 的默认实现就是抛出 ClassNotFoundException，子类应该重写该方法，实现自己的加载逻辑。类加载的栗子：可配置的策略很多应用使用面向接口的编程，接口具体的实现类可能有很多，适用于不同的场合，具体使用哪个实现类在配置文件中配置，通过更改配置，不用改变代码，就可以改变程序的行为，在设计模式中，这是一种策略模式。 定义一个服务接口 IService： public class ServiceB implements IService{ @Override public void action() { System.out.println(&quot;service B action&quot;); }} 查看配置文件，根据配置的实现类，自己加载，使用反射创建实例对象 public class ConfigurableStrategyDemo { public static IService createService() { try { Properties properties = new Properties(); String filename = &quot;D:\\\\happymaya\\\\notes-java\\\\dynamic-agent\\\\src\\\\main\\\\resources\\\\config.properties&quot;; properties.load(new FileInputStream(filename)); String className = properties.getProperty(&quot;service&quot;); Class&amp;lt;?&amp;gt; cls = Class.forName(className); return (IService)cls.newInstance(); } catch (Exception e) { throw new RuntimeException(e); } } public static void main(String[] args) { IService service = createService(); service.action(); }} config.properties 的内容为： service=cn.happymaya.dynamic.classloader.ServiceB ServiceB 的内容为： public class ServiceB implements IService{ @Override public void action() { System.out.println(&quot;service B action&quot;); }} 输出结果为： service B action 自定义 ClassLoaderJava 类加载机制的强大之处在于，可以创建自定义的 ClassLoader，自定义 Class-Loader 是 Tomcat实现应用隔离、支持SP、OSGI实现动态模块化的基础。自定义 ClassLoader，一般而言，继承类 ClassLoader，而后重写 findClass 就可以了。怎么实现 findClass 呢？使用自己的逻辑寻找 class 文件字节码的字节形式，找到后，使用如下方法转换为 Class 对象：// name 表示 类名// b 存放字节码数据的字节数组// off 有效数据的开始// len 有效数据的长度protected final class&amp;lt;?&amp;gt;defineclass(String name,byte[]b,int off,int len)栗子：public class MyClassLoader extends ClassLoader { private static final String BASE_DIR = &quot;&quot;; @Override protected Class&amp;lt;?&amp;gt; findClass(String name) throws ClassNotFoundException { String fileName = name.replaceAll(&quot;\\\\.&quot;, &quot;/&quot;); fileName = BASE_DIR + fileName + &quot;.class&quot;; try { byte[] bytes = BinaryFileUtils.readFileToByteArray(fileName); return defineClass(name, bytes, 0, bytes.length); } catch (IOException e) { e.printStackTrace(); } return super.findClass(name); }}MyClassLoader 从 BASE_DIR 下的路径中加载类，使用自定义实现的 readFileToByteArray 方法读取文件，并转换为 byte 数组。MyClassLoader 没有指定父 Class-Loader，默认是系统类加载器，即ClassLoader.getSystemClassLoader() 的返回值。不过，Class-Loader有一个可重写的构造方法，可以指定父 ClassLoader ：protected ClassLoader(ClassLoader parent) { this(checkCreateClassLoader(), parent);}将 BASE_DIR 加到 classpath ，确实可以，这只是基本用法，实际中，还可以从 Web 服务器、数据库或缓存服务器获取 bytes 数组，这就不是系统类加载器能做到的了。不过，不把 BASE_DIR 放到 classpath 中，而是使用 MyClassLoader 加载，还有一个很大的好处，那就是可以创建多个MyClassLoader，对同一个类，每个 MyClassLoader 都可以加载一次，得到同一个类的不同Class对象，比如：public static void main(String[] args) throws ClassNotFoundException { String className = &quot;cn.happymaya.dynamic.classloader.ServiceB&quot;; MyClassLoader classLoader1 = new MyClassLoader(); Class&amp;lt;?&amp;gt; class1 = classLoader1.loadClass(className); MyClassLoader classLoader2 = new MyClassLoader(); Class&amp;lt;?&amp;gt; class2 = classLoader2.loadClass(className); if (class2 == class1) { System.out.println(&quot;different classes&quot;); }}classLoader1 和 classLoader2 是两个不同的 ClassLoader。class1 和class2 对应的类名一样，但它们是不同的对象。它们的作用有亮点，如下： 实现隔离。一个复杂的程序，内部可能按模块组织，不同模块可能使用同一个类，但使用的是不同版本，如果使用同一个类加载器，它们是无法共存的，不同模块使用不同的类加载器就可以实现隔离，Tomcat 使用它隔离不同的 Web 应用，OSGI 使用它隔离不同模块。 实现热部署。使用同一个 ClassLoader，类只会被加载一次，加载后，即使 class 文件已经变了，再次加载，得到的也还是原来的 Class 对象，而使用 MyClassLoader，则可以先创建一个新的 ClassLoader，再用它加载 Class，得到的 Class 对象就是新的，从而实现动态更新。利用自定义的 ClassLoader 热部署热部署，就是在不重启应用的情况下，当类的定义（即字节码文件）修改后，能够替换该 Class 创建的对象。 使用面向接口编程方式，先定义一个接口 HelloService： public interface IHelloService { public void sayHello();} 实现类 HelloServiceImpl： public class HelloServiceImpl implements IHelloService{ @Override public void sayHello() { System.out.println(&quot;say Hello....&quot;); }} 演示类是 HotDeployDemo，主要定义了以下静态变量： public class HotDeployDemo { public static final String CLASS_NAME = &quot;cn.happymaya.dynamic.classloader.HelloServiceImpl&quot;; public static final String FILE_NAME = &quot;data/byte/&quot; + CLASS_NAME.replaceAll(&quot;\\\\.&quot;,&quot;/&quot;) + &quot;.class&quot;; private static volatile IHelloService helloService;} CLASS_NAME，表示实现类名称； FILE_NAME，表示具体的class文件路径； helloService 是 IHelloService 实例。 当 CLASS._NAME 代表的类字节码改变后，希望重新创建helloService,反映最新的代码，怎么做呢？先看用户端获取 IHelloService 的方法： public static IHelloService getHelloService() { if (helloService != null) { return helloService; } synchronized (HotDeployDemo.class) { if (helloService == null) { helloService = createHelloService(); } return helloService; }} 这只是一个单例模式，createHelloService() 的代码为： private static IHelloService createHelloService() { try { MyClassLoader classLoader = new MyClassLoader(); Class&amp;lt;?&amp;gt; cls = classLoader.loadClass(CLASS_NAME); if (cls != null) { return (IHelloService) cls.newInstance(); } } catch (Exception e) { e.printStackTrace(); } return null;} 它使用 MyClassLoader 加载类，并利用反射创建实例，它假定实现类有一个 public 无参构造方法。 在调用 HelloService 的方法时，客户端总是先通过 getHelloService 获取实例对象。模拟一个客户端线程，它不停地获取 IHelloService 对象，并调用其方法，然后睡眠 1 秒钟，其代码为： public static void client() { Thread t = new Thread() { @Override public void run() { try { while (true) { IHelloService helloService = getHelloService(); helloService.sayHello(); Thread.sleep(1000); } } catch (InterruptedException e) { throw new RuntimeException(e); } } }; t.start(); } 怎么晓得类的 class 文件发送变化，并重新创建 helloService 对象，使用一个单独的线程模拟这个过程： public static void monitor() { Thread t = new Thread() { private long lastModified = new File(FILE_NAME).lastModified(); @Override public void run() { try { while (true) { Thread.sleep(1000); long now = new File(FILE_NAME).lastModified(); if (now != lastModified) { lastModified = now; reloadHelloService(); } } } catch(InterruptedException e){ throw new RuntimeException(e); } } }; t.start();} 使用文件的最后修改时间来跟踪文件是否发生了变化，当文件修改后，调用 reloadHelloService() 来重新加载，代码为： public static void reloadHelloService() { helloService = createHelloService();jie} 它就是利用 MyClassLoader 重新创建 HelloService，创建后，赋值给 helloService，这样，下次 getHelloService() 获取到的就是最新的。 在主程序中启动 client 和 monitor 线程，代码为： public static void main(String[] args) { monitor(); client();} 在运行过程中，替换 Hellolmpl.class，可以看到行为会变化，目录下准备了两个不同的实现类：HelloImpl_origin.class7 和 HelloImpl_revised.class，在运行过程中替换，会看到输出不一样。 使用 cp 命令修改 Hellolmpl.class，如果其内容与 Hellolmpl_origin.class 一样，输出为”hello”；如果与HelloImpl_revised.class 一样，输出为”hello revised”。 总结Java 9 引入了模块的概念。在模块化系统中，类加载的过程有一些变化，扩展类的目录被删除掉了，原来的扩展类加载器没有了，增加了一个平台类加载器(Platform Class Loader)，角色类似于扩展类加载器，它分担了一部分启动类加载器的职责，另外，加载的顺序也有一些变化。之后再说！" }, { "title": "正向代理与反向代理", "url": "/posts/positive-proxy-and-reverse-proxy/", "categories": "Blog, Notes", "tags": "proxy, 正向代理, 反向代理", "date": "2018-11-02 03:34:00 +0000", "snippet": "代理 proxy代理其实就是一个中介，A 和 B 本来可以直连，中间插入一个 C，C 就是中介。刚开始的时候，代理多数是帮助内网 client 访问外网 server 用的。后来出现了反向代理，”反向”这个词在这儿的意思其实是指方向相反，即代理将来自外网客户端的请求转发到内网服务器，从外到内。正向代理 Forward Proxy正向代理类似一个跳板机，代理访问外网资源。就像，我国内访问谷歌，直接访问访是问不到，通常我们会通过一个 VPN 来访问 谷歌，而这个 VPN 就是一个正向代理服务器，请求会先发送到代理服务器，代理服务器再访问谷歌，然后取到返回数据，再返回给我们，这样我们就能访问谷歌了。正向代理的用途： 访问原来无法访问的资源，如 Google 可以做缓存，加速访问资源 对客户端访问授权，上网进行认证 代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息反向代理 Reverse Proxy反向代理（Reverse Proxy），实际运行方式是指以代理服务器来接受 Internet 上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给Internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。反向代理的作用： 保证内网的安全，阻止web攻击，大型网站，通常将反向代理作为公网访问地址，Web服务器是内网 负载均衡，通过反向代理服务器来优化网站的负载总结正向代理既是客户端代理，代理客户端，服务端不知道实际发起请求的客户端。反向代理既是服务端代理，代理服务端，客户端不知道实际提供服务的服务端。正向代理中，proxy 和 client 同属一个 LAN，对 server 透明；反向代理中，proxy 和 server 同属一个 LAN，对 client 透明。实际上proxy在两种代理中做的事都是代为收发请求和响应，不过从结构上来看正好左右互换了下，所以把后出现的那种代理方式叫成了反向代理总之，正向代理: 买票的黄牛；反向代理: 租房的代理。" }, { "title": "java 以及 tocmat 使用正向代理案例", "url": "/posts/java-implement-forward-proxy/", "categories": "Blog, Notes", "tags": "proxy, 正向代理, 反向代理, java, tomcat", "date": "2018-11-02 03:34:00 +0000", "snippet": "最近部署的应用服务器无法直接访问互联网, 只好在前置服务器上配置了 squid 做正向代理, 应用采用java语言, 这里回顾总结下 java 以及 tomcat 使用http正向代理的几种方法：squid使用3128端口java 正向代理设置设置 java 的启动参数# http 代理 -Dhttp.proxyHost=proxy server ip -Dhttp.proxyPort=proxy server ip port# https 代理-Dhttps.proxyHost=proxy server ip -Dhttps.proxyPort=proxy server ip port# https代理比如，在 tomcat 的 /bin/catalic.sh 下加入下面的代码：```bash在 java 代码初始化时设置环境变量 http 代理```bashSystem.setProperty(“http.proxyHost”, “代理ip”);System.setProperty(“http.proxyPort”, “代理ip端口`”); https 代理 System.setProperty(&quot;https.proxyHost&quot;, &quot;代理ip&quot;);System.setProperty(&quot;https.proxyPort&quot;, &quot;3128&quot;); 在 java 代码中设置使用代理：URL url = new URL(&quot;https://某网址&quot;);Proxy proxy = new Proxy(Proxy.Type.DIRECT.HTTP, new InetSocketAddress(&quot;代理ip&quot;, 3128)); HttpURLConnection conn = (HttpURLConnection) url.openConnection(proxy);如果操作系统已经配置好代理，可以直接使用System.setProperty(&quot;java.net.useSystemProxies&quot;, &quot;true&quot;);; 当然也可以在启动时增加 -Djava.net.useSystemProxies=true; 如果某些网址不需要使用代理，可以单独进行设置，比如：-Dhttp.nonProxyHosts=&quot;www.hongxuejing.com|localhost&quot;httpPost设置代理public JSONObject httpPost(String url, JSONObject jsonParam) { // post请求返回结果 CloseableHttpClient httpClient = HttpClients.createDefault(); JSONObject jsonResult = null; HttpPost httpPost = new HttpPost(url); HttpHost target = new HttpHost(&quot;qa.chetong.net/ailoss/getAiLossPageURL&quot;, 8080, &quot;http&quot;); HttpHost proxy = new HttpHost(&quot;10.1.200.95&quot;, 3128, &quot;http&quot;); // 设置请求和传输超时时间 RequestConfig requestConfig = RequestConfig.custom().setSocketTimeout(2000).setConnectTimeout(2000).setProxy(proxy).build(); httpPost.setConfig(requestConfig); try { if (null != jsonParam) { // 解决中文乱码问题 StringEntity entity = new StringEntity(jsonParam.toString(), &quot;utf-8&quot;); entity.setContentEncoding(&quot;UTF-8&quot;); entity.setContentType(&quot;application/json&quot;); httpPost.setEntity(entity); } CloseableHttpResponse result = httpClient.execute(target,httpPost); // 请求发送成功，并得到响应 if (result.getStatusLine().getStatusCode() == HttpStatus.SC_OK) { String str = &quot;&quot;; try { // 读取服务器返回过来的json字符串数据 str = EntityUtils.toString(result.getEntity(), &quot;utf-8&quot;); // 把json字符串转换成json对象 jsonResult = JSONObject.parseObject(str); } catch (Exception e) { log.error(&quot;post请求提交失败:&quot; + url, e); } } } catch (IOException e) { log.error(&quot;post请求提交失败:&quot; + url, e); System.out.println(e); } finally { httpPost.releaseConnection(); } return jsonResult; }//测试main方法public static void main(String[] args) { String url = &quot;&quot;; String json = &quot;&quot;; HttpClientUtil httpClient = new HttpClientUtil(); JSONObject jsonObject = httpClient.httpPost(url,json);}curl 正向代理常用切支持 http(s) 协议代理主要分为两大类: http 代理和 socks 代理，见下表：| 大类 | 小类 | 子类 | 描述 || ———- | —————— | ———————————— | ———————————————————— || http 代理 | http代理 https代理 | 透明代理 | http服务器知道浏览器端使用了代理，并能获取浏览器端原始IP； || | | 匿名代理 | http服务器知道浏览器端使用了代理，但无法获取浏览器端原始IP； || | | 高匿名代理 | http服务器不知道浏览器端使用了代理，且无法获取浏览器端原始IP； || SOCKS 代理 | SOCKS4 | 被称为全能代 理，支持http 和其他协议 | 只支持TCP应用； || | SOCKS4A | | 支持TCP应用；支持服务器端域名解析； || | SOCKS5 | | 支持TCP和UDP应用；支持服务器端域名解析； 支持多种身份验证；支持IPV6； |Linux curl 命令设置代理举例linux curl命令可以使用下面参数设置http(s)代理、socks代理, 已经设置它们的用户名、密码以及认证方式：| 参数 | 用法 || ———————————————————— | ———————————————————— || -x host:port-x [protocol://[user:pwd@]host[:port]-proxy [protocol://[user:pwd@]host[:port] | -x host:port-x [protocol://[user:pwd@]host[:port]–proxy [protocol://[user:pwd@]host[:port] || –socks4 &amp;lt;host[:port]&amp;gt;–socks4a &amp;lt;host[:port]&amp;gt;–socks5 &amp;lt;host[:port]&amp;gt; | 使用SOCKS4代理；使用SOCKS4A代理；使用SOCKS5代理；此参数会覆盖“-x”参数； || –proxy-anyauth–proxy-basic–proxy-diges–proxy-negotiate–proxy-ntlm | –proxy-anyauth–proxy-basic–proxy-diges–proxy-negotiate–proxy-ntlm || -U user:password–proxy-user user:password | 设置代理的用户名和密码； |Linux curl 命令设置 http 代理# 指定http代理IP和端口curl -x 113.185.19.192:80 http://aiezu.com/test.phpcurl --proxy 113.185.19.192:80 http://aiezu.com/test.php #指定为http代理curl -x http_proxy://113.185.19.192:80 http://aiezu.com/test.php #指定为https代理curl -x HTTPS_PROXY://113.185.19.192:80 http://aiezu.com/test.php #指定代理用户名和密码，basic认证方式curl -x aiezu:123456@113.185.19.192:80 http://aiezu.com/test.phpcurl -x 113.185.19.192:80 -U aiezu:123456 http://aiezu.com/test.phpcurl -x 113.185.19.192:80 --proxy-user aiezu:123456 http://aiezu.com/test.php #指定代理用户名和密码，ntlm认证方式curl -x 113.185.19.192:80 -U aiezu:123456 --proxy-ntlm http://aiezu.com/test.php #指定代理协议、用户名和密码，basic认证方式curl -x http_proxy://aiezu:123456@113.185.19.192:80 http://aiezu.com/test.phpLinux curl 命令设置 socks 代理#使用socks4代理，无需认证方式curl --socks4 122.192.32.76:7280 http://aiezu.com/test.phpcurl -x socks4://122.192.32.76:7280 http://aiezu.com/test.php #使用socks4a代理，无需认证方式curl --socks4a 122.192.32.76:7280 http://aiezu.com/test.phpcurl -x socks4a://122.192.32.76:7280 http://aiezu.com/test.php #使用socks5代理，basic认证方式curl --socks5 122.192.32.76:7280 -U aiezu:123456 http://aiezu.com/test.phpcurl -x socks5://aiezu:123456@122.192.32.76:7280 http://aiezu.com/test.php #使用socks5代理，basic认证方式，ntlm认证方式curl -x socks5://aiezu:123456@122.192.32.76:7280 --proxy-ntlm http://aiezu.com/test.phpcurl 设置代理 post 方式curl -H &quot;Content-Type: application/json&quot; -X POST -x http_proxy://代理ip:端口 请求地址" }, { "title": "注解", "url": "/posts/annotation/", "categories": "Java, Base", "tags": "annotation, 注解", "date": "2018-09-16 03:34:00 +0000", "snippet": "在 Java中，注解就是给程序添加一些信息，用字符@开头，这些信息用于修饰它后面紧挨着的其他代码元素，比如：类、接口、字段、方法、方法中的参数、构造方法等。注解可以被编译器、程序运行时和其他工具使用，用于增强或修改程序行为等。内置注解@OverrideJava 内置了一些常用注解：@Override、@Deprecated、@SuppressWarnings。@Override 修饰一个方法，表示该方法不是当前类首先声明的，而是在某个父类或实现的接口中声明的，当前类“重写”了该方法，比如：public class BaseDemo { static class Base { public void action(){}; } static class child extends Base { @Override public void action(){ System.out.println(&quot;child action&quot;); } @Override public String toString() { return &quot;child&quot;; } }} Child 的 action 方法重写了父类 Base 中的 action 方法； toString() 方法重写了 Object 类中的 toString 方法。虽然这个注解不写也不会改变这些方法是“重写”的本质，但是它可以减少一些编程错误。如果方法有 @Override注解，但没有任何父类或实现的接口声明该方法，则编译器会报错，强制修复该问题。@Deprecated@Deprecated 可以修饰的范围很广，包括类、方法、字段、参数等，它表示对应的代码已经过时，不应该使用它。不过，它是一种警告，而不是强制性的，在 IDE 如 Eclipse 中，会给 Deprecated 元素加一条删除线以示警告。比如，Date中很多方法就过时了：@Deprecatedpublic Date(int year,int month,int date)@Deprecatedpublic int getYear()在声明元素为 @Deprecated 时，应该用Java文档注释的方式同时说明替代方案，就像Date中的API文档那样，在调用@Deprecated方法时，应该先考虑其建议的替代方案。从 Java 9 开始，@Deprecated 多了两个属性： since，since 是一个字符串，表示是从哪个版本开始过时的； forRemoval，forRemoval 是一个 boolean 值，表示将来是否会删除。比如，Java9 中 Integer的一个构造方法就从版本 9 开始过时了，其代码为： @Deprecated(since=&quot;9&quot;)public Integer(int value){ this.valuevalue;} @SuppressWarning@SuppressWarnings 表示压制 Java 的编译警告，它有一个必填参数，表示压制哪种类型的警告，它也可以修饰大部分代码元素，在更大范围的修饰也会对内部元素起效，比如，在类上的注解会影响到方法，在方法上的注解会影响到代码行。对于Date方法的调用，可以这样压制警告：@SuppressWarnings({&quot;deprecation&quot;, &quot;unused&quot;})public static void main(String[] args) { Date date = new Date(2017, 4 ,12); int year = date.getYear();}Java 提供的内置注解比较少，在日常开发中使用的注解基本都是自定义的。不过，一般也不是自己定义的，而是由各种框架和库定义的，主要还是根据它们的文档直接使用。框架和库的注解JacksonJackson 是一个通用的序列化库，可以使用它提供的注解对序列化进行定制，比如： 使用@JsonIgnore和@JsonIgnoreProperties配置忽略字段； 使用@JsonManagedReference和@JsonBackReference配置互相引用关系； 使用@JsonProperty和@JsonFormat配置字段的名称和格式等。在 Java 提供注解功能之前，同样的配置功能也是可以实现的，一般通过配置文件实现，但是配置项和要配置的程序元素不在一个地方，难以管理和维护，使用注解就简单多了，代码和配置放在一起，一目了然，易于理解和维护。依赖注入容器Java 开发经常利用某种框架管理对象的生命周期及其依赖关系，这个框架一般称为 DI(Dependency Injection)容器。DI是指依赖注入，流行的框架有Spring、Guice等。在使用这些框架时，一般不通过 new 创建对象，而是由容器管理对象的创建，对于依赖的服务，也不需要自己管理，而是使用注解表达依赖关系。这么做的好处有： 代码更为简单，也更为灵活比如容器可以根据配置返回一个动态代理，实现 AOP。Servlet 3.0Servlet 是 Java 为 Web 应用提供的技术框架，早期的 Servlet 只能在 web.xml 中进行配置，而 Servlet3.0 则开始支持注解，可以使用 @WebServlet 配置一个类为 Servlet，比如：@WebServlet(urlPatterns =&quot;/async&quot;, asyncSupported true)public class AsyncDemoServlet extends Httpservlet {..}Web 应用框架在 Web 开发中，典型的架构都是 MVC(Model-View-Controller)， 典型的需求是配置哪个方法处理哪个 URL 的什么 HTTP 方法，然后将 HTTP 请求参数映射为 Java 方法的参数。各种框架如Spring MVC、Jersey 等都支持使用注解进行配置，比如，使用Jersey的一个配置示例为：@Path(&quot;/hello&quot;)public class HelloResource { @GET @Path(&quot;test&quot;) @Produces(MediaType.APPLICATION_JSON) public Map&amp;lt;string,object&amp;gt; test(@QueryParam(&quot;a&quot;)string a) { Map&amp;lt;string,object&amp;gt;map new HashMap&amp;lt;&amp;gt;(); map.put(&quot;status&quot;,&quot;ok&quot;); return map; }}HelloResource 将处理 Jersey 配置的根路径下/hello下的所有请求，而 test 方法将处理/hello/test的 GET请求，响应格式为 JSON，自动映射 HTTP 请求参数 a 到方法参数 String a。注解的好处通过上面的栗子，可以看出，注解有某种神奇的力量，通过简单的声明，就可以达到某种效果。在某些方面，它类似于序列化，序列化机制中通过简单的 Serializable 接口，Java 就能自动处理很多复杂的事情。它也类似于synchronized关键字，通过它可以自动实现同步访问。这些都是声明式编程风格，在这种风格中，程序都由三个组件组成： 声明的关键字和语法本身； 系统/框架/库，它们负责解释、执行声明式的语句； 应用程序，使用声明式风格写程序。在编程的世界里，访问数据库的 SQL 语言、编写网页样式的 CSS ,以及正则表达式、函数式编程都是这种风格，这种风格降低了编程的难度，提供了更为高级的语言，使得可以在更高的抽象层次上思考和解决问题，而不是陷于底层的细节实现。创建注解@Override 的定义如下：@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override {}定义注解与定义接口有点类似，都用了 interface，不过注解的 interface 前多了@。另外，它还有两个元注解@Target和@Retention，这两个注解专门用于定义注解本身。@Target：表示注解的目标，@Override 的目标是方法（ElementType.METHOD）。ElementType是一个枚举，主要可选值有： TYPE:表示类、接口（包括注解），或者枚举声明； FIELD:字段，包括枚举常量； METHOD:方法； PARAMETER:方法中的参数； CONSTRUCTOR:构造方法； LOCAL_VARIABLE:本地变量； MODULE:模块(Java9引入的)。目标可以有多个，用 {} 表示，比如@Suppress Warnings的@Target就有多个。Java 7 的定义为：@Target({TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE})@Retention(RetentionPolicy.SOURCE)public @interface SuppressWarnings { String[] value();}如果没有声明@Target，默认为适用于所有类型。@Retention表示注解信息保留到什么时候，取值只能有一个，类型为RetentionPolicy,它是一个枚举，有三个取值。 SOURCE：只在源代码中保留，编译器将代码编译为字节码文件后就会丢掉。 CLASS：保留到字节码文件中，但Java虚拟机将class文件加载到内存时不一定会在内存中保留。 RUNTIME：一直保留到运行时。如果没有声明@Retention，则默认为 CLASS。@Override和@SuppressWarnings都是给编译器用的，所以@Retention都是Retention-Policy.SOURCE。可以为注解定义一些参数，定义的方式是在注解内定义一些方法，比如@Suppress-Warnings内定义的方法 value，返回值类型表示参数的类型，这里是 String[]。使用 @Suppress-Warnings 时必须给 value 提供值，比如：@SuppressWarnings(value = {&quot;deprecation&quot;, &quot;unused&quot;})// 可简写为@SuppressWarnings({&quot;deprecation&quot;, &quot;unused&quot;})注解内参数的类型不是什么都可以的，合法的类型有基本类型、String、Class、枚举、注解，以及这些类型的数组。参数定义时可以使用 default 指定一个默认值，比如，Guice 中 Inject 注解的定义：@Target({METHOD,CONSTRUCTOR,FIELD }@Retention(RUNTIMB)@Documentedpublic @interface Inject { boolean optional() default false;}它有一个参数 optional， 默认值为 false。如果类型为 String，默认值可以为”“，但不能为null。如果定义了参数且没有提供默认值，在使用注解时必须提供具体的值，不能为 null。@Inject 多了一个元注解 @Documented，它表示注解信息包含到生成的文档中。与接口和类不同，注解不能继承。不过注解有一个与继承有关的元注解@Inherited，例子：public class InheritDemo { @Inherited @Retention(RetentionPolicy.RUNTIME) static @interface Test { } @Test static class Base { } static class Child extends Base { } public static void main(String[] args) { System.out.println(Child.class.isAnnotationPresent(Test.class)); }}查看注解信息创建了注解，就可以在程序中使用，注解指定的目标，提供需要的参数，但这还是不会影响到程序的运行。要影响程序，要先能查看这些信息。主要考虑@Retention为RetentionPolicy.RUNTIME的注解，利用反射机制在运行时进行查看和利用这些信息。反射相关类中与注解有关的方法有，Class、Field、Method、Constructor 中都有如下方法：// 获取所有的注解public Annotation[]getAnnotations()// 获取所有本元素上直接声明的注解，忍骆inherited来的public Annotation[]getDeclaredAnnotations()// 获取指定类型的注解，没有返回nul1public &amp;lt;A extends Annotation&amp;gt;A getAnnotation(Class&amp;lt;A&amp;gt;annotationclass)// 判断是否有指定类型的注解public boolean isAnnotationPresent(Class&amp;lt;?extends Annotation&amp;gt;annotationclass)Annotation 是一个接口，它表示注解，具体定义为：public interface Annotation { boolean equals(Object obj); int hashCode(); String toString(); // 返回真正的注解类型 Class&amp;lt;? extends Annotation&amp;gt; annotationType();}实际上，内部实现时，所有的注解类型都是扩展的 Annotation。对于 Method 和 Contructor，它们都有方法参数，而参数也可以有注解，所以它们都有如下方法：public Annotation[][] getParameterAnnotations()返回值是一个二维数组，每个参数对应一个一维数组：public class MethodAnnotations { @Target(ElementType.PARAMETER) @Retention(RetentionPolicy.RUNTIME) static @interface QueryParam { String value(); } @Target(ElementType.PARAMETER) @Retention(RetentionPolicy.RUNTIME) static @interface DefaultValue { String value() default &quot;&quot;; } public void hello(@QueryParam(&quot;action&quot;) String action) { // ... System.out.println(&quot;hello......&quot;); } public static void main(String[] args) throws NoSuchMethodException { Class&amp;lt;?&amp;gt; cls = MethodAnnotations.class; Method method = cls.getMethod(&quot;hello&quot;, new Class[]{String.class, String.class}); Annotation[][] annts = method.getParameterAnnotations(); for (int i = 0; i &amp;lt; annts.length; i++) { System.out.println(&quot;annotations for parameter &quot; + (i + 1)); Annotation[] anntArr = annts[i]; for (Annotation annt : anntArr) { if (annt instanceof QueryParam) { QueryParam qp = (QueryParam) annt; System.out.println(qp.annotationType().getSimpleName() + &quot;:&quot; + qp.value()); } else if (annt instanceof DefaultValue) { DefaultValue dv = (DefaultValue) annt; System.out.println(dv.annotationType().getSimpleName() + &quot;:&quot; + dv.value()); } } } }}注解应用的栗子：定制序列化注解应用的栗子：DI 容器定义两个注解： @SimpleInject @SimpleSingleton@SimpleInject 定义 @SimpleInject @Retention(RUNTIME)@Target(FIELD)public @interface SimpleInject {} 定于两个服务 ServiceA 和 ServiceB，ServiceA 依赖于 ServiceB，使用 @SimpleInject，如下： public class ServiceA { @SimpleInject ServiceB b; public void callB() { b.action(); }} public class ServiceB { public void action() { System.out.println(&quot;I&#39;m B&quot;); }} DI 容器的类为 SimpleContainer，如下： public class SimpleContainer { // SimpleContainer.getInstance 会创建需要的对象，并配置依赖关系 // 假定每个类型都有一个 public 默认构造方法，使用它创建对象，然后查看每个字段 // 如果有 SimpleInject 注解，就根据字段类型获取该类型的实例，并设置字段的值 public static &amp;lt;T&amp;gt; T getInstance(Class&amp;lt;T&amp;gt; cls) { try { T obj = cls.newInstance(); Field[] fields = cls.getDeclaredFields(); for (Field f : fields) { if (f.isAnnotationPresent(SimpleInject.class)) { if (!f.isAccessible()) { f.setAccessible(true); } Class&amp;lt;?&amp;gt; fieldCls = f.getType(); f.set(obj, getInstance(fieldCls)); } } return obj; } catch (Exception e) { throw new RuntimeException(e); } }} @SimpleSingleton在上面的代码中，每次获取一个类型的对象，都会新创建一个对象，实际开发中，这不是期望的结果，期望的模式可能是单例，即每个类型只创建一个对象，该对象被所有访问的代码共享，满足这种需求只需要增加一个注解@SimpleSingleton 用于修饰类，表示类型是单例，定义如下：@Retention(RUNTIME)@Target(TYPE)public @interface SimpleSingleton {}而后这样修饰 ServiceB：@SimpleSingletonpublic class ServiceB { public void action() { System.out.println(&quot;I&#39;m B&quot;); }}SimpleContainer 也需要修改，首先增加一个静态变量，缓存创建过的单例对象：private static Map&amp;lt;Class&amp;lt;?&amp;gt;, Object&amp;gt; instance = new ConcurrentHashMap();getInstance 的修改如下： public static &amp;lt;T&amp;gt; T getInstance(Class&amp;lt;T&amp;gt; cls) { try { boolean singleton = cls.isAnnotationPresent(SimpleSingleton.class); if (!singleton) { return createInstance(cls); } Object obj = instance.get(cls); if (obj != null) { return (T)obj; } synchronized (cls) { obj = instance.get(cls); if (obj == null) { obj = createInstance(cls); instance.put(cls, obj); } } return (T) obj; } catch (Exception e) { throw new RuntimeException(e); } }首先检查是否是单例，如果不是，直接调用 createInstance 创建对象。否则，检查缓存，如果有，直接返回，如果没有，则调用 createInstance 创建对象，并放入缓存中。createInstance 方法如下： private static &amp;lt;T&amp;gt; T createInstance(Class&amp;lt;T&amp;gt; cls) throws Exception { T obj = cls.newInstance(); Field[] fields = cls.getDeclaredFields(); for (Field f: fields) { if (f.isAnnotationPresent(SimpleInject.class)) { if (!f.isAccessible()){ f.setAccessible(true); } Class&amp;lt;?&amp;gt; fieldCls = f.getType(); f.set(obj, getInstance(fieldCls)); } } return obj; }总结 注解提升了 Java语言 的表达能力； 有效实现了应用功能和底层功能的分离，框架/库的程序员可以专注于底层实现，借助反射实现通用功能，提供注解给应用程序员使用，应用程序员可以专注于应用功能，通过简单的声明式注解与框架库进行协作。" }, { "title": "反射", "url": "/posts/reflect/", "categories": "Java, Base", "tags": "reflect, 反射", "date": "2018-09-14 03:34:00 +0000", "snippet": "利用 Java 中的一些特性，包括反射、注解、动态代理、类加载器等，可以优雅地实现一些灵活通用的功能，它们经常用于各种框架、库以及系统程序中。 例如 Jackson，利用反射和注解实现了通用的序列化机制； Spring MVC、Jersey 用于处理 Web 请求，利用反射和注解，可以方便地将用户的请求参数和内容转换为 Java 对象，将 Java 对象转变为响应内容； Spring、Guice 利用这些特性实现对象管理器，方便管理对象的生命周期以及其中复杂的依赖关系； 应用服务器，比如 Tomcat，利用类加载器实现不同应用之间的隔离，JSP 技术利用类加载器实现修改代码而不用重启就能生效的特性； 面向切面编程（AOP，Aspect Oriented Programming）将编程中通用的关注点（如日志记录、异常处理、安全检查等）与业务的主体逻辑分离，减少冗余代码，提高代码可维护性，AOP 需要依赖上面的这些特性来是实现。在操作数据的时候，依赖于数据类型，如： 根据类型，使用 new 创建对象； 根据类型定义变量，类型可能是基本类型、类、接口或数组； 将特定类型的对象传递给方法； 根据类型访问对象的属性，调用对象的方法 编译器也是根据类型进行代码的检查编译反射不一样，它是在运行的时候，而不是在编译的时候，动态获取类型的信息，比如接口信息、成员信息、方法信息、构造方法信息等，根据这些动态获取到的信息创建对象，访问/修改成员、调用方法等。Class 类每一个以及加载的类在内存都有一个份类信息，每个对象都有指向它所属类信息的引用。在 Java 中，类信息对应的类就是 java.lang.Class。所有类的根父类 Object 有一个方法，可以获取对象的 Class 对象：public final native Class&amp;lt;?&amp;gt; getClass();Class 是一个泛型类，有一个类型参数，getClass() 并不知道具体的类型，因此返回 Class&amp;lt;?&amp;gt;。 获取 Class 对象不一定需要实例对象，如果在写代码的时候知道类名，可以使用 &amp;lt;类名&amp;gt;.class 获取 Class 对象，比如： Class&amp;lt;Date&amp;gt; dateClass = Date.class; 同样，接口也有 Class 对象，并且这种方式对于接口也是适用的，比如： Class&amp;lt;Comparable&amp;gt; comparableClass = Comparable.class; 基本类型没有 getClass 方法，但也都有对应的 Class 对象，类型参数为对应的包装类型，比如： Class&amp;lt;Integer&amp;gt; integerClass = int.class;Class&amp;lt;Byte&amp;gt; byteClass = byte.class;Class&amp;lt;Character&amp;gt; characterClass = char.class;Class&amp;lt;Double&amp;gt; doubleClass = double.class; void 作为特殊的返回类型，也有对应的 Class，比如： Class&amp;lt;Void&amp;gt; voidClass = void.class; 对于数组，每种类型也都有对应数组类型的 Class 对象，每个维度都有一个，即一维数组有一个，二维数组有一个不同的类型，比如： String[] strArr = new String[10];int[][] twoDimArr = new int[3][2];int[] oneDimArr = new int[10];Class&amp;lt;? extends String[]&amp;gt; strArrClass = strArr.getClass();Class&amp;lt;? extends int[][]&amp;gt; twoDimArrClass = twoDimArr.getClass();Class&amp;lt;? extends int[]&amp;gt; oneDimArrClass = oneDimArr.getClass(); 枚举类型也有对应的 Class ，比如： enum Size{ SMALL, MEDIUM, BIG}Class&amp;lt;Size&amp;gt; sizeClass = Size.class; Class 有一个静态方法 forName，可以根据类名直接加载 Class，获取 Class 对象，比如： try { Class&amp;lt;?&amp;gt; cls = Class.forName(&quot;java.util.HashMap&quot;); System.out.println(cls.getName());} catch (ClassNotFoundException e) { e.printStackTrace();} 有了 Class 对象之后，就可以了解到关于类型的很多信息，并可以基于这些信息采取一些行动。Class 方法有很多，大体包括：名称信息、字段信息、方法信息、创建对象和构造函数、类型信息等。名称信息public String getName() {} // 返回的名称不带包信息public String getSimpleName() {} // 返回的是 Java 内部使用的真正的名称public String getCanonicalName() {} // 返回的更好public Package getPackage() {} // 返回的是包信息它们的不同具体如下： Class 对象 getName getSimpleName getCanonicalName getPackage int.class int int int null int[].class [I int[] int[] null int[][].class [[I int[][] int[][] null String.class java.lang.String String java.lang.String java.lang String[][].class [Ljava.lang.String;（注意这里有一个英文分号;） String[] java.lang.String[] null HashMap.class java.util.HashMap HashMap java.util.HashMap java.util Map.Entry.class java.util.Map$Entry Entry java.util.Map.Entry java.util 需要说明的是数组类型的 getName 返回值，它使用前缀 [表示数组，有几个[就表示是几维数组；数组的类型用一个字符表示，I 表示 int，L 表示类或接口，其他类型与字符的对应关系为： boolean(Z) byte(B) char(C) double(D) float(F) long(J) short(S) 对于引用类型的数组，注意最后有一个分号！！！字段信息类中定义的静态和实例变量都称为字段，用类 Filed 表示，位于包 java.lang.reflect。Class 有 4 个获取字段信息的方法，如下：// 返回所有的 public 字段，包括其父类的，如果没有字段，返回空数组public Field[] getFields() throws SecurityException {}// 返回本类声明的所有字段，包括非 public 的，但不包括父类的public Field[] getDeclaredFields() throws SecurityException {}// 返回本类或父类中指定名称的 public 字段，找不到则抛出异常 NoSuchFieldException public Field getField(String name){}// 返回本类中声明的指定名称的字段，找不到则抛出异常 NoSuchFieldExceptionpublic Field[] getDeclaredFields() throws SecurityException {}Field 也有很多方法，可以获取字段的信息，也可以通过 Field 访问和操作指定对象中该字段的值，基本方法有：public String getName() {return name;}对于静态变量，obj 被忽略，可以为 null；对于基本数据类型，自动在基本类型和对应的包装类型间进行转换；对于 private ，直接调用会返回，会抛出非法访问异常 IllegalAccessException，应该先调用 setAccessiable(true) 以关闭 Java 的检查机制，如下：List&amp;lt;String&amp;gt; obj = Arrays.asList(new String[]{&quot;刘备&quot;,&quot;关羽&quot;,&quot;刘备&quot;});Class&amp;lt;?&amp;gt; objClass = obj.getClass();for (Field f: objClass.getDeclaredFields()) { f.setAccessible(true); try { System.out.println(f.getName() + &quot; - &quot; + f.get(obj)); } catch (IllegalAccessException e) { e.printStackTrace(); }}除了以上，还有很多其他方法，比如：// 返回字段的修饰符，返回的 int，可以通过 Modifiers 类的静态方法进行解读public int getModifiers() { return modifiers; }// 返回字段的类型public Class&amp;lt;?&amp;gt; getType() { return type; }// 以基本类型操作字段public void setBoolean(Object obj, boolean z) throws IllegalArgumentException, IllegalAccessException {}public boolean getBoolean(Object obj) throws IllegalArgumentException, IllegalAccessException {}public void setDouble(Object obj, double d) throws IllegalArgumentException, IllegalAccessException {}public double getDouble(Object obj) throws IllegalArgumentException, IllegalAccessException {}// 查询字段的注解信息public &amp;lt;T extends Annotation&amp;gt; T getAnnotation(Class&amp;lt;T&amp;gt; annotationClass) {}public Annotation[] getDeclaredAnnotations() {}栗子：Field f = null;try { f = Demo.class.getField(&quot;MAX_NAME_LEN&quot;);} catch (NoSuchFieldException e) { e.printStackTrace();}int mod = f.getModifiers();System.out.println(Modifier.toString(mod));System.out.println(&quot;isPublic: &quot; + Modifier.isPublic(mod));System.out.println(&quot;isStatic: &quot; + Modifier.isStatic(mod));System.out.println(&quot;isFinal: &quot; + Modifier.isFinal(mod));System.out.println(&quot;isVolatile: &quot; + Modifier.isVolatile(mod));输出结果：public static finalisPublic: trueisStatic: trueisFinal: trueisVolatile: false方法信息类中定义的静态和实例方法都称为方法，用类 Method 表示。Class 有如下相关方法：// 返回所有的pub11c方法，包括其父类的，如果没有方法，返回空数组public Method[]getMethods()// 返回本类声明的所有方法，包括非pub11c的，但不包括父类的public Method[]getDeclaredMethods()// 返回本类或父类中指定名称和参数类型的publ1c方法，// 找不到抛出异常NoSuchMethodExceptionpublic Method getMethod(String name,class&amp;lt;?&amp;gt;...parameterTypes)// 返回本类中声明的指定名你和参数类型的方法，找不到抛出异常NoSuchi№thodExceptionpublic Method getDeclaredMethod(String name,class&amp;lt;?&amp;gt;...parameterTypes)通过 Method 可以获取方法的信息，也可以通过 Method 调用对象的方法，基本方法有：// 获取方法的名称public string getName()// f1ag设为true表示忽路Jawa的访问检查机制，以允许调用非oublic的方法public void setAccessible(boolean flag)// 在指定对象obj上调用ethod代表的方法，传递的渗数列表为argspublic object invoke(Object obj,object...args)throwsIllegalAccessException,Illegal-ArgumentException,InvocationTargetException对 invoke 方法： 如果 Method 为静态方法，obj 被忽略，可以为 null，args 可以为null，也可以为一个空的数组，方法调用的返回值被包装为 Objecti 返回； 如果实际方法调用抛出异常，异常被包装为 InvocationTargetException 重新抛出，可以通过 getCause方法得到原异常。 class&amp;lt;?&amp;gt; cls = Integer.class;try { Method method cls.getMethod(&quot;parseInt&quot;,new class[](String.class)); System.out.println(method.invoke(null,&quot;123&quot;));} catch (NoSuchMethodException e) { e.printstackTrace();} catch (InvocationTargetException e){ e.printstackTrace();} Mthode 还有很多方法，可以获取其修饰符、参数、返回值、注解等信息，具体就不列举了。创建对象和构造函数Class 有一个方法，可以用它来创建对象：public T newInstance() throws InstantiationException, IllegalAccessException {}这个方法会调用类的默认构造方法，如果类没有该构造方法，就会抛出 InstantialionException。类 Constructor 表示构造方法，通过它可以创建对象，方法为：Constructor&amp;lt;stringBuilder&amp;gt;contructor-StringBuilder.class.getConstructor(new class[]{int.class));StringBuilder sb contructor.newInstance(100);除此之外，Constructor 还有很多方法，可以获取关于构造方法的很多信息，包括参数、修饰符、注解等。类型检查和转换instanceof 关键字，用来判断变量指向的实际对象类型。instanceof 后面的类型是在代码中确定的，如果要检查的类型是动态的，就可以使用 Class 类中下面的方法：public native boolean isInstance(Object obj);除了判断类型，在程序中也需要进行强制类型转换，如果是动态的就可以用下面的方法：public T cast(Object obj) {}instance/cast 描述的都是对象和类之间的关系，Class 还有一个方法，可以判断 Class 之间的关系：// 检查参数类型 cls 能否赋给当前 Class 类型的变量public native boolean isAssignnableFrom(Class&amp;lt;?&amp;gt; cls)比如：Object.class.isAssignableFrom(String.class);String.class.isAssignableFrom(String.class);List.class.isAssignableFrom(ArrayList.class);Class 类型信息Class 代表的类型既可以是普通类、也可以是内部类、还可以是基本类型、数组等，对于一个给定的 Class 对象，可以用下面的方法进行检查是什么类型public native boolean isArray(); // 是否是数组public native boolean isPrimitive(); // 是否是基本类型public native boolean isInterface(); // 是否是接口public boolean isEnum() {} // 是否是枚举public boolean isAnnotation() {} // 是否是注解public boolean isAnonymousClass() {} // 是否是匿名内部类public boolean isMemberClass() {} // 是否是成员类，成员类定义在方法外，不是匿名类public boolean isLocalClass() {} // 是否是本地类，本地类定义在方法内，不是匿名类类的声明信息类的声明信息，如修饰符、父类、接口、注解等，可以通过下面的方法获取：//获取修饰符，返回值可通过od1fier类进行解读public native int getModifiers()//获取父类，如果为object,父类为nul1public native class&amp;lt;?super T&amp;gt;getSuperclass()//对于类，为自己声明实现的所有接口，对于接口，为直接扩展的接口，不包括通过父类继承的public native class&amp;lt;?&amp;gt;[]getInterfaces();//自己声明的注解public Annotation[]getDeclaredAnnotations()//所有的注解，包括继承得到的public Annotation[getAnnotations()//获阿或检查指定类型的注解，包括继承得到的public &amp;lt;A extends Annotation&amp;gt;A getAnnotation(class&amp;lt;A&amp;gt;annotationclass)public boolean isAnnotationPresent(class&amp;lt;?extends Annotation&amp;gt;annotationclass)类的加载Class 有两个静态方法，可以根据类名加载类：public static class&amp;lt;?&amp;gt;forName(String className)public static class&amp;lt;?&amp;gt;forName(String name,boolean initialize,ClassLoader loader)ClassLoader 表示类加载器，initialize表示加载后，是否执行类的初始化代码 (如static语句块)。第一个方法中没有传这些参数，相当于调用：// currentLoader 表示加载当前类的 ClassLoaderclass.forName(className,true,currentLoader)这里 className 与 Class.getName 的返回值是一致的。比如，对于 String 数组：String name &quot;[Ljava.lang.String;&quot;;class cls class.forName(name);System.out.println(cls String[].class);需要注意的是，基本类型不支持 forName 方法，也就是说，class.forName(&quot;int&quot;);会抛出异常 ClassNotFoundException。不过，可以对Class.forName进行一下包装，比如：public static class&amp;lt;?&amp;gt;forName(String className) throws classNotFoundException { if (&quot;int&quot;.equals(className)){ return int.class; } // 其他基本类型路 return class.forName(className);}Java9 还有一个 forName 方法，用于加载指定模块中指定名称的类：// Module 表示模块，是 Java 9 引入的类// 当找不到类的时候，不会抛出异常，而是返回 null，也不会执行类的初始化public static class&amp;lt;?&amp;gt; forName(Module module,String name)反射与数组对于数组类型，可以通过下面的方法，获取它的元素类型：public native class&amp;lt;?&amp;gt; getcomponentType()就像：String[]arr new String[]();System.out.printin(arr.getclass().getComponentType());输出为：class java.lang.Stringjava.lang.reflect 包中有一个针对数组的专门的类 Array(注意不是java.util中的Arrays)，提供了对于数组的一些反射支持，以便于统一处理多种类型的数组，主要方法有：// 创建指定元素类型、指定长度的数组public static object newInstance(class&amp;lt;?&amp;gt;componentType,int length)// 创建多维数组public static object newInstance(class&amp;lt;?&amp;gt;componentType,int...dimensions)// 获取数组array指定的索引位置index处的值public static native object get(object array,int index)// 修改数组array指定的索位置index处的值为valuepublic static native void set(object array,int index,object value)// 返回数组的长度public static native int getLength(object array)不过，在 Array 类中，数组是用 Object 而非 Object[] 表示的，这是因为方便处理多种类型的数组。int0、String0 都不能与 Objectl0 相互转换，但可以与 Object 相互转换，比如：int[] intArr = (int[])Array.newInstance(int.class,10);String[] strArr = (String[])Array.newInstance(String.class,10);除了 Object2 类型操作数组元素外，Array 也支持以各种基本类型操作数组元素，如：public static native double getDouble(Object array,int index)public static native void setDouble(object array,int index,double d)public static native void setLong(object array,int index,long 1)public static native long getLong(object array,int index)反射与枚举通过下面的方法，获取所有的枚举常量：public T[] getEnumConstants()反射与泛型泛型参数在运行时会被擦除，在类信息 CIass 中依然有关于泛型的一些信息，可以通过反射得到。Class 有如下方法，可以获取类的泛型参数信息：public Typevariable&amp;lt;class&amp;lt;T&amp;gt;&amp;gt;[]getTypeParameters()Field 有如下方法：public Type getGenericType()Method 有如以下方法：public Type getGenericReturnType()public Type[] getGenericParameterTypes()public Type[] getGenericExceptionTypes()Constructor 有如下方法：Type 是一个接口，Class 实现了 Type，Type 的其他子接口还有： Type Variable：类型参数，可以有上界，比如 T extends Number; ParameterizedType：参数化的类型，有原始类型和具体的类型参数，比如 List; WildcardType：通配符类型，比如 ？、？extends Number、?super Integer。.public class GenericDemo { static class GenericTest&amp;lt;U extends Comparable&amp;lt;U&amp;gt;, V&amp;gt; { U u; V v; List&amp;lt;String&amp;gt; list; public U test(List&amp;lt;? extends Number&amp;gt; numbers) { return null; } } public static void main(String[] args) throws NoSuchFieldException, NoSuchMethodException { Class&amp;lt;?&amp;gt; cls = GenericTest.class; // 类的类型参数 for (TypeVariable t : cls.getTypeParameters()) { System.out.println(t.getName() + &quot; extends &quot; + Arrays.toString(t.getBounds())); } // 字段：泛型类型 Field fu = cls.getDeclaredField(&quot;u&quot;); System.out.println(fu.getGenericType()); // 字段：参数化的类型 Field flist = cls.getDeclaredField(&quot;list&quot;); Type listType = flist.getGenericType(); if (listType instanceof ParameterizedType) { ParameterizedType pType = (ParameterizedType) listType; System.out.println(&quot;raw type: &quot; + pType.getRawType() + &quot;, type arguments: &quot; + Arrays.toString(pType.getActualTypeArguments())); } // 方法的泛型参数 Method m = cls.getMethod(&quot;test&quot;, new Class[]{List.class}); for (Type t: m.getGenericParameterTypes()) { System.out.println(t); } }}输出为：U extends [java.lang.Comparable&amp;lt;U&amp;gt;]V extends [class java.lang.Object]Uraw type: interface java.util.List, type arguments: [class java.lang.String]java.util.List&amp;lt;? extends java.lang.Number&amp;gt;应用栗子利用反射实现一个简单的通用序列化/反序列化类 SimpleMapper，有两个静态方法：// 将对象 obj 转换为字符串public static String tostring(object obj)// fromString 将字符串转换为对象 public static object fromstring(String str)我只实现最简单的类，即有默认构造方法，成员类型只有基本类型、包装类或 String。另外，序列化的格式也很简单，第一行为类的名称，后面每行表示一个字段，用字符 “=” 分隔，表示字段名称和字符串形式的值。public class SimpleMapperDemo { static class Student{ String name; int age; Double score; // 省路构造方法，getter/setter和tostring方法 } public static void main(String[]args) { Student zhangsan = new Student(&quot;张三&quot;, 18,89d); String str = SimpleMapper.tostring(zhangsan); Student zhangsan2 = (Student)simpleMapper.fromstring(str); System.out.println(zhangsan2); }}public class SimpleMapper { public static String tostring(Object obj) { try { Class&amp;lt;?&amp;gt; cls = obj.getClass(); StringBuilder sb = new StringBuilder(); sb.append(cls.getName() + &quot;\\n&quot;); for (Field f : cls.getDeclaredFields()) { if (!f.isAccessible()) { f.setAccessible(true); } sb.append(f.getName() + &quot;=&quot; + f.get(obj).toString() + &quot;\\n&quot;); } return sb.toString(); } catch (IllegalAccessException e) { throw new RuntimeException(); } } public static Object fromString(String str) { String[] lines = str.split(&quot;\\n&quot;); if (lines.length &amp;lt; 1) { throw new IllegalArgumentException(str); } try { Class&amp;lt;?&amp;gt; cls = Class.forName(lines[0]); Object obj = cls.newInstance(); if (lines.length &amp;gt; 1) { for (int i = 0; i &amp;lt; lines.length; i++) { String[] fv = lines[i].split(&quot;=&quot;); if (fv.length != 2) { throw new IllegalArgumentException(lines[i]); } Field f = cls.getDeclaredField(fv[0]); if (!f.isAccessible()) { f.setAccessible(true); } setFieldValue(f, obj, fv[i]); } } return obj; } catch (Exception e) { throw new RuntimeException(e); } } private static void setFieldValue(Field f, Object obj, String value) throws IllegalAccessException, NoSuchMethodException, InvocationTargetException, InstantiationException { Class&amp;lt;?&amp;gt; type = f.getType(); if (type == int.class) { f.setInt(obj, Integer.parseInt(value)); } else if (type == byte.class) { f.setByte(obj, Byte.parseByte(value)); } else if (type == short.class) { f.setShort(obj, Short.parseShort(value)); } else if (type == long.class) { f.setLong(obj, Long.parseLong(value)); } else if (type == float.class) { f.setFloat(obj, Float.parseFloat(value)); } else if (type == double.class) { f.setDouble(obj, Double.parseDouble(value)); } else if (type == char.class) { f.setChar(obj, value.charAt(0)); } else if (type == boolean.class) { f.setBoolean(obj, Boolean.parseBoolean(value)); } else if (type == String.class) { f.set(obj, value); } else { Constructor&amp;lt;?&amp;gt; ctor = type.getConstructor( new Class[]{String.class}); f.set(obj, ctor.newInstance(value)); } }}输出如：Student [name,age-18,score-89.0]总结反射虽然是灵活的，但一般情况下，并不是优先选择的，主要原因是： 反射容易出现运行时错误，使用显式的类和接口，编译器能帮助做类型检查，减少错误，但使用反射，类型是运行时才知道的，编译器无能为力； 反射的性能要低一些，在访问字段、调用方法前，反射先要查找对应的 Field/Method，要慢一些； 总之，如果能用接口实现同样的灵活性，就不要使用反射。" }, { "title": "动态代理", "url": "/posts/dynamic-agent/", "categories": "Java, Base", "tags": "dynamic agent, 动态代理", "date": "2018-09-11 03:34:00 +0000", "snippet": "动态代理，可以在运行时动态创建一个类，实现一个或多个接口。可以在不修改原有类的基础上动态为通过该类获取的对象添加方法、修改行为。这个特性被广泛的应用于各种系统程序、框架和库中，比如 Spring、Hibernate、MyBatis、Guice 等。动态代理是实现面向切面编程 AOP（Aspect Oriented Programming）的基础。切面的栗子有日志、性能监控、权限检查、数据库事务等，它们在程序的很多地方都会用到。代码也都差不多，但与某个具体的业务逻辑关系也不太密切，如果在每个用到的地方都写，代码会很冗余，也难以维护、AOP 将这些切面与主体业务逻辑分开，代码简单优雅很多。理解动态代理之前，首先要了解静态代理。静态代理代理是一个比较通用的词，作为一个软件模式，它在《设计模式》中被提出，基本概念和日常生活中的概念是类似的。代理背后一般至少有一个实际对象，并且代理的外部功能和实际对象一般是一样的。用户与代理打交道，不直接接触实际对象。虽然外部功能和实际对象一样，但代理的价值如下： 节省成本 执行权限检查，代理检查权限后，再调用实际对象； 屏蔽网络差异和复杂性，代理在本地，而实际对象在其他服务器上，调用本地代理时，本地代理请求其他服务器。代理模式的代码非常public class SimpleStaticProxyDemo { static interface IService { void sayHello(); } static class RealService implements IService { @Override public void sayHello() { System.out.println(&quot;Hello World&quot;); } } static class TraceProxy implements IService { private SimpleStaticProxyDemo.IService realService; TraceProxy(IService realService){ this.realService = realService; } @Override public void sayHello() { System.out.println(&quot;entering sayHello!&quot;); this.realService.sayHello(); System.out.println(&quot;leaving sayHello!&quot;); } } public static void main(String[] args) { IService realService = new RealService(); IService proxyService = new TraceProxy(realService); proxyService.sayHello(); }}代理和实际的对象一般都有相同的接口，在这个栗子中，共同的接口就是 IService，实际对象就是 RealService，代理 TraceProxy. TraceProxy 内部有一个 IService 的成员变量，指向实际对象，在构造方法中被初始化，对于方法 sayHello 的调用，它转发给了实际对象，在调用前后输出了一些跟踪调试信息，上面代码的输出为：entering sayHello!Hello Worldleaving sayHello! 设计模式中的适配器模式和装饰器模式，它们与代理模式有些相似，它们的背后都有一个实际对象，都是通过组合的方式执行该对象。不同之处在于： 适配器模式时提供了一个不一样的新接口； 装饰器模式是对原接口起到了“装饰”作用，可能是增加了新接口、修改了原有的行为； 代理模式一般不改变接口 上面的代码，想要达到的目的是在实际对象的方法调用前后加一些跟踪语句。为了在不修改原类的情况下达到这个目的，在代码中创建了一个代理类 TraceProxy，由于它的代码在写程序时是固定的，所以称为静态代理。输出跟踪调式信息是一个通用的，可以想象，如果每个类都需要，有不希望修改类定义，就需要为每个类创建代理，实现所有接口，这个工作就太繁琐。如果再有其他的切面要求，整个工作可能又要重来一遍。此时，就需动态代理。Java SDK 动态代理用法在静态代理中，代理类是直接定义在代码中的；在动态代理中，代理类是动态生成的，如下：public class SimpleJDKDynamicProxyDemo { static interface IService { void sayHello(); } static class RealService implements IService { @Override public void sayHello() { System.out.println(&quot;hello&quot;); } } static class SimpleInvocationHandler implements InvocationHandler { private Object realObject; public SimpleInvocationHandler (Object realObject) { this.realObject = realObject; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;entering &quot; + method.getName() + &quot;!&quot;); Object result = method.invoke(realObject, args); System.out.println(&quot;leaving &quot; + method.getName() + &quot;!&quot;); return result; } } public static void main(String[] args) { IService realService = new RealService(); IService proxyService = (IService) Proxy.newProxyInstance( IService.class.getClassLoader(), new Class&amp;lt;?&amp;gt;[]{ IService.class }, new SimpleInvocationHandler(realService)); proxyService.sayHello(); }}上面代码中，IService 和 RealService 的定义不变，程序的输出也没什么改变，但代理对象 proxyService 的创建方式变量，它使用 java.lang.reflect 包中的 Proxy 类的静态方法 newProxyInstance 来创建代理对象，这个方法的声明如下： public static Object newProxyInstance(ClassLoader loader, Class&amp;lt;?&amp;gt;[] interfaces,InvocationHandler h)throws IllegalArgumentException{}它有三个参数，具体如下： loader ，类加载器； interface，代理类要实现的接口列表，是一个数组，元素的类型只能是接口，不能是不同的类； h 的类型为 InvocationHandler，是一个接口，定义在 java.lang.reflect 包中，只定义了一个 invoke()，对代理接口所有方法的调用都会转给该方法；newProxyInstance 的返回值类型 Object，可以强制转换为 interfaces 数组中的某个接口类型。这里强制转换为 IService 类型，需要注意的是，它不能强制转换为某个类型，比如 RealService，即使它的实际代理的对象类型为 RealService。SimpleInvocationHandler 实现了 InvocationHandler ，它的构造方法接受一个参数，realObj 标识被代理的对象，invoke 方法处理所有的接口调用，它有三个参数： Proxy，代理对象本身，需要注意的是：它并不是被代理的对象，这个参数一般用处不大； method ，正在被调用的方法； args ，方法的参数。SimpleInvocationHandler 的 invoke 实现中，调用了 method 的 invoke 方法，传递了实际对象 realObject 作为参数，达到了调用实际对象对应方法的目的，在调用任何方法前后，输出了跟踪调试语句。需要注意的是，不能将 Proxy（代理类） 作为参数传递给 method.invoke，比如：Object result = method.invoke(realObject, args);上面的语句会出现死循环，因为 proxy 表示当前代理对象，这又会调用到 SimpleInvocationHanlder 的 invoke 方法。基本原理上面代码中，创建 proxyService 的代码可以用下面的代码代替： Class&amp;lt;?&amp;gt; proxyClass = Proxy.getProxyClass(IService.class.getClassLoader(), new Class&amp;lt;?&amp;gt;[]{IService.class}); Constructor&amp;lt;?&amp;gt; ctor = null; try { ctor = proxyClass.getConstructor(new Class&amp;lt;?&amp;gt;[]{InvocationHandler.class}); } catch (NoSuchMethodException e) { e.printStackTrace(); } InvocationHandler handler = new SimpleInvocationHandler(realService); IService proxyService2 = null; try { proxyService = (IService)ctor.newInstance(handler); } catch (InstantiationException e) { e.printStackTrace(); } catch (IllegalAccessException e) { e.printStackTrace(); } catch (InvocationTargetException e) { e.printStackTrace(); }分为三步： 通过 Proxy.getProxyClass 创建代理类定义，类定义会被缓存； 获取代理类的构造方法，构造方法有一个 InvocationHandler 类型的参数； 创建 InvocationHandler 对象，创建代理类对象Proxy.getProxyClass 需要两个参数，一个是 ClassLoader；另一个是接口数组。它会动态生成一个类，类名 以 $Proxy 开头，后面跟了一个数字。$Proxy 的父类是 Proxy，它有一个构造方法，接受一个 InvocationHanlder 类型的参数，保存了实例变量 h，h 定义在父类 Proxy 中，它实现了接口 IService ，对于每个方法，比如 sayHello，它调用了 InvocationHanlder 的 invoke 方法，对于 Object 中的方法，如 hashCode、equals 和 toString，$Proxy 同样转发给了 InvocationHanlder。可以看出，这个类定义本身与被代理的对象没有关系，与 InvocationHandler 的具体实现也没有关系，而主要与接口数组有关，给定这个接口数组，它动态创建每个接口的实现代码，实现就是转发给 InvocationHandler ，与被代理对象的关系以及对它的调用由 InvocationHandler 的实现。对于 Oracle 的 JVM， $Proxy 的定义可以通过下面的命令看到：java -Dsun.misc.ProxyGenerator.saveGeneratedFiles=true cn.happymaya.dynamic.agent.SimpleJDKDynamicProxyDemo以上命令会把动态生成的代理类 $Proxy() 保存到文件 $Proxy.class 中，通过反编译工具比如 D-GUI 就可以得到源码。由此可以，代理类的定义，就是获取构造方法，创建代理对象。动态代理的优点相比于静态代理，动态代理看起来麻烦很多。但是它的好处如下：使用动态代理，可以编写通用的代理逻辑，用于各种类型的被代理对象，而不需要为每个被代理的类型都创建一个静态代理类。public class GeneralProxyDemo { static interface IServiceA { void sayHello(); } static class ServiceAImpl implements IServiceA { @Override public void sayHello() { System.out.println(&quot;hello&quot;); } } static interface IServiceB { void fly(); } static class ServiceBImpl implements IServiceB { @Override public void fly() { System.out.println(&quot;flying&quot;); } } static class SimpleInvocationHandler implements InvocationHandler { private Object realObject; public SimpleInvocationHandler(Object realObject) { this.realObject = realObject; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;entering &quot; + realObject.getClass().getSimpleName() + &quot;::&quot; + method.getName()); Object result = method.invoke(realObject, args); System.out.println(&quot;leaving &quot; + realObject.getClass().getSimpleName() + &quot;::&quot; + method.getName()); return result; } } private static &amp;lt;T&amp;gt; T getProxy(Class&amp;lt;T&amp;gt; intf, T realObject){ return (T)Proxy.newProxyInstance(intf.getClassLoader(),new Class&amp;lt;?&amp;gt;[]{intf}, new SimpleInvocationHandler(realObject)); } public static void main(String[] args) { System.err.println(&quot;########################################&quot;); System.err.println(&quot;## ##&quot;); System.err.println(&quot;## General dynamic proxy ##&quot;); System.err.println(&quot;## ##&quot;); System.err.println(&quot;########################################&quot;); IServiceA a = new ServiceAImpl(); IServiceA aProxy = getProxy(IServiceA.class, a); aProxy.sayHello();; IServiceB b = new ServiceBImpl(); IServiceB bProxy = getProxy(IServiceB.class, b); bProxy.fly();; }}在这个栗子中，有两个接口 IServiceA 和 IServiceB ，它们对应的实现类是 ServiceAImpl 和 ServiceBImpl ，虽然它们的接口和实现不同，但利用动态代理，它们可以调用通用的方法 getProxy 获取代理对象，共享同样的代理逻辑 SimpleInvocationHandler，即在每个方法调用前后输出一条跟踪调试语句：程序输出为：entering ServiceAImpl::sayHellohelloleaving ServiceAImpl::sayHelloentering ServiceBImpl::flyflyingleaving ServiceBImpl::flyCGLIB 动态代理Java SDK 动态代理的缺点是：只能为接口创建代理，返回的代理对象也只能转换到某个接口类型。如果一个类没有接口，或者希望代理非接口中定义的方法，这就没有办法了。有一个三方类库 CGLIB(Code Generation Library)，可以做到上面一点，Spring、Hibernate 等都使用该类库。public class SimpleCGLIBDemo { static class RealService { public void sayHello() { System.out.println(&quot;hello&quot;); } } static class SimpleInterceptor implements MethodInterceptor { @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { System.out.println(&quot;entering &quot; + method.getName() + &quot;!&quot;); Object result = methodProxy.invokeSuper(o, objects); System.out.println(&quot;leaving &quot; + method.getName() + &quot;!&quot;); return result; } } private static &amp;lt;T&amp;gt; T getProxy(Class&amp;lt;T&amp;gt; cls){ Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(cls); enhancer.setCallback(new SimpleInterceptor()); return (T) enhancer.create(); } public static void main(String[] args) { RealService proxyService = getProxy(RealService.class); proxyService.sayHello(); }}RealService 表示被代理的类，它没有接口。getProxy() 为一个类生成代理对象，这个代理对象可以安全地转换为被代理类的类型，它使用了 CGLIB 的 Enhancer 类。Enhancer 类的 setSuperClass 设置被代理的类，setCallBack 设置被代理类的 public 非 final 方法被调用时的处理类。Enhancer 支持多种类型，这里使用的类实现了 MethodInterceptor 接口，它与 Java SDK 中的 InvocationHandler 有点类似，方法名变成了 intercept，多了一个 MethodProxy 类型的参数。与前面的 InvocationHandler 不同，SimpleInterceptor 中没有被代理的对象，它通过 MethodProxy 的 invokeSuper 方法调用被代理类的方法：Object result = methodProxy.invokeSuper(o, objects);不过，它不能这样调用被代理类的方法：Object result = methodProxy.invoke(object, args);object 是代理对象，调用这个方法还会调用到 SimpleInterceptor 的 intercept 方法，造成死循环。在 main 方法中，也没有创建被代理的对象，创建的对象直接就是代理对象。CGLIB 的实现机制与 Java SDK 不同，是通过继承实现的。也是动态创建了一个类，但这个类的父类是被代理的类，代理类重写了父类的所有 public 非 final 方法，改为调用 Callback 中的相关方法。上面代码中，调用 SimpleInterceptor 的 intercept 方法。Java SDK 代理与 CGLIB 代理的比较Java SDK 代理面向的是一组接口。它为这些接口动态创建了一个实现类。接口的具体实现逻辑是通过自定义的 InvocationHandler 实现的，这个实现是自定义的，也就是说，其背后都不一定有真正被代理的对象，也可能有多个实际对象，根据情况动态选择。CGLIB 代理面向的是一个具体的类，它动态创建了一个新类，继承了该类，重写了其方法。从代理的角度看，Java SDK 代理的是对象，需要先有一个实际对象，自定义的 InvocationHandler 引用该对象，然后创建一个代理类和代理对象，客户端访问的是代理对象，代理对象最后再调用实际对象的方法；CGLIB 代理的是类，创建的对象只有一个。如果目的都是为一个类的方法增强功能，Java SDK 要求该类要求必须有接口，且只能处理接口中的方法，CGLIB 没有这个限制。动态代理的应用：AOP利用 CGLIB 动态代理，可以实现一个极简单的 AOP 框架，从而学习了解 AOP 的基本思想和技术。用法 添加一个新的注解@Aspect，其定义为： @Retention(RUNTIME)@Target(TYPE)public @interface Aspect { Class&amp;lt;?&amp;gt;[] value();} 约定，切面类可以声明三个方法 before/after/exception，在主体类的方法 调用前/调用后/出现异常时 分别调用这三个方法，这三个方法的声明符合如下参数： public static void before(Object object, Method method, Object[] args) {} public static void after(Object object, Method method, Object[] args, Object result) {} public static void exception(Object object, Method method, Object[] args, Throwable e) {} @Aspect主要用于注解切面类，它有一个参数，可以指定要增强的类，比如： @Aspect({ServiceA.class,ServiceB.class})public class ServiceLogAspect { public static void before(Object object, Method method, Object[] args) { System.out.println( &quot;entering &quot; + method.getDeclaringClass().getSimpleName() + &quot;::&quot; + method.getName() + &quot;, args: &quot; + Arrays.toString(args) ); } public static void after(Object object, Method method, Object[] args, Object result) { System.out.println( &quot;leaving &quot; + method.getDeclaringClass().getSimpleName() + &quot;::&quot; + method.getName() + &quot;, result: &quot; + result ); }} ServiceLogAspect 就是一个切面，它负责 ServiceA 和 ServiceB 的日志切面，即为这两个增加日志功能。 目的是在类 ServiceA 和 ServiceB 所有方法的执行前后增加一些日志，而 ExceptionAspect 的目的是在类 ServiceB 的方法执行出现异常时收到通知并输出一些信息。 它们都没有修改类 ServiceA 和 ServiceB 本身，本身做的事是比较通用的，与 ServiceA 和 ServiceB 的具体逻辑关系也不密切，但又想改变 ServiceA/ServiceB 的行为，这就是 AOP 的思维。 异常切面，负责类的异常切面 @Aspect({ServiceB.class})public class ExceptionAspect { public static void exception(Object object, Method method, Object[] args, Throwable e) { System.err.println(&quot;exception when calling: &quot; + method.getName() + &quot;,&quot; + Arrays.toString(args)); }} object、method 和 args 与 CGLIB MethodInterceptor 中的 invoke 参数一样，after 中的 result 表示方法执行的结果，exception 中的 e 表示发生的异常类型。 ExceptionAspect 只实现 exception 方法，在异常发生的时候，会输出一些信息。 只是声明一个切面类是不起作用的，还需要和 DI 容器结合起来，实现一个新的容器 CGLibContaier ，有一个方法如下： 通过该方法获取 ServiceA 和 ServiceB ，它们的行为就会被改变。 通过 CGLibContaier 获取 ServiceA，会自动应用 ServiceLogAspect，比如： ServiceA a = CGLibContaier.getInstance(ServiceA.class); 输出为： entering ServiceA::callB, args: {}entering ServiceB::action, args: {}I&#39;m Bleaving ServiceB::action, result: nullleaving ServiceA::callB, result:null " }, { "title": "常用开发工具总结", "url": "/posts/tools/", "categories": "Blog, Notes", "tags": "tools", "date": "2018-08-10 16:34:00 +0000", "snippet": "IDEIdeaj ideaPlugins Atom Material Icons GsonFormatPlus Json Parser Material ThemeUI Rainbow Brackets REST Client plugin Code With Me Grazie IDE Features Trainer IDE Features Trainer: Git LessonsVS codeSSH Tabby" }, { "title": "自定义基于 jekyll-theme-chirpy 主题博客的主题", "url": "/posts/customize-the-favicon/", "categories": "Blog, Notes", "tags": "favicon", "date": "2018-08-10 16:34:00 +0000", "snippet": "The favicons of imaya are placed in the directory assets/img/favicons/. You may want to replace them with your own. The following sections will guide you to create and replace the default favicons.Generate the faviconPrepare a square image (PNG, JPG, or SVG) with a size of 512x512 or more, and then go to the online tool Real Favicon Generator and click the button Select your Favicon image to upload your image file.In the next step, the webpage will show all usage scenarios. You can keep the default options, scroll to the bottom of the page, and click the button Generate your Favicons and HTML code to generate the favicon.Download &amp;amp; ReplaceDownload the generated package, unzip and delete the following two from the extracted files: browserconfig.xml site.webmanifestAnd then copy the remaining image files (.PNG and .ICO) to cover the original files in the directory assets/img/favicons/ of your Jekyll site. If your Jekyll site doesn’t have this directory yet, just create one.The following table will help you understand the changes to the favicon files: File(s) From Online Tool From Chirpy *.PNG ✓ ✗ *.ICO ✓ ✗ ✓ means keep, ✗ means delete.The next time you build the site, the favicon will be replaced with a customized edition." }, { "title": "使用 jekyll-theme-chirpy 主题美化基于 jeykell + Giuhub Pages 的博客", "url": "/posts/getting-started/", "categories": "Blog, Notes", "tags": "getting started", "date": "2018-08-09 12:55:00 +0000", "snippet": "PrerequisitesFollow the instructions in the Jekyll Docs to complete the installation of Ruby, RubyGems, Jekyll, and Bundler. In addition, Git is also required to be installed.InstallationCreating a New SiteThere are two ways to create a new repository for this theme: Using the Chirpy Starter - Easy to upgrade, isolates irrelevant project files so you can focus on writing. Forking on GitHub - Convenient for custom development, but difficult to upgrade. Unless you are familiar with Jekyll and are determined to tweak or contribute to this project, this approach is not recommended.Option 1. Using the Chirpy StarterCreate a new repository from the Chirpy Starter and name it &amp;lt;GH_USERNAME&amp;gt;.github.io, where GH_USERNAME represents your GitHub username.Option 2. Forking on GitHubFork Chirpy on GitHub and rename it to &amp;lt;GH_USERNAME&amp;gt;.github.io. Please note that the default branch code is in development. If you want the site to be stable, please switch to the latest tag and start writing.And then execute:$ bash tools/init.sh If you don’t want to deploy your site on GitHub Pages, append option --no-gh at the end of the above command.The above command will: Removes some files or directories from your repository: .travis.yml files under _posts If the option --no-gh is provided, the directory .github will be deleted. Otherwise, set up the GitHub Action workflow by removing the extension .hook of .github/workflows/pages-deploy.yml.hook, and then remove the other files and directories in the folder .github. Removes item Gemfile.lock from .gitignore. Creates a new commit to save the changes automatically.Installing DependenciesBefore running for the first time, go to the root directory of your site, and install dependencies as follows:$ bundleUsageConfigurationUpdate the variables of _config.yml as needed. Some of them are typical options: url avatar timezone langCustoming StylesheetIf you need to customize the stylesheet, copy the theme’s assets/css/style.scss to the same path on your Jekyll site, and then add the custom style at the end of the style file.Starting from v4.1.0, if you want to overwrite the SASS variables defined in _sass/addon/variables.scss, create a new file _sass/variables-hook.scss and assign new values to the target variable in it.Running Local ServerYou may want to preview the site contents before publishing, so just run it by:$ bundle exec jekyll sOr run the site on Docker with the following command:$ docker run -it --rm \\ --volume=&quot;$PWD:/srv/jekyll&quot; \\ -p 4000:4000 jekyll/jekyll \\ jekyll serveAfter a while, the local service will be published at http://127.0.0.1:4000.DeploymentBefore the deployment begins, check out the file _config.yml and make sure the url is configured correctly. Furthermore, if you prefer the project site and don’t use a custom domain, or you want to visit your website with a base URL on a web server other than GitHub Pages, remember to change the baseurl to your project name that starts with a slash, e.g, /project-name.Now you can choose ONE of the following methods to deploy your Jekyll site.Deploy by Using Github ActionsFor security reasons, GitHub Pages build runs on safe mode, which restricts us from using plugins to generate additional page files. Therefore, we can use GitHub Actions to build the site, store the built site files on a new branch, and use that branch as the source of the GitHub Pages service.Quickly check the files needed for GitHub Actions build: Ensure your Jekyll site has the file .github/workflows/pages-deploy.yml. Otherwise, create a new one and fill in the contents of the sample file, and the value of the on.push.branches should be the same as your repo’s default branch name. Ensure your Jekyll site has file tools/deploy.sh. Otherwise, copy it from here to your Jekyll site. Furthermore, if you have committed Gemfile.lock to the repo, and your runtime system is not Linux, don’t forget to update the platform list in the lock file: $ bundle lock --add-platform x86_64-linux After the above steps, rename your repository to &amp;lt;GH_USERNAME&amp;gt;.github.io on GitHub.Now publish your Jekyll site by: Push any commit to remote to trigger the GitHub Actions workflow. Once the build is complete and successful, a new remote branch named gh-pages will appear to store the built site files. Browse to your repository on GitHub. Select the tab Settings, then click Pages in the left navigation bar, and then in the section Source of GitHub Pages, select the /(root) directory of branch gh-pages as the publishing source. Remember to click Save before leaving. Visit your website at the address indicated by GitHub. Manually Build and DeployOn self-hosted servers, you cannot enjoy the convenience of GitHub Actions. Therefore, you should build the site on your local machine and then upload the site files to the server.Go to the root of the source project, and build your site as follows:$ JEKYLL_ENV=production bundle exec jekyll bOr build the site on Docker:$ docker run -it --rm \\ --env JEKYLL_ENV=production \\ --volume=&quot;$PWD:/srv/jekyll&quot; \\ jekyll/jekyll \\ jekyll buildUnless you specified the output path, the generated site files will be placed in folder _site of the project’s root directory. Now you should upload those files to the target server.UpgradingIt depends on how you use the theme: If you are using the theme gem (there will be gem &quot;jekyll-theme-chirpy&quot; in the Gemfile), editing the Gemfile and update the version number of the theme gem, for example: - gem &quot;jekyll-theme-chirpy&quot;, &quot;~&amp;gt; 3.2&quot;, &quot;&amp;gt;= 3.2.1&quot;+ gem &quot;jekyll-theme-chirpy&quot;, &quot;~&amp;gt; 3.3&quot;, &quot;&amp;gt;= 3.3.0&quot; And then execute the following command: $ bundle update jekyll-theme-chirpy As the version upgrades, the critical files (for details, see the Startup Template) and configuration options will change. Please refer to the Upgrade Guide to keep your repo’s files in sync with the latest version of the theme. If you forked from the source project (there will be gemspec in the Gemfile of your site), then merge the latest upstream tags into your Jekyll site to complete the upgrade.The merge is likely to conflict with your local modifications. Please be patient and careful to resolve these conflicts. " }, { "title": "Markdown 的文本和排版总结", "url": "/posts/text-and-typography/", "categories": "Blog, Notes", "tags": "排版", "date": "2018-08-08 03:33:00 +0000", "snippet": "标题H1 - headingH2 - headingH3 - headingH4 - heading段落I wandered lonely as a cloudThat floats on high o’er vales and hills,When all at once I saw a crowd,A host, of golden daffodils;Beside the lake, beneath the trees,Fluttering and dancing in the breeze.Lists有序列表 Firstly Secondly Thirdly无序列表 Chapter Section Paragraph 任务列表 TODO Completed Defeat COVID-19 Vaccine production Economic recovery People smile again 描述列表 Sun the star around which the earth orbits Moon the natural satellite of the earth, visible by reflected light from the sunBlock Quote This line shows the block quote.提示 An example showing the info type prompt. An example showing the tip type prompt. An example showing the warning type prompt. An example showing the danger type prompt.表格 Company Contact Country Alfreds Futterkiste Maria Anders Germany Island Trading Helen Bennett UK Magazzini Alimentari Riuniti Giovanni Rovelli Italy 连接http://127.0.0.1:4000脚注Click the hook will locate the footnote1, and here is another footnote2.Images Default (with caption)Full screen width and center alignment Shadowshadow effect (visible in light mode) Left aligned Float to left “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Float to right “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Mermaid SVG gantt title Adding GANTT diagram functionality to mermaid apple :a, 2017-07-20, 1w banana :crit, b, 2017-07-23, 1d cherry :active, c, after b a, 1dMathematicsThe mathematics powered by MathJax:\\[\\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6}\\]When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\]Inline codeThis is an example of Inline Code.FilepathHere is the /path/to/the/file.extend.Code blockCommonThis is a common code snippet, without syntax highlight and line number.Specific LanguagesConsole$ env |grep SHELLSHELL=/usr/local/bin/bashPYENV_SHELL=bashShellif [ $? -ne 0 ]; then echo &quot;The command was not successful.&quot;; #do the needful / exitfi;Specific filename@import &quot;colors/light-typography&quot;, &quot;colors/dark-typography&quot;Reverse Footnote The footnote source &amp;#8617; The 2nd footnote source &amp;#8617; " }, { "title": "Socket 编程：epoll 为什么用红黑树？", "url": "/posts/socket-10/", "categories": "Internal Power, Network", "tags": "Socket, I/O, epoll", "date": "2018-07-10 14:32:00 +0000", "snippet": "Socket 对象、配置 Socket 地址、服务端 Socket、客户端 Socket 等名词时，是否可以明确理解这些概念？除了上面这些概念，还有 I/O 模型、异步编程、内存映射等概念。再往更深层次学习，你还会碰到 epoll/select 等编程模型。其实学习好这些知识有一条主线，就是抓住操作系统对 Socket 文件的设计。Socket 是什么？首先，Socket 是一种编程的模型。下图中，从编程的角度来看： 首先，客户端将数据发送给在客户端侧的Socket 对象， 然后，客户端侧的 Socket 对象将数据发送给服务端侧的 Socket 对象。Socket 对象负责提供通信能力，并处理底层的 TCP 连接/UDP 连接； 对服务端而言，每一个客户端接入，就会形成一个和客户端对应的 Socket 对象，如果服务器要读取客户端发送的信息，或者向客户端发送信息，就需要通过这个客户端 Socket 对象。但是如果从另一个角度去分析，Socket 还是一种文件，准确来说是一种双向管道文件。什么是管道文件呢？管道会将一个程序的输出，导向另一个程序的输入。那么什么是双向管道文件呢？双向管道文件连接的程序是对等的，都可以作为输入和输出。比如下面这段服务端侧程序：var serverSocket = new ServerSocket();serverSocket.bind(new InetSocketAddress(80));看起来我们创建的是一个服务端 Socket 对象，但如果单纯看这个对象，它又代表什么呢？如果我们理解成代表服务端本身合不合理呢——这可能会比较抽象，在服务端存在一个服务端 Socket。但如果从管道文件的层面去理解它，就会比较容易了： 其一，这是一个文件； 其二，它里面存的是所有客户端 Socket 文件的文件描述符。当一个客户端连接到服务端的时候，操作系统就会创建一个客户端 Socket 的文件。然后操作系统将这个文件的文件描述符写入服务端程序创建的服务端 Socket 文件中。服务端 Socket 文件，是一个管道文件。如果读取这个文件的内容，就相当于从管道中取走了一个客户端文件描述符。如上图所示，服务端 Socket 文件相当于一个客户端 Socket 的目录，线程可以通过 accept() 操作每次拿走一个客户端文件描述符。拿到客户端文件描述符，就相当于拿到了和客户端进行通信的接口。前面提到 Socket 是一个双向的管道文件： 当线程想要读取客户端传输来的数据时，就从客户端 Socket 文件中读取数据； 当线程想要发送数据到客户端时，就向客户端 Socket 文件中写入数据； 客户端 Socket 是一个双向管道，操作系统将客户端传来的数据写入这个管道，也将线程写入管道的数据发送到客户端。有同学会说，那既然可以双向传送，这不就是两个单向管道被拼凑在了一起吗？这里具体的实现取决于操作系统，Linux 中的管道文件都是单向的，因此 Socket 文件是一种区别于原有操作系统管道的单独的实现。总结下，Socket 首先是文件，存储的是数据。对服务端而言，分成服务端 Socket 文件和客户端 Socket 文件。服务端 Socket 文件存储的是客户端 Socket 文件描述符；客户端 Socket 文件存储的是传输的数据。读取客户端 Socket 文件，就是读取客户端发送来的数据；写入客户端文件，就是向客户端发送数据。对一个客户端而言， Socket 文件存储的是发送给服务端（或接收的）数据。综上，Socket 首先是文件，在文件的基础上，又封装了一段程序，这段程序提供了 API 负责最终的数据传输。服务端 Socket 的绑定为了区分应用，对于一个服务端 Socket 文件，我们要设置它监听的端口。比如 Nginx 监听 80 端口、Node 监听 3000 端口、SSH 监听 22 端口、Tomcat 监听 8080 端口。端口监听不能冲突，不然客户端连接进来创建客户端 Socket 文件，文件描述符就不知道写入哪个服务端 Socket 文件了。这样操作系统就会把连接到不同端口的客户端分类，将客户端 Socket 文件描述符存到对应不同端口的服务端 Socket 文件中。因此，服务端监听端口的本质，是将服务端 Socket 文件和端口绑定，这个操作也称为 bind。有时候我们不仅仅绑定端口，还需要绑定 IP 地址。这是因为有时候我们只想允许指定 IP 访问我们的服务端程序。扫描和监听对于一个服务端程序，可以定期扫描服务端 Socket 文件的变更，来了解有哪些客户端想要连接进来。如果在服务端 Socket 文件中读取到一个客户端的文件描述符，就可以将这个文件描述符实例化成一个 Socket 对象。之后，服务端可以将这个 Socket 对象加入一个容器（集合），通过定期遍历所有的客户端 Socket 对象，查看背后 Socket 文件的状态，从而确定是否有新的数据从客户端传输过来。上述的过程，我们通过一个线程就可以响应多个客户端的连接，也被称作I/O 多路复用技术。响应式（Reactive）在 I/O 多路复用技术中，服务端程序（线程）需要维护一个 Socket 的集合（可以是数组、链表等），然后定期遍历这个集合。这样的做法在客户端 Socket 较少的情况下没有问题，但是如果接入的客户端 Socket 较多，比如达到上万，那么每次轮询的开销都会很大。从程序设计的角度来看，像这样主动遍历，比如遍历一个 Socket 集合看看有没有发生写入（有数据从网卡传过来），称为命令式的程序。这样的程序设计就好像在执行一条条命令一样，程序主动地去查看每个 Socket 的状态。命令式会让负责下命令的程序负载过重，例如，在高并发场景下，上述讨论中循环遍历 Socket 集合的线程，会因为负担过重导致系统吞吐量下降。与命令式相反的是响应式（Reactive），响应式的程序就不会有这样的问题。在响应式的程序当中，每一个参与者有着独立的思考方式，就好像拥有独立的人格，可以自己针对不同的环境触发不同的行为。从响应式的角度去看 Socket 编程，应该是有某个观察者会观察到 Socket 文件状态的变化，从而通知处理线程响应。线程不再需要遍历 Socket 集合，而是等待观察程序的通知。当然，最合适的观察者其实是操作系统本身，因为只有操作系统非常清楚每一个 Socket 文件的状态。原因是对 Socket 文件的读写都要经过操作系统。在实现这个模型的时候，有几件事情要注意。 线程需要告诉中间的观察者自己要观察什么，或者说在什么情况下才响应？比如具体到哪个 Socket 发生了什么事件？是读写还是其他的事件？这一步我们通常称为注册。 中间的观察者需要实现一个高效的数据结构（通常是基于红黑树的二叉搜索树）。这是因为中间的观察者不仅仅是服务于某个线程，而是服务于很多的线程。当一个 Socket 文件发生变化的时候，中间观察者需要立刻知道，究竟是哪个线程需要这个信息，而不是将所有的线程都遍历一遍。为什么用红黑树？关于为什么要红黑树，考虑到中间观察者最核心的诉求有两个： 第一个核心诉求，是让线程可以注册自己关心的消息类型。比如线程对文件描述符 =123 的 Socket 文件读写都感兴趣，会去中间观察者处注册。当 FD=123 的 Socket 发生读写时，中间观察者负责通知线程，这是一个响应式的模型； 第二个核心诉求，是当 FD=123 的 Socket 发生变化（读写等）时，能够快速地判断是哪个线程需要知道这个消息。所以，中间观察者需要一个快速能插入（注册过程）、查询（通知过程）一个整数的数据结构，这个整数就是 Socket 的文件描述符。综合来看，能够解决这个问题的数据结构中，跳表和二叉搜索树都是不错的选择。因此，在 Linux 的 epoll 模型中，选择了红黑树。红黑树是二叉搜索树的一种，红与黑是红黑树的实现者才关心的内容，对于我们使用者来说不用关心颜色，Java 中的 TreeMap 底层就是红黑树。总结总结一下，Socket 既是一种编程模型，或者说是一段程序，同时也是一个文件，一个双向管道文件。也可以这样理解，Socket API 是在 Socket 文件基础上进行的一层封装，而 Socket 文件是操作系统提供支持网络通信的一种文件格式。在服务端有两种 Socket 文件，每个客户端接入之后会形成一个客户端的 Socket 文件，客户端 Socket 文件的文件描述符会存入服务端 Socket 文件。通过这种方式，一个线程可以通过读取服务端 Socket 文件中的内容拿到所有的客户端 Socket。这样一个线程就可以负责响应所有客户端的 I/O，这个技术称为 I/O 多路复用： 主动式的 I/O 多路复用，对负责 I/O 的线程压力过大，因此通常会设计一个高效的中间数据结构作为 I/O 事件的观察者线程通过订阅 I/O 事件被动响应，这就是响应式模型。 在 Socket 编程中，最适合提供这种中间数据结构的就是操作系统的内核，事实上 epoll 模型也是在操作系统的内核中提供了红黑树结构。epoll 为什么用红黑树： 在 Linux 的设计中有三种典型的 I/O 多路复用模型 select、poll、epoll； select 是一个主动模型，需要线程自己通过一个集合存放所有的 Socket，然后发生 I/O 变化的时候遍历。在 select 模型下，操作系统不知道哪个线程应该响应哪个事件，而是由线程自己去操作系统看有没有发生网络 I/O 事件，然后再遍历自己管理的所有 Socket，看看这些 Socket 有没有发生变化； poll 提供了更优质的编程接口，但是本质和 select 模型相同。因此千级并发以下的 I/O，你可以考虑 select 和 poll，但是如果出现更大的并发量，就需要用 epoll 模型； epoll 模型在操作系统内核中提供了一个中间数据结构，这个中间数据结构会提供事件监听注册，以及快速判断消息关联到哪个线程的能力（红黑树实现）。因此在高并发 I/O 下，可以考虑 epoll 模型，它的速度更快，开销更小。一个 epoll 的 hello world 例子：" }, { "title": "wireshark：使用 wireshark 对 TCP 进行抓包调试", "url": "/posts/wireshark-tcp-09/", "categories": "Internal Power, Network", "tags": "TCP, Wireshark", "date": "2018-07-09 14:45:22 +0000", "snippet": "网络调试工具——Wireshark 是世界上应用最广泛的网络协议分析器，它让我们在微观层面上看到整个网络正在发生的事情。Wireshark 本身是一个开源项目，所以也得到了很多志愿者的支持。同时，Wireshark 具有丰富的功能集，包括： 深入检查数百个协议，并不断添加更多协议； 实时捕获和离线分析； 支持 Windows、Linux、macOS、Solaris、FreeBSD、NetBSD，以及许多其他平台； 提供 GUI 浏览，也可以通过 TTY； 支持 VOIP； 支持 Gzip； 支持 IPSec。 ……是不是觉得Wireshark非常强大？无论你从事哪种开发工作，它都可以帮到你，因此也是面试经常考察的内容。比如本讲关联的面试题：如何进行 TCP 抓包和调试？下面请你带着问题，开始今天的学习吧。注：你可以到 Wireshark 的主页：https://www.wireshark.org/download.html下载 Wireshark。如果你是一个黑客、网络安全工程师，或者你的服务总是不稳定，就需要排查，那么你会如何 hack 这些网络连接、网络接口以及分析网络接口的封包呢？接口列表Whireshark 可以帮你看到整个网络交通情况，也可以帮你深入了解每个封包。而且 Whireshark 在 macOS、Linux、Windows 上的操作都是一致的，打开 Wireshark 会先看到如下图所示的一个选择网络接口的界面。我们要做的第一件事情就是选择一个网络接口（Network Interface）。Linux 下可以使用ifconfig指令看到所有的网络接口，Windows 下则使用 ipconfig。可以看到，上图中有很多网络接口，目前我教学这台机器上，连接路由器的接口是以太网 2。另外可以看到，我的机器上还有VMware的虚拟网络接口（你的机器可能和我的机器显示的不一样）。开启捕获功能选择好接口之后，点击左上角的按钮就可以开启捕获，开启后看到的是一个个数据条目。因为整个网络的数据非常多，大量的应用都在使用网络，你会看到非常多数据条目，每个条目是一次数据的发送或者接收。如下图所示：以下是具体捕获到的内容： 序号（No.）是 Wireshark 分配的一个从捕获开始的编号。 时间（Time）是从捕获开始过去的时间戳，具体可以在视图中设置，比如可以设置成中文的年月日等。这里有很多配置需要你自己摸索一下，我就不详细介绍了。 源地址和目标地址（Source 和 Destination）是 IP 协议，注意这里有 IPv6 的地址，也有 IPV4 的地址。 协议可能有很多种，比如 TCP/UDP/ICMP 等，ICMP 是 IP 协议之上搭建的一个消息控制协议（Internet Control Message Protocol），比如 Ping 用的就是 ICMP；还有 ARP 协议（Address Resolution Protocol）用来在局域网广播自己的 MAC 地址。 Length 是消息的长度（Bytes）。 Info 是根据不同协议显示的数据，比如你可以看到在TCP 协议上看到Seq 和 ACK。这里的 Seq 和 ACK 已经简化过了，正常情况下是一个大随机数，Whireshark 帮你共同减去了一个初始值。观察 TCP 协议如果你具体选择一个 TCP 协议的捕获，可以看到如下图所示的内容：然后在这下面可以观察到详情内容：我们可以从不同的层面来看这次捕获。从传输层看是 TCP 段；从网络层来看是 IP 封包；从链路层来看是 Frame。点开不同层面观察这个 TCP 段，就可以获得对它更具体的认识，例如下图是从 TCP 层面理解这次捕获：你可以看到这次捕获是一次 ACK（见 Flags）字段，从端口 58260 发往 443，那么大概率是 HTTPS 客户端给服务器的响应。消息视图如果你选中一条消息，下面会出现一个消息视图。还有一个二进制视图。二进制视图里面是数据的二进制形式，消息视图是对二进制形式的解读。Whireshark 追溯的是最底层网卡传输的 Frame（帧），可以追溯到数据链路层。因此对二进制形式的解读，也就是我们的消息视图也要分层。因为对于同样的数据，不同层的解读是不同的。 最上面是 Frame 数据，主要是关注数据的收发时间和大小。 接着是数据链路层数据，关注的是设备间的传递。你可以在这里看到源 MAC 地址和目标 MAC 地址。 然后是网络层数据，IP 层数据。这里有 IP 地址（源 IP 地址和目标 IP 地址）；也有头部的 Checksum（用来纠错的）。这里就不一一介绍了，你可以回到“[06 IPv4 协议：路由和寻址的区别是什么？](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=837#/detail/pc?id=7271&amp;amp;fileGuid=xxQTRXtVcqtHK6j8)”复习这块内容。 最下面是传输层数据。 也就是 TCP 协议。关注的是源端口，目标端口，Seq、ACK 等。 有的传输层上还有一个 TLS 协议，这是因为用 HTTPS 请求了数据。TLS 也是传输层。TLS 是建立在 TCP 之上，复用了 TCP 的逻辑。观察 HTTP 协议Wireshark 还可以用来观察其他的协议，比如说 HTTP 协议，下图是对 HTTP 协议的一次捕获：可以看到，Wireshark 不仅仅捕获了应用层，还可以看到这次 HTTP 捕获对应的传输层、网络层和链路层数据。过滤和筛选Wireshark 还提供了捕获的过滤，我们只需要输入过滤条件，就可以只看符合条件的捕获。比如我们想分析一次到百度的握手。首先开启捕获，然后在浏览器输入百度的网址，最后通过ping指令看下百度的 IP 地址，如下图所示：看到IP 地址之后，我们在 Wireshark 中输入表达式，如下图所示：这样看到的就是和百度关联的所有连接。上图中刚好是一次从建立 TCP 连接（3 次握手），到 HTTPS 协议传输握手的完整过程。你可以只看从192.168.1.5到14.215.177.39的请求。首先是从客户端（192.168.1.5）发出的 SYN 和百度返回的 SYN-ACK，如下图所示：然后是客户端返回给百度一个 ACK：接下来是 HTTPS 协议开始工作（开始握手）：可以看到 HTTPS 协议通过 TLSv1.2 发送了 Client Hello 到服务端。接下来是 Server 返回给客户端 ACK，然后再发送给客户端一个 Server Hello：之后百度回传了证书：最后开始交换密钥，直到 HTTPS 握手结束：报文颜色在抓包过程中，黑色报文代表各类报文错误；红色代表出现异常；其他颜色代表正常传输。总结在本讲，我对 Wireshark 做了一次开箱教学，希望你听完我的课程后，在自己的机器中也安装一个这个工具，以备不时之需。Wireshark 是个强大的工具，支持大量的协议。还有很多关于 Wireshark 的能力，希望你可以进一步探索，如下图中鼠标右键一次捕获，可以看到很多选项，都是可以深挖的。那么现在你可以尝试来回答我在本讲开头提出的问题：如何进行 TCP 抓包？答案就是用工具，例如 Wireshark。思考题最后给你留一道实战题目：请你用自己最熟悉的语言，写一个 UDP 连接程序，然后用 Wireshark 抓包。我建议你自己真正实际操作一遍，检验一下自己的学习成果。如果你对本次课程有什么建议和疑问，可以在评论区留言。如果你有所收获，也可以推荐给你的朋友。这一讲就到这里，发现求知的乐趣，我是林䭽。感谢你学习本次课程，下一讲是模块二思考题解答，希望你自己完成题目后再来看答案和分析。再见！" }, { "title": "局域网：NAT 是如何工作的？", "url": "/posts/nat-08/", "categories": "Internal Power, Network", "tags": "NAT, 局域网", "date": "2018-07-07 12:32:00 +0000", "snippet": "广域网是由很多的局域网组成的，比如公司网络、家庭网络、校园网络等。之前我们一直在讨论广域网的设计，今天我们到微观层面，看看局域网是如何工作的。IPv4 的地址不够，因此需要设计子网。当一个公司申请得到一个公网 IP 后，会在自己的公司内部设计一个局域网。这个局域网所有设备的 IP 地址，通常会以 192.168 开头。这个时候，假设你的职工小明，上班时间玩王者荣耀。当他用 UDP 协议向王者荣耀的服务器发送信息时，消息的源 IP 地址是一个内网 IP 地址，而王者荣耀的服务，是一个外网 IP 地址。这里我先向你提一个问题，数据到王者荣耀服务器可以通过寻址和路由找到目的地，但是数据从王者荣耀服务器回来的时候，王者荣耀服务器如何知道192.168开头的地址应该如何寻址呢？要想回答这个问题，就涉及网络地址转换协议（NAT 协议）。下面请你带着这个问题，开启今天的学习吧。内部网络和外部网络对一个组织、机构、家庭来说，我们通常把内部网络称为局域网，外部网络就叫作外网。下图是一个公司多个部门的网络架构。我们会看到外网通过路由器接入整个公司的局域网，和路由器关联的是三台交换机，代表公司的三个部门。交换机，或者称为链路层交换机，通常工作在链路层；而路由器通常也具有交换机的能力，工作在网络层和链路层。关于它们的详细区别，我们会在本文的后续讨论。光纤是一种透明的导光介质，多束光可以在一个介质中并行传播，不仅信号容量大，重量轻，并行度高而且传播距离远。当然，光纤不能弯曲，因此办公室里用来连接交换机和个人电脑的线路肯定不能是光纤，光线通常都用于主干网络。局域网数据交换（MAC 地址）接下来我们讨论下同一个局域网中的设备如何交换消息。首先，我们先明确一个概念，设备间通信的本质其实是设备拥有的网络接口（网卡）间的通信。为了区别每个网络接口，互联网工程任务组（IETF）要求每个设备拥有一个唯一的编号，这个就是 MAC 地址。你可能会问：IP 地址不也是唯一的吗？其实不然，一旦设备更换位置，比如你把你的电脑从北京邮寄的广州，那么 IP 地址就变了，而电脑网卡的 MAC 地址不会发生变化。总的来说，IP 地址更像现实生活中的地址，而 MAC 地址更像你的身份证号。然后，我们再明确另一个基本的概念。在一个局域网中，我们不可以将消息从一个接口（网卡）发送到另一个接口（网卡），而是要通过交换机。为什么是这样呢？因为两个网卡间没有线啊！所以数据交换，必须经过交换机，毕竟线路都是由网卡连接交换机的。总结下，数据的发送方，将自己的 MAC 地址、目的地 MAC 地址，以及数据作为一个分组（Packet），也称作 Frame 或者封包，发送给交换机。交换机再根据目的地 MAC 地址，将数据转发到目的地的网络接口（网卡）。最后一个问题，你可能问，这个分组或者 Frame，是不是 IP 协议的分组呢？——不是，这里提到的是链路层的数据交换，它支持 IP 协议工作，是网络层的底层。所以，如果 IP 协议要传输数据，就要将数据转换成为链路层的分组，然后才可以在链路层传输。链路层分组大小受限于链路层的网络设备、线路以及使用了链路层协议的设计。你有时候可能会看到 MTU 这个缩写词，它指的是 Maximun Transmission Unit，最大传输单元，意思是链路层网络允许的最大传输数据分组的大小。因此 IP 协议要根据 MTU 拆分封包。 之前在“[04 TCP 的稳定性：滑动窗口和流速控制是怎么回事？](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=837#/detail/pc?id=7268&amp;amp;fileGuid=xxQTRXtVcqtHK6j8)”介绍 TCP 协议滑动窗口的时候，还提到过一个词，叫作 MSS，这里我们复习下。MSS（Maximun Segment Size，最大段大小）是 TCP 段，或者称为 TCP 分组（TCP Packet）的最大大小。MSS 是传输层概念，MTU 是链路层概念。 聪明的同学可以能会意识到，这不就是下面这样一个数学关系吗？MTU = MSS + TCP Header + IP Header这个思路有一定道理，但是不对。先说说这个思路怎么来的，你可能会这么思考：TCP 传输数据大于 MSS，就拆包。每个封包加上 TCP Header ，之后经过 IP 协议，再加上 IP Header。于是这个加上 IP 头的分组（Packet）不能超过 MTU。固然这个思路很有道理，可惜是错的。因为 TCP 解决的是广域网的问题，MTU 是一个链路层的概念，要知道不同网络 MTU 是不同的，所以二者不可能产生关联。这也是为什么 IP 协议还可能会再拆包的原因。地址解析协议（ARP）上面我们讨论了 MAC 地址，链路层通过 MAC 地址定位网络接口（网卡）。在一个网络接口向另一个网络接口发送数据的时候，至少要提供这样 3 个字段： 源 MAC 地址 目标 MAC 地址 数据这里我们一起再来思考一个问题，对于一个网络接口，它如何能知道目标接口的 MAC 地址呢？我们在使用传输层协议的时候，清楚地知道目的地的 IP 地址，但是我们不知道 MAC 地址。这个时候，就需要一个中间服务帮助根据 IP 地址找到 MAC 地址——这就是地址解析协议（Address Resolution Protocol，ARP）。整个工作过程和 DNS 非常类似，如果一个网络接口已经知道目标 IP 地址对应的 MAC 地址了，它会将数据直接发送给交换机，交换机将数据转发给目的地，这个过程如下图所示：那么如果网络接口不知道目的地地址呢？这个时候，地址解析协议就开始工作了。发送接口会发送一个广播查询给到交换机，交换机将查询转发给所有接口。如果某个接口发现自己就是对方要查询的接口，则会将自己的 MAC 地址回传。接下来，会在交换机和发送接口的 ARP 表中，增加一个缓存条目。也就是说，接下来发送接口再次向 IP 地址 2.2.2.2 发送数据时，不需要再广播一次查询了。前面提到这个过程和 DNS 非常相似，采用的是逐级缓存的设计减少 ARP 请求。发送接口先查询本地的 ARP 表，如果本地没有数据，然后广播 ARP 查询。这个时候如果交换机中有数据，那么查询交换机的 ARP 表；如果交换机中没有数据，才去广播消息给其他接口。注意，ARP 表是一种缓存，也要考虑缓存的设计。通常缓存的设计要考虑缓存的失效时间、更新策略、数据结构等。比如可以考虑用 TTL（Time To Live）的设计，为每个缓存条目增加一个失效时间。另外，更新策略可以考虑利用老化（Aging）算法模拟 LRU。最后请你思考路由器和交换机的异同点。不知道你有没有在网上订购过家用无线路由器，通常这种家用设备也会提供局域网，具备交换机的能力。同时，这种设备又具有路由器的能力。所以，很多同学可能会分不清路由器和交换机。总的来说，家用的路由器，也具备交换机的功能。但是当 ARP 表很大的时候，就需要专门的、能够承载大量网络接口的交换设备。就好比，如果用数组实现 ARP 表，数据量小的时候，遍历即可；但如果数据量大的话，就需要设计更高效的查询结构和设计缓存。详细的缓存设计原理的介绍，可以参考《重学操作系统》专栏中关于 CPU 缓存的设计，以及 MMU 中 TLB 的设计的内容，分别在以下 3 讲： [05 存储器分级：L1 Cache 比内存和 SSD 快多少倍？](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=478#/detail/pc?id=4610&amp;amp;fileGuid=xxQTRXtVcqtHK6j8) [25 内存管理单元： 什么情况下使用大内存分页？](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=478#/detail/pc?id=4634&amp;amp;fileGuid=xxQTRXtVcqtHK6j8) [26 缓存置换算法： LRU 用什么数据结构实现更合理？](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=478#/detail/pc?id=4635&amp;amp;fileGuid=xxQTRXtVcqtHK6j8) 连接内网有时候，公司内部有多个子网。这个时候一个子网如果要访问另一个子网，就需要通过路由器。也就是说，图中的路由器，其实充当了两个子网通信的桥梁。在上述过程中，发送接口不能直接通过 MAC 地址发送数据到接收接口，因为子网 1 的交换机不知道子网 2 的接口。这个时候，发送接口需要通过 IP 协议，将数据发送到路由器，再由路由器转发信息到子网 2 的交换机。这里提一个问题，子网 2 的交换机如何根据 IP 地址找到接收接口呢？答案是通过查询 ARP 表。连接外网（网络地址转换技术，NAT）最后我们讨论下连接外网的问题。IPv4 协议因为存在网络地址耗尽的问题，不能为一个公司提供足够的地址，因此内网 IP 可能会和外网重复。比如内网 IP 地址192.168.0.1发送信息给22.22.22.22，这个时候，其实是跨着网络的。跨网络必然会通过多次路由，最终将消息转发到目的地。但是这里存在一个问题，寻找的目标 IP 地址22.22.22.22是一个公网 IP，可以通过正常的寻址 + 路由算法定位。当22.22.22.22寻找192.168.0.1的时候，是寻找一个私网 IP，这个时候是找不到的。解决方案就是网络地址转换技术（Network Address Translation）。NAT 技术转换的是 IP 地址，私有 IP 通过 NAT 转换为公网 IP 发送到服务器。服务器的响应，通过 NAT 转换为私有 IP，返回给客户端。通过这种方式，就解决了内网和外网的通信问题。总结总结一下，链路层发送数据靠的是 MAC 地址，MAC 地址就好像人的身份证一样。局域网中，数据不可能从一个终端直达另一个终端，而是必须经过交换机交换。交换机也叫作链路层交换机，它的工作就是不断接收数据，然后转发数据。通常意义上，交换机不具有路由功能，路由器往往具有交换功能。但是往往路由器交换的效率，不如交换机。已知 IP 地址，找到 MAC 地址的协议，叫作地址解析协议（ARP）。网络和网络的衔接，必须有路由器（或者等价的设备）。一个网络的设备不能直接发送链路层分组给另一个网络的设备，而是需要通过 IP 协议让路由器转发。那么，通过这一讲的学习，你可以来回答本讲关联的面试题目：网络地址转换协议是如何工作的？【解析】网络地址解析协议（NAT）解决的是内外网通信的问题。NAT 通常发生在内网和外网衔接的路由器中，由路由器中的 NAT 模块提供网络地址转换能力。从设计上看，NAT 最核心的能力，就是能够将内网中某个 IP 地址映射到外网 IP，然后再把数据发送给外网的服务器。当服务器返回数据的时候，NAT 又能够准确地判断外网服务器的数据返回给哪个内网 IP。你可以思考下 NAT 是如何做到这点的呢？需要做两件事。 NAT 需要作为一个中间层替换 IP 地址。 发送的时候，NAT 替换源 IP 地址（也就是将内网 IP 替换为出口 IP）；接收的时候，NAT 替换目标 IP 地址（也就是将出口 IP 替换回内网 IP 地址）。 NAT 需要缓存内网 IP 地址和出口 IP 地址 + 端口的对应关系。也就是说，发送的时候，NAT 要为每个替换的内网 IP 地址分配不同的端口，确保出口 IP 地址+ 端口的唯一性，这样当服务器返回数据的时候，就可以根据出口 IP 地址 + 端口找到内网 IP。思考题最后再给你提一道需要查资料的思考题：IPv6 协议还需要 NAT 吗？我建议你拿出几分钟的时间去查一下资料，然后把答案整理在留言区，我们一起讨论。如果你对本次课程有什么建议和疑问，可以在评论区留言。如果你有所收获，也可以推荐给你的朋友。 这一讲就到这里。发现求知的乐趣，我是林䭽。感谢你学习本次课程，下一讲我们将学习“09 TCP 实战：如何进行 TCP 抓包调试？”再见！ " }, { "title": "UDP 协议：TCP 协议和 UDP 协议的优势和劣势", "url": "/posts/udp-05/", "categories": "Internal Power, Network", "tags": "UDP", "date": "2018-07-05 12:32:00 +0000", "snippet": "TCP 和 UDP 是今天应用最广泛的传输层协议，拥有最核心的垄断地位。 TCP 最核心的价值是提供了可靠性； UDP 最核心的价值是灵活，几乎可以用它来做任何事情。例如：HTTP 协议 1.1 和 2.0 都基于 TCP，而到 HTTP 3.0 就开始用 UDP 了。如果打开 TCP 协议的 RFC文档，可以看到目录中一共有 85 页；如果你打开 UDP 的 RFC 文档，会看到目录中只有 3 页。一个只有 3 页的协议，能够成为今天最主流的传输层协议之一，那么它一定有非常值得学习的地方。UDP 在数据传输、网络控制、音视频、Web 技术中，都有很重要的地位，因此它也是面试常考的内容。设计系统时候，UDP 经常拿来和 TCP 比较。UDP 协议UDP（User Datagram Protocol），目标是在传输层提供直接发送报文（Datagram）的能力。Datagram 是数据传输的最小单位。UDP 协议不会帮助拆分数据，它的目标只有一个，就是发送报文。有细心的同学可能会问： 为什么不直接调用 IP 协议呢？ 如果裸发数据，IP 协议不香吗？这是因为传输层协议在承接上方应用层的调用，需要提供应用到应用的通信——因此要附上端口号。每个端口，代表不同的应用。传输层下层的 IP 协议，承接传输层的调用，将数据从主机传输到主机。IP 层不能区分应用，导致哪怕是在 IP 协议上进行简单封装，也需要单独一个协议。这就构成了 UDP 协议的市场空间。UDP 的封包格式UDP 的设计目标就是在允许用户直接发送报文的情况下，最大限度地简化应用的设计。下图是 UDP 的报文格式。UDP 的报文非常简化，只有 5 个部分： Source Port 是源端口号。因为 UDP 协议的特性（不需要 ACK），因此这个字段是可以省略的。但有时候对于防火墙、代理来说，Source Port 有很重要的意义，它们需要用这个字段行过滤和路由； Destination Port 是目标端口号（这个字段不可以省略）； Length 是消息体长度； Checksum 是校验和，作用是检查封包是否出错； Data octets 就是一个字节一个字节的数据，Octet 是 8 位。校验和（Checksum）机制，这个机制在很多的网络协议中都会存在，因为校验数据在传输过程中有没有丢失、损坏是一个普遍需求。在一次网络会话中，传输的内容可能是：“你好！”，但事实上传输的是 01 组成的二进制。思考这样一个算法，把数据分成一个一个 byte，然后将所有 byte 相加，再将最终的结果取反。比如现在数据有 4 个 byte：a,b,c,d，那么一种最简单的校验和就是：checksum = (a+b+c+d) ^ 0xffUDP 与 TCP的区别目的差异这两个协议的目的不同： TCP 协议的核心目标是：提供可靠的网络传输； UDP 协议的核心目标是：在提供报文交换能力基础上，尽可能地简化协议轻装上阵。可靠性差异 TCP 核心是要在保证可靠性提供更好的服务。TCP 会有握手的过程，需要建立连接，保证双方同时在线。而且TCP 有时间窗口持续收集无序的数据，直到这一批数据都可以合理地排序组成连续的结果； UDP 并不具备以上这些特性，它只管发送数据封包，而且 UDP 不需要 ACK，这意味着消息发送出去成功与否 UDP 是不管的。连接 vs 无连接 TCP 是一个面向连接的协议（Connection-oriented Protocol），传输数据必须先建立连接； UDP 是一个无连接协议（Connection-less Protocol），数据随时都可以发送，只提供发送封包（Datagram）的能力。流控技术（Flow Control） TCP 使用了流控技术来确保发送方不会因为一次发送过多的数据包而使接收方不堪重负，TCP 在发送缓冲区中存储数据，并在接收缓冲区中接收数据。当应用程序准备就绪时，它将从接收缓冲区读取数据。如果接收缓冲区已满，接收方将无法处理更多数据，并将其丢弃； UDP 没有提供类似的能力。传输速度 UDP 协议简化，封包小，没有连接、可靠性检查等，因此单纯从传输速度上讲，UDP 更快。场景差异TCP 每个数据封包都需要确认，因此天然不适应高速数据传输场景，比如观看视频（流媒体应用）、网络游戏（TCP 有延迟）等。具体来说，如果网络游戏用 TCP，每个封包都需要确认，可能会造成一定的延迟；再比如音、视频传输天生就允许一定的丢包率；Ping 和 DNSLookup，这类型的操作只需要一次简单的请求/返回，不需要建立连接，用 UDP 就足够了。近些年有一个趋势，TCP/UDP 的边界逐渐变得模糊，UDP 应用越来越多。比如传输文件，如果考虑希望文件无损到达，可以用 TCP。如果考虑希望传输足够块，就可能会用 UDP。再比如 HTTP 协议，如果考虑请求/返回的可靠性，用 TCP 比较合适。但是像 HTTP 3.0 这类应用层协议，从功能性上思考，暂时没有找到太多的优化点，但是想要把网络优化到极致，就会用 UDP 作为底层技术，然后在 UDP 基础上解决可靠性。所以理论上，任何一个用 TCP 协议构造的成熟应用层协议，都可以用 UDP 重构。这就好比，本来用一个工具可以解决所有问题，但是如果某一类问题体量非常大，就会专门为这类问题创造工具。因此，UDP 非常适合需要定制工具的场景。场景分成三类： TCP 应用场景： 远程控制（SSH） File Transfer Protocol（FTP） 邮件（SMTP、IMAP）等 点对点文件传出（微信等） UDP 应用场景； 网络游戏 音视频传输 DNS Ping 直播 模糊地带（TCP、UDP 都可以考虑）： HTTP（目前以 TCP 为主） 文件传输 总之，UDP 不提供可靠性，不代表不能解决可靠性。UDP 的核心价值是灵活、轻量，构造了最小版本的传输层协议。在这个之上，还可以实现连接（Connection），实现会话（Session），实现可靠性（Reliability）……总结协议对于我们来说是非常重要的，协议的制定让所有参与者一致、有序地工作。学习协议的设计，对你的工作非常有帮助。比如： 学习 TCP 协议可以培养你思维的缜密性——序号的设计、滑动窗口的设计、快速重发的设计、内在状态机的设计，都是非常精妙的想法； 学习 UDP 协议可以带动我们反思自己的技术架构，有时候简单的工具更受欢迎。Linux 下每个工具都是那么简单、专注，容易理解。相比 TCP 协议，UDP 更容易理解。从程序架构上来说，今天我们更倾向于简单专注的设计，我们更期望有解决报文传输的工具、有解决可靠性的工具、有解决流量控制的工具、有解决连接和会话的工具……我相信这应该是未来的趋势——由大量优质的工具逐渐取代历史上沉淀下来完整统一的系统。从这个角度，我希望通过学习传输层的知识，能够帮助你重新审视自己的系统设计，看看自己还有哪些进步的空间。TCP 协议和 UDP 协议的优势和劣势【解析】TCP 最核心的价值就是提供封装好的一套解决可靠性的优秀方案。 可以看到解决可靠性是非常复杂的，要考虑非常多的因素。TCP 在确保吞吐量、延迟、丢包率的基础上，保证可靠性。历史上 TCP 也是靠可靠性起家的，有一次著名的实验，TCP 协议的设计者做了一次演示——利用 TCP 协议将数据在卫星和地面之间传播了很多次，没有发生任何数据损坏。从那个时候开始，研发人员开始大量选择 TCP 协议。然后随着生态的发展，逐渐提供了流控等能力。TCP 的成功在于它给人们提供了很多现成、好用的能力。UDP 则不同，UDP 提供了最小版的实现，只支持 Checksum。UDP 最核心的价值是灵活、轻量、传输速度快。考虑到不同应用的特性，如果不使用一个大而全的方案，为自己的应用特性量身定做，可能会做得更好。比如网络游戏中游戏客户端不断向服务端发送玩家的位置，如果某一次消息丢失了，只要这个消息不影响最终的游戏结果，就可以只看下一个消息。不同应用有不同的特性，需要的可靠性级别不一样，这就是越来越多的应用开始使用 UDP 的原因之一。其实对于我们来说，TCP 协议和 UDP 协议根本不存在什么优势和劣势，只不过是场景不同，选择不同而已。最后还有一个非常重要的考虑因素就是成本，如果没有足够专业的团队解决网络问题，TCP 无疑会是更好的选择。Moba 类游戏的网络应该用 TCP 还是 UDP所有在线联机游戏都有件非常重要的事情需要完成，就是确定事件发生的唯一性，这个性质和聊天工具是类似的。听我这么说，是不是有点迷？请听我慢慢道来。你在王者荣耀中控制后羿释放技能，这是一个事件。同时，王昭君放了大招，这是第二个事件。两个事件一定要有先后顺序吗？答案是当然要有。因为游戏在同一时刻只能有一个状态。类比一下，多个线程同时操作内存，发生了竞争条件（具体分析可以参见《重学操作系统》专栏关于“线程”的内容），那么是不是意味着，内存在同一时刻有两个状态呢？当然不是，内存同时刻只能有一个状态，所以多个线程的操作必须有先有后。回到 Moba 游戏的问题，每个事件，游戏服务器必须给一个唯一的时序编号，对应后羿的技能和王昭君的技能。所以，在线竞技类游戏，事实上是玩家在不断向服务器竞争一个自增序列号的过程。无论客户端发生怎样的行为，只有竞争到自增 ID 才能进步。也就是说，服务器要尽快响应多个客户端提交的事件，并以最快的速度分配自增序号，然后返回给客户端。所以，Moba 服务端的核心是自增序号的计算和尽量缩减延迟。从这个角度出发，你再来看看，应该用 TCP 协议还是 UDP 协议呢？虽然TCP 协议有 3 次握手，但是连接上之后，双方就不会再有额外的传输成本，因此创建连接的成本，可以忽略不计。同时，TCP 协议还提供稳定性支持，不需要自己实现稳定性。如果规模较小的在线竞技类游戏，TCP 完全适用。但是当游戏玩家体量上升后，TCP 协议的头部（数据封包）较大，会增加服务器额外的 I/O 压力。要发送更多的数据，自然有更大的 I/O 压力。从这个角度来看，UDP 就有了用武之地。经验： 当你既要保证 FIFO，又要提供多处理的数据结构时，可以想到滑动窗口； 当你设计请求/响应模型的时，可以想到多路复用； 当你为自己的应用选择协议时，可以想到实现可靠性最基本的思路。" }, { "title": "TCP 的稳定性：滑动窗口和流速控制", "url": "/posts/tcp-stability-04/", "categories": "Internal Power, Network", "tags": "UDP", "date": "2018-07-04 14:32:00 +0000", "snippet": "TCP 利用发送字节数和接收字节数，这个二元组的唯一性保证顺序。本文研究下保证顺序的具体算法，以及如何在保证顺序的基础上，同时追求更高的吞吐量。我认为，这部分知识也是 TCP 协议中最有趣的部分 —— TCP 的滑动窗口算法。TCP 作为一个传输层协议，最核心的能力是传输。传输需要保证可靠性，还需要控制流速，这两个核心能力均由滑动窗口提供。而滑动窗口中解决的问题，是在今后的工作中可以长期使用的，比如设计一个分布式的 RPC 框架、实现一个消息队列或者分布式的文件系统等。请求/响应模型TCP 中每个发送的请求都需要响应。如果一个请求没有收到响应，发送方就会认为这次发送出现了故障，会触发重发。大体的模型，与下图很像。但是如果完全和下图一样，每一个请求收到响应之后，再发送下一个请求，吞吐量会很低。因为这样的设计，会产生网络的空闲时间，说白了，就是浪费带宽。带宽没有用满，意味着可以同时发送更多的请求，接收更多的响应。一种改进的方式，就是让发送方有请求就发送出去，而不是等待响应。通过这样的处理方式，发送的数据连在了一起，响应的数据也连在了一起，吞吐量就提升了。但是如果可以同时发送的数据真的非常多呢？比如成百上千个 TCP 段都需要发送，这个时候带宽可能会不足。像下图这样，很多个数据封包都需要发送，该如何处理呢？排队（Queuing）在这种情况下，通常会考虑排队（Queuing）机制。考虑这样一个模型，如上图所示，在 TCP 层实现一个队列。新元素从队列的一端（左侧）排队，作为一个未发送的数据封包。开始发送的数据封包，从队列的右侧离开。缺点这样做就需要多个队列，我们要将未发送的数据从队列中取出，加入发送中的队列。然后再将发送中的数据，收到 ACK 的部分取出，放入已接收的队列。而发送中的封包，何时收到 ACK 是一件不确定的事情，这样使用队列似乎也有一定的问题。滑动窗口（Sliding Window）在上面的模型当中，之所以觉得算法不好设计，是因为用错了数据结构。有个说法叫作如果程序写复杂了，那就是写错了。这里其实应该用一种叫作滑动窗口的数据结构去实现。如上图所示： 浅绿色代表发送了，但是没有收到 ACK 的段； 白色代表没有发送的段； 紫色代表暂时不能发送的段下面重新设计一下不同类型封包的顺序，将已发送的数据放到最左边，发送中的数据放到中间，未发送的数据放到右边。假设最多同时发送 5 个封包，也就是窗口大小 = 5。窗口中的数据被同时发送出去，然后等待 ACK。如果一个封包 ACK 到达，就将它标记为已接收（深绿色）。如下图所示，有两个封包的 ACK 到达，因此标记为绿色。这个时候滑动窗口可以向右滑动，如下图所示：重传如果发送过程中，部分数据没能收到 ACK 会怎样呢？这就可能发生重传。如果发生下图这样的情况，段 4 迟迟没有收到 ACK。这个时候滑动窗口只能右移一个位置，如下图所示：在这个过程中，如果后来段 4 重传成功（接收到 ACK），那么窗口就会继续右移。如果段 4 发送失败，还是没能收到 ACK，那么接收方也会抛弃段 5、段 6、段 7。这样从段 4 开始之后的数据都需要重发。快速重传在 TCP 协议中，如果接收方想丢弃某个段，可以选择不发 ACK。发送端超时后，会重发这个 TCP 段。而有时候，接收方希望催促发送方尽快补发某个 TCP 段，这个时候可以使用快速重传能力。例如段 1、段 2、段 4 到了，但是段 3 没有到。 接收方可以发送多次段 3 的 ACK。如果发送方收到多个段 3 的 ACK，就会重发段 3。这个机制称为快速重传。这和超时重发不同，是一种催促的机制。为了不让发送方误以为段 3 已经收到了，在快速重传的情况下，接收方即便收到发来的段 4，依然会发段 3 的 ACK（不发段 4 的 ACK），直到发送方把段 3 重传。思考：窗口大小的单位是？窗口大小的单位是多少呢？在上面所有的图片中，窗口大小是 TCP 段的数量。实际操作中，每个 TCP 段的大小不同，限制数量会让接收方的缓冲区不好操作，因此实际操作中窗口大小单位是字节数。流速控制发送、接收窗口的大小可以用来控制 TCP 协议的流速。窗口越大，同时可以发送、接收的数据就越多，支持的吞吐量也就越大。当然，窗口越大，如果数据发生错误，损失也就越大，因为需要重传越多的数据。举个例子：用 RTT 表示 Round Trip Time，就是消息一去一回的时间。假设 RTT = 1ms，带宽是 1mb/s。如果窗口大小为 1kb，那么 1ms 可以发送一个 1kb 的数据（含 TCP 头），1s 就可以发送 1mb 的数据，刚好可以将带宽用满。如果 RTT 再慢一些，比如 RTT = 10ms，那么这样的设计就只能用完 1/10 的带宽。 当然你可以提高窗口大小提高吞吐量，但是实际的模型会比这个复杂，因为还存在重传、快速重传、丢包等因素。而实际操作中，也不可以真的把带宽用完，所以会使用折中的方案，在延迟、丢包率、吞吐量中进行选择，毕竟鱼和熊掌不可兼得。总结为了提高传输速率，TCP 协议选择将多个段同时发送，为了让这些段不至于被接收方拒绝服务，在发送前，双方要协商好发送的速率。但是不可能完全确定网速，所以协商的方式，就变成确定窗口大小。有了窗口，发送方利用滑动窗口算法发送消息；接收方构造缓冲区接收消息，并给发送方 ACK。滑动窗口的实现只需要数组和少量的指针即可，是一个非常高效的算法。像这种算法，简单又实用，比如求一个数组中最大的连续 k 项和，就可以使用滑动窗口算法。如果你对这个问题感兴趣，不妨用你最熟悉的语言尝试解决一下。滑动窗口和流速控制是怎么回事： 滑动窗口是 TCP 协议控制可靠性的核心。发送方将数据拆包，变成多个分组。然后将数据放入一个拥有滑动窗口的数组，依次发出，仍然遵循先入先出（FIFO）的顺序，但是窗口中的分组会一次性发送。窗口中序号最小的分组如果收到 ACK，窗口就会发生滑动；如果最小序号的分组长时间没有收到 ACK，就会触发整个窗口的数据重新发送。 另一方面，在多次传输中，网络的平均延迟往往是相对固定的，这样 TCP 协议可以通过双方协商窗口大小控制流速。补充下，上面我们说的分组和 TCP 段是一个意思。 既然发送方有窗口，那么接收方也需要有窗口吗： 接收方收到发送方的每个数据分组（或者称为 TCP Segment），接收方肯定需要缓存。举例来说，如果发送方发送了：1, 2, 3, 4。 那么接收方可能收到的一种情况是：1，4，3。注意，没有收到 2 的原因可能是延迟、丢包等。这个时候，接收方有两种选择。 选择一：什么都不做（这样分组 2 的 ACK 就不会发送给发送方，发送方发现没有收到 2 的 ACK，过一段时间就有可能重发 2,3,4,5）。 当然具体设计还需要探讨，比如不重发整个分组，只重发已发送没有收到 ACK 的分组。这种方法的缺陷是性能太差，重发了整个分组（或部分）。因此我们可以考虑另一种选择。 选择二：如果重发一个窗口，或部分窗口，问题就不会太大了。虽然增加了网络开销，但是毕竟有进步（1 进步了，不会再重发）。 性能方面最大的开销是等待超时的时间，就是发送方要等到超时时间才重发窗口，这样操作性能太差。因此，TCP 协议有一个快速重传的机制——接收方发现接收到了 1，但是没有接收到 2，那么马上发送 3 个分组 2 的 ACK 给到发送方，这样发送方收到多个 ACK，就知道接收方没有收到 2，于是马上重发 2。 无论是上面哪种方案，接收方也维护一个滑动窗口，是一个不错的选择。接收窗口的状态，可以和发送窗口的状态相互对应了。 " }, { "title": "TCP 的封包格式：TCP 为什么要粘包和拆包", "url": "/posts/tcp-packet-03/", "categories": "Internal Power, Network", "tags": "UDP", "date": "2018-07-03 12:32:00 +0000", "snippet": "从稳定性角度深挖 TCP 协议的运作机制。如今，大半个互联网都建立在 TCP 协议之上，使用的 HTTP 协议、消息队列、存储、缓存，都需要用到 TCP 协议—— 这是因为 TCP 协议提供了可靠性。简单来说，可靠性就是让数据无损送达。但若是考虑到成本，就会变得非常复杂 —— 因为还需要尽可能地提升吞吐量、降低延迟、减少丢包率。TCP 协议具有很强的实用性，而可靠性又是 TCP 最核心的能力，所以理所当然成为面试的香饽饽。具体来说，从一个终端有序地发出多个数据包，经过一个复杂的网络环境，到达目的地的时候会变得无序，而可靠性要求数据恢复到原始的顺序。不禁有两个问题： TCP 协议是如何恢复数据的顺序的？ 拆包和粘包的作用是什么？TCP 的拆包和粘包TCP 是一个传输层协议。TCP 发送数据的时候，往往不会将数据一次性发送，像下图这样：而是将数据拆分成很多个部分，然后再逐个发送。像下图这样：同样的，在目的地，TCP 协议又需要逐个接收数据。TCP 为什么不一次发送完所有的数据？比如要传一个大小为 10M 的文件，对于应用层而言，就是一次传送完成的。而传输层的协议为什么不选择将这个文件一次发送完呢？这里有很多原因，比如为了稳定性，一次发送的数据越多，出错的概率越大。再比如说为了效率，网络中有时候存在着并行的路径，拆分数据包就能更好地利用这些并行的路径。再有，比如发送和接收数据的时候，都存在着缓冲区。如下图所示：缓冲区是在内存中开辟的一块区域，目的是缓冲。因为大量的应用频繁地通过网卡收发数据，这个时候，网卡只能一个一个处理应用的请求。当网卡忙不过来的时候，数据就需要排队，也就是将数据放入缓冲区。如果每个应用都随意发送很大的数据，可能导致其他应用实时性遭到破坏。比如内存的最小分配单位是页表，如果数据的大小超过一个页表，可能会存在页面置换问题，造成性能的损失。总之，方方面面的原因：在传输层封包不能太大。这种限制，往往是以缓冲区大小为单位的。也就是 TCP 协议，会将数据拆分成不超过缓冲区大小的一个个部分。每个部分有一个独特的名词，叫作 TCP 段（TCP Segment）。在接收数据的时候，一个个 TCP 段又被重组成原来的数据。像这样，数据经过拆分，然后传输，然后在目的地重组，俗称拆包。所以拆包是将数据拆分成多个 TCP 段传输。那么粘包是什么呢？有时候，如果发往一个目的地的多个数据太小了，为了防止多次发送占用资源，TCP 协议有可能将它们合并成一个 TCP 段发送，在目的地再还原成多个数据，这个过程俗称粘包。所以粘包是将多个数据合并成一个 TCP 段发送。TCP Segment下图是一个 TCP 段的格式：可以看到，TCP 的很多配置选项和数据粘在了一起，作为一个 TCP 段。显然，把每一部分都记住似乎不太现实，只好把其中最主要的部分理解。TCP 协议就是依靠每一个 TCP 段工作的，所以你每认识一个 TCP 的能力，几乎都会找到在 TCP Segment 中与之对应的字段。 Source Port/Destination Port ，描述的是发送端口号和目标端口号，代表发送数据的应用程序和接收数据的应用程序。比如 80 往往代表 HTTP 服务，22 往往是 SSH 服务…… Sequence Number 和 Achnowledgment Number， 保证可靠性的两个关键； Data Offset，偏移量，这存在的原因是 TCP Header 部分的长度是可变的，因此需要一个数值来描述数据从哪个字节开始。 Reserved，保留的一个区域，用于日后扩展能力。 URG/ACK/PSH/RST/SYN/FIN， 几个标志位，用于描述 TCP 段的行为，也就是一个 TCP 封包到底是做什么用的 URG，代表这是一个紧急数据，比如远程操作的时候，用户按下了 Ctrl+C，要求终止程序，这种请求需要紧急处理； ACK，代表响应，所有的消息都必须有 ACK，这是 TCP 协议确保稳定性的一环； PSH，代表数据推送，也就是在传输数据的意思； SYN，同步请求，也就是申请握手； FIN，终止请求，也就是挥手； 特别的，这 5 个标志位，每个占了一个比特，可以混合使用。 比如 ACK 和 SYN 同时为 1，代表同步请求和响应被合并了。这也是 TCP 协议，为什么是三次握手的原因之一。 Window 也是 TCP 保证稳定性并进行流量控制的工具； Checksum 是校验和，用于校验 TCP 段有没有损坏。 Urgent Pointer 指向最后一个紧急数据的序号（Sequence Number）。它存在的原因是：有时候紧急数据是连续的很多个段，所以需要提前告诉接收方进行准备。 Options 中存储了一些可选字段，比如接下来我们要讨论的 MSS（Maximun Segment Size）。 Padding 存在的意义是因为 Options 的长度不固定，需要 Pading 进行对齐。 Sequence Number 和 Acknowledgement Number在 TCP 协议的设计当中，数据被拆分成很多个部分，部分增加了协议头。合并成为一个 TCP 段，进行传输。这个过程，俗称拆包。这些 TCP 段经过复杂的网络结构，由底层的 IP 协议，负责传输到目的地，然后再进行重组。稳定性要求数据无损地传输，也就是说拆包获得数据，又需要恢复到原来的样子。而在复杂的网络环境当中，即便所有的段是顺序发出的，也不能保证它们顺序到达，因此，发出的每一个 TCP 段都需要有序号。这个序号，就是 Sequence Number（Seq）。如上图所示。发送数据的时候，为每一个 TCP 段分配一个自增的 Sequence Number。接收数据的时候，虽然得到的是乱序的 TCP 段，但是可以通过 Seq 进行排序。但是这样又会产生一个新的问题——接收方如果要回复发送方，也需要这个 Seq。而网络的两个终端，去同步一个自增的序号是非常困难的。因为任何两个网络主体间，时间都不能做到完全同步，又没有公共的存储空间，无法共享数据，更别说实现一个分布式的自增序号了。其实这个问题的本质就好像两个人在说话一样，要确保他们说出去的话，和回答之间的顺序。因为 TCP 是一个双工的协议，两边可能会同时说话。所以聪明的科学家想到了确定一句话的顺序，需要两个值去描述——也就是发送的字节数和接收的字节数。重新定义一下 Seq（如上图所示），对于任何一个接收方，如果知道了发送者发送某个 TCP 段时，已经发送了多少字节的数据，那么就可以确定发送者发送数据的顺序。但是这里有一个问题。如果接收方也向发送者发送了数据请求（或者说双方在对话），接收方就不知道发送者发送的数据到底对应哪一条自己发送的数据了。举个例子：下面 A 和 B 的对话中，可以确定他们彼此之间接收数据的顺序。但是无法确定数据之间的关联关系，所以只有 Sequence Number 是不够的。A: 今天天气好吗？A: 今天你开心吗？B: 开心B：天气不好人类很容易理解这几句话的顺序，但是对于机器来说就需要特别的标注。因此还需要另一个数据，就是每个 TCP 段发送时，发送方已经接收了多少数据。用 Acknowledgement Number 表示，下面简写为 ACK。下图中，终端发送了三条数据，并且接收到四条数据，通过观察，根据接收到的数据中的 Seq 和 ACK，将发送和接收的数据进行排序。例如上图中，发送方发送了 100 字节的数据，而接收到的（Seq = 0 和 Seq =100）的两个封包，都是针对发送方（Seq = 0）这个封包的。发送 100 个字节，所以接收到的 ACK 刚好是 100。说明（Seq= 0 和 Seq= 100）这两个封包是针对接收到第 100 个字节数据后，发送回来的。这样就确定了整体的顺序。注意，无论 Seq 还是 ACK，都是针对“对方”而言的。是对方发送的数据和对方接收到的数据。在实际的工作当中，可以通过 Whireshark 调试工具观察两个 TCP 连接的 Seq和 ACK。MSS（Maximun Segment Size）MSS，也是面试经常会问到的一个 TCP Header 中的可选项（Options），这个可选项控制了 TCP 段的大小，它是一个协商字段（Negotiate）。协议是双方都要遵循的标准，因此配置往往不能由单方决定，需要双方协商。TCP 段的大小（MSS）涉及发送、接收缓冲区的大小设置，双方实际发送接收封包的大小，对拆包和粘包的过程有指导作用，因此需要双方去协商。如果这个字段设置得非常大，就会带来一些影响： 首先对方可能会拒绝，作为服务的提供方，你可能不会愿意接收太大的 TCP 段。因为大的 TCP 段，会降低性能，比如内存使用的性能。具体你可以参考《重学操作系统》课程中关于页表的讨论。 还有就是资源的占用。一个用户占用服务器太多的资源，意味着其他的用户就需要等待或者降低他们的服务质量。 其次，支持 TCP 协议工作的 IP 协议，工作效率会下降。TCP 协议不肯拆包，IP 协议就需要拆出大量的包。那么 IP 协议为什么需要拆包呢？这是因为在网络中，每次能够传输的数据不可能太大，这受限于具体的网络传输设备，也就是物理特性。但是 IP 协议，拆分太多的封包并没有意义。因为可能会导致属于同个 TCP 段的封包被不同的网络路线传输，这会加大延迟。同时，拆包，还需要消耗硬件和计算资源。当然，也不是 MSS 越小越好： MSS 太小的情况下，会浪费传输资源（降低吞吐量）。因为数据被拆分之后，每一份数据都要增加一个头部。如果 MSS 太小，那头部的数据占比会上升，这让吞吐量成为一个灾难。所以在使用的过程当中，MSS 的配置，往往都是一个折中的方案。根据 Unix 的哲学，不要去猜想什么样的方案是最合理的，而是要尝试去用实验证明它，一切都要用实验依据说话。总结TCP 协议的设计像一台巨大而严密的机器，每次我重新温习 TCP 协议，都会感叹“它庞大，而且很琐碎”。每一个细节的设计，都有很深的思考。比如 Sequence Number 和 Acknowledge Number 的设计，就非常巧妙地利用发送字节数和接收字节数解决了顺序的问题。TCP 协议是如何恢复数据的顺序的: TCP 利用（发送字节数、接收字节数）的唯一性来确定封包之间的顺序关系。TCP 拆包和粘包的作用是什么： TCP 拆包的作用是将任务拆分处理，降低整体任务出错的概率，以及减小底层网络处理的压力。拆包过程需要保证数据经过网络的传输，又能恢复到原始的顺序。这中间，需要数学提供保证顺序的理论依据。 粘包是为了防止数据量过小，导致大量的传输，而将多个 TCP 段合并成一个发送。 不需要死记 Seq，Ack的计算步骤。 核心是要理解一个顺序问题，需要理解的核心问题是，TCP 如何排序的 —— 用累计值确认。 有了这个理解，计算过程是可以推导的。思考这样一个问题，你和小明聊天， 你会发现，你发出的消息，你可以确定顺序。因为用了聊天工具，接收到消息的时间不确定，你需要理解内容才能确认小明具体那句话回复的是你的哪句话。 那么如果是机器，如何确定小明哪句话是针对哪句话回复呢？那就需要小明的回复带上(Seq, Ack)，看到小明的Ack，就知道小明是收到多少自己发送的消息后才进行的回复。通过ACK，你知道小明回复的是你的第1条？第2条？第3条？……还是第k条信息。ACK是小明累计收到的信息数。 如果你要针对小明的回复（Seq, ACK, 内容）进行回复，你就把ACK设置成小明的 Seq。" }, { "title": "传输层协议 TCP：TCP 为什么握手是 3 次、挥手是 4 次？", "url": "/posts/tcp-02/", "categories": "Internal Power, Network", "tags": "UDP", "date": "2018-07-02 12:32:00 +0000", "snippet": "TCP 和 UDP 是今天应用最广泛的传输层协议，拥有最核心的垄断地位。今天互联网的整个传输层，几乎都是基于这两个协议打造的。无论是应用开发、框架设计选型、做底层和优化，还是定位线上问题，只要碰到网络，就逃不开 TCP 协议相关的知识。在面试中 TCP 一直是一个高频考察内容，外加 TCP 关联的知识比较多，因此面试题五花八门。其中一道高频面试题：TCP 协议为什么握手是 3 次，挥手却是 4 次？TCP 协议TCP（Transport Control Protocol）： 一个传输层协议； 提供 Host-To-Host 数据的可靠传输；‘ 支持全双工； 是一个连接导向的协议。这里面牵涉很多概念，比如主机到主机、连接、会话、双工/单工及可靠性等。主机到主机（Host-To-Host）TCP 提供的是 Host-To-Host 传输，一台主机通过 TCP 发送数据给另一台主机。这里的主机（Host）是一个抽象的概念，可以是手机、平板、手表等。收发数据的设备都是主机，所以双方是平等的。TCP 协议往上是应用到应用（Application-To-Application）的协议。比如用微信发信息给张三，微信客户端、微信聊天服务都是应用。微信有自己的聊天协议，微信的聊天协议是应用到应用的协议；如果微信的聊天协议想要工作，就需要一个主机到主机的协议帮助它实现通信。而 TCP 上层有太多的应用，不仅仅有微信，还有原神、抖音、网易云音乐……因此 TCP 上层的应用层协议使用 TCP 能力的时候，需要告知 TCP 是哪个应用——这就是端口号。端口号用于区分应用，下文中我们还会详细讨论。TCP 要实现主机到主机通信，就需要知道主机们的网络地址（IP 地址），但是 TCP 不负责实际地址到地址（Address-To-Address）的传输，因此 TCP 协议把 IP 地址给底层的互联网层处理。互联网层，也叫网络层（Network Layer），提供地址到地址的通信，IP 协议就在这一层工作。互联网层解决地址到地址的通信，但是不负责信号在具体两个设备间传递。因此，网络层会调用下方的链路层在两个相邻设备间传递信息。当信号在两个设备间传递的时候，科学家又设计出了物理层封装最底层的物理设备、传输介质等，由最下方的物理层提供最底层的传输能力。以上的 5 层架构，称为互联网协议群，也称作 TCP/IP 协议群。总之，主机到主机（Host-To-Host）为应用提供应用间通信的能力。连接和会话连接（Connection） 数据传输双方（通信双方）的契约（约定）； 目标是让两个在通信的程序之间产生一个默契，保证两个程序都在线，而且尽快地响应对方的请求； 设计上，连接是一种传输数据的行为。传输之前，建立一个连接。具体来说，数据收发双方的内存中都建立一个用于维护数据传输状态的对象，比如双方 IP 和端口是多少？现在发送了多少数据了？状态健康吗？传输速度如何等。所以，连接是网络行为状态的记录。会话（Session） 和连接关联的一个名词，叫作会话（Session）； 会话是应用的行为。比如微信里张三和你聊天，那么张三和你建立一个会话。你要和张三聊天，你们创建一个聊天窗口，这个就是会话。你开始 Typing，开始传输数据，你和微信服务器间建立一个连接。如果你们聊一段时间，各自休息了，约定先不要关微信，1 个小时后再回来。那么连接会断开，因为聊天窗口没关，所以会话还在； 在有些系统设计中，会话会自动重连（也就是重新创建连接），或者帮助创建连接； 会话也负责在多次连接中保存状态，比如 HTTP Session 在多次 HTTP 请求（连接）间保持状态（如用户信息）。总结下，会话是应用层的概念，连接是传输层的概念。双工/单工问题 如果在任何一个时刻，如果数据只能单向发送，就是单工，所以单工需要至少一条线路； 如果在某个时刻数据可以向一个方向传输，也可以向另一个方向反方向传输，而且交替进行，叫作半双工，半双工需要至少 1 条线路； 如果任何时刻数据都可以双向收发，就是全双工，全双工需要大于 1 条线路。当然这里的线路，是一个抽象概念，你可以并发地处理信号，达到模拟双工的目的。TCP 是一个双工协议，数据任何时候都可以双向传输。这就意味着客户端和服务端可以平等地发送、接收信息。正因为如此，客户端和服务端在 TCP 协议中有一个平等的名词——Host（主机）。可靠性 可靠性指数据保证无损传输。 如果发送方按照顺序发送，然后数据无序地在网络间传递，就必须有一种算法在接收方将数据恢复原有的顺序。 另外，如果发送方同时要把消息发送给多个接收方，这种情况叫作多播，可靠性要求每个接收方都无损收到相同的副本。 多播情况还有强可靠性，就是如果有一个消息到达任何一个接收者，那么所有接受者都必须收到这个消息。TCP 的握手和挥手TCP 是一个连接导向的协议，设计有建立连接（握手）和断开连接（挥手）的过程。TCP 没有设计会话（Session），因为会话通常是一个应用的行为。TCP 协议的基本操作TCP 协议有这样几个基本操作： 如果一个 Host 主动向另一个 Host 发起连接，称为 SYN（Synchronization），请求同步； 如果一个 Host 主动断开请求，称为 FIN（Finish），请求完成； 如果一个 Host 给另一个 Host 发送数据，称为 PSH（Push），数据推送。以上 3 种情况，接收方收到数据后，都需要给发送方一个 ACK（Acknowledgement）响应。请求/响应的模型是可靠性的要求，如果一个请求没有响应，发送方可能会认为自己需要重发这个请求。建立连接的过程（三次握手）由于要保持连接和可靠性约束，TCP 协议要保证每一条发出的数据必须给返回，返回数据叫作 ACK（也就是响应）。按照这个思路，建立连接是需要 3 次握手： 客户端发消息给服务端（SYN）； 服务端准备好进行连接； 服务端针对客户端的 SYN 给一个 ACK有个疑惑，到这里不就可以了吗？2 次握手就足够了。其实不是，因为服务端还没有确定客户端是否准备好了。比如步骤 3 之后，服务端马上给客户端发送数据，这个时候客户端可能还没有准备好接收数据。因此还需要增加一个过程。接下来还会发生以下操作： 服务端发送一个 SYN 给客户端； 客户端准备就绪； 客户端给服务端发送一个 ACK；上面不是 6 个步骤吗？ 怎么是 3 次握手呢？其中缘由如下： 步骤 1 是 1 次握手； 步骤 2 是服务端的准备，不是数据传输，因此不算握手； 步骤 3 和步骤 4，因为是同时发生的，可以合并成一个 SYN-ACK 响应，作为一条数据传递给客户端，因此是第 2 次握手； 步骤 5 不算握手； 步骤 6 是第 3 次握手。过程如下图所示：SYN、ACK、PSH 这些常见的标识位（Flag）在传输中如何表示： 一种思路是为 TCP 协议增加协议头。在协议头中取多个位（bit），其中 SYN、ACK、PSH 都占有 1 个位。比如 SYN 位，1 表示 SYN 开启，0 表示关闭。因此，SYN-ACK 就是 SYN 位和 ACK 位都置 1。这种设计，称为标识（Flag）。标识位是放在 TCP 头部的。断开连接的过程（4 次挥手）继续上面的思路，断开连接需要几次握手？可以在脑海中这样构思： 客户端要求断开连接，发送一个断开的请求，这个叫作（FIN）； 服务端收到请求，然后给客户端一个 ACK，作为 FIN 的响应； 需要思考一个问题，可不可以像握手那样马上传 FIN 回去？ 这个时候服务端不能马上传 FIN，因为断开连接要处理的问题比较多，比如： 服务端可能还有发送出去的消息没有得到 ACK； 也有可能服务端自己有资源要释放。 因此断开连接不能像握手那样操作——将两条消息合并。所以，服务端经过一个等待，确定可以关闭连接了，再发一条 FIN 给客户端。 客户端收到服务端的 FIN，同时客户端也可能有自己的事情需要处理完，比如客户端有发送给服务端没有收到 ACK 的请求，客户端自己处理完成后，再给服务端发送一个 ACK。 经过以上分析，就可以回答上面这个问题了。刚刚好 4 次挥手？过程如下图所示：总结 TCP 提供连接（Connection），让双方的传输更加稳定、安全； TCP 没有直接提供会话，因为应用对会话的需求多种多样，比如聊天程序会话在保持双方的聊天记录，电商程序会话在保持购物车、订单一致，所以会话通常在 TCP 连接上进一步封装，在应用层提供； TCP 是一个面向连接的协议（Connection -oriented Protocol），说的就是 TCP 协议参与的双方（Host）在收发数据之前会先建立连接（UDP 协议，UDP 是一个面向报文（Datagram-oriented）的协议——协议双方不需要建立连接，直接传送报文/数据)。 最后，连接需要消耗更多的资源。比如说，在传输数据前，必须先协商建立连接。因此，不是每种场景都应该用连接导向的协议。像视频播放的场景，如果使用连接导向的协议，服务端每向客户端推送一帧视频，客户端都要给服务端一次响应，这是不合理的。TCP 为什么是 3 次握手，4 次挥手？ TCP 是一个双工协议，为了让双方都保证，建立连接的时候，连接双方都需要向对方发送 SYC（同步请求）和 ACK（响应）； 握手阶段双方都没有烦琐的工作，因此一方向另一方发起同步（SYN）之后，另一方可以将自己的 ACK 和 SYN 打包作为一条消息回复，因此是 3 次握手——需要 3 次数据传输。 到了挥手阶段，双方都可能有未完成的工作。收到挥手请求的一方，必须马上响应（ACK），表示接收到了挥手请求。 类比现实世界中，收到一个 Offer，出于礼貌你先回复考虑一下，然后思考一段时间再回复 HR 最后的结果。最后等所有工作结束，再发送请求中断连接（FIN），因此是 4 次挥手。 一台内存在 8G 左右的服务器，可以同时维护多少个连接？ 连接是内存中的状态对象，从理论上分析，连接本身不太占用内存。不同语言连接对象大小不等，但是通常很小。下面我提供一段 Java 程序，你可以感受一下： public class Server {    public static void main(String[] argv) throws IOException {        var serverSocket = new ServerSocket();        var addr = new InetSocketAddress(3001);        serverSocket.bind(addr);        var list = new LinkedList&amp;lt;&amp;gt;();        while(true) {            var client = serverSocket.accept();            list.add(client);            System.out.println(list.size());        }    }}public class Client {    public static void main(String[] argv) throws IOException, InterruptedException {        var clients = new LinkedList&amp;lt;&amp;gt;();        for(int i = 0; i &amp;lt; 1000000; i++) {            var client = new Socket(&quot;127.0.0.1&quot;, 3001);            clients.add(client);        }        Thread.sleep(10000000);    }} 通过运行上面这段程序，可以观察到以下这几个现象： 创建 100W 连接速度不是很快，这说明 TCP 连接创建有成本（3 次握手，都是网络 IO）； 用jps找到对应的进程的id，在用sudo cat /proc/{进程ID}/status | grep VmHWM可以看到实际的内存占用。按照这种增长趋势，8G 内存空间可以轻轻松松存放 100W 个连接。 但是如果单机建立太多的连接，会报一个Cannot assign requested address的异常，这是因为客户端连接服务端时，操作系统要为每个客户端分配一个端口，上面的程序很快会把端口号用尽。 所以，核心的问题是，通信需要缓冲区，通信需要 I/O。这是因为通信占用资源，连接本身占用资源少。 tcp 连接数上限其实受限于机器的内存，以8G内存为例，假设一个tcp连接需要占用的最小内存是8k（发送接收缓存各4k，当然还要考虑socket描述符），那么最大连接数为：8*1024*1024/8=1048576个，即约 100 万个 tcp 长连接。不过这只是理论数值，并未考虑实际业务。" }, { "title": "漫游互联网：什么是蜂窝移动网络", "url": "/posts/Honeycomb-mobile-network-01/", "categories": "Internal Power, Network", "tags": "UDP", "date": "2018-07-01 13:12:00 +0000", "snippet": "从 DataReportal 2021 年 1 月的统计数据来看，全球 78 亿人口中，有 52 亿手机用户，46 亿互联网用户。能够接入网络的设备越来越多，体量越来越大，这样一个庞大的世界是如何被构造出来的？思科（Cisco，世界 500 强通信设备提供商）在一篇报告中曾指出，2016 年年底全球 IP 流量超过 1 个 Zettabyte，也就是 1021 个字节，相当于一万亿 GB。如此庞大的流量体系，是何种结构去承接？网络的组成大家习惯称今天的时代为云时代，整个世界可以看作一张巨大的、立体的网。在这个时代里产生的各种服务，就好像水和电一样，打开即用。透过这张巨大的网去观察，里面还会有一个个小型的网络。可以想象，用无数个节点构成一个个小型网络，再用小型网络组成中型网络，再组成大型网络，以此类推，最后组成完整的一个如星河般的世界。公司内网如果仔细分析一个小型网络，比如一个公司网络，就会得到下图 1 所示的结构：公司网络从本地网络服务提供商 （Internet Service Provider） 接入，然后内部再分成一个个子网。上图 1 中，看到的线路，被称作通信链路（Communication Link），用于传输网络信号。可以观察到，有的网络节点，同时接入了 2 条以上的链路，这个时候因为路径发生了分叉，数据传输到这些节点需要选择方向，因此这些节点需要进行交换（Switch）。数据发生交换的时候，会先从一条链路进入交换设备，然后缓存下来，再转发（切换）到另一条路径，如下图 2 所示：交换技术的本质，就是让数据切换路径。因为，网络中的数据是以分组或封包（Packet）的形式传输，因此这个技术也称作封包交换技术（Packet Switch）。比如说，传递一首 2Mb 的 MP3 的歌曲，歌曲本身不是一次传输完成的，而是被拆分成很多个封包。每个封包只有歌曲中的一部分数据，而封包一旦遇到岔路口，就需要封包交换技术帮助每个封包选择最合理的路径。在网络中，常见的具有交换能力的设备是路由器（Router）和链路层交换机（Link-Layer Switch）。通常情况下，两个网络可以通过路由器进行连接，多台设备可以通过交换机进行连接。但是路由器通常也具有交换机的功能。在上图 1 中，公司内部网络也被分成了多级子网。每个路由器、交换机构成一级子网。最高级的路由器在公司网络的边缘，它可以将网络内部节点连接到其他的网络（网络外部）。本地网络提供商（ISP）提供的互联网先到达边缘的路由器，然后再渗透到内部的网络节点。公司内部的若干服务器可以通过交换机形成一个局域网络；公司内部的办公设备，比如电脑和笔记本，也可以通过无线路由器或者交换机形成局域网络。局域网络之间，可以通过路由器、交换机进行连接，从而构成一个更大的局域网。移动网络网络传输需要通信链路（Communication Link），而通信链路是一个抽象概念。这里说的抽象，就是面向对象中抽象类和继承类的关系，比如同轴电缆是通信链路，无线信号的发送接收器可以构成通信链路，蓝牙信道也可以构成通信链路。在移动网络中，无线信号构成了通信链路。在移动网络的设计中，通信的核心被称作蜂窝塔（Cellular Tower），有时候也称作基站（BaseStation）。之所以有这样的名称，是因为每个蜂窝塔只覆盖一个六边形的范围，如果要覆盖一个很大的区域就需要很多的蜂窝塔（六边形）排列在一起，像极了蜜蜂的巢穴。这种六边形的结构，可以让信号无死角地覆盖。想象一下，如果是圆形结构，那么圆和圆之间就会有间隙，造成一部分无法覆盖的信号死角，而六边形就完美地解决了这个问题。对于构成移动网络最小的网络结构——蜂窝网络来说，构造大体如图 4 所示：图 4 中，国家或全球网络提供商将网络供给处于蜂窝网络边缘的路由器，路由器连接蜂窝塔，再通过蜂窝塔（基站）提供给处于六边形地区中的设备。通常是国家级别的网络服务提供商负责部署基站，比如中国电信、中国联通。将网络提供给一个子网的行为，通常称为网络提供（Network Provider），反过来，对一个子网连接提供商的网络，称为网络接入（Network Access）。随着移动网络的发展，一个蜂窝网格中的设备越来越多，也出现了基站覆盖有重叠关系的网格，如下图 5 所示：这样设计的好处是，当一个基站过载、出现故障，或者用户设备周边信号出现不稳定，就可以切换到另一个基站的网络，不影响用户继续使用网络服务。另一方面，在一定范围内的区域，离用户较近的地方还可以部署服务器，帮助用户完成计算。这相当于计算资源的下沉，称为边缘计算。相比中心化的计算，边缘计算延迟低、链路短，能够将更好的体验带给距离边缘计算集群最近的节点。从而让用户享受到更优质、延迟更低、算力更强的服务。家用网络近些年，家用联网设备越来越多。比如说冰箱、空调、扫地机器人、灯光、电动窗帘……如上图 7 所示，家用网络现在已经发展成一种网格状的连接： 一方面通过路由器接入本地 ISP 提供的网络服务； 另一方面，一些设备，比如电脑、笔记本、手机、冰箱等都可以直接和路由器连接。路由器也承担了链路层网关的作用，作为家用电器之间信息的交换设备。还有一些家用设备，比如说 10 多块钱的灯泡，不太适合内部再嵌入一个几十块钱可以接收 WI-FI 的芯片，这个时候就可以考虑用蓝牙控制电灯。路由器提供蓝牙不现实，因此一些家用电器也承担了蓝牙设备的控制器——比如说智能音箱。上图 7 中的智能音箱把家用网络带向了一个网格状，有的设备会同时连接路由器（WI-FI）和智能音箱，这样手机和音箱都可以直接控制这些设备。这样的设计，即便网络断开，仍然可以控制这些家用设备。整体关系公司网络、移动网络和家用网络的整体关系如下图 8 所示：最顶部的全球或国家大型的 ISP 之间联网，构成了网络的主干。然后区域性的 ISP 承接主干网络，在这个基础之上再向家庭和公司提供接入服务。移动蜂窝网络因为部署复杂，往往也是由大型 ISP 直接提供。数据的传输上述的网络结构中，由庞大数目的个人、公司、组织、基站，形成一个个网络。在这些网络中，传递数据不是一件容易的事情。为了传递数据，在网络中有几个特别重要的抽象： 终端（Terminal），或者端系统（End System），有时候简单称为主机（Host），最终提供服务或者享受服比如说：电脑、手机、冰箱、汽车等都可以看作是一个主机（Host）。 网络传输分成两类： 一类是端到端（Host-to-Host）的能力，由 TCP/IP 协议群提供； 一类是广播的能力，是一对多、多对多的能力，可以看作是端到端（Host-to-Host）能力的延伸。 思考一下，一个北京的主机（Host）向一个深圳的主机（Host）发送消息。那么，中间会穿越大量的网络节点，这些节点可以是路由器、交换机、基站等。 在发送消息的过程中，会跨越很多网络、通过很多边缘，也可能会通过不同的网络提供商提供的网络……而且，传输过程中，可能会使用不同材质的通信链路（Communication Link），比如同轴电缆、双绞线、光纤，或者通过无线传输的 WI-FI、卫星等。 封包（Packet）。网络基础设施往往不能一次性传输太大的数据量，因此通常会将数据分片传输。比如传输一个 MP3，会将 MP3 内容切分成很多个组，每个组也称作一个封包。如果一个封包损坏，只需要重发损坏的封包，而不需要重发所有数据。可以类比下中文的活字印刷技术。 封包交换技术，另一方面，网络中两点间的路径非常多，如果一条路径阻塞了，部分封包可以考虑走其他路径。发送端将数据拆分成封包（Packet），封包在网络中遇到岔路，由交换器和路由器节点决定走向。图 9 中是对封包交换技术的一个演示。 总结互联网是一个非常庞大的结构，从整体来看，互联网是一个立体的、庞大的网状结构。如果将它放大、再放大，将镜头拉近，在微观层面，会看到一个个网络、一台台设备，还会看到大量的封包在交换、有设备在不断地改变封包的走向、损坏的封包被重发、一个个光电信号被转化和传输。这个过程看似复杂，但任何一个局部的设计都在井然有序地运行着。每次想要深入研究互联网的设计时，都不禁再次感叹它的浩瀚——如今你看到的网络，是经过几个时代的发展沉淀下来的“宝藏”。不是某个人、某个团队就可以设计出来。所以从这个角度，可以把计算机网络看作是优秀的分层设计、精密的模块组装、准确的数值运算等一系列设计思想、工程方法的集合。因此，我认为互联网就是学校软件架构一个最好的参照。有的光猫有交换功能，还带wifi…所以现在多合一设备比较多。但本质上，光猫是调制解调器，实现光和数字信号间转换。封包交换 是假设一首 mp3 歌曲 传输过程中 在网络畅通情况下 各个包永远走一条路径传输吗 还是只在拥堵的时候 会选择其他路径传输 - 这个是整个网络在做的事情，看路由策略。 只能说可能出现同时分两条路径发送的情况。" }, { "title": "使用 JmsTemplate 集成 ActiveMQ", "url": "/posts/activeMQ-template/", "categories": "Spring", "tags": "SpringBoot, JmsTemplate, ActiveMQ", "date": "2018-05-24 09:32:00 +0000", "snippet": "基于 JmsTemplate 模板工具类为 spring-css 添加对应的消息通信机制。JMS 规范与 ActiveMQJMS（Java Messaging Service是一种 Java 消费服务，它基于消息传递语义，提供了一整套经过抽象的公共 API。目前，业界存在一批 JMS 规范的实现框架，其中具备代表性的就是 ActiveMQ。JMS 规范JMS 规范提供了一批核心接口供开发人员，而这些接口构成了客户端的 API 体系，如下图所示：上图中可以看到，可以通过 ConnectionFactory 创建 Connection，作为客户端的 MessageProducer 和 MessageConsumer 通过 Connection 提供的会话（Session）与服务器进行交互，而交互的媒介就是各种经过封装、包含目标地址（Destination）的消息。JMS 的消息由两大部分组成，即消息头（Header）和消息体（Payload）。消息体只包含具体的业务数据；消息头包含了 JMS 规范定义的通用属性，比如消息的唯一标识 MessageId、目标地址 Destination、接收消息的时间 Timestamp、有效期 Expiration、优先级 Priority、持久化模式 DeliveryMode **等都是常见的通用属性，这些通用属性构成了消息通信的基础元数据（Meta Data）**，由消息通信系统默认设置。JMS 规范中的点对点模型表现为队列（Queue），队列为消息通信提供了一对一顺序发送和消费的机制。点对点模型 API 在通用 API 基础上，专门区分生产者 QueueSender **和消费者 QueueReceiver**。而 Topic 是 JMS 规范中对发布-订阅模型的抽象，JMS 同样提供了专门的 TopicPublisher 和 TopicSubscriber。对于 Topic 而言，因多个消费者存在同时消费一条消息的情况，所以消息有副本的概念。相较点对点模型，发布-订阅模型通常用于更新、事件、通知等非响应式请求场景。在这些场景中，消费者和生产者之间是透明的，消费者可以通过配置文件进行静态管理，也可以在运行过程中动态被创建，同时还支持取消订阅操作。ActiveMQJMS 规范存在 ActiveMQ、WMQ、TIBCO 等多种第三方实现方式，其中较主流的是 ActiveMQ。针对 ActiveMQ，目前有两个实现项目可供选择： 一个是经典的 5.x 版本； 另一个是下一代的 Artemis。关于这两者之间的关系，可以简单地认为 Artemis 是 ActiveMQ 的未来版本，代表 ActiveMQ 的发展趋势。因此，我使用 Artemis 练习消息通信机制。如果想启动 Artemis 服务，首先需要通过如下所示的命名创建一个服务实例：artemis.cmd create D:\\happymaya --user spring-css --passwordspring-css-password然后，执行如下命令，就可以正常启动这个 Artemis 服务实例了。D:\\happymaya \\bin\\artemis runSpring 提供了对 JMS 规范及各种实现的友好集成，通过直接配置 Queue 或 Topic，就可以使用 JmsTemplate 提供的各种方法简化对 Artemis 的操作了。使用 JmsTemplate 集成 ActiveMQ如果想基于 Artemis 使用 JmsTemplate，首先需要在 Spring Boot 应用程序中添加对 spring-boot-starter-artemis 的依赖，如下代码所示：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-artemis&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;在讨论如何使用 JmsTemplate 实现消息发送和消费之前，先来分析消息生产者和消费者的工作模式。消息生产者和消费者的工作模式通常，生产者行为模式单一，而消费者根据消费方式的不同有一些特定的分类，比如常见的有推送型消费者（Push Consumer）和拉取型消费者（Pull Consumer）： 推送型方式，指的是应用系统向消费者对象注册一个 Listener 接口并通过回调 Listener 接口方法实现消息消费； 拉取方式，应用系统通常主动调用消费者的拉取消息方法消费消息，主动权由应用系统控制。在消息通信的两种基本模型中: 发布-订阅模型，支持生产者/消费者之间的一对多关系，属于一种典型的推送消费者实现机制； 点对点模型，其中有且仅有一个消费者，它们主要通过基于间隔性拉取的轮询（Polling）方式进行消息消费。Kafka 中消费消息的方式是一种典型的推送型消费者，所以 KafkaTemplate 只提供了发送消息的方法而没有提供实现消费消息的方法。而 JmsTemplate 则不同，它同时支持推送型消费和拉取型消费。使用 JmsTemplate 发送消息JmsTemplate 中存在一批 send 方法用来实现消息发送，如下代码所示：public class JmsTemplate extends JmsDestinationAccessor implements JmsOperations { ...... public void send(MessageCreator messageCreator) throws JmsException { Destination defaultDestination = this.getDefaultDestination(); if (defaultDestination != null) { this.send(defaultDestination, messageCreator); } else { this.send(this.getRequiredDefaultDestinationName(), messageCreator); } } public void send(Destination destination, MessageCreator messageCreator) throws JmsException { this.execute((session) -&amp;gt; { this.doSend(session, destination, messageCreator); return null; }, false); } public void send(String destinationName, MessageCreator messageCreator) throws JmsException { this.execute((session) -&amp;gt; { Destination destination = this.resolveDestinationName(session, destinationName); this.doSend(session, destination, messageCreator); return null; }, false); } ......}这些 send 方法： 一方面指定了目标 Destination； 另一方面提供了一个用于创建消息对象的MessageCreator接口，如下代码所示： @FunctionalInterfacepublic interface MessageCreator { Message createMessage(Session var1) throws JMSException;} 通过 send 方法发送消息的典型实现方式如下代码所示：public void sendDemoObject(DemoObject demoObject) { jmsTemplate.send(&quot;demo.queue&quot;, new MessageCreator() { @Override public Message createMessage(Session session) throws JMSException { return session.createObjectMessage(demoObject); } });}与 KakfaTemplate 不同，JmsTemplate 还提供了一组更为简便的方法实现消息发送，即 convertAndSend 方法，如下代码所示：public void convertAndSend(Object message) throws JmsException { Destination defaultDestination = this.getDefaultDestination(); if (defaultDestination != null) { this.convertAndSend(defaultDestination, message); } else { this.convertAndSend(this.getRequiredDefaultDestinationName(), message); }}public void convertAndSend(Destination destination, Object message) throws JmsException { this.send(destination, (session) -&amp;gt; { return this.getRequiredMessageConverter().toMessage(message, session); });}public void convertAndSend(String destinationName, Object message) throws JmsException { this.send(destinationName, (session) -&amp;gt; { return this.getRequiredMessageConverter().toMessage(message, session); });}public void convertAndSend(Object message, MessagePostProcessor postProcessor) throws JmsException { Destination defaultDestination = this.getDefaultDestination(); if (defaultDestination != null) { this.convertAndSend(defaultDestination, message, postProcessor); } else { this.convertAndSend(this.getRequiredDefaultDestinationName(), message, postProcessor); }}public void convertAndSend(Destination destination, Object message, MessagePostProcessor postProcessor) throws JmsException { this.send(destination, (session) -&amp;gt; { Message msg = this.getRequiredMessageConverter().toMessage(message, session); return postProcessor.postProcessMessage(msg); });}public void convertAndSend(String destinationName, Object message, MessagePostProcessor postProcessor) throws JmsException { this.send(destinationName, (session) -&amp;gt; { Message msg = this.getRequiredMessageConverter().toMessage(message, session); return postProcessor.postProcessMessage(msg); });}通过 convertAndSend 方法，可以直接传入任意业务对象，且该方法能自动将业务对象转换为消息对象并进行消息发送，具体的示例代码如下所示：public void sendDemoObject(DemoObject demoObject) { jmsTemplate.convertAndSend(&quot;demo.queue&quot;, demoObject); }在以上代码中，注意到 convertAndSend 方法还存在一批重载方法，它包含了消息后处理功能。上述方法中的 MessagePostProcessor 就是一种消息后处理器，它用来在构建消息过程中添加自定义的消息属性，它的一种典型的使用方法如下代码所示：public void sendDemoObject(DemoObject demoObject) { jmsTemplate.convertAndSend(&quot;demo.queue&quot;, demoObject, new MessageCreator() { @Override public Message createMessage(Session session) throws JMSException { // 针对 Message 的处理 return message; } });}使用 JmsTemplate 的最后一步就是在配置文件中添加配置项，如下代码所示：spring: artemis: host: localhost port: 61616 user: springcss password: springcss_password embedded: enabled: false这里指定了 artemis 服务器的地址、端口、用户名和密码等信息。同时，也可以在配置文件中指定 Destination 信息，具体配置方式如下代码所示：spring: jms: template: default-destination: springcss.account.queue使用 JmsTemplate 消费消息基于前面，知道 JmsTemplate 同时支持推送型消费和拉取型消费两种消费类型。我们先来看一下如何实现拉取型消费模式。实现拉取型消费模式在 JmsTemplate 中提供了一批 receive 方法供我们从 artemis 中拉取消息，如下代码所示：@Nullablepublic Message receive() throws JmsException { Destination defaultDestination = this.getDefaultDestination(); return defaultDestination != null ? this.receive(defaultDestination) : this.receive(this.getRequiredDefaultDestinationName());}@Nullablepublic Message receive(Destination destination) throws JmsException { return this.receiveSelected((Destination)destination, (String)null);}@Nullablepublic Message receive(String destinationName) throws JmsException { return this.receiveSelected((String)destinationName, (String)null);}到这一步需要注意一点：调用上述方法时，当前线程会发生阻塞，直到一条新消息的到来。针对阻塞场景，这时 receive 方法的使用方式如下代码所示：public DemoEvent receiveEvent() { Message message = jmsTemplate.receive(&quot;demo.queue&quot;); return (DemoEvent)messageConverter.fromMessage(message);}这里使用了一个 messageConverter 对象将消息转化为领域对象。在使用 JmsTemplate 时，可以使用 Spring 提供的 MappingJackson2MessageConverter、MarshallingMessageConverter、MessagingMessageConverter，以及 SimpleMessageConverter 实现消息转换。一般系统默认使用 SimpleMessageConverter。而在日常开发过程中，通常会使用 MappingJackson2MessageConverter 来完成 JSON 字符串与对象之间的转换。同时，JmsTemplate 还提供了一组更为高阶的 receiveAndConvert 方法完成消息的接收和转换，如下代码所示：public Object receiveAndConvert(Destination destination) throws JmsException {}顾名思义，receiveAndConvert 方法能在接收消息后完成对消息对象的自动转换，使得接收消息的代码更为简单，如下代码所示：public DemoEvent receiveEvent() { return (DemoEvent)jmsTemplate.receiveAndConvert(&quot;demo.queue&quot;); }当然，在消费者端，同样需要指定与发送者端完全一致的 MessageConverter 和 Destination 来分别实现消息转换和设置消息目的地。推模式下的消息消费方法，实现方法也很简单，如下代码所示：@JmsListener(queues = “demo.queue”)public void handlerEvent(DemoEvent event) {    //TODO：添加消息处理逻辑}在推模式下，只需要在 @JmsListener 注解中指定目标队列，就能自动接收来自该队列的消息。在 SpringCSS 案例中集成 ActiveMQ实现 account-service 消息生产者在 Spring Boot 中，可以通过 Profile 有效管理针对不同场景和环境的配置信息。而在 SpringCSS 案例中，Kafka、ActiveMQ 及 16 讲将要介绍的 RabbitMQ 都是消息中间件，在案例系统运行过程中，我们需要选择其中一种中间件演示消息发送和接收到过程，这样我们就需要针对不同的中间件设置不同的 Profile 了。在 account-service 中，我们可以根据 Profile 构建如下所示的配置文件体系：从以上图中可以看到，根据三种不同的中间件，分别提供了三个配置文件。以其中的 application-activemq.yml 为例，其包含的配置项如下代码所示：spring: jms: template: default-destination: springcss.account.queue artemis: host: localhost port: 61616 user: springcss password: springcss_password embedded: enabled: false在主配置文件 application.yml 中，只需要将 Profile 设置为 activemq 即可。实现消息发送的 ActiveMQAccountChangedPublisher 类，如下代码所示：@Component(&quot;activeMQAccountChangedPublisher&quot;)public class ActiveMQAccountChangedPublisher extends AbstractAccountChangedPublisher { @Autowired private JmsTemplate jmsTemplate; @Override protected void publishEvent(AccountChangedEvent event) { jmsTemplate.convertAndSend(AccountChannels.SPRINGCSS_ACCOUNT_QUEUE, event, this::addEventSource); } private Message addEventSource(Message message) throws JMSException { message.setStringProperty(&quot;EVENT_SYSTEM&quot;, &quot;SpringCSS&quot;); return message; }}以上代码中，基于 JmsTemplate 的 convertAndSend 方法完成了消息的发送。同时，注意到：这里也使用了另一种实现 MessagePostProcessor 的方法，即 lambda 语法（参考这种语法简化代码的组织方式）。另一方面，在练习中，希望使用 MappingJackson2MessageConverter 完成对消息的转换。因此，可以在 account-service 中添加一个 ActiveMQMessagingConfig 初始化具体的 MappingJackson2MessageConverter 对象，如下代码所示：@Configurationpublic class ActiveMQMessagingConfig { @Bean public MappingJackson2MessageConverter activeMQMessageConverter() { MappingJackson2MessageConverter messageConverter = new MappingJackson2MessageConverter(); messageConverter.setTypeIdPropertyName(&quot;_typeId&quot;); Map&amp;lt;String, Class&amp;lt;?&amp;gt;&amp;gt; typeIdMappings = new HashMap&amp;lt;String, Class&amp;lt;?&amp;gt;&amp;gt;(); typeIdMappings.put(&quot;accountChangedEvent&quot;, AccountChangedEvent.class); messageConverter.setTypeIdMappings(typeIdMappings); return messageConverter; }}上述代码的核心作用是定义了 typeId 到 Class 的 Map，这样做的目的在于为消息的转换提供了灵活性。比如可以在 account-service 中发送了一个 Id 为accountChangedEvent且类型为 AccountChangedEvent 的业务对象，而在消费该消息的场景中，只需要指定同一个 Id，对应的消息就可以自动转化为另一种业务对象（不一定是这里使用到 AccountChangedEvent）。实现 customer-service 消息消费者先回到 customer-service 服务，看看如何消费来自 account-service 的消息，如下代码所示：@Component(&quot;activeMQAccountChangedReceiver&quot;)public class ActiveMQAccountChangedReceiver extends AbstractAccountChangedReceiver { @Autowired private JmsTemplate jmsTemplate; @Override protected AccountChangedEvent receiveEvent() { return (AccountChangedEvent) jmsTemplate.receiveAndConvert(AccountChannels.SPRINGCSS_ACCOUNT_QUEUE); }}这里，只是简单通过 JmsTemplate 的 receiveAndConvert 方法拉取来自 ActiveMQ 的消息。请注意，因为 receiveAndConvert 方法的执行过程是阻塞性的拉取行为，所以可以实现一个新的 Controller 专门测试该方法的有效性。如下代码所示：@RestController@RequestMapping(value=&quot;messagereceive&quot;)public class MessageReceiveController { @Autowired private ActiveMQAccountChangedReceiver accountChangedReceiver; @RequestMapping(value = &quot;&quot;, method = RequestMethod.GET) public void receiveAccountChangedEvent() { accountChangedReceiver.receiveAccountChangedEvent();    }}一旦访问了这个端点，系统就会拉取 ActiveMQ 中目前尚未消费的消息。如果 ActiveMQ 没有待消费的消息，这个方法就会阻塞，且一直处于等待状态，直到新消息的到来。如果想使用消息推送的方式来消费消息，实现过程更加简单，如下代码所示：@Override@JmsListener(destination = AccountChannels.SPRINGCSS_ACCOUNT_QUEUE)public void handlerAccountChangedEvent(AccountChangedEvent event) { AccountMessage account = event.getAccountMessage(); String operation = event.getOperation(); System.out.print(accountMessage.getId() + &quot;:&quot; + accountMessage.getAccountCode() + &quot;:&quot; + accountMessage.getAccountName());}以上代码中可以看到，可以直接通过 @JmsListener 注解消费从 ActiveMQ 推送过来的消息。这里只是把消息打印了出来，可以根据实际需要对消息进行任何形式的处理。PS：使用 JmsTemplate 时，如何分别实现基于推模式和拉模式的消息消费？" }, { "title": "使用 Actuator 组件实现系统监控", "url": "/posts/actuator/", "categories": "Spring", "tags": "SpringBoot, Actuator", "date": "2018-05-23 09:32:00 +0000", "snippet": "系统监控是 Spring Boot 中引入的一项全新功能，它对应用程序运行状态的管理非常有效。Spring Boot Actuator 组件主要通过一系列 HTTP 端点提供的系统监控功能来实现系统监控。引入 Spring Boot Actuator 组件在初始化 Spring Boot 系统监控功能之前，首先需要引入 Spring Boot Actuator 组件，具体操作为在 pom 中添加如下所示的 Maven 依赖：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;请注意，引入 Spring Boot Actuator 组件后，并不是所有的端点都对外暴露。例如，启动 customer-service 时，在启动日志中发现如下所示内容：Exposing 2 endpoint(s) beneath base path &#39;/actuator&#39;访问 http://localhost:8080/actuator 端点后，会得到如下所示结果：{     &quot;_links&quot;:{         &quot;self&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator&quot;,             &quot;templated&quot;:false         },         &quot;health-path&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/health/{*path}&quot;,             &quot;templated&quot;:true         },         &quot;health&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/health&quot;,             &quot;templated&quot;:false         },         &quot;info&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/info&quot;,             &quot;templated&quot;:false         }     } }这种结果就是 HATEOAS 风格的 HTTP 响应。如果想看到默认情况下看不到的所有端点，则需要在配置文件中添加如下所示的配置信息。management:  endpoints:    web:      exposure:        include: &quot;*&quot;  重启应用后，就能获取到 Spring Boot Actuator 暴露的所有端点，如下代码所示：{     &quot;_links&quot;:{         &quot;self&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator&quot;,             &quot;templated&quot;:false         },         &quot;beans&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/beans&quot;,             &quot;templated&quot;:false         },         &quot;health&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/health&quot;,             &quot;templated&quot;:false         },         &quot;health-path&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/health/{*path}&quot;,             &quot;templated&quot;:true         },         &quot;info&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/info&quot;,             &quot;templated&quot;:false         },         &quot;conditions&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/conditions&quot;,             &quot;templated&quot;:false         },         &quot;configprops&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/configprops&quot;,             &quot;templated&quot;:false         },         &quot;env&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/env&quot;,             &quot;templated&quot;:false         },         &quot;env-toMatch&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/env/{toMatch}&quot;,             &quot;templated&quot;:true         },         &quot;loggers&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/loggers&quot;,             &quot;templated&quot;:false         },         &quot;loggers-name&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/loggers/{name}&quot;,             &quot;templated&quot;:true         },         &quot;heapdump&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/heapdump&quot;,             &quot;templated&quot;:false         },         &quot;threaddump&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/threaddump&quot;,             &quot;templated&quot;:false         },         &quot;metrics-requiredMetricName&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/metrics/{requiredMetricName}&quot;,             &quot;templated&quot;:true         },         &quot;metrics&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/metrics&quot;,             &quot;templated&quot;:false         },         &quot;scheduledtasks&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/scheduledtasks&quot;,             &quot;templated&quot;:false         },         &quot;mappings&quot;:{             &quot;href&quot;:&quot;http://localhost:8080/actuator/mappings&quot;,             &quot;templated&quot;:false         }     } }根据端点所起到的作用，Spring Boot Actuator 提供的原生端点分为如下三类： 应用配置类： 主要用来获取应用程序中加载的应用配置、环境变量、自动化配置报告等配置类信息，它们与 Spring Boot 应用密切相关； 度量指标类： 主要用来获取应用程序运行过程中用于监控的度量指标，比如内存信息、线程池信息、HTTP 请求统计等； 操作控制类： 在原生端点中只提供了一个关闭应用的端点，即 /shutdown 端点。根据 Spring Boot Actuator 默认提供的端点列表，常见端点的类型、路径和描述梳理在如下表格中：| 类型 | 路径 | 描述 || :——–: | :———: | :———————————————————-: || 应用配置类 | /beans | 该端点用来获取应用程序中所创建的所有JavaBean信息 || 应用配置类 | /env | 该端点用来获取应用程序中所有可用的环境属性，包括环境变量、JVM属性、应用配置信息等 || 应用配置类 | /info | 该端点用来返回一些应用自定义的信息。开发人员可以对其进行扩展，这一讲后续会有详细案例 || 应用配置类 | /mappings | 该端点用来返回所有Controller中RequestMapping所表示的映射信息 || 度量指标类 | /metrics | 该端点用来返回当前应用程序的各类重要度量指标，如内存信息、线程信息、垃圾回收信息等 || 度量指标类 | /threaddump | 该端点用来暴露应用程序运行中的线程信息 || 度量指标类 | /health | 该端点用来获取应用的各类健康指标信息，这些指标信息由HealthIndicator的实现类提供。这一讲后续同样会有扩展HealthIndicator的代码案例 || 度量指标类 | /trace | 该端点用来返回基本的HTTP跟踪信息 || 操作控制类 | /shutdown | 该端点用来关闭应用程序，要求将endpoints.shutdown.enabled设置为True |通过访问上表中的各个端点，就可以获取自己感兴趣的监控信息了。例如访问了http://localhost:8082/actuator/health 端点，就可以得到如下所示的 account-service 基本状态。{ &quot;status&quot;: &quot;UP&quot;}此时，看到这个健康状态信息非常简单。如果想要获取更详细的状态信息呢，只需要在配置文件中添加如下所示的配置项即可：management:   endpoint:    health:      show-details: always上述配置项指定了针对 health 端点需要显示它的详细信息。此外，重启 Spring Boot 应用程序，并重新访问 http://localhost:8082/actuator/health 端点，就可以获取如下所示的详细信息。{     &quot;status&quot;:&quot;UP&quot;,     &quot;components&quot;:{         &quot;diskSpace&quot;:{             &quot;status&quot;:&quot;UP&quot;,             &quot;details&quot;:{                 &quot;total&quot;:201649549312,                 &quot;free&quot;:3434250240,                 &quot;threshold&quot;:10485760             }         },         &quot;ping&quot;:{             &quot;status&quot;:&quot;UP&quot;         }     }}如果 Spring Boot Actuator 默认提供的端点信息不能满足业务需求，可以对其进行修改和扩展。此时，常见实现方案有两种： 一种是扩展现有的监控端点； 另一种是自定义新的监控端点。扩展 Actuator 端点Spring Boot 默认暴露了日常开发中最常见的两个端点：Info 端点和Health 端点。扩展 Info 端点Info 端点用于暴露 Spring Boot 应用的自身信息。在 Spring Boot 内部，它把这部分工作委托给了一系列 InfoContributor 对象，而 Info 端点会暴露所有 InfoContributor 对象所收集的各种信息。在 Spring Boot 中包含了很多自动配置的 InfoContributor 对象，常见的 InfoContributor 及其描述如下表所示：| InfoContributor 名称 | 描述 || ————————– | ———————————————————— || EnvironmentInfoContributor | 暴露 Environment 中 key 为 info 的所有 key || GitInfoContributor | 暴露 git 信息，如果存在 git.properties 文件 || BuildInfoContributor | 暴露构建信息，如果存在 META-lNF/build-info.properties 文件 |以上表中的 EnvironmentInfoContributor 为例，通过在配置文件中添加格式以 info 作为前缀的配置段。就可以定义 Info 端点暴露的数据。添加完成后，将看到所有在 info 配置段下的属性都将被自动暴露。比如可以将如下所示配置信息添加到配置文件 application.yml 中：info: app: encoding: UTF-8 java: source: 1.8.0_31 target: 1.8.0_31现在访问 Info 端点，就能得到如下的 Environment 信息。{ &quot;app&quot;:{ &quot;encoding&quot;:&quot;UTF-8&quot;, &quot;java&quot;: { &quot;source&quot;:&quot;1.8.0_31&quot;, &quot;target&quot;:&quot;1.8.0_31&quot; } }}同时，还可以在服务构建时扩展 Info 属性，而不是硬编码这些值。假设使用 Maven，就可以按照如下所示的配置重写前面的示例并得到同样的效果。info: app: encoding: @project.build.sourceEncoding@ java: source: @java.version@ target: @java.version@很多时候，Spring Boot 自身提供的 Info 端点并不能满足我们的业务需求，这就需要编写一个自定义的 InfoContributor 对象。方法也很简单，直接实现 InfoContributor 接口的 contribute() 方法即可。例如，希望在 Info 端点中暴露该应用的构建时间，就可以采用如下所示的代码进行操作。@Componentpublic class CustomBuildInfoContributor implements InfoContributor { @Override public void contribute(Builder builder) { builder.withDetail(&quot;build&quot;, Collections.singletonMap(&quot;timestamp&quot;, new Date())); }}重新构建应用并访问 Info 端口后，就能获取如下所示信息：{ &quot;app&quot;:{ &quot;encoding&quot;:&quot;UTF-8&quot;, &quot;java&quot;: { &quot;source&quot;:&quot;1.8.0_31&quot;, &quot;target&quot;:&quot;1.8.0_31&quot; } }, &quot;build&quot;: { &quot;timestamp&quot;: 1604307503710 }}这里可以看到，CustomBuildInfoContributor 为 Info 端口新增了时间属性。扩展 Health 端点Health 端点用于检查正在运行的应用程序健康状态，而健康状态信息由 HealthIndicator 对象从 Spring 的 ApplicationContext 中获取。和 Info 端点一样，Spring Boot 内部也提供了一系列 HealthIndicator 对象来实现定制化。在默认情况下，HealthAggregator 会根据 HealthIndicator 的有序列表对每个状态进行排序，从而得到最终的系统状态。常见的 HealthIndicator 如下表所示：| HealthIndicator 名称 | 描述 || :————————–: | :—————————–: || DiskSpaceHealthIndicator | 检查磁盘空间是否足够 || DataSourceHealthIndicator | 检查是否可以获得连接 DataSource || ElasticsearchHealthIndicator | 检查 Elasticsearch 集群是否启动 || JmsHealthlndicator | 检查 JMS 代理是否启动 || MailHealthIndicator | 检查邮件服务器是否启动 || MongoHealthIndicator | 检查 Mongo 数据库是否启动 || RabbitHealthIndicator | 检查 RabbitMQ 服务器是否启动 || RedisHealthIndicator | 检查 Redis 服务器是否启动 || SolrHealthlndicator | 检查 Solr 服务器是否已启动 |Health 端点信息的丰富程度取决于当下应用程序所处的环境，而一个真实的 Health 端点信息如下代码所示：{     &quot;status&quot;:&quot;UP&quot;,     &quot;components&quot;:{         &quot;db&quot;:{             &quot;status&quot;:&quot;UP&quot;,             &quot;details&quot;:{                 &quot;database&quot;:&quot;MySQL&quot;,                 &quot;result&quot;:1,                 &quot;validationQuery&quot;:&quot;/* ping */ SELECT 1&quot;             }         },         &quot;diskSpace&quot;:{             &quot;status&quot;:&quot;UP&quot;,             &quot;details&quot;:{                 &quot;total&quot;:201649549312,                 &quot;free&quot;:3491287040,                 &quot;threshold&quot;:10485760             }         },         &quot;ping&quot;:{             &quot;status&quot;:&quot;UP&quot;         } }}通过以上这些信息，就可以判断该环境中是否包含了 MySQL 数据库。如果还想在 Health 端点中暴露 customer-service 当前运行时状态。为了进一步明确该服务的状态，可以自定义一个 CustomerServiceHealthIndicator 端点专门展示 customer-service 的状态信息，CustomerServiceHealthIndicator 的定义如下所示：@Componentpublic class CustomerServiceHealthIndicator implements HealthIndicator { @Override public Health health() { try { URL url = new URL(&quot;http://localhost:8083/health/&quot;); HttpURLConnection conn = (HttpURLConnection) url.openConnection(); int statusCode = conn.getResponseCode(); if (statusCode &amp;gt;= 200 &amp;amp;&amp;amp; statusCode &amp;lt; 300) { return Health.up().build(); } else { return Health.down().withDetail(&quot;HTTP Status Code&quot;, statusCode).build(); } } catch (IOException e) { return Health.down(e).build(); } }}需要提供 health() 方法的具体实现并返回一个 Health 结果。该 Health 结果应该包括一个状态，并且可以根据需要添加任何细节信息。以上代码中，使用了一种简单且直接的方式判断配置中心服务customerservice是否正在运行。然后构建一个 HTTP 请求，并根据 HTTP 响应得出了健康诊断的结论。如果 HTTP 响应的状态码处于 200~300 之间，认为该服务正在运行，此时，Health.up().build() 方法就会返回一种 Up 响应，如下代码所示：{    &quot;status&quot;: &quot;UP&quot;,    &quot;details&quot;: {        &quot;customerservice&quot;:{            &quot;status&quot;: &quot;UP&quot;        }        …    }}如果状态码不处于这个区间（例如返回 404，代表服务不可用），Health.down().withDetail().build() 方法就会返回一个 Down 响应，并给出具体的状态码，如下代码所示：{    &quot;status&quot;: &quot;DOWN&quot;,    &quot;details&quot;: {        &quot;customerservice&quot;:{            &quot;status&quot;: &quot;DOWN&quot;,            &quot;details&quot;: {                &quot;HTTP Status Code&quot;: &quot;404&quot;            }        },        …    }}如果 HTTP 请求直接抛出了异常，Health.down().build() 方法同样会返回一个 Down 响应，并返回异常信息，效果如下代码所示：{    &quot;status&quot;: &quot;DOWN&quot;,    &quot;details&quot;: {        &quot;customerservice&quot;:{            &quot;status&quot;: &quot;DOWN&quot;,            &quot;details&quot;: {                &quot;error&quot;: &quot;java.net.ConnectException: Connection refused: connect&quot;            }        },        …    }}显然，通过扩展 Health 端点，时监控系统中各个服务的正常运行状态提供了很好的支持，也可以根据需要构建一系列有用的 HealthIndicator 实现类，并添加报警等监控手段。系统监控的一大目标是收集和分析系统运行时的度量指标，并基于这些指标判断当前的运行时状态。 在使用 Spring Boot 时，如何实现自定义的健康监测功能？" }, { "title": "使用 Admin Server 管理 Spring 应用程序", "url": "/posts/admin-server/", "categories": "Spring", "tags": "SpringBoot, Spring Security", "date": "2018-05-22 12:32:00 +0000", "snippet": "基于 Actuator 暴露的各种 HTTP 端点，可以获取系统的运行时状态。而端点是一种底层的监控技术，这就要求对HTTP 协议和 Spring Boot 应用程序的构建方式有一定的了解。还有一个更简单、基于可视化的方式，就是 Spring Boot Admin 组件。引入 Spring Boot Admin 组件Spring Boot Admin 是一个用于监控 Spring Boot 的应用程序，它的基本原理是通过统计、集成 Spring Boot Actuator 中提供的各种 HTTP 端点，从而提供简洁的可视化 WEB UI，如下图所示：从上图中，不难看出，Spring Boot Admin 的整体架构中存在两大角色，即服务器端组件 Admin Server 和客户端组件 Admin Client。其中，Admin Client 实际上是一个普通的 Spring Boot 应用程序，而 Admin Server 则是一个独立服务，需要进行专门构建。构建 Admin Server 的两种实现方式： 一种是简单的基于独立的 Admin 服务； 另一种则相对复杂，需要依赖服务注册中心的服务注册和发现机制。基于独立服务构建 Admin Server无论使用哪种方式实现 Admin Server，首先都需要创建一个 Spring Boot 应用程序，并在 pom 文件中添加如下所示的依赖项：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;de.codecentric&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-admin-server&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;de.codecentric&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-admin-server-ui&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;请注意： Spring Boot Admin 组件并不是 Spring 家族官方提供的组件，而是来自一个 codecentric AG 团队。如果想将普通的 Spring Boot 应用程序转变为 Spring Boot Admin Server，只需要在 Bootstrap 类上添加一个 @EnableAdminServer 注解即可，添加完该注解的 BootStrap 类如下代码所示：@SpringBootApplication@EnableAdminServerpublic class AdminApplication { public static void main(String[] args) { SpringApplication.run(AdminApplication.class, args); }}从图中可以看到，目前还没有一个应用程序与 Admin Server 有关联。如果想将应用程序与 Admin Server 进行关联，还需要对原有的 Spring Boot 应用程序做一定的改造。首先，在 Maven 依赖中引入对 Spring Boot Admin Client 组件的依赖，如下代码所示：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;de.codecentric&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-admin-starter-client&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;然后，在配置文件中添加如下配置信息，以便该应用程序能够与 Admin Server 进行关联。spring: boot: admin: client: url: http://localhost:9000注意：这里的 9000 就是 Admin Server 的服务器端口。现在启动这个应用程序，就会发现 Admin Server 中已经出现了这个应用的名称和地址，如下图所示： 在图中，看到 APPLICATIONS 和 INSTANCES 的数量都是 1，代表 Admin Server 管理着一个应用程序，而该应用程序只有一个运行实例。在界面的下方，我们还能看到这个应用的名称及实例地址。这里你可以尝试使用不同的端口启动应用程序的不同实例，然后观察这个列表的变化。基于注册中心构建 Admin Server虽然基于独立服务构建 Admin Server 和 Admin Client 非常简单，但是需要在每个应用程序中添加对 Spring Boot Admin 的 Maven 依赖，并指定 Admin Server 地址。这实际上是一种代码侵入，意味着应用程序与 Admin Server 之间有一种强耦合。联想到 Admin Server 和 Admin Client 之间需要建立类似服务注册的关联关系，可以认为这是服务注册和发现机制的一种表现形式。在 Spring 家族中，存在一个用于构建微服务架构的 Spring Cloud 框架，而该框架中恰好存在一款专门实现服务注册和发现的组件——服务注册中心 Spring Cloud Netflix Eureka ，且 Spring Boot Admin 内置了与这款注册中心实现工具的无缝集成。基于注册中心，Admin Server 与各个 Admin Client 之间的交互方式如下图所示：使用 Eureka 构建注册中心的过程也很简单，首先我们创建一个独立的 Spring Boot 应用程序，并在 pom 文件中添加如下所示的用于提供 Eureka 服务端功能的 Maven 依赖：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-cloud-starter-netflix-eureka-server&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;引入 Maven 依赖后，我们就可以创建 Spring Boot 的启动类。在示例代码中，我们把该启动类命名为 EurekaServerApplication，如下代码所示：@SpringBootApplication@EnableEurekaServerpublic class EurekaServerApplication { public static void main(String[] args) {        SpringApplication.run(EurekaServerApplication.class, args);    }}注意：在上面的代码中，在启动类上加了一个 @EnableEurekaServer 注解。在 SpringCloud 中，包含 @EnableEurekaServer 注解的服务也就是一个 Eureka 服务器组件。这样，Eureka 服务就构建完毕了。同样，Eureka 服务还为我们提供了一个可视化的 UI 界面，它可以用来观察当前注册到 Eureka 中的应用程序信息，如下图所示：接下来，需要 Admin Server 也做相应调整。首先，我们在 pom 文件中添加一个对 spring-cloud-starter-netflix-eureka-client 这个 Eureka 客户端组件的依赖：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-cloud-starter-netflix-eureka-client&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;这时 Admin Server 相当于 Eureka 的客户端，因此，需要在它的 BootStrap 类上添加 @EnableEurekaClient 注解，以便将 Admin Server 注册到 Eureka 上。重构 Admin Server 的最后一步是调整配置信息，此时需要在配置文件中添加如下所示的配置项来指定 Eureka 服务器地址。eureka:  client:    registerWithEureka: true    fetchRegistry: true    serviceUrl:   defaultZone: http://localhost:8761/eureka/好了，现在 Admin Server 已经重构完毕。引入注册中心的目的是降低 Admin Client 与 Admin Server 之间的耦合度，关于这点从 Maven 依赖上就可以得到印证。有了注册中心后，Admin Client 就不再依赖 spring-boot-admin-starter-client 组件了，而是直接使用如下所示的 Eureka 客户端组件。&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-cloud-starter-netflix-eureka-client&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;在配置文件中，需要去掉对 Admin Server 地址的引用，直接使用 Eureka 服务端地址即可，且无须对 Admin Client 中的 Bootstrap 类做任何修改。通过以上调整，各个 Admin Client 就能通过 Eureka 注册中心完成与 Admin Server 的关联了。使用 Admin Server 监控系统根据 Spring Boot Admin 官方 Github 上的介绍，Admin Server 监控系统提供了一套完整的可视化方案。基于 Admin Server，健康状态、JVM、内存、Micrometer 的度量、线程、HTTP 跟踪等核心功能都可以通过可视化的 UI 界面进行展示。监控系统运行时关键指标注意到 Admin Server 菜单中有一个Wallboard，点击该菜单，就可以看到一面应用墙，如下图所示：点击应用墙中的某个应用，就能进入针对该应用的监控信息主界面。在该界面的左侧，包含了监控功能的各级目录，如下图所示：在图中，我们看到了最重要的“Health”信息，显然，这一信息来自 Spring Boot Actuator 组件的 Health 端点，这里你可以参考《服务监控：如何使用 Actuator 组件实现系统监控？》的内容进行回顾。在这个界面上继续往下滑动，我们将看到一些与 JVM 相关的监控信息，比如非常有用的线程、垃圾回收、内存状态等数据，如下图所示：这些 JVM 数据都是通过可视化的方式进行展现，并随着运行时状态的变化而实时更新。在 Admin Server 中，同样存在一个Metrics菜单，展示效果如下图所示：在Metrics菜单中，开发人员可以通过对各种条件进行筛选，然后添加对应的度量指标。比如上图中，针对 HTTP 请求中 /actuator/health 端点进行了过滤，从而得到了度量结果。接着系统环境方面的属性，因为这方面的属性非常之多，所以 Admin Server 也提供了一个过滤器，如下图所示：在上图中，通过输入spring.参数，就能获取一系列与该参数相关的环境属性。日志也是监控系统的一个重要途径，在 Admin Server 的Loggers菜单中，可以看到该应用程序的所有日志信息，如下图所示：通过springcss关键词对这些日志进行过滤，就可以获取 spring-ces 案例中的日志详细了，图中也显示了每个日志记录器对应的日志级别。最后，看一下 Admin Server 中的JVM菜单，该菜单下存在两个子菜单：Thread Dump和Heap Dump。以Thread Dump为例，尽管 Actuator 提供了 /threaddump 端点，但开发人员只能获取触发该端点时的 Dump 信息，而 Admin Server 则提供了一个连续性的可视化监控界面，如下图所示：点击图中的色条，就可以获取每一个线程的详细信息了，而后尝试做一些分析。控制访问安全性Admin Server 的功能非常强大，而这些功能显然也不应该暴露给所有人。因此，需要控制 Admin Server 的访问安全性。想做到这一点也非常简单，只需要集成 Spring Security 即可。在 Spring Boot 应用程序中添加一个对 spring-boot-starter-security 的 Maven 依赖：&amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;spring-boot-starter-security&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;然后，在配置文件中添加如下配置项：spring:  security:    user:      name: &quot;springcss_admin&quot;      password: &quot;springcss_password&quot;重启 Admin Server 后，再次访问 Web 界面时，就需要我们输入用户名和密码了，如下图所示： 在使用 Spring Boot Admin 组件时，构建 Admin Server 有哪两种方法 ？" }, { "title": "JDBC 关系型数据库访问规范", "url": "/posts/base-rules-of-jdbc/", "categories": "Blog, Notes", "tags": "JDBC", "date": "2018-05-20 09:32:00 +0000", "snippet": "JDBC 关系型数据库访问规范无论是互联网应用还是传统软件，对于任何一个系统而言，数据的存储和访问都是不可缺少的。数据访问层的构建可能会涉及多种不同形式的数据存储媒介，本课程关注的是最基础也是最常用的数据存储媒介，即关系型数据库，针对关系型数据库，Java 中应用最广泛的就是 JDBC 规范，今天我们将对这个经典规范展开讨论。JDBC 是 Java Database Connectivity 的全称，它的设计初衷是提供一套能够应用于各种数据库的统一标准，这套标准需要不同数据库厂家之间共同遵守，并提供各自的实现方案供 JDBC 应用程序调用。作为一套统一标准，JDBC 规范具备完整的架构体系，如下图所示：从上图中可以看到，Java 应用程序通过 JDBC 所提供的 API 进行数据访问，而这些 API 中包含了所需要掌握的各个核心编程对象，JDBC 规范中核心编程对象？对于日常开发而言，JDBC 规范中的核心编程对象包括： DriverManger DataSource Connection Statement ResultSetDriverManager正如前面的 JDBC 规范整体架构图中所示，JDBC 中的 DriverManager 主要负责加载各种不同的驱动程序（Driver），并根据不同的请求向应用程序返回相应的数据库连接（Connection），应用程序再通过调用 JDBC API 实现对数据库的操作。JDBC 中的 Driver 定义如下，其中最重要的是第一个获取 Connection 的 connect 方法：public interface Driver {    //获取数据库连接    Connection connect(String url, java.util.Properties info)        throws SQLException;    boolean acceptsURL(String url) throws SQLException;    DriverPropertyInfo[] getPropertyInfo(String url, java.util.Properties info)                         throws SQLException;    int getMajorVersion();    int getMinorVersion();    boolean jdbcCompliant();    public Logger getParentLogger() throws SQLFeatureNotSupportedException;}针对 Driver 接口，不同的数据库供应商分别提供了自身的实现方案。例如，MySQL 中的 Driver 实现类如下代码所示：public class Driver extends NonRegisteringDriver implements java.sql.Driver {    // 通过 DriverManager 注册 Driver    static {        try {            java.sql.DriverManager.registerDriver(new Driver());        } catch (SQLException E) {            throw new RuntimeException(&quot;Can&#39;t register driver!&quot;);        } } …}这里就使用用了 DriverManager，而 DriverManager 除提供了上述用于注册 Driver 的 registerDriver 方法之外，还提供了 getConnection 方法用于针对具体的 Driver 获取 Connection 对象。DataSource通过前面知道在 JDBC 规范中可直接通过 DriverManager 获取 Connection，也知道获取 Connection 的过程需要建立与数据库之间的连接，而这个过程会产生较大的系统开销。为了提高性能，通常我们首先会建立一个中间层将 DriverManager 生成的 Connection 存放到连接池中，再从池中获取 Connection。可以认为 DataSource 就是这样一个中间层，它作为 DriverManager 的替代品而推出，是获取数据库连接的首选方法。DataSource 在 JDBC 规范中代表的是一种数据源，核心作用是获取数据库连接对象 Connection。在日常开发过程中，我们通常会基于 DataSource 获取 Connection。DataSource 接口的定义如下代码所示：public interface DataSource  extends CommonDataSource, Wrapper {  Connection getConnection() throws SQLException;  Connection getConnection(String username, String password)    throws SQLException;}从上面可以看到，DataSource 接口提供了两个获取 Connection 的重载方法，并继承了 CommonDataSource 接口。CommonDataSource 是 JDBC 中关于数据源定义的根接口，除了 DataSource 接口之外，它还有另外两个子接口，如下图所示：其中，DataSource 是官方定义的获取 Connection 的基础接口，XADataSource 用来在分布式事务环境下实现 Connection 的获取，而 ConnectionPoolDataSource 是从连接池 ConnectionPool 中获取 Connection 的接口。所谓的 ConnectionPool 相当于预先生成一批 Connection 并存放在池中，从而提升 Connection 获取的效率。请注意 DataSource 接口同时还继承了一个 Wrapper 接口。从接口的命名上看，我们可以判断该接口起到一种包装器的作用。事实上，因为很多数据库供应商提供了超越标准 JDBC API 的扩展功能，所以 Wrapper 接口可以把一个由第三方供应商提供的、非 JDBC 标准的接口包装成标准接口。以 DataSource 接口为例，如果我们想自己实现一个定制化的数据源类 MyDataSource，就可以提供一个实现了 Wrapper 接口的 MyDataSourceWrapper 类来完成包装和适配，如下图所示：在 JDBC 规范中，除了 DataSource 之外，Connection、Statement、ResultSet 等核心对象也都继承了这个 Wrapper 接口。作为一种基础组件，它同样不需要开发人员自己实现 DataSource，因为业界已经存在了很多优秀的实现方案，如 DBCP、C3P0 和 Druid 等。例如 Druid 提供了 DruidDataSource，它不仅提供了连接池的功能，还提供了诸如监控等其他功能，它的类层结构如下图所示：ConnectionDataSource 的目的是获取 Connection 对象。我们可以把 Connection 理解为一种会话（Session）机制，Connection 代表一个数据库连接，负责完成与数据库之间的通信。所有 SQL 的执行都是在某个特定 Connection 环境中进行的，同时它还提供了一组重载方法分别用于创建 Statement 和 PreparedStatement。另一方面，Connection 也涉及事务相关的操作。Connection 接口中定义的方法很丰富，其中最核心的几个方法如下代码所示：public interface Connection  extends Wrapper, AutoCloseable { //创建 Statement Statement createStatement() throws SQLException; //创建 PreparedStatement PreparedStatement prepareStatement(String sql) throws SQLException; //提交 void commit() throws SQLException; //回滚 void rollback() throws SQLException; //关闭连接 void close() throws SQLException;}Statement/PreparedStatementJDBC 规范中的 Statement 存在两种类型，一种是普通的 Statement，一种是支持预编译的 PreparedStatement。所谓预编译，是指数据库的编译器会对 SQL 语句提前编译，然后将预编译的结果缓存到数据库中，下次执行时就可以通过替换参数并直接使用编译过的语句，从而大大提高 SQL 的执行效率。当然，这种预编译也需要一定成本，因此在日常开发中，如果对数据库只执行一次性读写操作时，用 Statement 对象进行处理会比较合适；而涉及 SQL 语句的多次执行时，我们可以使用 PreparedStatement。如果需要查询数据库中的数据，我们只需要调用 Statement 或 PreparedStatement 对象的 executeQuery 方法即可。这个方法以 SQL 语句作为参数，执行完后返回一个 JDBC 的 ResultSet 对象。当然，Statement 或 PreparedStatement 还提供了一大批执行 SQL 更新和查询的重载方法，我们无意一一展开。以 Statement 为例，它的核心方法如下代码所示：public interface Statement extends Wrapper, AutoCloseable { //执行查询语句 ResultSet executeQuery(String sql) throws SQLException; //执行更新语句 int executeUpdate(String sql) throws SQLException; //执行 SQL 语句 boolean execute(String sql) throws SQLException; //执行批处理    int[] executeBatch() throws SQLException;}ResultSet一旦通过 Statement 或 PreparedStatement 执行了 SQL 语句并获得了 ResultSet 对象，就可以使用该对象中定义的一大批用于获取 SQL 执行结果值的工具方法，如下代码所示：public interface ResultSet extends Wrapper, AutoCloseable { //获取下一个结果 boolean next() throws SQLException; //获取某一个类型的结果值 Value getXXX(int columnIndex) throws SQLException; …}ResultSet 提供了 next() 方法，便于实现对整个结果集的遍历。如果 next() 方法返回为 true，意味着结果集中存在数据，可以调用 ResultSet 对象的一系列 getXXX() 方法来取得对应的结果值。使用 JDBC 规范访问数据库JDBC API 是访问数据库的主要途径，如果使用 JDBC 开发一个访问数据库的执行流程，常见的代码风格如下所示（省略了异常处理）：// 创建池化的数据源PooledDataSource dataSource = new PooledDataSource ();// 设置 MySQL DriverdataSource.setDriver (&quot;com.mysql.jdbc.Driver&quot;);// 设置数据库 URL、用户名和密码dataSource.setUrl (&quot;jdbc:mysql://localhost:3306/test&quot;);dataSource.setUsername(&quot;root&quot;);dataSource.setPassword(&quot;root&quot;);// 获取连接Connection connection = dataSource.getConnection();// 执行查询PreparedStatement statement = connection.prepareStatement (&quot;select * from user&quot;);// 获取查询结果进行处理ResultSet resultSet = statement.executeQuery();while (resultSet.next()) { …}// 关闭资源statement.close();resultSet.close();connection.close();上述代码主要面向查询场景，而针对用于插入数据的处理场景，只需要在上述代码中替换几行代码，即将“执行查询”和“获取查询结果进行处理”部分的查询操作代码替换为插入操作代码就行。梳理一下基于 JDBC 规范进行数据库访问的整个开发流程，如下所示： 创建 DataSource 获取 Connection 创建 Statement 执行 SQL 豫剧 处理 ResultSet 关闭资源对象基于 JDBC 规范访问关系型数据库的操作分成两大部分： 一部分是准备和释放资源以及执行 SQL 语句 另一部分则是处理 SQL 执行结果。对于任何数据访问而言，前者实际上都是重复的。在上图所示的整个开发流程中，事实上只有“处理 ResultSet ”部分的代码需要根据具体的业务对象进行定制化处理。这种抽象为整个执行过程提供了优化空间。诸如 Spring 框架中 JdbcTemplate 这样的模板工具类就应运而生了。" }, { "title": "实现自定义度量指标和 Actuator 端点", "url": "/posts/customized-measurement-index-actuator/", "categories": "Spring", "tags": "SpringBoot, Spring Boot Actuator", "date": "2018-05-15 09:32:00 +0000", "snippet": "引入了 Spring Boot Actuator 组件来满足 Spring Boot 应用程序的系统监控功能，并重点介绍了如何扩展常见的 Info 和 Health 监控端点的实现方法。创建自定义 Actuator 的实现方法，以便应对默认端点无法满足需求的应用场景。Actuator 中的度量指标对于系统监控而言，度量是一个很重要的维度。 在 Spring Boot 2.X 版本中，Actuator 组件主要使用内置的 Micrometer 库实现度量指标的收集和分析。Micrometer 度量库Micrometer 是一款监控指标的度量类库，为 Java 平台上的性能数据收集提供了一套通用的 API。在应用程序中，只使用 Micrometer 提供的通用 API 即可收集度量指标。Micrometer 中包含的几个核心概念。首先是计量器 Meter，它是一个接口，代表的是需要收集的性能指标数据。关于 Meter 的定义如下：public interface Meter extends AutoCloseable { //Meter 的唯一标识，是名称和标签的一种组合 Id getId(); //一组测量结果 Iterable&amp;lt;Measurement&amp;gt; measure(); //Meter 的类型枚举值    enum Type {        COUNTER,        GAUGE,        LONG_TASK_TIMER,        TIMER,        DISTRIBUTION_SUMMARY,        OTHER    }}通过上述代码，Meter 中存在一个 Id 对象，该对象的作用是定义 Meter 的名称和标签。从 Type 的枚举值中，不难看出 Micrometer 中包含的所有计量器类型。1.Meter 的名称：对于计量器来说，每个计量器都有自己的名称，而且在创建时它们都可以指定一系列标签。2.Meter 的标签：标签的作用在于监控系统可以通过这些标签对度量进行分类过滤。在日常开发过程中，常用的计量器类型主要分为计数器 Counter、计量仪 Gauge 和计时器 Timer 这三种。 Counter：这个计量器的作用和它的名称一样，就是一个不断递增的累加器，我们可以通过它的 increment 方法实现累加逻辑； Gauge：与 Counter 不同，Gauge 所度量的值并不一定是累加的，我们可以通过它的 gauge 方法指定数值； Timer：这个计量器比较简单，就是用来记录事件的持续时间。既然已经明确了常用的计量器及其使用场景，那么如何创建这些计量器呢？在 Micrometer 中，提供了一个计量器注册表 MeterRegistry，它主要负责创建和维护各种计量器。关于 MeterRegistry 的创建方法如下代码所示：public abstract class MeterRegistry implements AutoCloseable { protected abstract &amp;lt;T&amp;gt; Gauge newGauge(Meter.Id id, @Nullable T obj, ToDoubleFunction&amp;lt;T&amp;gt; valueFunction); protected abstract Counter newCounter(Meter.Id id); protected abstract Timer newTimer(Meter.Id id, DistributionStatisticConfig distributionStatisticConfig, PauseDetector pauseDetector); …}以上代码只是创建 Meter 的一种途径，从中可以看到 MeterRegistry 针对不同的 Meter 提供了对应的创建方法。而创建 Meter 的另一种途径是使用某个 Meter 的具体 builder 方法。以 Counter 为例，它的定义中包含了一个 builder 方法和一个 register 方法，如下代码所示：public interface Counter extends Meter {    static Builder builder(String name) {        return new Builder(name); }    default void increment() {        increment(1.0); } void increment(double amount); double count();    @Override    default Iterable&amp;lt;Measurement&amp;gt; measure() {        return Collections.singletonList(new Measurement(this::count, Statistic.COUNT)); }     …    public Counter register(MeterRegistry registry) {        return registry.counter(new Meter.Id(name, tags, baseUnit, description, Type.COUNTER));    }}注意到最后的 register 方法就是将当前的 Counter 注册到 MeterRegistry 中，因此需要创建一个 Counter。通常，会采用如下所示代码进行创建。Counter counter = Counter.builder(&quot;counter1&quot;)        .tag(&quot;tag1&quot;, &quot;value1&quot;)        .register(registry);扩展 Metrics 端点在 Spring Boot 中，它为提供了一个 Metrics 端点用于实现生产级的度量工具。访问 actuator/metrics 端点后，将得到如下所示的一系列度量指标。{     &quot;names&quot;:[         &quot;jvm.memory.max&quot;,         &quot;jvm.threads.states&quot;,         &quot;jdbc.connections.active&quot;,         &quot;jvm.gc.memory.promoted&quot;,         &quot;jvm.memory.used&quot;,         &quot;jvm.gc.max.data.size&quot;,         &quot;jdbc.connections.max&quot;,         &quot;jdbc.connections.min&quot;,         &quot;jvm.memory.committed&quot;,         &quot;system.cpu.count&quot;,         &quot;logback.events&quot;,         &quot;http.server.requests&quot;,         &quot;jvm.buffer.memory.used&quot;,         &quot;tomcat.sessions.created&quot;,         &quot;jvm.threads.daemon&quot;,         &quot;system.cpu.usage&quot;,         &quot;jvm.gc.memory.allocated&quot;,         &quot;hikaricp.connections.idle&quot;,         &quot;hikaricp.connections.pending&quot;,         &quot;jdbc.connections.idle&quot;,         &quot;tomcat.sessions.expired&quot;,         &quot;hikaricp.connections&quot;,         &quot;jvm.threads.live&quot;,         &quot;jvm.threads.peak&quot;,         &quot;hikaricp.connections.active&quot;,         &quot;hikaricp.connections.creation&quot;,         &quot;process.uptime&quot;,         &quot;tomcat.sessions.rejected&quot;,         &quot;process.cpu.usage&quot;,         &quot;jvm.classes.loaded&quot;,         &quot;hikaricp.connections.max&quot;,         &quot;hikaricp.connections.min&quot;,         &quot;jvm.gc.pause&quot;,         &quot;jvm.classes.unloaded&quot;,         &quot;tomcat.sessions.active.current&quot;,         &quot;tomcat.sessions.alive.max&quot;,         &quot;jvm.gc.live.data.size&quot;,         &quot;hikaricp.connections.usage&quot;,         &quot;hikaricp.connections.timeout&quot;,         &quot;jvm.buffer.count&quot;,         &quot;jvm.buffer.total.capacity&quot;,         &quot;tomcat.sessions.active.max&quot;,         &quot;hikaricp.connections.acquire&quot;,         &quot;process.start.time&quot;     ] }以上代码中涉及的指标包括常规的系统内存总量、空闲内存数量、处理器数量、系统正常运行时间、堆信息等。也包含引入 JDBC 和 HikariCP 数据源组件之后的数据库连接信息等。此时，如果想了解某项指标的详细信息，在 actuator/metrics 端点后添加对应指标的名称即可。例如想了解当前内存的使用情况，就可以通过 actuator/metrics/jvm.memory.used 端点进行获取，如下代码所示：{     &quot;name&quot;:&quot;jvm.memory.used&quot;,     &quot;description&quot;:&quot;The amount of used memory&quot;,     &quot;baseUnit&quot;:&quot;bytes&quot;,     &quot;measurements&quot;:[         {             &quot;statistic&quot;:&quot;VALUE&quot;,             &quot;value&quot;:115520544         }     ],     &quot;availableTags&quot;:[         {             &quot;tag&quot;:&quot;area&quot;,             &quot;values&quot;:[                 &quot;heap&quot;,                 &quot;nonheap&quot;             ]         },         {             &quot;tag&quot;:&quot;id&quot;,             &quot;values&quot;:[                 &quot;Compressed Class Space&quot;,                 &quot;PS Survivor Space&quot;,                 &quot;PS Old Gen&quot;,                 &quot;Metaspace&quot;,                 &quot;PS Eden Space&quot;,                 &quot;Code Cache&quot;             ]         }     ]}Metrics 指标体系中包含支持 Counter 和 Gauge 这两种级别的度量指标。通过将 Counter 或 Gauge 注入业务代码中，就可以记录自己想要的度量指标。其中，Counter 用来暴露 increment() 方法，而 Gauge 用来提供一个 value() 方法。下面以 Counter 为例介绍在业务代码中嵌入自定义 Metrics 指标的方法，如下代码所示：@Componentpublic class CounterService {       public CounterService() {        Metrics.addRegistry(new SimpleMeterRegistry());    }    public void counter(String name, String... tags) {        Counter counter = Metrics.counter(name, tags);        counter.increment();    }}在这段代码中，构建了一个公共服务 CounterService，并开放了一个 Counter 方法供业务系统进行使用。当然，也可以自己实现类似的工具类完成对各种计量器的封装。另外，Micrometer 还提供了一个 MeterRegistry 工具类供我们创建度量指标。因此，十分推荐使用 MeterRegistry 对各种自定义度量指标的创建过程进行简化。使用 MeterRegistry比如希望系统每创建一个客服工单，就对所创建的工单进行计数，并作为系统运行时的一项度量指标，该效果的实现方式如下代码所示：@Servicepublic class CustomerTicketService {    @Autowired    private MeterRegistry meterRegistry; public CustomerTicket generateCustomerTicket(Long accountId, String orderNumber) { CustomerTicket customerTicket = new CustomerTicket();    …        meterRegistry.summary(&quot;customerTickets.generated.count&quot;).record(1);        return customerTicket;    }   }在上述 generateCustomerTicket 方法中，通过 MeterRegistry 我们实现了每次创建 CustomerTicket 时自动添加一个计数的功能。而且，MeterRegistry 还提供了一些类工具方法用于创建自定义度量指标。这些类工具方法除了常规的 counter、gauge、timer 等对应具体 Meter 的工具方法之外，还包括上述代码中的 summary 方法，且 Summary 方法返回的是一个 DistributionSummary 对象，关于它的定义如下代码所示：public interface DistributionSummary extends Meter, HistogramSupport {    static Builder builder(String name) {        return new Builder(name);    }    //记录数据    void record(double amount);    //记录操作执行的次数    long count();    //记录数据的数量    double totalAmount();    //记录数据的平均值    default double mean() {        return count() == 0 ? 0 : totalAmount() / count();    }    //记录数据的最大值 double max(); …}因为 DistributionSummary 的作用是记录一系列的事件并对这些事件进行处理，所以在 CustomerTicketService 中添加的meterRegistry.summary(&quot;customertickets.generated.count&quot;).record(1) 这行代码相当于每次调用 generateCustomerTicket 方法时，都会对这次调用进行记录。现在访问 actuator/metrics/customertickets.generated.count 端点，就能看到如下所示的随着服务调用不断递增的度量信息。{     &quot;name&quot;:&quot;customertickets.generated.count&quot;,     &quot;measurements&quot;:[         {             &quot;statistic&quot;:&quot;Count&quot;,             &quot;value&quot;:1         },         {             &quot;statistic&quot;:&quot;Total&quot;,             &quot;value&quot;:19         }     ] }显然，通过 MeterRegistry 实现自定义度量指标的使用方法更加简单。这里，也可以结合业务需求尝试该类的不同功能。再来看一个相对比较复杂的使用方式。在 customer-service 中，同样希望系统存在一个度量值，该度量值用于记录所有新增的 CustomerTicket 个数，这次的示例代码如下所示：@Componentpublic class CustomerTicketMetrics extends AbstractRepositoryEventListener&amp;lt;CustomerTicket&amp;gt; {    private MeterRegistry meterRegistry;    public CustomerTicketMetrics(MeterRegistry meterRegistry) {        this.meterRegistry = meterRegistry;    }    @Override    protected void onAfterCreate(CustomerTicket customerTicket) { meterRegistry.counter(&quot;customerTicket.created.count&quot;).increment();  }}首先，这里使用了 MeterRegistry 的 Counter 方法初始化一个 counter，然后调用它的 increment 方法增加度量计数（这部分内容我们已经很熟悉了）。注意到这里，同时还引入了一个 AbstractRepositoryEventListener 抽象类，这个抽象类能够监控 Spring Data 中 Repository 层操作所触发的事件 RepositoryEvent，例如实体创建前后的 BeforeCreateEvent 和 AfterCreateEvent 事件、实体保存前后的 BeforeSaveEvent 和 AfterSaveEvent 事件等。针对这些事件，AbstractRepositoryEventListener 能捕捉并调用对应的回调函数。关于 AbstractRepositoryEventListener 类的部分实现如下代码所示：public abstract class AbstractRepositoryEventListener&amp;lt;T&amp;gt; implements ApplicationListener&amp;lt;RepositoryEvent&amp;gt; {    public final void onApplicationEvent(RepositoryEvent event) {        …        Class&amp;lt;?&amp;gt; srcType = event.getSource().getClass();        if (event instanceof BeforeSaveEvent) {            onBeforeSave((T) event.getSource());        } else if (event instanceof BeforeCreateEvent) {            onBeforeCreate((T) event.getSource());        } else if (event instanceof AfterCreateEvent) {            onAfterCreate((T) event.getSource());        } else if (event instanceof AfterSaveEvent) {            onAfterSave((T) event.getSource());        }        … }}在这段代码中，可以看到 AbstractRepositoryEventListener 直接实现了 Spring 容器中的 ApplicationListener 监听器接口，并在 onApplicationEvent 方法中根据所传入的事件类型触发了回调函数。以案例中的需求场景为例，可以在创建 Account 实体之后执行度量操作。也就是说，可以把度量操作的代码放在 onAfterCreate 回调函数中，正如案例代码中所展示那样。现在执行生成客户工单操作，并访问对应的 Actuator 端点，同样可以看到度量数据在不断上升。自定义 Actuator 端点在日常开发过程中，扩展现有端点有时并不一定能满足业务需求，而自定义 Spring Boot Actuator 监控端点算是一种更灵活的方法。假设需要提供一个监控端点以获取当前系统的用户信息和计算机名称，就可以通过一个独立的 MySystemEndPoint 进行实现，如下代码所示：@Configuration@Endpoint(id = &quot;mysystem&quot;, enableByDefault = true)public class MySystemEndpoint { @ReadOperation public Map&amp;lt;String, Object&amp;gt; getMySystemInfo() { Map&amp;lt;String, Object&amp;gt; result = new HashMap&amp;lt;&amp;gt;(); Map&amp;lt;String, String&amp;gt; map = System.getenv(); result.put(&quot;username&quot;, map.get(&quot;USERNAME&quot;)); result.put(&quot;computername&quot;, map.get(&quot;COMPUTERNAME&quot;)); return result; }}在这段代码中可以看到，MySystemEndpoint 主要通过系统环境变量获取所需监控信息。注意，这里引入了一个新的注解 @Endpoint，该注解定义如下代码所示：@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Endpoint { String id() default &quot;&quot;; boolean enableByDefault() default true;}这段代码中的 @Endpoint 注解主要用于设置端点 id 及是否默认启动的标志位。且在案例中，指定了 id 为ysystem，enableByDefault 标志为 true。事实上，在 Actuator 中也存在一批类似 @Endpoint 的端点注解。其中被 @Endpoint 注解的端点可以通过 **JMX **和 Web 访问应用程序，对应的被 @JmxEndpoint 注解的端点只能通过 JMX 访问，而被 @WebEndpoint 注解的端点只能通过 Web 访问。在示例代码中，还看到了一个 @ReadOperation 注解，该注解作用于方法，用于标识读取数据操作。在 Actuator 中，除了提供 @ReadOperation 注解之外，还提供 @WriteOperation 和 @DeleteOperation 注解，它们分别对应写入操作和删除操作。现在，通过访问 http://localhost:8080/actuator/mysystem，就能获取如下所示监控信息。{ &quot;computername&quot;:&quot;LAPTOP-EQB59J5P&quot;, &quot;username&quot;:&quot;user&quot;}有时为了获取特定的度量信息，我们需要对某个端点传递参数，而 Actuator 专门提供了一个 @Selector 注解标识输入参数，示例代码如下所示：@Configuration@Endpoint(id = &quot;account&quot;, enableByDefault = true)public class AccountEndpoint {    @Autowired    private AccountRepository accountRepository;         @ReadOperation    public Map&amp;lt;String, Object&amp;gt; getMySystemInfo(@Selector String arg0) {        Map&amp;lt;String, Object&amp;gt; result = new HashMap&amp;lt;&amp;gt;();        result.put(accountName, accountRepository.findAccountByAccountName(arg0));        return result;    }} 在使用 Micrometer 时，实现度量数据的采集方法有哪些？" }, { "title": "使用 RabbitTemplate 集成 RabbitMQ", "url": "/posts/rabbitMQ-template/", "categories": "Spring", "tags": "SpringBoot, RabbitTemplate, RabbitMQ", "date": "2018-05-12 09:32:00 +0000", "snippet": "AMQP 规范与 RabbitMQAMQP（Advanced Message Queuing Protocol）是一个提供统一消息服务的应用层标准高级消息队列规范。和 JMS 规范一样，AMQP 描述了一套模块化的组件及组件之间进行连接的标准规则，用于明确客户端与服务器交互的语义。而业界也存在一批实现 AMQP 规范的框架，其中极具代表性的是 RabbitMQ。AMQP 规范在 AMQP 规范中存在三个核心组件，分别是： 交换器（Exchange）：用于接收应用程序发送的消息，并根据一定的规则将这些消息路由发送到消息队列中； 消息队列（Queue）：用于存储消息，直到这些消息被消费者安全处理完毕； 绑定（Binding）：定义了交换器和消息队列之间的关联，为它们提供了路由规则。在 AMQP 规范中并没有明确指明类似 JMS 中一对一的点对点模型和一对多的发布-订阅模型，不过通过控制 Exchange 与 Queue 之间的路由规则，可以很容易地模拟 Topic 这种典型消息中间件的概念。如果存在多个 Queue，Exchange 如何知道把消息发送到哪个 Queue 中呢？通过 Binding 规则设置路由信息即可。在与多个 Queue 关联之后，Exchange 中会存在一个路由表，这个表中维护着每个 Queue 存储消息的限制条件。消息中包含一个路由键（Routing Key），它由消息发送者产生，并提供给 Exchange 路由这条消息的标准。而 Exchange 会检查 Routing Key，并结合路由算法决定将消息路由发送到哪个 Queue 中。通过下面 Exchange 与 Queue 之间的路由关系图，我们可以看到一条来自生产者的消息通过 Exchange 中的路由算法可以发送给一个或多个 Queue，从而实现点对点和发布订阅功能。上图中，不同的路由算法存在不同的 Exchange 类型，而 AMQP 规范中指定了直接式交换器（Direct Exchange）、广播式交换器（Fanout Exchange）、主题式交换器（Topic Exchange）和消息头式交换器（Header Exchange）这几种 Exchange 类型。通过精确匹配消息的 Routing Key，直接式交换器可以将消息路由发送到零个或多个队列中，如下图所示：RabbitMQ 基本架构RabbitMQ 使用 Erlang 语言开发的 AMQP 规范标准实现框架，而 ConnectionFactory、Connection、Channel 是 RabbitMQ 对外提供的 API 中最基本的对象，都需要遵循 AMQP 规范的建议。其中，Channel 是应用程序与 RabbitMQ 交互过程中最重要的一个接口，因为大部分的业务操作需要通过 Channel 接口完成，如定义 Queue、定义 Exchange、绑定 Queue **与 **Exchange、发布消息等。如果想启动 RabbitMQ，只需要运行 rabbitmq-server.sh 文件即可。不过，因为 RabbitMQ 依赖于 Erlang，所以首先需要确保安装上 Erlang 环境。使用 RabbitTemplate 集成 RabbitMQ如果想使用 RabbitTemplate 集成 RabbitMQ，首先需要在 Spring Boot 应用程序中添加对 spring-boot-starter-amqp 的依赖，如下代码所示：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-amqp&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;使用 RabbitTemplate 发送消息和其他模板类一样，RabbitTemplate 也提供了一批 send 方法用来发送消息，如下代码所示：@Overridepublic void send(Message message) throws AmqpException {    send(this.exchange, this.routingKey, message);}@Overridepublic void send(String routingKey, Message message) throws AmqpException { send(this.exchange, routingKey, message);}@Overridepublic void send(final String exchange, final String routingKey, final Message message) throws AmqpException {    send(exchange, routingKey, message, null);}在这里可以看到，指定了消息发送的 Exchange 及用于消息路由的路由键 RoutingKey。因为这些 send 方法发送的是原生消息对象，所以在与业务代码进行集成时，需要将业务对象转换为 Message 对象，示例代码如下所示：public void sendDemoObject(DemoObject demoObject) { MessageConverter converter = rabbitTemplate.getMessageConverter(); MessageProperties props = new MessageProperties(); Message message = converter.toMessage(demoObject, props); rabbitTemplate.send(&quot;demo.queue&quot;, message); }如果不想在业务代码中嵌入 Message 等原生消息对象，还可以使用 RabbitTemplate 的 convertAndSend 方法组进行实现，如下代码所示：@Overridepublic void convertAndSend(Object object) throws AmqpException { convertAndSend(this.exchange, this.routingKey, object, (CorrelationData) null);}@Overridepublic void correlationConvertAndSend(Object object, CorrelationData correlationData) throws AmqpException { convertAndSend(this.exchange, this.routingKey, object, correlationData);}@Overridepublic void convertAndSend(String routingKey, final Object object) throws AmqpException { convertAndSend(this.exchange, routingKey, object, (CorrelationData) null);}@Overridepublic void convertAndSend(String routingKey, final Object object, CorrelationData correlationData) throws AmqpException { convertAndSend(this.exchange, routingKey, object, correlationData);}@Overridepublic void convertAndSend(String exchange, String routingKey, final Object object) throws AmqpException { convertAndSend(exchange, routingKey, object, (CorrelationData) null);}上述 convertAndSend 方法组在内部就完成了业务对象向原生消息对象的自动转换过程，因此，可以使用如下所示的代码来简化消息发送过程。public void sendDemoObject(DemoObject demoObject) { rabbitTemplate.convertAndSend(&quot;demo.queue&quot;, demoObject); }当然，有时候需要在消息发送的过程中为消息添加一些属性，这就不可避免需要操作原生 Message 对象，而 RabbitTemplate 也提供了一组 convertAndSend 重载方法应对这种场景，如下代码所示：@Overridepublic void convertAndSend(String exchange, String routingKey, final Object message,            final MessagePostProcessor messagePostProcessor, CorrelationData correlationData) throws AmqpException {        Message messageToSend = convertMessageIfNecessary(message);        messageToSend = messagePostProcessor.postProcessMessage(messageToSend, correlationData);        send(exchange, routingKey, messageToSend, correlationData);}注意这里，使用了一个 MessagePostProcessor 类对所生成的消息进行后处理，MessagePostProcessor 的使用方式如下代码所示：rabbitTemplate.convertAndSend(&quot;demo.queue&quot;, event, new MessagePostProcessor() {     @Override     public Message postProcessMessage(Message message) throws AmqpException {          //针对 Message 的处理          return message;     }});使用 RabbitTemplate 的最后一步是在配置文件中添加配置项，在配置时我们需要指定 RabbitMQ 服务器的地址、端口、用户名和密码等信息，如下代码所示：spring:  rabbitmq:    host: 127.0.0.1    port: 5672    username: guest    password: guest    virtual-host: DemoHost注意，出于对多租户和安全因素的考虑，AMQP 还提出了虚拟主机（Virtual Host）概念，因此这里出现了一个 virtual-host 配置项。Virtual Host 类似于权限控制组，内部可以包含若干个 Exchange 和 Queue。多个不同用户使用同一个 RabbitMQ 服务器提供的服务时，可以将其划分为多个 Virtual Host，并在自己的 Virtual Host 中创建相应组件，如下图所示：使用 RabbitTemplate 消费消息和 JmsTemplate 一样，使用 RabbitTemplate 消费消息时，也可以使用推模式和拉模式。拉模式在拉模式下，使用 RabbitTemplate 的典型示例如下代码所示：public DemoEvent receiveEvent() { return (DemoEvent) rabbitTemplate.receiveAndConvert(&quot;demo.queue&quot;);}这里，使用了 RabbitTemplate 中的 receiveAndConvert 方法，该方法可以从一个指定的 Queue 中拉取消息，如下代码所示：@Overridepublic Object receiveAndConvert(String queueName) throws AmqpException { return receiveAndConvert(queueName, this.receiveTimeout);}这里请注意，内部的 receiveAndConvert 方法中出现了第二个参数 receiveTimeout，这个参数的默认值是 0，意味着即使调用 receiveAndConvert 时队列中没有消息，该方法也会立即返回一个空对象，而不会等待下一个消息的到来，这点与 JmsTemplate 存在本质性的区别。如果想实现与 JmsTemplate 一样的阻塞等待，设置好 receiveTimeout 参数即可，如下代码所示：`public DemoEvent receiveEvent() { return (DemoEvent)rabbitTemplate.receiveAndConvert(&quot;demo.queue&quot;, 2000ms); }如果不想每次方法调用都指定 receiveTimeout，可以在配置文件中通过添加配置项的方式设置 RabbitTemplate 级别的时间，如下代码所示：spring:  rabbitmq:    template:      receive-timeout: 2000当然，RabbitTemplate 也提供了一组支持接收原生消息的 receive 方法，但还是建议使用 receiveAndConvert 方法实现拉模式下的消息消费。推模式它的实现方法也很简单，如下代码所示：@RabbitListener(queues = &quot;demo.queue&quot;)public void handlerEvent(DemoEvent event) { // TODO：添加消息处理逻辑}开发人员在 @RabbitListener 中指定目标队列即可自动接收来自该队列的消息，这种实现方式与 @JmsListener 完全一致。在 SpringCSS 案例中集成 RabbitMQ因为这三种模板工具类的使用方式非常类似，都可以用来提取公共代码形成统一的接口和抽象类，所以，我想对 SpringCSS 案例中的三种模板工具类的集成方式进行抽象。实现 account-service 消息生产者在消息生产者的 account-service 中，我们提取了如下所示的 AccountChangedPublisher 作为消息发布的统一接口。public interface AccountChangedPublisher { void publishAccountChangedEvent(Account account, String operation);}请注意，这是一个面向业务的接口，没有使用用于消息通信的 AccountChangedEvent 对象。而将在 AccountChangedPublisher 接口的实现类 AbstractAccountChangedPublisher 中完成对 AccountChangedEvent 对象的构建，如下代码所示：public abstract class AbstractAccountChangedPublisher implements AccountChangedPublisher {    @Override    public void publishAccountChangedEvent(Account account, String operation) {        AccountMessage accountMessage = new AccountMessage(account.getId(), account.getAccountCode(), account.getAccountName());        AccountChangedEvent event = new AccountChangedEvent(AccountChangedEvent.class.getTypeName(), operation.toString(), accountMessage);        publishEvent(event);    }    protected abstract void publishEvent(AccountChangedEvent event);}AbstractAccountChangedPublisher 是一个抽象类，基于传入的业务对象构建了一个消息对象 AccountChangedEvent，并通过 publishEvent 抽象方法发送消息。针对不同的消息中间件，需要分别实现对应的 publishEvent 方法。以 Kafka 为例，重构了原有代码并提供了如下所示的 KafkaAccountChangedPublisher 实现类。@Component(&quot;kafkaAccountChangedPublisher&quot;)public class KafkaAccountChangedPublisher extends AbstractAccountChangedPublisher {    @Autowired    private KafkaTemplate&amp;lt;String, AccountChangedEvent&amp;gt; kafkaTemplate;    @Override    protected void publishEvent(AccountChangedEvent event) {        kafkaTemplate.send(AccountChannels.SPRINGCSS_ACCOUNT_TOPIC, event);    }}对 RabbitMQ 而言，RabbitMQAccountChangedPublisher 的实现方式也是类似，如下代码所示：@Component(&quot;rabbitMQAccountChangedPublisher&quot;)public class RabbitMQAccountChangedPublisher extends AbstractAccountChangedPublisher {    @Autowired    private RabbitTemplate rabbitTemplate;    @Override    protected void publishEvent(AccountChangedEvent event) {        rabbitTemplate.convertAndSend(AccountChannels.SPRINGCSS_ACCOUNT_QUEUE, event, new MessagePostProcessor() {            @Override            public Message postProcessMessage(Message message) throws AmqpException {                MessageProperties props = message.getMessageProperties();                props.setHeader(&quot;EVENT_SYSTEM&quot;, &quot;SpringCSS&quot;);                return message;            }        });    }}对于 RabbitMQ 而言，在使用 RabbitMQAccountChangedPublisher 发送消息之前，需要先初始化 Exchange、Queue，以及两者之间的 Binding 关系，因此实现了如下所示的 RabbitMQMessagingConfig 配置类。@Configurationpublic class RabbitMQMessagingConfig {    public static final String SPRINGCSS_ACCOUNT_DIRECT_EXCHANGE = &quot;springcss.account.exchange&quot;;    public static final String SPRINGCSS_ACCOUNT_ROUTING = &quot;springcss.account.routing&quot;;    @Bean    public Queue SpringCssDirectQueue() {        return new Queue(AccountChannels.SPRINGCSS_ACCOUNT_QUEUE, true);    }    @Bean    public DirectExchange SpringCssDirectExchange() {        return new DirectExchange(SPRINGCSS_ACCOUNT_DIRECT_EXCHANGE, true, false);    }    @Bean    public Binding bindingDirect() {        return BindingBuilder.bind(SpringCssDirectQueue()).to(SpringCssDirectExchange()).with(SPRINGCSS_ACCOUNT_ROUTING);    }    @Bean    public Jackson2JsonMessageConverter rabbitMQMessageConverter() {        return new Jackson2JsonMessageConverter();    }}上述代码中初始化了一个 DirectExchange、一个 Queue ，并设置了两者之间的绑定关系，同时还初始化了一个 Jackson2JsonMessageConverter 用于在消息发送过程中将消息转化为序列化对象，以便在网络上进行传输。实现 customer-service 消息消费者回到 customer-service 服务，我们先看看提取用于接收消息的统一化接口 AccountChangedReceiver，如下代码所示：public interface AccountChangedReceiver {    //Pull 模式下的消息接收方法    void receiveAccountChangedEvent();    //Push 模式下的消息接收方法    void handlerAccountChangedEvent(AccountChangedEvent event);}AccountChangedReceiver 分别定义了拉取模式和推送模式下的消息接收方法，同样也提取了一个抽象实现类 AbstractAccountChangedReceiver，如下代码所示：public abstract class AbstractAccountChangedReceiver implements AccountChangedReceiver {    @Autowired    LocalAccountRepository localAccountRepository;    @Override    public void receiveAccountChangedEvent() {        AccountChangedEvent event = receiveEvent();        handleEvent(event);    }    protected void handleEvent(AccountChangedEvent event) {        AccountMessage account = event.getAccountMessage();        String operation = event.getOperation();        operateAccount(account, operation);    }    private void operateAccount(AccountMessage accountMessage, String operation) {        System.out.print(accountMessage.getId() + &quot;:&quot; + accountMessage.getAccountCode() + &quot;:&quot; + accountMessage.getAccountName());        LocalAccount localAccount = new LocalAccount(accountMessage.getId(), accountMessage.getAccountCode(),accountMessage.getAccountName());        if (operation.equals(&quot;ADD&quot;) || operation.equals(&quot;UPDATE&quot;)) {            localAccountRepository.save(localAccount);        } else {            localAccountRepository.delete(localAccount);        }    }    protected abstract AccountChangedEvent receiveEvent();}这里实现了 AccountChangedReceiver 接口的 receiveAccountChangedEvent 方法，并定义了一个 receiveEvent 抽象方法接收来自不同消息中间件的 AccountChangedEvent 消息。一旦 receiveAccountChangedEvent 方法获取了消息，将根据其中的 Account 对象及对应的操作更新本地数据库。AbstractAccountChangedReceiver 中的一个实现类 RabbitMQAccountChangedReceiver，如下代码所示：@Component(&quot;rabbitMQAccountChangedReceiver&quot;)public class RabbitMQAccountChangedReceiver extends AbstractAccountChangedReceiver {    @Autowired    private RabbitTemplate rabbitTemplate;    @Override    public AccountChangedEvent receiveEvent() {        return (AccountChangedEvent) rabbitTemplate.receiveAndConvert(AccountChannels.SPRINGCSS_ACCOUNT_QUEUE);    }    @Override    @RabbitListener(queues = AccountChannels.SPRINGCSS_ACCOUNT_QUEUE)    public void handlerAccountChangedEvent(AccountChangedEvent event) {        super.handleEvent(event);    }}上述 RabbitMQAccountChangedReceiver 同时实现了 AbstractAccountChangedReceiver 的 receiveEvent 抽象方法及 AccountChangedReceiver 接口中的 handlerAccountChangedEvent 方法。其中 receiveEvent 方法用于主动拉取消息，而 handlerAccountChangedEvent 方法用于接受推动过来的消息，在该方法上添加了 @RabbitListener 注解。接着来看下同样继承了 AbstractAccountChangedReceiver 抽象类的 KafkaAccountChangedListener 类，如下代码所示：@Componentpublic class KafkaAccountChangedListener extends AbstractAccountChangedReceiver {    @Override    @KafkaListener(topics = AccountChannels.SPRINGCSS_ACCOUNT_TOPIC)    public void handlerAccountChangedEvent(AccountChangedEvent event) {        super.handleEvent(event);    }    @Override    protected AccountChangedEvent receiveEvent() {        return null;    }}Kafka 只能通过推送方式获取消息，所以它只实现了 handlerAccountChangedEvent 方法，而 receiveEvent 方法为空。ps：在使用 RabbitMQ 时，如何完成 Exchange、Queue ，以及它们之间绑定关系的初始化？" }, { "title": "构建 RESTful 风格的 Web 服务", "url": "/posts/restful-web/", "categories": "Spring", "tags": "SpringBoot, RESTful, Web", "date": "2018-05-10 09:32:00 +0000", "snippet": "创建 RESTful 服务在当下的分布式系统及微服务架构中，RESTful 风格是一种主流的 Web 服务表现方式。首先来理解什么是 RESTful 服务。理解 RESTful 架构风格REST（Representational State Transfer，表述性状态转移）本质上只是一种架构风格而不是一种规范。这种架构风格把位于服务器端的访问入口看作一个资源，每个资源都使用 URI（Universal Resource Identifier，统一资源标识符） 得到一个唯一的地址，且在传输协议上使用标准的 HTTP 方法，比如最常见的 GET、PUT、POST 和 DELETE。下表是 RESTful 风格的一些具体示例：| URL | HTTP 方法 | 描述 || :—————————————: | :——-: | :————————————: || http://www.example.com/accounts | GET | 获取 Account 对象列表 || http://www.example.com/accounts | PUT | 更新一组 Accoun t对象 || http://www.example.com/accounts | POST | 新增一组 Account 对象 || http://www.example.com/accounts | DELETE | 删除所有 Account 对象 || http://www.example.com/accounts/jianxiang | GET | 根据账户名 jianxiang 获取 Account 对象 || http://www.example.com/accounts/jianxiang | PUT | 根据账户名 jianxiang 更新 Account 对象 || http://www.example.com/accounts/jianxiang | POST | 添加账户名为jianxiang的新Account对象 || http://www.example.com/accounts/jianxiang | DELETE | 根据账户名 jianxiang 删除 Account 对象 |另一方面，客户端与服务器端的数据交互涉及序列化问题。关于序列化完成业务对象在网络环境上的传输的实现方式有很多，常见的有文本和二进制两大类。目前 JSON 是一种被广泛采用的序列化方式，因此所有的代码实例我都将 JSON 作为默认的序列化方式。使用基础注解在原有 Spring Boot 应用程序的基础上，可以通过构建一系列的 Controller 类暴露 RESTful 风格的 HTTP 端点。。这里的 Controller 与 Spring MVC 中的 Controller 概念上一致，最简单的 Controller 类如下代码所示：@RestControllerpublic class HelloController { @GetMapping(&quot;/&quot;) public String index() { return &quot;Hello World!&quot;; } }从以上代码中可以看到，包含了 @RestController 和 @GetMapping 这两个注解。 @RestController 注解继承自 Spring MVC 中的 @Controller 注解，顾名思义就是一个基于 RESTful 风格的 HTTP 端点，并且会自动使用 JSON 实现 HTTP 请求和响应的序列化/反序列化方式。通过这个特性，在构建 RESTful 服务时，可以使用 @RestController 注解代替 @Controller 注解以简化开发; 另外一个 @GetMapping 注解也与 Spring MVC 中的 @RequestMapping 注解类似。@RequestMapping 注解的定义，该注解所提供的属性都比较容易理解，如下代码所示：@Target({ElementType.TYPE, ElementType.METHOD})@Retention(RetentionPolicy.RUNTIME)@Documented@Mappingpublic @interface RequestMapping { String name() default &quot;&quot;; @AliasFor(&quot;path&quot;) String[] value() default {}; @AliasFor(&quot;value&quot;) String[] path() default {}; RequestMethod[] method() default {}; String[] params() default {}; String[] headers() default {}; String[] consumes() default {}; String[] produces() default {};}@GetMapping 的注解的定义与 @RequestMapping 非常类似，只是默认使用了 RequestMethod.GET 指定 HTTP 方法，如下代码所示：@Target({ElementType.METHOD})@Retention(RetentionPolicy.RUNTIME)@Documented@RequestMapping(method = {RequestMethod.GET})public @interface GetMapping { @AliasFor( annotation = RequestMapping.class ) String name() default &quot;&quot;; @AliasFor( annotation = RequestMapping.class ) String[] value() default {}; @AliasFor( annotation = RequestMapping.class ) String[] path() default {}; @AliasFor( annotation = RequestMapping.class ) String[] params() default {}; @AliasFor( annotation = RequestMapping.class ) String[] headers() default {}; @AliasFor( annotation = RequestMapping.class ) String[] consumes() default {}; @AliasFor( annotation = RequestMapping.class ) String[] produces() default {};}Spring Boot 2 中引入的一批新注解中，除了 @GetMapping ，还有 @PutMapping、@PostMapping、@DeleteMapping 等注解，这些注解极大方便了显式指定 HTTP 的请求方法。当然，可以继续使用原先的 @RequestMapping 实现同样的效果。一个更加具体的示例，以下代码展示了 account-service 中的 AccountController。@RestController@RequestMapping(value = &quot;accounts&quot;)public class AccountController { @GetMapping(value = &quot;/{accountId}&quot;) public Account getAccountById(@PathVariable(&quot;accountId&quot;) Long accountId) { Account account = new Account(); account.setId(1L); account.setAccountCode(&quot;DemoCode&quot;); account.setAccountName(&quot;DemoName&quot;); return account; }}在该 Controller 中，通过静态的业务代码完成了根据账号编号（accountId）获取用户账户信息的业务流程。这里用到了两层 Mapping： 第一层的 @RequestMapping 注解在服务层级定义了服务的根路径“/accounts”； 第二层的 @GetMapping 注解则在操作级别定义了 HTTP 请求方法的具体路径及参数信息。到这里，一个典型的 RESTful 服务已经开发完成了，而后可以通过 java –jar 命令直接运行 Spring Boot 应用程序了。在启动日志中，发现了以下输出内容（为了显示效果，部分内容做了调整），可以看到自定义的这个 AccountController 已经成功启动并准备接收响应。RequestMappingHandlerMapping : Mapped &quot;{[/accounts/{accountId}], methods=[GET]}&quot; onto public cn.happymaya.springcss.account.domain.Account cn.happymaya.account.controller.AccountController.getAccountById (java.lang.Long)而后，可以通过 Postman 访问“http://localhost:8082/accounts/1”端点以得到响应结果。除此之外，还有一个新的注解：@PathVariable，该注解作用于输入的参数，控制请求输入和输出使用注解简化请求输入Spring Boot 提供了一系列简单有用的注解来简化对请求输入的控制过程，常用的包括 @PathVariable、@RequestParam 和 @RequestBody。其中 @PathVariable 注解用于获取路径参数，即从类似 url/{id} 这种形式的路径中获取 {id} 参数的值。该注解的定义如下代码所示：@Target({ElementType.PARAMETER})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface PathVariable { @AliasFor(&quot;name&quot;) String value() default &quot;&quot;; @AliasFor(&quot;value&quot;) String name() default &quot;&quot;; boolean required() default true;}通常，使用 @PathVariable 注解时，只需要指定一个参数的名称即可。再看一个示例，如下代码所示：@GetMapping(value = &quot;/{accountName}&quot;)public Account getAccountByAccountName(@PathVariable(&quot;accountName&quot;) String accountName) { return accountService.getAccountByAccountName(accountName);}@RequestParam 注解的作用与 @PathVariable 注解类似，也是用于获取请求中的参数，但是它面向类似 url?id=XXX 这种路径形式。该注解的定义如下代码所示，相较 @PathVariable 注解，它只是多了一个设置默认值的 defaultValue 属性。@Target({ElementType.PARAMETER})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface RequestParam { @AliasFor(&quot;name&quot;) String value() default &quot;&quot;; @AliasFor(&quot;value&quot;) String name() default &quot;&quot;; boolean required() default true; String defaultValue() default ValueConstants.DEFAULT_NONE;}在 HTTP 协议中，content-type 属性用来指定所传输的内容类型，可以通过 @RequestMapping 注解中的 produces 属性来设置这个属性。在设置这个属性时，通常会将其设置为application/json，如下代码所示：@RestController@RequestMapping(value = &quot;accounts&quot;, produces=&quot;application/json&quot;)public class AccountController {}@RequestBody 注解用来处理 content-type 为 application/json 类型时的编码内容，通过 @RequestBody 注解可以将请求体中的 JSON 字符串绑定到相应的 JavaBean 上。如下代码所示就是一个使用 @RequestBody 注解来控制输入的场景。@PutMapping(value = &quot;/&quot;)public void updateAccount(@RequestBody Account account) {}如果使用 @RequestBody 注解，我们可以在 Postman 中输入一个 JSON 字符串来构建输入对象，如下所示：关于控制请求输入的规则，关键在于按照 RESTful 风格的设计原则设计 HTTP 端点，对于这点业界也存在一些约定。 以 Account 这个领域实体为例，如果我们把它视为一种资源，那么 HTTP 端点的根节点命名上通常采用复数形式，即“/accounts”，正如前面的示例代码所示。 在设计 RESTful API 时，我们需要基于 HTTP 语义设计对外暴露的端点的详细路径。针对常见的 CRUD 操作，我们展示了 RESTful API 与非 RESTful API 的一些区别。 业务操作 非 RESTful API RESTful API 获取用户账户 /account/query/1 /accounts/1 GET 新增用户账户 /account/add /accounts POST 更新用户账户 /account/edit /accounts PUT 删除用户账户 /account/delete /accounts DELETE 基于以上的控制请求输入的实现方法，可以给出 account-service 中 AccountController 类的完整实现过程，如下代码所示：@RestController@RequestMapping(value = &quot;accounts&quot;, produces=&quot;application/json&quot;)public class AccountController { @Autowired private AccountService accountService; private static final Logger logger = LoggerFactory.getLogger(AccountController.class); @GetMapping(value = &quot;/{accountId}&quot;) public Account getAccountById(@PathVariable(&quot;accountId&quot;) Long accountId) { logger.info(&quot;Get account by id: [{}] &quot;, accountId); Account account = new Account(); account.setId(1L); account.setAccountCode(&quot;DemoCode&quot;); account.setAccountName(&quot;DemoName&quot;); return account; } @GetMapping(value = &quot;accountname/{accountName}&quot;) public Account getAccountByAccountName(@PathVariable(&quot;accountName&quot;) String accountName) { Account account = accountService.getAccountByAccountName(accountName); return account; } @PostMapping(value = &quot;/&quot;) public void addAccount(@RequestBody Account account) { accountService.addAccount(account); } @PutMapping(value = &quot;/&quot;) public void updateAccount(@RequestBody Account account) { accountService.updateAccount(account); } @DeleteMapping(value = &quot;/&quot;) public void deleteAccount(@RequestBody Account account) { accountService.deleteAccount(account); }}控制请求的输出相较输入控制，输出控制就要简单很多，因为 Spring Boot 所提供的 @RestController 注解已经屏蔽了底层实现的复杂性，只需要返回一个普通的业务对象即可。@RestController 注解相当于是 Spring MVC 中 @Controller 和 @ResponseBody 这两个注解的组合，它们会自动返回 JSON 数据。例如 order-service 中的 OrderController 实现过程，如下代码所示：@RestController@RequestMapping(value=&quot;orders/jpa&quot;)public class JpaOrderController { @Autowired JpaOrderService jpaOrderService; @GetMapping(value = &quot;/{orderId}&quot;) public JpaOrder getOrderById(@PathVariable Long orderId) { JpaOrder order = jpaOrderService.getOrderById(orderId); return order; } @GetMapping(value = &quot;orderNumber/{orderNumber}&quot;) public JpaOrder getOrderByOrderNumber(@PathVariable String orderNumber) { return jpaOrderService.getOrderByOrderNumberBySpecification(orderNumber);rder; } @PostMapping(value = &quot;&quot;) public JpaOrder addOrder(@RequestBody JpaOrder order) { return jpaOrderService.addOrder(order);; }} 在使用 Spring Boot 构建 Web 服务时，可以使用哪些注解实现对输入参数的有效控制？ produces 是生成的格式，consumes 是接收的格式，指定生产者所提供的内容格式。" }, { "title": "RestTemplate 远程调用实现原理", "url": "/posts/rest-template-remote/", "categories": "Spring", "tags": "SpringBoot, RestTemplate", "date": "2018-05-07 09:32:00 +0000", "snippet": "RestTemplate 访问 HTTP 端点的使用方法，它涉及 RestTemplate 初始化、发起请求及获取响应结果等核心环节。初始化 RestTemplate 实例通过 RestTemplate 提供的几个构造函数对 RestTemplate 进行初始化。在分析这些构造函数之前，有必要先看一下 RestTemplate 类的定义，如下代码所示：public class RestTemplate extends InterceptingHttpAccessor implements RestOperation{}从上述代码中，可以看到 RestTemplate 扩展了 InterceptingHttpAccessor 抽象类，并实现了 RestOperations 接口。接下来围绕 RestTemplate 的方法定义进行设计思路的梳理。首先，来看看 RestOperations 接口的定义，这里截取了部分核心方法，如下代码所示：public interface RestOperations { @Nullable &amp;lt;T&amp;gt; T getForObject(String var1, Class&amp;lt;T&amp;gt; var2, Object... var3) throws RestClientException; @Nullable &amp;lt;T&amp;gt; T getForObject(String var1, Class&amp;lt;T&amp;gt; var2, Map&amp;lt;String, ?&amp;gt; var3) throws RestClientException; @Nullable &amp;lt;T&amp;gt; T getForObject(URI var1, Class&amp;lt;T&amp;gt; var2) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; getForEntity(String var1, Class&amp;lt;T&amp;gt; var2, Object... var3) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; getForEntity(String var1, Class&amp;lt;T&amp;gt; var2, Map&amp;lt;String, ?&amp;gt; var3) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; getForEntity(URI var1, Class&amp;lt;T&amp;gt; var2) throws RestClientException; HttpHeaders headForHeaders(String var1, Object... var2) throws RestClientException; HttpHeaders headForHeaders(String var1, Map&amp;lt;String, ?&amp;gt; var2) throws RestClientException; HttpHeaders headForHeaders(URI var1) throws RestClientException; @Nullable URI postForLocation(String var1, @Nullable Object var2, Object... var3) throws RestClientException; @Nullable URI postForLocation(String var1, @Nullable Object var2, Map&amp;lt;String, ?&amp;gt; var3) throws RestClientException; @Nullable URI postForLocation(URI var1, @Nullable Object var2) throws RestClientException; @Nullable &amp;lt;T&amp;gt; T postForObject(String var1, @Nullable Object var2, Class&amp;lt;T&amp;gt; var3, Object... var4) throws RestClientException; @Nullable &amp;lt;T&amp;gt; T postForObject(String var1, @Nullable Object var2, Class&amp;lt;T&amp;gt; var3, Map&amp;lt;String, ?&amp;gt; var4) throws RestClientException; @Nullable &amp;lt;T&amp;gt; T postForObject(URI var1, @Nullable Object var2, Class&amp;lt;T&amp;gt; var3) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; postForEntity(String var1, @Nullable Object var2, Class&amp;lt;T&amp;gt; var3, Object... var4) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; postForEntity(String var1, @Nullable Object var2, Class&amp;lt;T&amp;gt; var3, Map&amp;lt;String, ?&amp;gt; var4) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; postForEntity(URI var1, @Nullable Object var2, Class&amp;lt;T&amp;gt; var3) throws RestClientException; void put(String var1, @Nullable Object var2, Object... var3) throws RestClientException; void put(String var1, @Nullable Object var2, Map&amp;lt;String, ?&amp;gt; var3) throws RestClientException; void put(URI var1, @Nullable Object var2) throws RestClientException; @Nullable &amp;lt;T&amp;gt; T patchForObject(String var1, @Nullable Object var2, Class&amp;lt;T&amp;gt; var3, Object... var4) throws RestClientException; @Nullable &amp;lt;T&amp;gt; T patchForObject(String var1, @Nullable Object var2, Class&amp;lt;T&amp;gt; var3, Map&amp;lt;String, ?&amp;gt; var4) throws RestClientException; @Nullable &amp;lt;T&amp;gt; T patchForObject(URI var1, @Nullable Object var2, Class&amp;lt;T&amp;gt; var3) throws RestClientException; void delete(String var1, Object... var2) throws RestClientException; void delete(String var1, Map&amp;lt;String, ?&amp;gt; var2) throws RestClientException; void delete(URI var1) throws RestClientException; Set&amp;lt;HttpMethod&amp;gt; optionsForAllow(String var1, Object... var2) throws RestClientException; Set&amp;lt;HttpMethod&amp;gt; optionsForAllow(String var1, Map&amp;lt;String, ?&amp;gt; var2) throws RestClientException; Set&amp;lt;HttpMethod&amp;gt; optionsForAllow(URI var1) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; exchange(String var1, HttpMethod var2, @Nullable HttpEntity&amp;lt;?&amp;gt; var3, Class&amp;lt;T&amp;gt; var4, Object... var5) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; exchange(String var1, HttpMethod var2, @Nullable HttpEntity&amp;lt;?&amp;gt; var3, Class&amp;lt;T&amp;gt; var4, Map&amp;lt;String, ?&amp;gt; var5) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; exchange(URI var1, HttpMethod var2, @Nullable HttpEntity&amp;lt;?&amp;gt; var3, Class&amp;lt;T&amp;gt; var4) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; exchange(String var1, HttpMethod var2, @Nullable HttpEntity&amp;lt;?&amp;gt; var3, ParameterizedTypeReference&amp;lt;T&amp;gt; var4, Object... var5) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; exchange(String var1, HttpMethod var2, @Nullable HttpEntity&amp;lt;?&amp;gt; var3, ParameterizedTypeReference&amp;lt;T&amp;gt; var4, Map&amp;lt;String, ?&amp;gt; var5) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; exchange(URI var1, HttpMethod var2, @Nullable HttpEntity&amp;lt;?&amp;gt; var3, ParameterizedTypeReference&amp;lt;T&amp;gt; var4) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; exchange(RequestEntity&amp;lt;?&amp;gt; var1, Class&amp;lt;T&amp;gt; var2) throws RestClientException; &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; exchange(RequestEntity&amp;lt;?&amp;gt; var1, ParameterizedTypeReference&amp;lt;T&amp;gt; var2) throws RestClientException; @Nullable &amp;lt;T&amp;gt; T execute(String var1, HttpMethod var2, @Nullable RequestCallback var3, @Nullable ResponseExtractor&amp;lt;T&amp;gt; var4, Object... var5) throws RestClientException; @Nullable &amp;lt;T&amp;gt; T execute(String var1, HttpMethod var2, @Nullable RequestCallback var3, @Nullable ResponseExtractor&amp;lt;T&amp;gt; var4, Map&amp;lt;String, ?&amp;gt; var5) throws RestClientException; @Nullable &amp;lt;T&amp;gt; T execute(URI var1, HttpMethod var2, @Nullable RequestCallback var3, @Nullable ResponseExtractor&amp;lt;T&amp;gt; var4) throws RestClientException;}显然，RestOperations 接口定义了 get/post/put/delete/exhange 等所有远程调用方法组，这些方法都遵循 RESTful 架构风格而设计。RestTemplate 为这些接口提供了实现机制，这是它的一条代码支线。InterceptingHttpAccessor，是一个抽象类，包含的核心变量如下代码所示：public abstract class InterceptingHttpAccessor extends HttpAccessor { private final List&amp;lt;ClientHttpRequestInterceptor&amp;gt; interceptors = new ArrayList(); @Nullable private volatile ClientHttpRequestFactory interceptingRequestFactory; public InterceptingHttpAccessor() { } public void setInterceptors(List&amp;lt;ClientHttpRequestInterceptor&amp;gt; interceptors) { if (this.interceptors != interceptors) { this.interceptors.clear(); this.interceptors.addAll(interceptors); AnnotationAwareOrderComparator.sort(this.interceptors); } } public List&amp;lt;ClientHttpRequestInterceptor&amp;gt; getInterceptors() { return this.interceptors; } public void setRequestFactory(ClientHttpRequestFactory requestFactory) { super.setRequestFactory(requestFactory); this.interceptingRequestFactory = null; } public ClientHttpRequestFactory getRequestFactory() { List&amp;lt;ClientHttpRequestInterceptor&amp;gt; interceptors = this.getInterceptors(); if (!CollectionUtils.isEmpty(interceptors)) { ClientHttpRequestFactory factory = this.interceptingRequestFactory; if (factory == null) { factory = new InterceptingClientHttpRequestFactory(super.getRequestFactory(), interceptors); this.interceptingRequestFactory = (ClientHttpRequestFactory)factory; } return (ClientHttpRequestFactory)factory; } else { return super.getRequestFactory(); } }}通过变量定义，明确了 InterceptingHttpAccessor 包含两部分处理功能： 一部分负责设置和管理请求拦截器 ClientHttpRequestInterceptor； 另一部分负责获取用于创建客户端 HTTP 请求的工厂类 ClientHttpRequestFactory。同时，注意到 InterceptingHttpAccessor 同样存在一个父类 HttpAccessor，这个父类值真正实现了 ClientHttpRequestFactory 的创建及如何通过 ClientHttpRequestFactory 获取代表客户端请求的 ClientHttpRequest 对象。HttpAccessor 的核心变量如下代码所示：public abstract class HttpAccessor { protected final Log logger = HttpLogging.forLogName(this.getClass()); private ClientHttpRequestFactory requestFactory = new SimpleClientHttpRequestFactory(); private final List&amp;lt;ClientHttpRequestInitializer&amp;gt; clientHttpRequestInitializers = new ArrayList(); public HttpAccessor() { } public void setRequestFactory(ClientHttpRequestFactory requestFactory) { Assert.notNull(requestFactory, &quot;ClientHttpRequestFactory must not be null&quot;); this.requestFactory = requestFactory; } public ClientHttpRequestFactory getRequestFactory() { return this.requestFactory; } public void setClientHttpRequestInitializers(List&amp;lt;ClientHttpRequestInitializer&amp;gt; clientHttpRequestInitializers) { if (this.clientHttpRequestInitializers != clientHttpRequestInitializers) { this.clientHttpRequestInitializers.clear(); this.clientHttpRequestInitializers.addAll(clientHttpRequestInitializers); AnnotationAwareOrderComparator.sort(this.clientHttpRequestInitializers); } } public List&amp;lt;ClientHttpRequestInitializer&amp;gt; getClientHttpRequestInitializers() { return this.clientHttpRequestInitializers; } protected ClientHttpRequest createRequest(URI url, HttpMethod method) throws IOException { ClientHttpRequest request = this.getRequestFactory().createRequest(url, method); this.initialize(request); if (this.logger.isDebugEnabled()) { this.logger.debug(&quot;HTTP &quot; + method.name() + &quot; &quot; + url); } return request; } private void initialize(ClientHttpRequest request) { this.clientHttpRequestInitializers.forEach((initializer) -&amp;gt; { initializer.initialize(request); }); }}从以上代码可以看到，HttpAccessor 中创建了 SimpleClientHttpRequestFactory 作为系统默认的 ClientHttpRequestFactory。由此可见，RestTemplate 的类层结构，如下图所示：在 RestTemplate 的类层结构中，就能快速理解它的设计思想。整个类层结构清晰地分成两条支线： 左边支线用于完成与 HTTP 请求相关的实现机制； 而右边支线提供了基于 RESTful 风格的操作入口，并使用了面向对象中的接口和抽象类完成这两部分功能的聚合。RestTemplate 核心执行流程作为用于远程调用的模板工具类，可以从具备多种请求方式的 exchange 方法入手，该方法的定义如下代码所示：public &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; exchange(String url, HttpMethod method, @Nullable HttpEntity&amp;lt;?&amp;gt; requestEntity, Class&amp;lt;T&amp;gt; responseType, Object... uriVariables) throws RestClientException { // 构建请求回调 RequestCallback requestCallback = this.httpEntityCallback(requestEntity, responseType); // 构建响应体抽取器 ResponseExtractor&amp;lt;ResponseEntity&amp;lt;T&amp;gt;&amp;gt; responseExtractor = this.responseEntityExtractor(responseType); // 执行远程调用 return (ResponseEntity)nonNull(this.execute(url, method, requestCallback, responseExtractor, uriVariables));}显然，应该进一步关注这里的 execute 方法。事实上，无论采用 get/put/post/delete 中的哪种方法发起请求，RestTemplate 负责执行远程调用时，使用的都是 execute 方法，该方法定义如下代码所示：@Nullablepublic &amp;lt;T&amp;gt; T execute(String url, HttpMethod method, @Nullable RequestCallback requestCallback, @Nullable ResponseExtractor&amp;lt;T&amp;gt; responseExtractor, Object... uriVariables) throws RestClientException { URI expanded = this.getUriTemplateHandler().expand(url, uriVariables); return this.doExecute(expanded, method, requestCallback, responseExtractor);}从以上代码中，发现 execute 方法首先通过 UriTemplateHandler 构建了一个 URI，然后将请求过程委托给 doExecute 方法进行处理，该方法定义如下代码所示：@Nullableprotected &amp;lt;T&amp;gt; T doExecute(URI url, @Nullable HttpMethod method, @Nullable RequestCallback requestCallback, @Nullable ResponseExtractor&amp;lt;T&amp;gt; responseExtractor) throws RestClientException { Assert.notNull(url, &quot;URI is required&quot;); Assert.notNull(method, &quot;HttpMethod is required&quot;); ClientHttpResponse response = null; Object var14; try { // 创建请求对象 ClientHttpRequest request = this.createRequest(url, method); if (requestCallback != null) { // 执行对请求的回调 requestCallback.doWithRequest(request); } // 获取调用结果 response = request.execute(); // 处理调用结果 this.handleResponse(url, method, response); // 使用结果提取从结果中提取数据 var14 = responseExtractor != null ? responseExtractor.extractData(response) : null; } catch (IOException var12) { String resource = url.toString(); String query = url.getRawQuery(); resource = query != null ? resource.substring(0, resource.indexOf(63)) : resource; throw new ResourceAccessException(&quot;I/O error on &quot; + method.name() + &quot; request for \\&quot;&quot; + resource + &quot;\\&quot;: &quot; + var12.getMessage(), var12); } finally { if (response != null) { response.close(); } } return var14;}创建请求对象创建请求对象的入口方法如下代码所示：ClientHttpRequest request = createRequest(url, method);通过跟踪上面的 createRequest 方法，发现流程执行到了前面介绍的 HttpAccessor 类。创建 ClientHttpRequest 的过程是一种典型的工厂模式应用场景，这里直接创建了一个实现 ClientHttpRequestFactory 接口的 SimpleClientHttpRequestFactory 对象，然后再通过这个对象的 createRequest 方法创建了客户端请求对象 ClientHttpRequest 并返回给上层组件进行使用。ClientHttpRequestFactory 接口的定义如下代码所示：@FunctionalInterfacepublic interface ClientHttpRequestFactory { // 创建客户端请求对象 ClientHttpRequest createRequest(URI var1, HttpMethod var2) throws IOException;}在 Spring 中，存在一批 ClientHttpRequestFactory 接口的实现类，而SimpleClientHttpRequestFactory 是它的默认实现，在实现自定义的 ClientHttpRequestFactory 时，开发人员也可以根据需要自行选择。为简单起见，直接跟踪 SimpleClientHttpRequestFactory 的代码，来看它的 createRequest 方法，如下代码所示：private boolean bufferRequestBody = true;@Overridepublic ClientHttpRequest createRequest(URI uri, HttpMethod httpMethod) throws IOException { HttpURLConnection connection = openConnection(uri.toURL(), this.proxy); prepareConnection(connection, httpMethod.name()); if (this.bufferRequestBody) { return new SimpleBufferingClientHttpRequest(connection, this.outputStreaming); } else { return new SimpleStreamingClientHttpRequest(connection, this.chunkSize, this.outputStreaming); }}在上述 createRequest 中，首先通过传入的 URI 对象构建了一个 HttpURLConnection 对象，然后对该对象进行一些预处理，最后构造并返回一个 ClientHttpRequest 实例。通过翻阅代码，我们发现上述的 openConnection 方法只是通过 URL 对象的 openConnection 方法返回了一个 UrlConnection，而 prepareConnection 方法也只是完成了对 HttpUrlConnection 超时时间、请求方法等常见属性的设置。在这里，注意到 bufferRequestBody 参数的值为 true，因此通过 createRequest 方法最终返回的结果是一个 SimpleBufferingClientHttpRequest 对象。执行远程调用一旦获取了请求对象，我们就可以发起远程调用并获取响应了，RestTemplate 中的入口方法如下代码所示：response = request.execute();这里的 request 就是前面创建的 SimpleBufferingClientHttpRequest 类，可以先来看一下该类的类层结构，如下图所示：在上图的 AbstractClientHttpRequest 中，定义了如下代码所示的 execute 方法：@Overridepublic final ClientHttpResponse execute() throws IOException { assertNotExecuted(); ClientHttpResponse result = executeInternal(this.headers); this.executed = true; return result;}protected abstract ClientHttpResponse executeInternal(HttpHeaders headers) throws IOException;AbstractClientHttpRequest 类的作用是防止 HTTP 请求的 Header 和 Body 被多次写入，所以在 execute 方法返回之前，我们设置了一个 executed 标志位。同时，在 execute 方法中，最终调用了一个抽象方法 executeInternal，这个方法的实现在 AbstractClientHttpRequest 的子类 AbstractBufferingClientHttpRequest 中，如下代码所示：@Overrideprotected ClientHttpResponse executeInternal(HttpHeaders headers) throws IOException { byte[] bytes = this.bufferedOutput.toByteArray(); if (headers.getContentLength() &amp;lt; 0) { headers.setContentLength(bytes.length); } ClientHttpResponse result = executeInternal(headers, bytes); this.bufferedOutput = new ByteArrayOutputStream(0); return result;}protected abstract ClientHttpResponse executeInternal(HttpHeaders headers, byte[] bufferedOutput)    throws IOException;和 AbstractClientHttpRequest 类一样，进一步梳理了一个抽象方法 executeInternal，这个抽象方法通过最底层的 SimpleBufferingClientHttpRequest 类实现，如下代码所示：@Overrideprotected ClientHttpResponse executeInternal(HttpHeaders headers, byte[] bufferedOutput) throws IOException { addHeaders(this.connection, headers); // JDK &amp;lt;1.8 doesn&#39;t support getOutputStream with HTTP DELETE if (getMethod() == HttpMethod.DELETE &amp;amp;&amp;amp; bufferedOutput.length == 0) { this.connection.setDoOutput(false); } if (this.connection.getDoOutput() &amp;amp;&amp;amp; this.outputStreaming) { this.connection.setFixedLengthStreamingMode(bufferedOutput.length); } this.connection.connect(); if (this.connection.getDoOutput()) { FileCopyUtils.copy(bufferedOutput, this.connection.getOutputStream()); } else { // Immediately trigger the request in a no-output scenario as well            this.connection.getResponseCode(); } return new SimpleClientHttpResponse(this.connection);}这里通过 FileCopyUtils.copy 工具方法，我们把结果写入输出流上了，executeInternal 方法最终返回的结果是一个包装了 Connection 对象的 SimpleClientHttpResponse。处理响应结果一个 HTTP 请求处理的最后一步是从 ClientHttpResponse 中读取输入流，然后格式化为一个响应体并将其转化为业务对象，入口代码如下所示：//处理调用结果handleResponse(url, method, response);//使用结果提取从结果中提取数据return (responseExtractor != null ? responseExtractor.extractData(response) : null);先来看这里的 handleResponse 方法，定义如下代码所示：protected void handleResponse(URI url, HttpMethod method, ClientHttpResponse response) throws IOException { ResponseErrorHandler errorHandler = getErrorHandler(); boolean hasError = errorHandler.hasError(response); if (logger.isDebugEnabled()) { try { logger.debug(method.name() + &quot; request for \\&quot;&quot; + url + &quot;\\&quot; resulted in &quot; + response.getRawStatusCode() + &quot; (&quot; + response.getStatusText() + &quot;)&quot; + (hasError ? &quot;; invoking error handler&quot; : &quot;&quot;)); } catch (IOException ex) {                // ignore        } } if (hasError) {            errorHandler.handleError(url, method, response); }}以上代码中，通过 getErrorHandler 方法我们获取了一个 ResponseErrorHandler，如果响应的状态码错误，我们可以调用 handleError 来处理错误并抛出异常。在这里，我们发现这段代码实际上并没有真正处理返回的数据，而只是执行了错误处理。而获取响应数据并完成转化的工作是在 ResponseExtractor 中，该接口定义如下代码所示：public interface ResponseExtractor&amp;lt;T&amp;gt; {    @Nullable    T extractData(ClientHttpResponse response) throws IOException;}在 RestTemplate 类中，定义了一个 ResponseEntityResponseExtractor 内部类实现了ResponseExtractor 接口，如下代码所示：private class ResponseEntityResponseExtractor &amp;lt;T&amp;gt; implements ResponseExtractor&amp;lt;ResponseEntity&amp;lt;T&amp;gt;&amp;gt; { @Nullable private final HttpMessageConverterExtractor&amp;lt;T&amp;gt; delegate; public ResponseEntityResponseExtractor(@Nullable Type responseType) { if (responseType != null &amp;amp;&amp;amp; Void.class != responseType) { this.delegate = new HttpMessageConverterExtractor&amp;lt;&amp;gt;(responseType, getMessageConverters(), logger); } else { this.delegate = null; } } @Override public ResponseEntity&amp;lt;T&amp;gt; extractData(ClientHttpResponse response) throws IOException { if (this.delegate != null) { T body = this.delegate.extractData(response); return ResponseEntity.status(response.getRawStatusCode()).headers(response.getHeaders()).body(body); } else { return ResponseEntity.status(response.getRawStatusCode()).headers(response.getHeaders()).build(); } }}在上述代码中，ResponseEntityResponseExtractor 中的 extractData 方法本质上是将数据提取部分的工作委托给了一个代理对象 delegate，而这个 delegate 的类型就是 HttpMessageConverterExtractor。从命名上看，不难看出 HttpMessageConverterExtractor 类的内部使用 HttpMessageConverter 实现消息的转换，如下代码所示（代码做了裁剪）：public class HttpMessageConverterExtractor&amp;lt;T&amp;gt; implements ResponseExtractor&amp;lt;T&amp;gt; {    private final List&amp;lt;HttpMessageConverter&amp;lt;?&amp;gt;&amp;gt; messageConverters; @Override    @SuppressWarnings({&quot;unchecked&quot;, &quot;rawtypes&quot;, &quot;resource&quot;})    public T extractData(ClientHttpResponse response) throws IOException {        MessageBodyClientHttpResponseWrapper responseWrapper = new MessageBodyClientHttpResponseWrapper(response);        if (!responseWrapper.hasMessageBody() || responseWrapper.hasEmptyMessageBody()) {            return null;        }        MediaType contentType = getContentType(responseWrapper);        try {            for (HttpMessageConverter&amp;lt;?&amp;gt; messageConverter : this.messageConverters) {                if (messageConverter instanceof GenericHttpMessageConverter) {                    GenericHttpMessageConverter&amp;lt;?&amp;gt; genericMessageConverter =                            (GenericHttpMessageConverter&amp;lt;?&amp;gt;) messageConverter;                    if (genericMessageConverter.canRead(this.responseType, null, contentType)) {                        return (T) genericMessageConverter.read(this.responseType, null, responseWrapper);                    }                }                if (this.responseClass != null) {                    if (messageConverter.canRead(this.responseClass, contentType)) {                        return (T) messageConverter.read((Class) this.responseClass, responseWrapper);                    }                }            }        } …}上述方法看上去有点复杂，但核心逻辑很简单，首先遍历 HttpMessageConveter 列表，然后判断其是否能够读取数据，如果能就调用 read 方法读取数据。最后， HttpMessageConveter 中如何实现 read 方法。先来看 HttpMessageConveter 接口的抽象实现类 AbstractHttpMessageConverter，在它的 read 方法中我们同样定义了一个抽象方法 readInternal，如下代码所示：@Overridepublic final T read(Class&amp;lt;? extends T&amp;gt; clazz, HttpInputMessage inputMessage)            throws IOException, HttpMessageNotReadableException {    return readInternal(clazz, inputMessage);}protected abstract T readInternal(Class&amp;lt;? extends T&amp;gt; clazz, HttpInputMessage inputMessage) throws IOException, HttpMessageNotReadableException;Spring 提供了一系列的 HttpMessageConveter 实现消息的转换，而最简单的实现方式是 StringHttpMessageConverter，该类的 read 方法如下代码所示：@Overrideprotected String readInternal(Class&amp;lt;? extends String&amp;gt; clazz, HttpInputMessage inputMessage) throws IOException {    Charset charset = getContentTypeCharset(inputMessage.getHeaders().getContentType());    return StreamUtils.copyToString(inputMessage.getBody(), charset);}StringHttpMessageConverter 的实现过程：首先从输入消息 HttpInputMessage 中通过 getBody 方法获取消息体，也就是一个 ClientHttpResponse 对象，再通过 copyToString 方法从该对象中读取数据，并返回字符串结果。至此，通过 RestTemplate 发起、执行及响应整个 HTTP 请求的完整流程就介绍完毕了。" }, { "title": "使用 RestTemplate 消费 RESTful 服务", "url": "/posts/rest-template-restful/", "categories": "Spring", "tags": "SpringBoot, RestTemplate, RESTful", "date": "2018-05-03 09:32:00 +0000", "snippet": "完成 Web 服务的构建后，接下来需要做的事情就是如何对服务进行消费。使用 RestTemplate 访问 HTTP 端点RestTemplate 是 Spring 提供的用于访问 RESTful 服务的客户端的模板工具类，它位于 org.springframework.web.client 包下。在设计上，RestTemplate 完全满足 RESTful 架构风格的设计原则。相较传统 Apache 中的 HttpClient 客户端工具类，RestTemplate 在编码的简便性以及异常的处理等方面都做了很多改进。创建一个 RestTemplate 对象，并通过该对象所提供的大量工具方法实现对远程 HTTP 端点的高效访问，如下文。创建 RestTemplate想创建一个 RestTemplate 对象，最简单且最常见的方法是直接 new 一个该类的实例，如下代码所示：@Beanpublic RestTemplate restTemplate() { return new RestTemplate();}这里我创建了一个 RestTemplate 实例，并通过 @Bean 注解将其注入 Spring 容器中。通過 RestTemplate 的无参构造函数，可以看到创建它的实例时，RestTemplate 都做下面的一下事情，如下代码所示： public RestTemplate() { this.messageConverters = new ArrayList(); this.errorHandler = new DefaultResponseErrorHandler(); this.headersExtractor = new RestTemplate.HeadersExtractor(); this.messageConverters.add(new ByteArrayHttpMessageConverter()); this.messageConverters.add(new StringHttpMessageConverter()); this.messageConverters.add(new ResourceHttpMessageConverter(false)); try { this.messageConverters.add(new SourceHttpMessageConverter()); } catch (Error var2) { } this.messageConverters.add(new AllEncompassingFormHttpMessageConverter()); if (romePresent) { this.messageConverters.add(new AtomFeedHttpMessageConverter()); this.messageConverters.add(new RssChannelHttpMessageConverter()); } if (jackson2XmlPresent) { this.messageConverters.add(new MappingJackson2XmlHttpMessageConverter()); } else if (jaxb2Present) { this.messageConverters.add(new Jaxb2RootElementHttpMessageConverter()); } if (jackson2Present) { this.messageConverters.add(new MappingJackson2HttpMessageConverter()); } else if (gsonPresent) { this.messageConverters.add(new GsonHttpMessageConverter()); } else if (jsonbPresent) { this.messageConverters.add(new JsonbHttpMessageConverter()); } if (jackson2SmilePresent) { this.messageConverters.add(new MappingJackson2SmileHttpMessageConverter()); } if (jackson2CborPresent) { this.messageConverters.add(new MappingJackson2CborHttpMessageConverter()); } this.uriTemplateHandler = initUriTemplateHandler(); }可以清楚地看到 RestTemplate 的无参构造函数只做了一件事情，添加了一批用于实现消息转换的 HttpMessageConverter 对象。通过 RestTemplate 发送的请求和获取的响应，都是以 JSON 作为序列化方式，而调用后续的getForObject、exchange 等方法时所传入的参数及获取的结果都是普通的 Java 对象，我们就是通过使用 RestTemplate 中的 HttpMessageConverter 自动做了这一层转换操作。此外， RestTemplate 还有另外一个更强大的有参构造函数，如下代码所示：public RestTemplate(ClientHttpRequestFactory requestFactory) { this(); this.setRequestFactory(requestFactory);}从以上代码中，可以看到这个构造函数一方面调用了前面的无参构造函数，另一方面可以设置一个 ClientHttpRequestFactory 接口。而基于这个 ClientHttpRequestFactory 接口的各种实现类，可以对 RestTemplate 中的行为进行精细化控制。这方面典型的应用场景是设置 HTTP 请求的超时时间等属性，如下代码所示：@Beanpublic RestTemplate customRestTemplate() { HttpComponentsClientHttpRequestFactory httpRequestFactory = new HttpComponentsClientHttpRequestFactory(); httpRequestFactory.setConnectionRequestTimeout(3000); httpRequestFactory.setConnectTimeout(3000); httpRequestFactory.setReadTimeout(3000); return new RestTemplate(httpRequestFactory);}这里我创建了一个 HttpComponentsClientHttpRequestFactory 工厂类，它是 ClientHttpRequestFactory 接口的一个实现类。通过设置连接请求超时时间 ConnectionRequestTimeout、连接超时时间 ConnectTimeout 等属性，从而对 RestTemplate 的默认行为进行了定制化处理。使用 RestTemplate 访问 Web 服务在远程服务访问上，RestTemplate 内置了一批常用的工具方法，可以根据 HTTP 的语义以及 RESTful 的设计原则对这些工具方法进行分类，如下表所示： HTTP 方法 RestTemplate 方法组 GET getForObject/getForEntity POST postForLocation/postForObject/postForEntity PUT put DELETE delete Header headForHeaders 不限 exchange/execute 接下来，基于该表对 RestTemplate 中的工具方法进行练习。不过在此之前，需要先了解请求的 URL。在一个 Web 请求中，可以通过请求路径携带参数。在使用 RestTemplate 时，也可以在它的 URL 中嵌入路径变量，示例代码如下所示：(&quot;http://localhost:8082/account/{id}&quot;, 1)这里我对 account-service 提供的一个端点进行了参数设置：定义了一个拥有路径变量名为 id 的 URL，实际访问时，将该变量值设置为 1。其实，在URL 中也可以包含多个路径变量，因为 Java 支持不定长参数语法，多个路径变量的赋值可以按照参数依次设置。如下所示的代码中，在 URL 中使用了 pageSize 和 pageIndex 这两个路径变量进行分页操作，实际访问时它们将被替换为 20 和 2。(&quot;http://localhost:8082/account/{pageSize}/{pageIndex}&quot;, 20, 2)而路径变量也可以通过 Map 进行赋值。如下所示的代码同样定义了拥有路径变量 pageSize 和 pageIndex 的 URL，但实际访问时，会从 uriVariables 这个 Map 对象中获取值进行替换，从而得到最终的请求路径为 http://localhost:8082/account/20/2。Map&amp;lt;String, Object&amp;gt; uriVariables = new HashMap&amp;lt;&amp;gt;();uriVariables.put(&quot;pageSize&quot;, 20);uriVariables.put(&quot;pageIndex&quot;, 2);webClient.getForObject() (&quot;http://localhost:8082/account/{pageSize}/{pageIndex}&quot;, Account.class, uriVariables);请求 URL 一旦准备好了，就可以使用 RestTemplates 所提供的一系列工具方法完成远程服务的访问。GET 方法组先来看一下 get 方法组，它包括 getForObject 和 getForEntity 这两组方法，每组各有三个方法。 @Nullable public &amp;lt;T&amp;gt; T getForObject(String url, Class&amp;lt;T&amp;gt; responseType, Object... uriVariables) throws RestClientException { RequestCallback requestCallback = this.acceptHeaderRequestCallback(responseType); HttpMessageConverterExtractor&amp;lt;T&amp;gt; responseExtractor = new HttpMessageConverterExtractor(responseType, this.getMessageConverters(), this.logger); return this.execute(url, HttpMethod.GET, requestCallback, responseExtractor, (Object[])uriVariables); } @Nullable public &amp;lt;T&amp;gt; T getForObject(String url, Class&amp;lt;T&amp;gt; responseType, Map&amp;lt;String, ?&amp;gt; uriVariables) throws RestClientException { RequestCallback requestCallback = this.acceptHeaderRequestCallback(responseType); HttpMessageConverterExtractor&amp;lt;T&amp;gt; responseExtractor = new HttpMessageConverterExtractor(responseType, this.getMessageConverters(), this.logger); return this.execute(url, HttpMethod.GET, requestCallback, responseExtractor, (Map)uriVariables); } @Nullable public &amp;lt;T&amp;gt; T getForObject(URI url, Class&amp;lt;T&amp;gt; responseType) throws RestClientException { RequestCallback requestCallback = this.acceptHeaderRequestCallback(responseType); HttpMessageConverterExtractor&amp;lt;T&amp;gt; responseExtractor = new HttpMessageConverterExtractor(responseType, this.getMessageConverters(), this.logger); return this.execute(url, HttpMethod.GET, requestCallback, responseExtractor); }从以上方法定义上，不难看出它们之间的区别只是传入参数的处理方式不同。这里，注意到第三个 getForObject 方法只有两个参数（后面的两个 getForObject 方法分别支持不定参数以及一个 Map 对象），如果我们想在访问路径上添加一个参数，则需要我们构建一个独立的 URI 对象，示例如下代码所示：例如，getForObject 方法组中的三个方法如下代码所示：String url = &quot;http://localhost:8080/hello?name=&quot; + URLEncoder.encode(name, &quot;UTF-8&quot;);URI uri = URI.create(url);比如 AccountController，如下代码所示：@RestController@RequestMapping(value = &quot;accounts&quot;)public class AccountController {    @GetMapping(value = &quot;/{accountId}&quot;)    public Account getAccountById(@PathVariable(&quot;accountId&quot;) Long accountId) { …    }}对于上述端点，我们可以通过 getForObject 方法构建一个 HTTP 请求来获取目标 Account 对象，实现代码如下所示：Account result = restTemplate.getForObject(&quot;http://localhost:8082/accounts/{accountId}&quot;, Account.class, accountId);在以上代码中，可以看到 getForEntity 方法的返回值是一个 ResponseEntity 对象，在这个对象中还包含了 HTTP 消息头等信息，而 getForObject 方法返回的只是业务对象本身。这是这两个方法组的主要区别，可以根据个人需要自行选择。POST 方法组与 GET 请求相比，RestTemplate 中的 POST 请求除提供了 postForObject 和 postForEntity 方法组以外，还额外提供了一组 postForLocation 方法。假设有如下所示的 OrderController ，它暴露了一个用于添加 Order 的端点。那么，通过 postForEntity 发送 POST 请求的示例如下代码所示：Order order = new Order();order.setOrderNumber(&quot;Order0001&quot;);order.setDeliveryAddress(&quot;DemoAddress&quot;);ResponseEntity&amp;lt;Order&amp;gt; responseEntity = restTemplate.postForEntity(&quot;http://localhost:8082/orders&quot;, order, Order.class);return responseEntity.getBody();从以上代码中可以看到，构建了一个 Order 实体，通过 postForEntity 传递给了 OrderController 所暴露的端点，并获取了该端点的返回值。（特殊说明：postForObject 的操作方式也与此类似。）掌握了 get 和 post 方法组后，理解 put 方法组和 delete 方法组就会非常容易了。其中 put 方法组与 post 方法组相比只是操作语义上的差别，而 delete 方法组的使用过程也和 get 方法组类似。exchange 方法组对于 RestTemplate 而言，exchange 是一个通用且统一的方法，它既能发送 GET 和 POST 请求，也能用于发送其他各种类型的请求。 public &amp;lt;T&amp;gt; ResponseEntity&amp;lt;T&amp;gt; exchange(String url, HttpMethod method, @Nullable HttpEntity&amp;lt;?&amp;gt; requestEntity, Class&amp;lt;T&amp;gt; responseType, Object... uriVariables) throws RestClientException { RequestCallback requestCallback = this.httpEntityCallback(requestEntity, responseType); ResponseExtractor&amp;lt;ResponseEntity&amp;lt;T&amp;gt;&amp;gt; responseExtractor = this.responseEntityExtractor(responseType); return (ResponseEntity)nonNull(this.execute(url, method, requestCallback, responseExtractor, uriVariables)); }注意，这里的 requestEntity 变量是一个 HttpEntity 对象，它封装了请求头和请求体，而 responseType 用于指定返回数据类型。 假如前面的 OrderController 中存在一个根据订单编号 OrderNumber 获取 Order 信息的端点，那么我们使用 exchange 方法发起请求的代码就变成这样了，如下代码所示。ResponseEntity&amp;lt;Order&amp;gt; result = restTemplate.exchange(&quot;http://localhost:8082/orders/{orderNumber}&quot;, HttpMethod.GET, null, Order.class, orderNumber);而比较复杂的一种使用方式是分别设置 HTTP 请求头及访问参数，并发起远程调用，示例代码如下所示：//设置 HTTP HeaderHttpHeaders headers = new HttpHeaders();headers.setContentType(MediaType.APPLICATION_JSON_UTF8);//设置访问参数HashMap&amp;lt;String, Object&amp;gt; params = new HashMap&amp;lt;&amp;gt;();params.put(&quot;orderNumber&quot;, orderNumber);//设置请求 EntityHttpEntity entity = new HttpEntity&amp;lt;&amp;gt;(params, headers);ResponseEntity&amp;lt;Order&amp;gt; result = restTemplate.exchange(url, HttpMethod.GET, entity, Order.class);RestTemplate 其他使用技巧实现常规的 HTTP 请求，RestTemplate 还有一些高级用法，如指定消息转换器、设置拦截器和处理异常等。指定消息转换器在 RestTemplate 中，实际上还存在第三个构造函数，如下代码所示：public RestTemplate(List&amp;lt;HttpMessageConverter&amp;lt;?&amp;gt;&amp;gt; messageConverters) { this.messageConverters = new ArrayList(); this.errorHandler = new DefaultResponseErrorHandler(); this.headersExtractor = new RestTemplate.HeadersExtractor(); this.validateConverters(messageConverters); this.messageConverters.addAll(messageConverters); this.uriTemplateHandler = initUriTemplateHandler();}从以上代码中不难看出，可以通过传入一组 HttpMessageConverter 来初始化 RestTemplate，这也为消息转换器的定制提供了途径。假如，希望把支持 Gson 的 GsonHttpMessageConverter 加载到 RestTemplate 中，就可以使用如下所示的代码。@Beanpublic RestTemplate restTemplate() { List&amp;lt;HttpMessageConverter&amp;lt;?&amp;gt;&amp;gt; messageConverters = new ArrayList&amp;lt;HttpMessageConverter&amp;lt;?&amp;gt;&amp;gt;(); messageConverters.add(new GsonHttpMessageConverter()); return new RestTemplate(messageConverters);}原则上，可以根据需要实现各种自定义的 HttpMessageConverter ，并通过以上方法完成对 RestTemplate 的初始化。设置拦截器如果想对请求做一些通用拦截设置，那么可以使用拦截器。不过，使用拦截器之前，首先需要实现 ClientHttpRequestInterceptor 接口。这方面最典型的应用场景是在 Spring Cloud 中通过 @LoadBalanced 注解为 RestTemplate 添加负载均衡机制。可以在 LoadBalanceAutoConfiguration 自动配置类中找到如下代码。public RestTemplateCustomizer restTemplateOnCustomizer (final LoadBalancerInterceptor loadBalanceInterceptor) { return restTemplate -&amp;gt; {List&amp;lt;ClientHttpRequestInterceptor&amp;gt; list = new ArrayList&amp;lt;&amp;gt;( restTemplate.getInterceptors()); list.add(loadBalanceInterceptor); restTemplate.setInterceptors(list); };}处理异常请求状态码不是返回 200 时，RestTemplate 在默认情况下会抛出异常，并中断接下来的操作，如果我们不想采用这个处理过程，那么就需要覆盖默认的 ResponseErrorHandler。示例代码结构如下所示：public void restTemplateHandlerErrot () { RestTemplate restTemplate = new RestTemplate(); ResponseErrorHandler responseErrorHandler = new ResponseErrorHandler() { @Override public boolean hasError(ClientHttpResponse clientHttpResponse) throws IOException { return true; } @Override public void handleError(ClientHttpResponse clientHttpResponse) throws IOException { // 添加定制化的异常处理代码 } }; restTemplate.setErrorHandler(responseErrorHandler);}在上面的 handleError 方法中，可以实现任何自己想控制的异常处理代码。在 spring-css 中的实现服务交互在其中的 customer-service 的 CustomerService 类中用于完成与 account-service 和 order-service 进行集成的 generateCustomerTicket 方法的代码结构，如下代码所示：public CustomerTicket generateCustomerTicket(Long accountId, String orderNumber) { logger.debug(&quot;Generate customer ticket record with account: {} and order: {}&quot;, accountId, orderNumber); // 创建客服工单对象 CustomerTicket customerTicket = new CustomerTicket(); // 从本地数据库中获取Account信息 LocalAccount account = getAccountById(accountId); if (account != null) { return customerTicket; } // 从远程 order-service 中获取 Order 信息 OrderMapper order = getRemoteOrderByOrderNumber(orderNumber); if (order == null) { return customerTicket; } logger.debug(&quot;Get remote order: {} is successful&quot;, orderNumber); // 创建并保存CustomerTicket信息 customerTicket.setAccountId(accountId); customerTicket.setOrderNumber(order.getOrderNumber()); customerTicket.setCreateTime(new Date()); customerTicket.setDescription(&quot;TestCustomerTicket&quot;); customerTicketRepository.save(customerTicket); // 添加Metrics // customerTicketCounterService.counter(&quot;customerTicket.created.count&quot;); meterRegistry.summary(&quot;customerTickets.generated.count&quot;).record(1); return customerTicket;}这里以 getRemoteOrderByOrderNumber 方法为例，来对它的实现过程进行展开，getRemoteOrderByOrderNumber 方法定义代码如下：@Autowiredprivate OrderClient orderClient;private OrderMapper getRemoteOrderByOrderNumber(String orderNumber) { return orderClient.getOrderByOrderNumber(orderNumber);}getRemoteAccountById 方法的实现过程也类似。接下来构建了一个 OrderClient 类完成对 order-service 的远程访问，如下代码所示：@Componentpublic class OrderClient { private static final Logger logger = LoggerFactory.getLogger(OrderClient.class); @Autowired RestTemplate restTemplate; public OrderMapper getOrderByOrderNumber(String orderNumber) { logger.debug(&quot;Get order: {}&quot;, orderNumber); ResponseEntity&amp;lt;OrderMapper&amp;gt; restExchange = restTemplate.exchange( &quot;http://localhost:8083/orders/{orderNumber}&quot;, HttpMethod.GET, null, OrderMapper.class, orderNumber); OrderMapper result = restExchange.getBody(); return result; } @Bean public RestTemplate restTemplate() { List&amp;lt;HttpMessageConverter&amp;lt;?&amp;gt;&amp;gt; messageConverters = new ArrayList&amp;lt;HttpMessageConverter&amp;lt;?&amp;gt;&amp;gt;(); messageConverters.add(new GsonHttpMessageConverter()); return new RestTemplate(messageConverters); } @Bean public RestTemplate customRestTemplate() { HttpComponentsClientHttpRequestFactory httpRequestFactory = new HttpComponentsClientHttpRequestFactory(); httpRequestFactory.setConnectionRequestTimeout(3000); httpRequestFactory.setConnectTimeout(3000); httpRequestFactory.setReadTimeout(3000); return new RestTemplate(httpRequestFactory); } public void restTemplateHandlerErrot () { RestTemplate restTemplate = new RestTemplate(); ResponseErrorHandler responseErrorHandler = new ResponseErrorHandler() { @Override public boolean hasError(ClientHttpResponse clientHttpResponse) throws IOException { return true; } @Override public void handleError(ClientHttpResponse clientHttpResponse) throws IOException { // 添加定制化的异常处理代码 } }; restTemplate.setErrorHandler(responseErrorHandler); }}注意：这里我注入了一个 RestTemplate 对象，并通过它的 exchange 方法完成对远程 order-serivce 的请求过程。且这里的返回对象是一个 OrderMapper，而不是一个 Order 对象。最后，RestTemplate 内置的 HttpMessageConverter 完成 OrderMapper 与 Order 之间的自动映射。事实上，OrderMapper 与 Order 对象的内部字段一一对应，它们分别位于两个不同的代码工程中，为了以示区别我们才故意在命名上做了区分。 在使用 RestTemplate 时，如何实现对请求超时时间等控制逻辑的定制化处理？" }, { "title": "使用 Spring 测试 Web 服务层组件", "url": "/posts/service-test/", "categories": "Spring", "tags": "SpringBoot, Test", "date": "2018-05-01 09:32:00 +0000", "snippet": "与位于底层的数据库访问层（Dao）不同，业务服务层（Service）和控制层（Controller）都依赖它的下一层组件，即: 业务服务层（Service）依赖于数据访问层（Dao）; 控制层（Controller） 层依赖于服务层（Service）。对于业务服务层（Service）和控制层（Controller）这两层进行测试，需要使用不同的方案和技术。使用 Environment 测试配置信息在 Spring Boot 应用程序中，Service 层通常依赖于配置文件，所以也需要对配置信息进行测试。配置信息的测试方案分为两种： 第一种依赖于物理配置文件； 第二种则是在测试时动态注入配置信息。第一种测试方案比较简单，在 src/test/resources 目录下添加配置文件时，Spring Boot 能读取这些配置文件中的配置项并应用于测试案例中。Environment 接口，定义如下：public interface Environment extends PropertyResolver { String[] getActiveProfiles(); String[] getDefaultProfiles(); boolean acceptsProfiles(String... profiles);}在上述代码中可以看到，Environment 接口的主要作用是处理 Profile，而它的父接口 PropertyResolver 定义如下代码所示：public interface PropertyResolver {    boolean containsProperty(String key);    String getProperty(String key);    String getProperty(String key, String defaultValue);    &amp;lt;T&amp;gt; T getProperty(String key, Class&amp;lt;T&amp;gt; targetType);    &amp;lt;T&amp;gt; T getProperty(String key, Class&amp;lt;T&amp;gt; targetType, T defaultValue);    String getRequiredProperty(String key) throws IllegalStateException;    &amp;lt;T&amp;gt; T getRequiredProperty(String key, Class&amp;lt;T&amp;gt; targetType) throws IllegalStateException;    String resolvePlaceholders(String text);    String resolveRequiredPlaceholders(String text) throws IllegalArgumentException;}显然，PropertyResolver 的作用是根据各种配置项的 Key 获取配置属性值。现在，假设 src/test/resources 目录中的 application.properties 存在如下配置项：springcss.order.point=10那么，就可以设计如下所示的测试用例了：@RunWith(SpringRunner.class)@SpringBootTestpublic class EnvironmentTests{ @Autowired public Environment environment; @Test public void testEnvValue(){ Assert.assertEquals(10, Integer.parseInt(environment.getProperty(&quot;springcss.order.point&quot;))); } }这里注入了一个 Environment 接口，并调用了它的 getProperty 方法来获取测试环境中的配置信息。除了在配置文件中设置属性，也可以使用 @SpringBootTest 注解指定用于测试的属性值，示例代码如下：@RunWith(SpringRunner.class)@SpringBootTest(properties = {&quot; springcss.order.point = 10&quot;})public class EnvironmentTests{ @Autowired public Environment environment; @Test public void testEnvValue(){ Assert.assertEquals(10, Integer.parseInt(environment.getProperty(&quot;springcss.order.point&quot;))); } }使用 Mock 测试 Service 层Service 层依赖于数据访问层。因此，对 Service 层进行测试时，还需要引入新的技术体系，也就是应用非常广泛的 Mock 机制。Mock 机制Mock 的意思是模拟，它可以用来对系统、组件或类进行隔离。在测试过程中，通常关注测试对象本身的功能和行为，而对测试对象涉及的一些依赖，仅仅关注它们与测试对象之间的交互（比如是否调用、何时调用、调用的参数、调用的次数和顺序，以及返回的结果或发生的异常等），并不关注这些被依赖对象如何执行这次调用的具体细节。因此，Mock 机制就是使用 Mock 对象替代真实的依赖对象，并模拟真实场景来开展测试工作。使用 Mock 对象完成依赖关系测试的示意图如下所示：从图中可以看出，在形式上，Mock 是在测试代码中直接 Mock 类和定义 Mock 方法的行为，通常测试代码和 Mock 代码放一起。因此，测试代码的逻辑从测试用例的代码上能很容易地体现出来。使用 Mock@SpringBootTest 注解中的 SpringBootTest.WebEnvironment.MOCK 选项，该选项用于加载 WebApplicationContext 并提供一个 Mock 的 Servlet 环境，内置的 Servlet 容器并没有真实启动。接下来，我们针对 Service 层演示一下这种测试方式。首先，先看一种简单场景，在 customer-service 中存在如下 CustomerTicketService 类：@Servicepublic class CustomerTicketService { @Autowired private CustomerTicketRepository customerTicketRepository; public CustomerTicket getCustomerTicketById(Long id) { return customerTicketRepository.getOne(id);    }    …}以上方法只是简单地通过 CustomerTicketRepository 完成了数据查询操作。显然，对以上 CustomerTicketService 进行集成测试时，还需要提供一个 CustomerTicketRepository 依赖。下面，通过以下代码演示一下如何使用 Mock 机制完成对 CustomerTicketRepository 的隔离。@RunWith(SpringRunner.class)@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.MOCK)public class CustomerServiceTests { @MockBean private CustomerTicketRepository customerTicketRepository; @Test public void testGetCustomerTicketById() throws Exception { Long id = 1L; Mockito.when(customerTicketRepository.getOne(id)).thenReturn(new CustomerTicket(1L, 1L, &quot;Order00001&quot;, &quot;DemoCustomerTicket1&quot;, new Date())); CustomerTicket actual = customerTicketService.getCustomerTicketById(id); assertThat(actual).isNotNull(); assertThat(actual.getOrderNumber()).isEqualTo(&quot;Order00001&quot;); }}首先，通过 @MockBean 注解注入了 CustomerTicketRepository；然后，基于第三方 Mock 框架 Mockito 提供的 when/thenReturn 机制完成了对 CustomerTicketRepository 中 getCustomerTicketById() 方法的 Mock。当然，如果希望在测试用例中直接注入真实的CustomerTicketRepository，这时就可以使用@SpringBootTest 注解中的 SpringBootTest.WebEnvironment.RANDOM_PORT 选项，示例代码如下：@RunWith(SpringRunner.class)@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)public class CustomerServiceTests {    @Autowired    private CustomerTicketRepository customerTicketRepository;     @Test    public void testGetCustomerTicketById() throws Exception {        Long id = 1L;        CustomerTicket actual = customerTicketService.getCustomerTicketById(id);        assertThat(actual).isNotNull();        assertThat(actual.getOrderNumber()).isEqualTo(&quot;Order00001&quot;);    }}运行上述代码后就会以一个随机的端口启动整个 Spring Boot 工程，并从数据库中真实获取目标数据进行验证。以上集成测试的示例中只包含了对 Repository 层的依赖，而有时候一个 Service 中可能同时包含 Repository 和其他 Service 类或组件，下面回到如下所示的 CustomerTicketService 类：@Servicepublic class CustomerTicketService { @Autowired private OrderClient orderClient; private OrderMapper getRemoteOrderByOrderNumber(String orderNumber) { return orderClient.getOrderByOrderNumber(orderNumber); } ...}从这里可以看到，在该代码中，除了依赖 CustomerTicketRepository 之外，还同时依赖了 OrderClient。请注意：以上代码中的 OrderClient 是在 customer-service 中通过 RestTemplate 访问 order-service 的远程实现类，其代码如下所示：@Componentpublic class OrderClient { @Autowired RestTemplate restTemplate; public OrderMapper getOrderByOrderNumber(String orderNumber) { logger.debug(&quot;Get order: {}&quot;, orderNumber); ResponseEntity&amp;lt;OrderMapper&amp;gt; restExchange = restTemplate.exchange( &quot;http://localhost:8083/orders/{orderNumber}&quot;, HttpMethod.GET, null, OrderMapper.class, orderNumber); OrderMapper result = restExchange.getBody(); return result; }}CustomerTicketService 类实际上并不关注 OrderClient 中如何实现远程访问的具体过程。因为对于集成测试而言，它只关注方法调用返回的结果，所以将同样采用 Mock 机制完成对 OrderClient 的隔离。对 CustomerTicketService 这部分功能的测试用例代码如下所示，可以看到，采用的是同样的测试方式。@Testpublic void testGenerateCustomerTicket() throws Exception { Long accountId = 100L; String orderNumber = &quot;Order00001&quot;; Mockito.when(this.orderClient.getOrderByOrderNumber(&quot;Order00001&quot;)) .thenReturn(new OrderMapper(1L, orderNumber, &quot;deliveryAddress&quot;)); Mockito.when(this.localAccountRepository.getOne(accountId)) .thenReturn(new LocalAccount(100L, &quot;accountCode&quot;, &quot;accountName&quot;)); CustomerTicket actual = customerTicketService.generateCustomerTicket(accountId, orderNumber);    assertThat(actual.getOrderNumber()).isEqualTo(orderNumber);}测试 Controller 层对 Controller 层进行测试之前，需要先来提供一个典型的 Controller 类，它来自 customer-service，如下代码所示：@RestController@RequestMapping(value=&quot;customers&quot;)public class CustomerController {    @Autowired    private CustomerTicketService customerTicketService;     @PostMapping(value = &quot;/{accountId}/{orderNumber}&quot;)    public CustomerTicket generateCustomerTicket( @PathVariable(&quot;accountId&quot;) Long accountId, @PathVariable(&quot;orderNumber&quot;) String orderNumber) { CustomerTicket customerTicket = customerTicketService.generateCustomerTicket(accountId, orderNumber); return customerTicket;    }}关于上述 Controller 类的测试方法，相对来说比较丰富，比如有 TestRestTemplate、@WebMvcTest 注解和 MockMvc 这三种。使用 TestRestTemplateSpring Boot 提供的 TestRestTemplae 与 RestTemplate 非常类似，只不过它专门用在测试环境中。如果想在测试环境中使用 @SpringBootTest，则可以直接使用 TestRestTemplate 来测试远程访问过程，示例代码如下：@RunWith(SpringRunner.class)@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)public class CustomerController2Tests { private TestRestTemplate testRestTemplate; public CustomerController2Tests(TestRestTemplate testRestTemplate) { this.testRestTemplate = testRestTemplate; } @MockBean private CustomerTicketService customerTicketService; public void testGenerateCustomerTicket() throws Exception { Long accountId = 100L; String orderNumber = &quot;Order00001&quot;; given(this.customerTicketService.generateCustomerTicket(accountId, orderNumber)) .willReturn(new CustomerTicket(1L, accountId, orderNumber, &quot;DemoCustomerTicket1&quot;, new Date())); CustomerTicket actual = testRestTemplate.postForObject(&quot;/customers/&quot; + accountId+ &quot;/&quot; + orderNumber, null, CustomerTicket.class); assertThat(actual.getOrderNumber()).isEqualTo(orderNumber); }}上述测试代码中： 首先，注意到 @SpringBootTest 注解通过使用 SpringBootTest.WebEnvironment.RANDOM_PORT 指定了随机端口的 Web 运行环境； 然后，基于 TestRestTemplate 发起了 HTTP 请求并验证了结果。特别说明：这里使用 TestRestTemplate 发起请求的方式与 RestTemplate 完全一致。使用 @WebMvcTest 注解@WebMvcTest，该注解将初始化测试 Controller 所必需的 Spring MVC 基础设施，CustomerController 类的测试用例如下所示：@RunWith(SpringRunner.class)@WebMvcTest(CustomerController.class)public class CustomerControllerTestsWithMockMvc { @Autowired private MockMvc mvc; @MockBean private CustomerTicketService customerTicketService; @Test public void testGenerateCustomerTicket() throws Exception { Long accountId = 100L; String orderNumber = &quot;Order00001&quot;; given(this.customerTicketService.generateCustomerTicket(accountId, orderNumber)) .willReturn(new CustomerTicket(1L, 100L, &quot;Order00001&quot;, &quot;DemoCustomerTicket1&quot;, new Date())); this.mvc.perform(post(&quot;/customers/&quot; + accountId+ &quot;/&quot; + orderNumber).accept(MediaType.APPLICATION_JSON)).andExpect(status().isOk()); }}以上代码的关键是 MockMvc 工具类，所以接下来我们有必要对它进一步展开说明。MockMvc 类提供的基础方法分为以下 6 种，下面一一对应来看下： Perform：执行一个 RequestBuilder 请求，会自动执行 SpringMVC 流程并映射到相应的 Controller 进行处理； get/post/put/delete：声明发送一个 HTTP 请求的方式，根据 URI 模板和 URI 变量值得到一个 HTTP 请求，支持 GET、POST、PUT、DELETE 等 HTTP 方法； param：添加请求参数，发送 JSON 数据时将不能使用这种方式，而应该采用 @ResponseBody 注解； andExpect：添加 ResultMatcher 验证规则，通过对返回的数据进行判断来验证 Controller 执行结果是否正确； andDo：添加 ResultHandler 结果处理器，比如调试时打印结果到控制台； andReturn：最后返回相应的 MvcResult，然后执行自定义验证或做异步处理。执行该测试用例后，从输出的控制台日志中不难发现，整个流程相当于启动了 CustomerController 并执行远程访问，而 CustomerController 中使用的 CustomerTicketService 则做了 Mock。显然，测试 CustomerController 的目的在于验证其返回数据的格式和内容。在上述代码中： 先定义了 CustomerController 将会返回的 JSON 结果； 然后通过 perform、accept 和 andExpect 方法模拟了 HTTP 请求的整个过程，最终验证了结果的正确性。使用 @AutoConfigureMockMvc 注解需要注意的是：@SpringBootTest 注解不能和 @WebMvcTest 注解同时使用。在使用 @SpringBootTest 注解的场景下，如果想使用 MockMvc 对象，那么可以引入 @AutoConfigureMockMvc 注解。通过将 @SpringBootTest 注解与 @AutoConfigureMockMvc 注解相结合，@AutoConfigureMockMvc 注解将通过 @SpringBootTest 加载的 Spring 上下文环境中自动配置 MockMvc 这个类。使用 @AutoConfigureMockMvc 注解的测试代码如下所示：@RunWith(SpringRunner.class)@SpringBootTest@AutoConfigureMockMvcpublic class CustomerControllerTestsWithAutoConfigureMockMvc { @Autowired private MockMvc mvc; @MockBean private CustomerTicketService customerTicketService; @Test public void testGenerateCustomerTicket() throws Exception { Long accountId = 100L; String orderNumber = &quot;Order00001&quot;; given(this.customerTicketService.generateCustomerTicket(accountId, orderNumber)) .willReturn(new CustomerTicket(1L, 100L, &quot;Order00001&quot;, &quot;DemoCustomerTicket1&quot;, new Date())); this.mvc.perform(post(&quot;/customers/&quot; + accountId+ &quot;/&quot; + orderNumber).accept(MediaType.APPLICATION_JSON)).andExpect(status().isOk()); }}在使用 @SpringBootTest 注解的场景下，如果想使用** MockMvc 对象**，那么可以引入 @AutoConfigureMockMvc 注解。通过将 @SpringBootTest 注解与 @AutoConfigureMockMvc注解相结合，@AutoConfigureMockMvc 注解将通过 @SpringBootTest 加载的 Spring 上下文环境中自动配置 MockMvc 这个类。使用 @AutoConfigureMockMvc 注解的测试代码如下所示：@RunWith(SpringRunner.class)@SpringBootTest@AutoConfigureMockMvcpublic class CustomerControllerTestsWithAutoConfigureMockMvc { @Autowired private MockMvc mvc; @MockBean private CustomerTicketService customerTicketService; @Test public void testGenerateCustomerTicket() throws Exception { Long accountId = 100L; String orderNumber = &quot;Order00001&quot;; given(this.customerTicketService.generateCustomerTicket(accountId, orderNumber)) .willReturn(new CustomerTicket(1L, 100L, &quot;Order00001&quot;, &quot;DemoCustomerTicket1&quot;, new Date())); this.mvc.perform(post(&quot;/customers/&quot; + accountId+ &quot;/&quot; + orderNumber).accept(MediaType.APPLICATION_JSON)).andExpect(status().isOk()); }}在上述代码中，使用了 MockMvc 工具类完成了对 HTTP 请求的模拟，并基于返回状态验证了 Controller 层组件的正确性。Spring Boot 中的测试注解总结经常使用的测试注解如下：| 注解名称 | 注解描述 || ——————— | —————————————————- || @Test | JUit中使用的基础测试注解，用来表明需要执行的测试用例 || @RunWith | JUnt框架提供的用于设置测试运行器的基础注解 || @SpringBootTest | Spring Boot应用程序专用的测试注解 || @DataJpaTest | 专门用于测试关系型数据库的测试注解 || @MockBean | 用于实现Mock机制的测试注解 || @WebMvcTest | 在Web容器环境中嵌入MockMvc的注解 || @AutoConfigureMockMvc | 与@SpringBootTest组合嵌入MockMvc的注解 |" }, { "title": "使用 Spring 测试数据访问层组件", "url": "/posts/data-test/", "categories": "Spring", "tags": "SpringBoot, Data Access Layer", "date": "2018-04-29 09:32:00 +0000", "snippet": "对于 Web 应用程序而言，测试是一个难点，是经常被忽略的一套技术体系。一个应用程序中涉及数据层、服务层、Web 层，以及各种外部服务之间的交互关系。除了对各层组件的单元测试之外，还需要充分引入集成测试保证服务的正确性和稳定性。Spring Boot 中的测试解决方案和 Spring Boot 1.x 版本一样，Spring Boot 2.x 也提供了一个用于测试的 spring-boot-starter-test 组件。在 Spring Boot 中，集成该组件的方法是在 pom 文件中添加如下所示依赖：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-test&amp;lt;/artifactId&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.junit.platform&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;junit-platform-launcher&amp;lt;/artifactId&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;其中，最后一个依赖用于导入与 JUnit 相关的功能组件。然后，通过 Maven 查看 spring-boot-starter-test 组件的依赖关系，可以得到如下所示的组件依赖图：从上图中可以看到，在代码工程的构建路径中，引入了一系列组件初始化测试环境。比如 JUnit、JSON Path、AssertJ、Mockito、Hamcrest 等： JUnit：JUnit 是一款非常流行的基于 Java 语言的单元测试框架，在我们的课程中主要使用该框架作为基础的测试框架； JSON Path：类似于 XPath 在 XML 文档中的定位，JSON Path 表达式通常用来检索路径或设置 JSON 文件中的数据； AssertJ：AssertJ 是一款强大的流式断言工具，它需要遵守 3A 核心原则，即 Arrange（初始化测试对象或准备测试数据）——&amp;gt; Actor（调用被测方法）——&amp;gt;Assert（执行断言）； Mockito：Mockito 是 Java 世界中一款流行的 Mock 测试框架，它主要使用简洁的 API 实现模拟操作。在实施集成测试时，将大量使用到这个框架； Hamcrest：Hamcrest 提供了一套匹配器（Matcher），其中每个匹配器的设计用于执行特定的比较操作； JSONassert：JSONassert 是一款专门针对 JSON 提供的断言框架； Spring Test &amp;amp; Spring Boot Test：为 Spring 和 Spring Boot 框架提供的测试工具。以上组件的依赖关系都是自动导入，无须做任何变动。而对于某些特定场景而言，就需要手工导入一些组件以满足测试需求，例如引入专用针对测试场景的嵌入式关系型数据库 H2。测试 Spring Boot 应用程序导入 spring-boot-starter-test 依赖后，就可以使用它提供的各项功能应对复杂的测试场景了。初始化测试环境对于 Spring Boot 应用程序而言，其 Bootstrap 类中的 main() 入口将通过 SpringApplication.run() 方法启动 Spring 容器，如下所示的 CustomerApplication 类就是一个典型的 Spring Boot 启动类 ：@SpringBootApplicationpublic class CustomerApplication { public static void main(String[] args) { SpringApplication.run(CustomerApplication.class, args); }}针对上述 Bootstrap 类，可以通过编写测试用例的方式，验证 Spring 容器能否正常启动。为了添加测试用例，有必要梳理一下代码的组织结构，梳理完后就呈现了如下图所示的 customer-service 工程中代码的基本目录结构。基于 Maven 的默认风格，将在 src/test/java 和 src/test/resources 包下添加各种测试用例代码和配置文件，正如上图所示。打开上图中的 ApplicationContextTests.java 文件，可以得到如下所示的测试用例代码：@SpringBootTest@RunWith(SpringRunner.class)public class ApplicationContextTests {    @Autowired    private ApplicationContext applicationContext;     @Test    public void testContextLoads() throws Throwable {        Assert.assertNotNull(this.applicationContext);    }}该代码中的 testContextLoaded() 就是一个有效的测试用例，这里可以看到该用例对 Spring 中的 ApplicationContext 作了简单非空验证。执行该测试用例后，从输出的控制台信息中，可以看到 Spring Boot 应用程序被正常启动，同时测试用例本身也会给出执行成功的提示。上述测试用例虽然简单，但是已经包含了测试 Spring Boot 应用程序的基本代码框架。其中，最重要的是 ApplicationContextTests 类上的 @SpringBootTest 和 @RunWith 注解，对于 Spring Boot 应用程序而言，这两个注解构成了一套完成的测试方案。@SpringBootTest 注解因为 SpringBoot 程序的入口是 Bootstrap 类，所以 SpringBoot 专门提供了一个 @SpringBootTest 注解测试 Bootstrap 类。同时 @SpringBootTest 注解也可以引用 Bootstrap 类的配置，因为所有配置都会通过 Bootstrap 类去加载。在上面的例子中，是通过直接使用 @SpringBootTest 注解提供的默认功能对作为 Bootstrap 类的 Application 类进行测试。而更常见的做法是在 @SpringBootTest 注解中指定该 Bootstrap 类，并设置测试的 Web 环境，如下代码所示：@SpringBootTest(classes = CustomerApplication.class, webEnvironment = SpringBootTest.WebEnvironment.MOCK)在以上代码中，@SpringBootTest 注解中的 webEnvironment 可以有四个选项，分别是 MOCK、RANDOM_PORT、DEFINED_PORT和NONE。 MOCK：加载 WebApplicationContext 并提供一个 Mock 的 Servlet 环境，此时内置的 Servlet 容器并没有正式启动； RANDOM_PORT：加载 EmbeddedWebApplicationContext 并提供一个真实的 Servlet 环境，然后使用一个随机端口启动内置容器； DEFINED_PORT：这个配置也是通过加载 EmbeddedWebApplicationContext 提供一个真实的 Servlet 环境，但使用的是默认端口，如果没有配置端口就使用 8080； NONE：加载 ApplicationContext 但并不提供任何真实的 Servlet 环境。在 Spring Boot 中，@SpringBootTest 注解主要用于测试基于自动配置的 ApplicationContext，它允许设置测试上下文中的 Servlet 环境。在多数场景下，一个真实的 Servlet 环境对于测试而言过于重量级，通过 MOCK 环境则可以缓解这种环境约束所带来的成本和挑战。@RunWith 注解与 SpringRunner还看到一个由 JUnit 框架提供的 @RunWith 注解，它用于设置测试运行器。例如，可以通过 @RunWith(SpringJUnit4ClassRunner.class) 让测试运行于 Spring 测试环境。虽然指定的是 SpringRunner.class，实际上，SpringRunner 就是 SpringJUnit4ClassRunner 的简化，它允许 JUnit 和 Spring TestContext 整合运行，而 Spring TestContext 则提供了用于测试 Spring 应用程序的各项通用的支持功能。在后续的测试用例中，大量使用 SpringRunner。执行测试用例单元测试的应用场景是一个独立的类，如下所示的 CustomerTicket 类就是一个非常典型的独立类：@Entity@Table(name = &quot;customer_ticket&quot;)public class CustomerTicket { @Id @GeneratedValue private Long id; private Long accountId; private String orderNumber; private String description; private Date createTime; public CustomerTicket(Long accountId, String orderNumber) { super(); Assert.notNull(accountId, &quot;Account Id must not be null&quot;); Assert.notNull(orderNumber, &quot;Order Number must not be null&quot;); Assert.isTrue(orderNumber.length() == 10, &quot;Order Number must be exactly 10 characters&quot;); this.accountId = accountId; this.orderNumber = orderNumber; } ......}从中可以看到，该类对客服工单做了封装，并在其构造函数中添加了校验机制。先来看看如何对正常场景进行测试。例如 CustomerTicket 中orderNumber 的长度问题，可以使用如下测试用例，通过在构造函数中传入字符串来验证规则的正确性：@RunWith(SpringRunner.class)public class CustomerTicketTests { private static final String ORDER_NUMBER = &quot;Order00001&quot;; @Test public void testOrderNumberIsExactly10Chars() throws Exception { CustomerTicket customerTicket = new CustomerTicket(100L, ORDER_NUMBER); assertThat(customerTicket.getOrderNumber().toString()).isEqualTo(ORDER_NUMBER); }}执行这个单元测试后，就可以看到执行的过程及结果。而这些单元测试用例只是演示了最基本的测试方式，后续的各种测试机制将在此基础上进行扩展和演化。使用 @DataJpaTest 注解测试数据访问组件首先，使用关系型数据库的场景，并引入针对 JPA 数据访问技术的 @DataJpaTest 注解。@DataJpaTest 注解会自动注入各种 Repository 类，并初始化一个内存数据库和及访问该数据库的数据源。在测试场景下，一般可以使用 H2 作为内存数据库，并通过 MySQL 实现数据持久化，因此需要引入以下所示的 Maven 依赖：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.h2database&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;h2&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;mysql&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;mysql-connector-java&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;另一方面，需要准备数据库 DDL 用于初始化数据库表，并提供 DML 脚本完成数据初始化。其中，schema-mysql.sql 和 data-h2.sql 脚本分别充当了 DDL 和 DML 的作用。在 customer-service 的 schema-mysql.sql 中包含了 CUSTOMER 表的创建语句，如下代码所示：DROP TABLE IF EXISTS `customerticket`;create table `customerticket` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `account_id` bigint(20) not null, `order_number` varchar(50) not null, `description` varchar(100) not null, `create_time` timestamp not null DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`));在 data-h2.sql 中，插入了一条测试需要使用的数据，具体的初始化数据过程如下代码所示：INSERT INTO customerticket (`account_id`, `order_number`,`description`) values (1, &#39;Order00001&#39;, &#39; DemoCustomerTicket1&#39;);接下来是提供具体的 Repository 接口，先通过如下所示代码回顾一下 CustomerRepository 接口的定义：public interface CustomerTicketRepository extends JpaRepository&amp;lt;CustomerTicket, Long&amp;gt; { List&amp;lt;CustomerTicket&amp;gt; getCustomerTicketByOrderNumber(String orderNumber);}这里存在一个方法名衍生查询 getCustomerTicketByOrderNumber，它会根据 OrderNumber 获取 CustomerTicket。基于上述 CustomerRepository，可以编写如下所示的测试用例：@RunWith(SpringRunner.class)@DataJpaTestpublic class CustomerRepositoryTest { @Autowired private TestEntityManager entityManager; @Autowired private CustomerTicketRepository customerTicketRepository; @Test public void testFindCustomerTicketById() throws Exception { this.entityManager.persist(new CustomerTicket(1L, &quot;Order00001&quot;, &quot;DemoCustomerTicket1&quot;, new Date())); CustomerTicket customerTicket = this.customerTicketRepository.getOne(1L); assertThat(customerTicket).isNotNull(); assertThat(customerTicket.getId()).isEqualTo(1L); } @Test public void testFindCustomerTicketByOrderNumber() throws Exception { String orderNumber = &quot;Order00001&quot;; this.entityManager.persist(new CustomerTicket(1L, orderNumber, &quot;DemoCustomerTicket1&quot;, new Date())); this.entityManager.persist(new CustomerTicket(2L, orderNumber, &quot;DemoCustomerTicket2&quot;, new Date())); List&amp;lt;CustomerTicket&amp;gt; customerTickets = this.customerTicketRepository.getCustomerTicketByOrderNumber(orderNumber); assertThat(customerTickets).size().isEqualTo(2); CustomerTicket actual = customerTickets.get(0); assertThat(actual.getOrderNumber()).isEqualTo(orderNumber); } @Test public void testFindCustomerTicketByNonExistedOrderNumber() throws Exception { this.entityManager.persist(new CustomerTicket(1L, &quot;Order00001&quot;, &quot;DemoCustomerTicket1&quot;, new Date())); this.entityManager.persist(new CustomerTicket(2L, &quot;Order00002&quot;, &quot;DemoCustomerTicket2&quot;, new Date())); List&amp;lt;CustomerTicket&amp;gt; customerTickets = this.customerTicketRepository.getCustomerTicketByOrderNumber(&quot;Order00003&quot;); assertThat(customerTickets).size().isEqualTo(0); }}这里可以看到，使用了 @DataJpaTest 实现 CustomerRepository 的注入。同时，还注意到另一个核心测试组件 TestEntityManager，它的效果相当于不使用真正的 CustomerRepository 完成数据的持久化，从而提供了一种数据与环境之间的隔离机制。执行这些测试用例后，需要关注它们的控制台日志输入，其中核心日志如下所示（为了显示做了简化处理）：Hibernate: drop table customer_ticket if existsHibernate: drop sequence if exists hibernate_sequenceHibernate: create sequence hibernate_sequence start with 1 increment by 1Hibernate: create table customer_ticket (id bigint not null, account_id bigint, create_time timestamp, description varchar(255), order_number varchar(255), primary key (id))Hibernate: create table localaccount (id bigint not null, account_code varchar(255), account_name varchar(255), primary key (id))…Hibernate: call next value for hibernate_sequenceHibernate: call next value for hibernate_sequenceHibernate: insert into customer_ticket (account_id, create_time, description, order_number, id) values (?, ?, ?, ?, ?)Hibernate: insert into customer_ticket (account_id, create_time, description, order_number, id) values (?, ?, ?, ?, ?)Hibernate: select customerti0_.id as id1_0_, customerti0_.account_id as account_2_0_, customerti0_.create_time as create_t3_0_, customerti0_.description as descript4_0_, customerti0_.order_number as order_nu5_0_ from customer_ticket customerti0_ where customerti0_.order_number=?…Hibernate: drop table customer_ticket if existsHibernate: drop sequence if exists hibernate_sequence从以上日志中，不难看出执行各种 SQL 语句的效果，此时也可以修改这些测试用例，并观察执行结果。" }, { "title": "使用 JdbcTemplate 访问关系型数据库", "url": "/posts/jdbc-template/", "categories": "Spring", "tags": "SpringBoot, JdbcTemplate", "date": "2018-04-20 09:32:00 +0000", "snippet": "JDBC 规范是 Java 领域中使用最广泛的数据访问标准，目前市面上主流的数据访问框架都是构建在 JDBC 规范之上。因为 JDBC 是偏底层的操作规范，所以关于如何使用 JDBC 规范进行关系型数据访问的实现方式有很多（区别在于对 JDBC 规范的封装程度不同），而在 Spring 中，同样提供了 JdbcTemplate 模板工具类实现数据访问，它简化了 JDBC 规范的使用方法。数据模型和 Repository 层设计以 spring-css 为例子演示。一个订单中往往涉及一个或多个商品，所以在 spring-css，主要通过一对多的关系来展示数据库设计和实现方面的技巧。为了使描述更简单，把具体的业务字段做了简化。Order 类的定义如下代码所示：public class Order{ private Long id; private String orderNumber; private String deliveryAddress; private List&amp;lt;Goods&amp;gt; goodsList; // 省略了 getter/setter}其中代表商品的 Goods 类定义如下：public class Goods { private Long id; private String goodsCode; private String goodsName; private Double price; // 省略了 getter/setter}从以上代码，不难看出一个订单可以包含多个商品，因此设计关系型数据库表时，我们首先会构建一个中间表来保存 Order 和 Goods 这层一对多关系。在 spring-css，使用 MySQL 作为关系型数据库，对应的数据库 Schema 定义如下代码所示：DROP TABLE IF EXISTS `order`;DROP TABLE IF EXISTS `goods`;DROP TABLE IF EXISTS `order_goods`;create table `order` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `order_number` varchar(50) not null, `delivery_address` varchar(100) not null, `create_time` timestamp not null DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`));create table `goods` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `goods_code` varchar(50) not null, `goods_name` varchar(50) not null, `price` double not null, `create_time` timestamp not null DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`));create table `order_goods` ( `order_id` bigint(20) not null, `goods_id` bigint(20) not null, foreign key(`order_id`) references `order`(`id`), foreign key(`goods_id`) references `goods`(`id`));基于以上数据模型，将完成 order-server 中的 Repository 层组件的设计和实现。首先，设计一个 OrderRepository 接口，用来抽象数据库访问的入口，如下代码所示：public interface IOrderRepository { Order addOrder(Order order); Order getOrderById(Long orderId); Order getOrderDetailByOrderNumber(String orderNumber);}这个接口非常简单，方法都是自解释的。不过请注意，这里的 OrderRepository 并没有继承任何父接口，完全是一个自定义的、独立的 Repository。针对上述 OrderRepository 中的接口定义，我构建了一系列的实现类: OrderRawJdbcRepositoryImpl：使用原生 JDBC 进行数据库访问 OrderJdbcRepositoryImpl：使用 JdbcTemplate 进行数据库访问 IOrderJpaRepositoryImpl：使用 Spring Data JPA 进行数据库访问OrderRawJdbcRepositoryImpl 类中实现方式如下代码所示：/** * 使用原生 JDBC 进行数据库访问 */@Repository(&quot;orderRawJdbcRepository&quot;)public class OrderRawJdbcRepositoryImpl implements IOrderRepository { @Autowired private DataSource dataSource; @Override public Order addOrder(Order order) { // 不做实现 return null; } @Override public Order getOrderById(Long orderId) { Connection connection = null; PreparedStatement statement = null; ResultSet resultSet = null; try { connection = dataSource.getConnection(); statement = connection.prepareStatement(&quot;select id, order_number, delivery_address from `order` where id=?&quot;); statement.setLong(1, orderId); resultSet = statement.executeQuery(); Order order = null; if (resultSet.next()) { order = new Order(resultSet.getLong(&quot;id&quot;), resultSet.getString(&quot;order_number&quot;), resultSet.getString(&quot;delivery_address&quot;)); } return order; } catch (SQLException e) { System.out.print(e); } finally { if (resultSet != null) { try { resultSet.close(); } catch (SQLException e) { } } if (statement != null) { try { statement.close(); } catch (SQLException e) { } } if (connection != null) { try { connection.close(); } catch (SQLException e) { } } } return null; } @Override public Order getOrderDetailByOrderNumber(String orderNumber) { // 不做实现 return null; }}这里，值得注意的是，首先需要在类定义上添加 @Repository 注解，标明这是能够被 Spring 容器自动扫描的 Javabean，再在 @Repository 注解中指定这个 Javabean 的名称为”orderRawJdbcRepository”，方便 Service 层中根据该名称注入 OrderRawJdbcRepository 类。可以看到，上述代码使用了 JDBC 原生 DataSource、Connection、PreparedStatement、ResultSet 等核心编程对象完成针对“order”表的一次查询。代码流程看起来比较简单，其实也比较烦琐，学到这里，我们可以结合上一课时的内容理解上述代码。请注意，如果想运行这些代码，千万别忘了在 Spring Boot 的配置文件中添加对 DataSource 的定义。使用 JdbcTemplate 操作数据库首先引入对它的依赖，如下代码所示：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-data-jpa&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;JdbcTemplate 提供了一系列的 query、update、execute 重载方法应对数据的 CRUD 操作。使用 JdbcTemplate 实现查询基于 SpringCSS ，最简单的查询操作，对 OrderRawJdbcRepository 中的 getOrderById 方法进行重构。为此，构建了一个新的 OrderJdbcRepositoryImpl 类并同样实现了 OrderRepository 接口，如下代码所示：package cn.happymaya.order.repository.impl;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;import java.sql.Statement;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;import cn.happymaya.order.domain.Goods;import cn.happymaya.order.domain.Order;import cn.happymaya.order.repository.IOrderRepository;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.dao.DataAccessException;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.core.PreparedStatementCreator;import org.springframework.jdbc.core.ResultSetExtractor;import org.springframework.jdbc.core.simple.SimpleJdbcInsert;import org.springframework.jdbc.support.GeneratedKeyHolder;import org.springframework.jdbc.support.KeyHolder;import org.springframework.stereotype.Repository;/** * 使用 JdbcTemplate 进行数据库访问 */@Repository(&quot;orderJdbcRepository&quot;)public class OrderJdbcRepositoryImpl implements IOrderRepository { private JdbcTemplate jdbcTemplate; @Autowired public OrderJdbcRepositoryImpl(JdbcTemplate jdbcTemplate) { // 通过构造函数注入了 JdbcTemplate 模板类 this.jdbcTemplate = jdbcTemplate; }}OrderJdbcRepository 的 getOrderById 方法实现过程如下代码所示：@Overridepublic Order getOrderById(Long orderId) { return jdbcTemplate.queryForObject(&quot;select id, order_number, delivery_address from `order` where id=?&quot;, this::mapRowToOrder, orderId);}这里使用了 JdbcTemplate 的 queryForObject 方法执行查询操作，该方法传入目标 SQL、参数以及一个 RowMapper 对象。其中 RowMapper 定义如下：package org.springframework.jdbc.core;import java.sql.ResultSet;import java.sql.SQLException;import org.springframework.lang.Nullable;@FunctionalInterfacepublic interface RowMapper&amp;lt;T&amp;gt; { @Nullable T mapRow(ResultSet var1, int var2) throws SQLException;}从 mapRow 方法定义中，不难看出 RowMapper 的作用就是处理来自 ResultSet 中的每一行数据，并将来自数据库中的数据映射成领域对象。例如，使用 getOrderById 中用到的 mapRowToOrder 方法完成对 Order 对象的映射，如下代码所示：private Order mapRowToOrder(ResultSet rs, int rowNum) throws SQLException { return new Order(rs.getLong(&quot;id&quot;), rs.getString(&quot;order_number&quot;), rs.getString(&quot;delivery_address&quot;));}getOrderById 方法实际上只是获取了 Order 对象中的订单部分信息，并不包含商品数据。又设计一个 getOrderDetailByOrderNumber 方法，根据订单编号获取订单以及订单中所包含的所有商品信息，如下代码所示： @Override public Order getOrderDetailByOrderNumber(String orderNumber) { //获取Order基础信息 Order order = jdbcTemplate.queryForObject( &quot;select id, order_number, delivery_address from `order` where order_number=?&quot;, this::mapRowToOrder, orderNumber); if (order == null) return order; //获取Order与Goods之间的关联关系，找到给Order中的所有GoodsId Long orderId = order.getId(); List&amp;lt;Long&amp;gt; goodsIds = jdbcTemplate.query(&quot;select order_id, goods_id from order_goods where order_id=?&quot;, new ResultSetExtractor&amp;lt;List&amp;lt;Long&amp;gt;&amp;gt;() { public List&amp;lt;Long&amp;gt; extractData(ResultSet rs) throws SQLException, DataAccessException { List&amp;lt;Long&amp;gt; list = new ArrayList&amp;lt;Long&amp;gt;(); while (rs.next()) { list.add(rs.getLong(&quot;goods_id&quot;)); } return list; } }, orderId); //根据GoodsId分别获取Goods信息并填充到Order对象中 for (Long goodsId : goodsIds) { Goods goods = getGoodsById(goodsId); order.addGoods(goods); } return order; }使用 JdbcTemplate 实现插入通过 update 方法实现数据的插入和更新。针对 Order 和 Goods 中的关联关系，插入一个 Order 对象需要同时完成两张表的更新，即 order 表和 order_goods 表，因此插入 Order 的实现过程也分成两个阶段，如下代码所示的 addOrderWithJdbcTemplate 方法展示了这一过程：private Order addOrderDetailWithJdbcTemplate(Order order) { //插入Order基础信息 Long orderId = saveOrderWithJdbcTemplate(order); order.setId(orderId); //插入Order与Goods的对应关系 List&amp;lt;Goods&amp;gt; goodsList = order.getGoods(); for (Goods goods : goodsList) { saveGoodsToOrderWithJdbcTemplate(goods, orderId); } return order;}这里同样先是插入 Order 的基础信息，然后再遍历 Order 中的 Goods 列表并逐条进行插入。其中的 saveOrderWithJdbcTemplate 方法如下代码所示：private Long saveOrderWithJdbcTemplate(Order order) { PreparedStatementCreator psc = new PreparedStatementCreator() { @Override public PreparedStatement createPreparedStatement(Connection con) throws SQLException { PreparedStatement ps = con.prepareStatement( &quot;insert into `order` (order_number, delivery_address) values (?, ?)&quot;, Statement.RETURN_GENERATED_KEYS); s.setString(1, order.getOrderNumber()); ps.setString(2, order.getDeliveryAddress()); return ps; } }; KeyHolder keyHolder = new GeneratedKeyHolder(); jdbcTemplate.update(psc, keyHolder); return keyHolder.getKey().longValue();}上述 saveOrderWithJdbcTemplate 的方法比想象中要复杂，主要原因在于我们需要在插入 order 表的同时返回数据库中所生成的自增主键，因此，这里使用了 PreparedStatementCreator 工具类封装 PreparedStatement 对象的构建过程，并在 PreparedStatement 的创建过程中设置了 Statement.RETURN_GENERATED_KEYS 用于返回自增主键。然后我们构建了一个 GeneratedKeyHolder 对象用于保存所返回的自增主键。这是使用 JdbcTemplate 实现带有自增主键数据插入的一种标准做法，你可以参考这一做法并应用到日常开发过程中。至于用于插入 Order 与 Goods 关联关系的 saveGoodsToOrderWithJdbcTemplate 方法就比较简单了，直接调用 JdbcTemplate 的 update 方法插入数据即可，如下代码所示：private void saveGoodsToOrderWithJdbcTemplate(Goods goods, long orderId) { jdbcTemplate.update(&quot;insert into order_goods (order_id, goods_id) &quot; + &quot;values (?, ?)&quot;, orderId, goods.getId());}需要实现插入 Order 的整个流程，先实现 Service 类和 Controller 类，如下代码所示：@Servicepublic class OrderService {    @Autowired    @Qualifier(&quot;orderJdbcRepository&quot;)    private OrderRepository orderRepository;    public Order addOrder(Order order) {        return orderRepository.addOrder(order);    } }@RestController@RequestMapping(value=&quot;orders&quot;)public class OrderController {    @RequestMapping(value = &quot;&quot;, method = RequestMethod.POST)    public Order addOrder(@RequestBody Order order) {        Order result = orderService.addOrder(order);    return result;    }}通过 Postman 向http://localhost:8081/orders端点发起 Post 请求后，可以发现 order 表和 order_goods 表中的数据都已经正常插入。使用 SimpleJdbcInsert 简化数据插入过程虽然通过 JdbcTemplate 的 update 方法可以完成数据的正确插入，不禁发现这个实现过程还是比较复杂，尤其是涉及自增主键的处理时，代码显得有点臃肿。Spring Boot 针对数据插入场景专门提供了一个 SimpleJdbcInsert 工具类，SimpleJdbcInsert 本质上是在 JdbcTemplate 的基础上添加了一层封装，提供了一组 execute、executeAndReturnKey 以及 executeBatch 重载方法来简化数据插入操作。可以在 Repository 实现类的构造函数中对 SimpleJdbcInsert 进行初始化，如下代码所示:/** * 使用 JdbcTemplate 进行数据库访问 */@Repository(&quot;orderJdbcRepository&quot;)public class OrderJdbcRepositoryImpl implements IOrderRepository { private JdbcTemplate jdbcTemplate; private SimpleJdbcInsert orderInserter; private SimpleJdbcInsert orderGoodsInserter; @Autowired public OrderJdbcRepositoryImpl(JdbcTemplate jdbcTemplate) { this.jdbcTemplate = jdbcTemplate; this.orderInserter = new SimpleJdbcInsert(jdbcTemplate).withTableName(&quot;`order`&quot;).usingGeneratedKeyColumns(&quot;id&quot;); this.orderGoodsInserter = new SimpleJdbcInsert(jdbcTemplate).withTableName(&quot;order_goods&quot;); }}可以看到，这里首先注入了一个 JdbcTemplate 对象，然后我们基于 JdbcTemplate 并针对 order 表和 order_goods 表分别初始化了两个 SimpleJdbcInsert 对象 orderInserter 和 orderGoodsInserter。其中 orderInserter 中还使用了 usingGeneratedKeyColumns 方法设置自增主键列。基于 SimpleJdbcInsert，完成 Order 对象的插入就非常简单了，实现方式如下所示：private Long saveOrderWithSimpleJdbcInsert(Order order) { Map&amp;lt;String, Object&amp;gt; values = new HashMap&amp;lt;String, Object&amp;gt;(); values.put(&quot;order_number&quot;, order.getOrderNumber()); values.put(&quot;delivery_address&quot;, order.getDeliveryAddress()); Long orderId = orderInserter.executeAndReturnKey(values).longValue(); return orderId;}通过构建一个 Map 对象，然后把需要添加的字段设置成一个个键值对。通过SimpleJdbcInsert 的 executeAndReturnKey 方法在插入数据的同时直接返回自增主键。同样，完成 order_goods 表的操作只需要几行代码就可以了，如下代码所示：private void saveGoodsToOrderWithSimpleJdbcInsert(Goods goods, long orderId) { Map&amp;lt;String, Object&amp;gt; values = new HashMap&amp;lt;&amp;gt;(); values.put(&quot;order_id&quot;, orderId); values.put(&quot;goods_id&quot;, goods.getId()); orderGoodsInserter.execute(values);}这里用到了 SimpleJdbcInsert 提供的 execute 方法，可以把这些方法组合起来对 addOrderDetailWithJdbcTemplate 方法进行重构，从而得到如下所示的 addOrderDetailWithSimpleJdbcInsert 方法：// addOrderDetailWithSimpleJdbcInsertprivate Order addOrderDetailWithSimpleJdbcInsert(Order order) { //插入Order基础信息 Long orderId = saveOrderWithSimpleJdbcInsert(order); order.setId(orderId); //插入Order与Goods的对应关系 List&amp;lt;Goods&amp;gt; goodsList = order.getGoods(); for (Goods goods : goodsList) { saveGoodsToOrderWithSimpleJdbcInsert(goods, orderId); } return order;}在使用 JdbcTemplate 时，如果想要返回数据库的自增主键值有哪些实现方法？JdbcTemplate 模板工具类是一个基于 JDBC 规范实现数据访问的强大工具，是一个优秀的工具类（本质上就是对 JDBC 的封装，也可以算是一种半自动化的 ORM 技术，谈不上是对 Mybatis 的替代方案，只是 Spring 提供的一个工具类）。它对常见的 CRUD 操作做了封装并提供了一大批简化的 API。" }, { "title": "JdbcTemplate 数据访问实现原理", "url": "/posts/jdbc-template-yuanli/", "categories": "Spring", "tags": "SpringBoot, JdbcTemplate", "date": "2018-04-15 09:32:00 +0000", "snippet": "通过 JdbcTemplate 不仅简化了数据库操作，还避免了使用原生 JDBC 带来的代码复杂度和冗余性问题。那么，JdbcTemplate 在 JDBC 基础上如何实现封装的呢？从模板方法模式和回调机制说起从命名上 JdbcTemplate 显然是一种模板类，这就不禁联想起设计模式中的模板方法模式。模板方法设计模式模板方法模式的原理非常简单，它主要是利用了面向对象中类的继承机制，目前应用非常广泛的，且在实现上往往与抽象类一起使用，比如 Spring 框架中也大量应用了模板方法实现基类和子类之间的职责分离和协作。按照定义，完成一系列步骤时，这些步骤需要遵循统一的工作流程，个别步骤的实现细节除外，这时就需要考虑使用模板方法模式处理了。模板方法模式的结构示意图如下所示：上图中，抽象模板类 AbstractClass 定义了一套工作流程，而具体实现类 ConcreteClassA 和 ConcreteClassB 对工作流程中的某些特定步骤进行了实现。回调机制在软件开发过程中，回调（Callback）是一种常见的实现技巧，回调的含义如下图所示：上图中： ClassA 的 operation1() 方法调用 ClassB 的 operation2() 方法； ClassB 的 operation2() 方法执行完毕再主动调用 ClassA 的 callback() 方法； 这就是回调机制，体现的是一种双向的调用方式。从上面描述可以看到，回调在任务执行过程中不会造成任何的阻塞，任务结果一旦就绪，回调就会被执行，显然在方法调用上这是一种异步执行的方式。同时，回调还是实现扩展性的一种简单而直接的模式。在上图中，看到执行回调时，代码会从一个类中的某个方法跳到另一个类中的某个方法，这种思想同样可以扩展到组件级别，即代码从一个组件跳转到另一个组件。只要预留回调的契约，原则上我们可以实现运行时根据调用关系动态来实现组件之间的跳转，从而满足扩展性的要求。事实上，JdbcTemplate 正是基于模板方法模式和回调机制，才真正解决了原生 JDBC 中的复杂性问题。接下来，我们结合 07 讲中给出的 SpringCSS 案例场景从 JDBC 的原生 API 出发，讨论 JdbcTemplate 的演进过程。JDBC API 到 JdbcTemplate 的演变JDBC 到 JdbcTemplate 的整个过程中，不难发现创建 DataSource、获取 Connection、创建 Statement 等步骤实际上都是重复的，只有处理 ResultSet 部分需要针对不同的 SQL 语句和结果进行定制化处理，因为每个结果集与业务实体之间的对应关系不同。这样，我们的思路就来了，首先我们想到的是如何构建一个抽象类实现模板方法。在 JDBC API 中添加模板方法模式假设将这个抽象类命名为 AbstractJdbcTemplate，那么该类的代码结构应该是这样的：public abstract class AbstractJdbcTemplate { @Autowired private DataSource dataSource; public final Object execute(String sql){ Connection connection = null; Statement statement = null; ResultSet resultSet = null; try { connection = dataSource.getConnection(); statement = connection.createStatement(); resultSet = statement.executeQuery(sql); Object object = handleResultSet(resultSet); return object; } catch (SQLException e) { System.out.print(e); } finally { if (resultSet != null) { try { resultSet.close(); } catch (SQLException e) { } } if (statement != null) { try { statement.close(); } catch (SQLException e) { } } if (connection != null) { try { connection.close(); } catch (SQLException e) { } } } return null; } protected abstract Object handleResultSet(ResultSet rs) throws SQLException; }AbstractJdbcTemplate 是一个抽象类，而 execute 方法主体代码我们都很熟悉，基本照搬了 07 所构建的 OrderRawJdbcRepository 类中的代码。唯一需要注意的是，这里出现了一个模板方法 handleResultSet 用来处理 ResultSet。再构建一个 AbstractJdbcTemplate 的实现类 OrderJdbcTemplate，如下代码所示：public class OrderJdbcTemplate extends AbstractJdbcTemplate { @Override protected Object handleResultSet(ResultSet rs) throws SQLException { List&amp;lt;Order&amp;gt; orders = new ArrayList&amp;lt;Order&amp;gt;(); while (rs.next()) { Order order = new Order(rs.getLong(&quot;id&quot;), rs.getString(&quot;order_number&quot;), rs.getString(&quot;delivery_address&quot;)); orders.add(order); } return orders; }}显然，这里获取了 ResultSet 中的结果，并构建了业务对象进行返回。再使用 OrderJdbcTemplate 执行目标 SQL 语句，如下代码所示：public class AbstractJdbcTemplateTest { @SuppressWarnings(&quot;unchecked&quot;) public void test() { AbstractJdbcTemplate jdbcTemplate = new OrderJdbcTemplate(); List&amp;lt;Order&amp;gt; orders = (List&amp;lt;Order&amp;gt;) jdbcTemplate.execute(&quot;select * from Order&quot;); }}就这样，一个添加了模板方法模式的 JdbcTemplate 就构建完成了，很简单。在 JDBC API 中添加回调机制如果需要对各种业务对象实现数据库操作，那势必需要提供各种类似 OrderJdbcTemplate 这样的实现类，这点显然很不方便。一方面需要创建和维护一批新类，另一方面如果抽象方法数量很多，子类就需要实现所有抽象方法，尽管有些方法中子类并不会用到，这时该如何解决呢？实际上，这种问题本质在于使用了抽象类。如果不想使用抽象类，则可以引入回调机制。使用回调机制的第一步，先定义一个回调接口来剥离业务逻辑，将其命名为 StatementCallback，如下代码所示：public interface StatementCallback { Object handleStatement(Statement statement) throws SQLException; }然后，重新创建一个新的 CallbackJdbcTemplate 用来执行数据库访问，如下代码所示:public class CallbackJdbcTemplate { @Autowired private DataSource dataSource; public final Object execute(StatementCallback callback){ Connection connection = null; Statement statement = null; ResultSet resultSet = null; try { connection = dataSource.getConnection(); statement = connection.createStatement(); Object object = callback.handleStatement(statement); return object; } catch (SQLException e) { System.out.print(e); } finally { if (resultSet != null) { try { resultSet.close(); } catch (SQLException e) { } } if (statement != null) { try { statement.close(); } catch (SQLException e) { } } if (connection != null) { try { connection.close(); } catch (SQLException e) { } } } return null; }}注意，与 AbstractJdbcTemplate 类相比，CallbackJdbcTemplate 存在两处差异点。 首先，CallbackJdbcTemplate 不是一个抽象类。 其次，execute 方法签名上传入的是一个 StatementCallback 对象。而具体的定制化处理是通过 Statement 传入到 Callback 对象中完成的，也可以认为是把原有需要子类抽象方法实现的功能转嫁到了 StatementCallback 对象上。基于 CallbackJdbcTemplate 和 StatementCallback，我们可以构建具体数据库访问的执行流程，如下代码所示： public Object queryOrder(final String sql) { class OrderStatementCallback implements StatementCallback { public Object handleStatement(Statement statement) throws SQLException { ResultSet rs = statement.executeQuery(sql); List&amp;lt;Order&amp;gt; orders = new ArrayList&amp;lt;Order&amp;gt;(); while (rs.next()) { Order order = new Order(rs.getLong(&quot;id&quot;), rs.getString(&quot;order_number&quot;), rs.getString(&quot;delivery_address&quot;)); orders.add(order); } return orders; } } CallbackJdbcTemplate jdbcTemplate = new CallbackJdbcTemplate(); return jdbcTemplate.execute(new OrderStatementCallback()); }这里，定义了一个 queryOrder 方法并传入 SQL 语句中用来实现对 Order 表的查询。注意到，在 queryOrder 方法中构建了一个 OrderStatementCallback 内部类，该类实现了 StatementCallback 接口并提供了具体操作 SQL 的定制化代码。然后我们创建了一个 CallbackJdbcTemplate 对象并将内部类 OrderStatementCallback 传入该对象的 execute 方法中。针对这种场景，实际上也可以不创建 OrderStatementCallback 内部类，因为该类只适用于这个场景中，此时更为简单的处理方法是使用匿名类，如下代码所示： public Object queryOrder2(final String sql) { CallbackJdbcTemplate jdbcTemplate = new CallbackJdbcTemplate(); return jdbcTemplate.execute(new StatementCallback() { public Object handleStatement(Statement statement) throws SQLException { ResultSet rs = statement.executeQuery(sql); List&amp;lt;Order&amp;gt; orders = new ArrayList&amp;lt;&amp;gt;(); while (rs.next()) { Order order = new Order(rs.getLong(&quot;id&quot;), rs.getString(&quot;order_number&quot;), rs.getString(&quot;delivery_address&quot;)); orders.add(order); } return orders; } }); }匿名类的实现方式比较简洁点，且在日常开发过程中，也经常使用这种方式实现回调接口。JdbcTemplate 源码理解了 JDBC API 到 JdbcTemplate 的演变过程，可以发现 Spring Boot 所提供的 JdbcTemplate 模板工具类的源码部分，同样采用了这种设计思路。JdbcTemplate 的 execute(StatementCallback action) 方法，如下代码所示：public &amp;lt;T&amp;gt; T execute(StatementCallback&amp;lt;T&amp;gt; action) throws DataAccessException {        Assert.notNull(action, &quot;Callback object must not be null&quot;);        Connection con = DataSourceUtils.getConnection(obtainDataSource());        Statement stmt = null;        try {            stmt = con.createStatement();            applyStatementSettings(stmt);            T result = action.doInStatement(stmt);            handleWarnings(stmt);            return result;        } catch (SQLException ex) {            String sql = getSql(action);            JdbcUtils.closeStatement(stmt);            stmt = null;            DataSourceUtils.releaseConnection(con, getDataSource());            con = null;            throw translateException(&quot;StatementCallback&quot;, sql, ex);        } finally {            JdbcUtils.closeStatement(stmt);            DataSourceUtils.releaseConnection(con, getDataSource());        }}从以上代码中可以看出，execute 方法中同样接收了一个 StatementCallback 回调接口，然后通过传入 Statement 对象完成 SQL 语句的执行，这与前面我们给出的实现方法完全一致。StatementCallback 回调接口定义代码如下：public interface StatementCallback&amp;lt;T&amp;gt; { T doInStatement(Statement stmt) throws SQLException, DataAccessException;}同样，发现 StatementCallback 回调接口的定义也很类似。我们来看看上述 execute(StatementCallback action) 方法的具体使用方法。事实上，在 JdbcTemplate 中，还存在另一个 execute(final String sql) 方法，该方法中恰恰使用了 execute(StatementCallback action) 方法，如下代码所示：public void execute(final String sql) throws DataAccessException {        if (logger.isDebugEnabled()) {            logger.debug(&quot;Executing SQL statement [&quot; + sql + &quot;]&quot;);        }        class ExecuteStatementCallback implements StatementCallback&amp;lt;Object&amp;gt;, SqlProvider {            @Override            @Nullable            public Object doInStatement(Statement stmt) throws SQLException {                stmt.execute(sql);                return null;            }            @Override            public String getSql() {                return sql;            }        }        execute(new ExecuteStatementCallback());}这里，同样采用了内部类的实现方式创建 StatementCallback 回调接口的实现类 ExecuteStatementCallback，然后通过传入 Statement 对象完成 SQL 语句的执行，最后通过调用 execute(StatementCallback action) 方法实现整个执行过程。从源码解析到日常开发无论是模板方法还是回调机制(板方法设计模式和回调机制的作用：可以预留抽象方法在子类中实现，也可以传入回调接口通过接口的不同实现完成)，在技术实现上都没有难度，有难度的是应用的场景以及对问题的抽象。JdbcTemplate 基于 JDBC 的原生 API，把模板方法和回调机制结合在了一起，为提供了简洁且高扩展的实现方案，值得分析和应用。" }, { "title": "使用 KafkaTemplate 集成 Kafka", "url": "/posts/kafka-template/", "categories": "Spring", "tags": "SpringBoot, KafkaTemplate, Kafka", "date": "2018-04-10 09:32:00 +0000", "snippet": "消息通信是 Web 应用程序中间层组件中的代表性技术体系，主要用于构建复杂而又灵活的业务流程。在互联网应用中，消息通信被认为是实现系统解耦和高并发的关键技术体系。本节课我们将在 SpringCSS 案例中引入消息通信机制来实现多个服务之间的异步交互。spring-css 案例中的消息通信场景在 spring-css 案例中，一个用户的账户信息变动并不会太频繁。因为 account-service 和 customer-service 分别位于两个服务中，为了降低远程交互的成本，很多时候会想到先在 customer-service 本地存放一份用户账户的拷贝信息，并在客户工单生成过程时直接从本地数据库中获取用户账户。在这样的设计和实现方式下，如果某个用户的账户信息发生变化，如何正确且高效地应对呢？此时消息驱动机制从系统扩展性角度提供了一种很好的实现方案。在用户账户信息变更时，account-service 首先会发送一个消息告知某个用户账户信息已经发生变化，然后通知所有对该消息感兴趣的服务。而在 spring-css 案例中，这个服务就是 customer-service，相当于是这个消息的订阅者和消费者。通过这种方式，customer-service 就可以快速获取用户账户变更消息，从而正确且高效地处理本地的用户账户数据。整个场景的示意图见下图：消息通信机制使得我们不必花费太大代价即可实现整个交互过程，简单而方便。消息通信机制消息通信机制的整体流程见下图所示：上图中位于流程中间的就是各种消息中间件，消息中间件一般提供了消息的发送客户端和消息接收客户端组件，这些客户端组件会嵌入业务服务中。消息的生产者负责产生消息，在实际业务中一般由业务系统充当生产者；而消息的消费者负责消费消息，在实际业务中一般是后台系统负责异步消费。消息通信有两种基本模型，即： 发布-订阅（Pub-Sub）模型，支持生产者消费者之间的一对多关系 点对点（Point to Point）模型，点对点模型中有且仅有一个消费者。上述概念构成了消息通信系统最基本的模型，围绕这个模型，业界已经有了一些实现规范和工具，代表性的规范有 JMS 、AMQP ，以及它们的实现框架 ActiveMQ 和 RabbitMQ 等，而 Kafka 等工具并不遵循特定的规范，但也提供了消息通信的设计和实现方案。与 JdbcTemplate 和 RestTemplate 类似，Spring Boot 作为一款支持快速开发的集成性框架，同样提供了一批以 -Template 命名的模板工具类用于实现消息通信。对于 Kafka 而言，这个工具类就是 KafkaTemplate。使用 KafkaTemplate 集成 Kafka在讨论如何使用 KafkaTemplate 实现与 Kafka 之间的集成方法之前，先来了解 Kafka 的基本架构，再引出 Kafka 中的几个核心概念。Kafka 基本架构Kafka 基本架构参考下图，从中可以看到 Broker、Producer、Consumer、Push、Pull 等消息通信系统常见概念在 Kafka 中都有所体现，生产者使用 Push 模式将消息发布到 Broker，而消费者使用 Pull 模式从 Broker 订阅消息。在上图中注意到，Kafka 架构图中还使用了 Zookeeper。Zookeeper 中存储了 Kafka 的元数据及消费者消费偏移量（Offset），其作用在于实现 Broker 和消费者之间的负载均衡。因此，如果想要运行 Kafka，首先需要启动 Zookeeper，再启动 Kafka 服务器。在 Kafka 中还存在 Topic 这么一个核心概念，它是 Kafka 数据写入操作的基本单元，每一个 Topic 可以存在多个副本（Replication）以确保其可用性。每条消息属于且仅属于一个 Topic，因此开发人员通过 Kafka 发送消息时，必须指定将该消息发布到哪个 Topic。同样，消费者订阅消息时，也必须指定订阅来自哪个 Topic 的信息。另一方面，从组成结构上讲，一个 Topic 中又可以包含一个或多个分区（Partition），因此在创建 Topic 时我们可以指定 Partition 个数。KafkaTemplate 是 Spring 中提供的基于 Kafka 完成消息通信的模板工具类，而要想使用这个模板工具类，我们必须在消息的发送者和消费者应用程序中都添加如下 Maven 依赖：&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-kafka&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;使用 KafkaTemplate 发送消息KafkaTemplate 提供了一系列 send 方法用来发送消息，典型的 send 方法定义如下代码所示：@Overridepublic ListenableFuture&amp;lt;SendResult&amp;lt;K, V&amp;gt;&amp;gt; send(String topic, @Nullable V data) {}在上述方法实际传入了两个参数： 一个是消息对应的 Topic； 另一个是消息体的内容。通过该方法，就能完成最基本的消息发送过程。请注意，在使用 Kafka 时，推荐事先创建好 Topic 供消息生产者和消费者使用， 通过命令行创建 Topic 的方法如下代码所示：bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 3 --topic springcss.account.topic这里创建了一个名为springcss.account.topic的 Topic，并指定它的副本数量和分区数量都是 3。事实上，在调用 KafkaTemplate 的 send 方法时，如果 Kafka 中不存在该方法中指定的 Topic，它就会自动创建一个新的 Topic。另一方面，KafkaTemplate 也提供了一组 sendDefault 方法，它通过使用默认的 Topic 来发送消息，如下代码所示:@Overridepublic ListenableFuture&amp;lt;SendResult&amp;lt;K, V&amp;gt;&amp;gt; sendDefault(V data) { return send(this.defaultTopic, data);}从代码中可以看到，在上述 sendDefault 方法内部中也是使用了 send 方法完成消息的发送过程。那么，如何指定这里的 defaultTopic 呢？在 Spring Boot 中，可以使用如下配置项完成这个工作：spring: kafka: bootstrap-servers: - localhost:9092 template: default-topic: springcss.account.topic现在，已经了解了通过 KafkaTemplate 发送消息的实现方式，KafkaTemplate 高度抽象了消息的发送过程，整个过程非常简单。接下来切换下视角，如何消费所发送的消息？使用 @KafkaListener 注解消费消息通过翻阅 KafkaTemplate 提供的类定义，并未找到有关接收消息的任何方法，这实际上与 Kafka 的设计思想有很大关系。这点也与 JmsTemplate 和 RabbitTemplate 存在很大区别，因为它们都提供了明确的 receive 方法来接收消息。从前面提供的 Kafka 架构图中可以看出，在 Kafka 中消息通过服务器推送给各个消费者，而 Kafka 的消费者在消费消息时，需要提供一个监听器（Listener）对某个 Topic 实现监听，从而获取消息，这也是 Kafka 消费消息的唯一方式。在 Spring 中提供了一个 @KafkaListener 注解实现监听器，该注解定义如下代码所示：@Target({ElementType.TYPE, ElementType.METHOD, ElementType.ANNOTATION_TYPE})@Retention(RetentionPolicy.RUNTIME)@MessageMapping@Documented@Repeatable(KafkaListeners.class)public @interface KafkaListener { String id() default &quot;&quot;; String containerFactory() default &quot;&quot;; // 消息 Topic String[] topics() default {}; // Topic 的模式匹配表达式 String topicPattern() default &quot;&quot;; // Topic 分区 TopicPartition[] topicPartitions() default {}; String containerGroup() default &quot;&quot;; String errorHandler() default &quot;&quot;; // 消息分组 Id String groupId() default &quot;&quot;; boolean idIsGroup() default true; String clientIdPrefix() default &quot;&quot;; String beanRef() default &quot;__listener&quot;; String concurrency() default &quot;&quot;; String autoStartup() default &quot;&quot;; String[] properties() default {}; boolean splitIterables() default true;}上述代码中可以看到 @KafkaListener 的定义比较复杂，我把日常开发中常见的几个配置项做了注释。在使用 @KafkaListener 时，最核心的操作是设置 Topic，而 Kafka 还提供了一个模式匹配表达式可以对目标 Topic 实现灵活设置。在这里，有必要留意 groupId 这个属性，这涉及 Kafka 中另一个核心概念：消费者分组（Consumer Group）。设计消费者组的目的是应对集群环境下的多服务实例问题。显然，如果采用发布-订阅模式会导致一个服务的不同实例可能会消费到同一条消息。为了解决这个问题，Kafka 中提供了消费者组的概念。一旦使用了消费组，一条消息只能被同一个组中的某一个服务实例所消费。消费者组的基本结构如下图所示：使用 @KafkaListener 注解时，把它直接添加在处理消息的方法上即可，如下代码所示：@KafkaListener(topics = &quot;demo.topic&quot;)public void handlerEvent() { // TODO：添加消息处理逻辑}当然，还需要在消费者的配置文件中指定用于消息消费的配置项，如下代码所示：spring: kafka: bootstrap-servers: - localhost:9092 template: default-topic: springcss.account.topic consumer: group-id: springcss_customer可以看到，这里除了指定 template.default-topic 配置项之外，还指定了 consumer.group-id 配置项来指定消费者分组信息。在 spring-css 案例中集成 Kafka在 spring-css 案例中引入 Kafka 实现 account-service 与 customer-service 之间的消息通信。实现 account-service 消息生产者 首先，新建一个 Spring Boot 工程，用来保存用于多个服务之间交互的消息对象，以供各个服务使用； 将这个代码工程简单命名为 message，并添加一个代表消息主体的事件 AccountChangedEvent，如下代码所示： public class AccountChangedEvent implements Serializable { private static final long serialVersionUID = 1L; //事件类型 private String type; //事件所对应的操作 private String operation; //事件对应的领域模型 private AccountMessage accountMessage; // 省略 getter/setter} 上述 AccountChangedEvent 类包含了 AccountMessage 对象本身以及它的操作类型，而 AccountMessage 与 Account 对象的定义完全一致，只不过 AccountMessage 额外实现了用于序列化的 Serializable 接口而已，如下代码所示：public class AccountMessage implements Serializable { private static final long serialVersionUID = 1L; private Long id; private String accountCode; private String accountName; }定义完消息实体之后，我在 account-service 中引用了一个 message 工程，并添加了一个 KafkaAccountChangedPublisher 类用来实现消息的发布，如下代码所示：在这里可以看到，我们注入了一个 KafkaTemplate 对象，然后通过它的 send 方法向目标 Topic 发送了消息。这里的 AccountChannels.SPRINGCSS_ACCOUNT_TOPIC 就是 spring-css.account.topic，需要在 account-service 中的配置文件中指定同一个 Topic，如下代码所示：@Component(&quot;kafkaAccountChangedPublisher&quot;)public class KafkaAccountChangedPublisher { @Autowired private KafkaTemplate&amp;lt;String, AccountChangedEvent&amp;gt; kafkaTemplate; @Override protected void publishEvent(AccountChangedEvent event) { kafkaTemplate.send(AccountChannels.SPRINGCSS_ACCOUNT_TOPIC, event); }}在这里可以看到，我注入了一个 KafkaTemplate 对象，然后通过它的 send 方法向目标 Topic 发送了消息。这里的 AccountChannels.SPRINGCSS_ACCOUNT_TOPIC 就是 springcss.account.topic，我们需要在 account-service 中的配置文件中指定同一个 Topic，如下代码所示：spring: kafka: bootstrap-servers: - localhost:9092 template: default-topic: springcss.account.topic producer: keySerializer: org.springframework.kafka.support.serializer.JsonSerializer valueSerializer: org.springframework.kafka.support.serializer.JsonSerializer注意到这里，使用了 JsonSerializer 对发送的消息进行序列化。实现 customer-service 消息消费者针对服务消费者 customer-service，它的配置信息，如下代码所示：spring: kafka: bootstrap-servers: - localhost:9092 template: default-topic: springcss.account.topic consumer: value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer group-id: springcss_customer properties: spring.json.trusted.packages: cn.happymaya.message相较消息生产者中的配置信息，消息消费者的配置信息多了两个配置项，其中一个是 group-id，这是 Kafka 消费者特有的一个配置项，用于指定消费者组。而另一个配置项是 spring.json.trusted.packages，用于设置 JSON 序列化的可行包名称，这个名称需要与 AccountChangedEvent 类所在的包结构一致，即这里指定的 cn.happymaya.message。 在 Kafka 中，消费者组的作用是什么，如何都消费者组进行配置？ 如何动态的从数据库里面加载？意思就是我的消费者队列名从数据库里面查出来，之后让 kafka 监听这个队列 - 跟普通的查询数据库一样，把查询出来的队列名通过代码的方式设置到Kafka中，Kafka既支持配置的方式使用队列，也可以使用代码的方式动态配置" }, { "title": "Spring Boot 中的配置体系", "url": "/posts/spring-boot-config-system/", "categories": "Spring", "tags": "SpringBoot, Config System", "date": "2018-04-01 09:32:00 +0000", "snippet": "创建第一个 Spring Boot Web 应用程序基于 Spring Boot 创建 Web 应用程序的方法有很多，但最简单、最直接的方法是使用 Spring 官方提供的 Spring Initializer 初始化模板。初始化使用操作：直接访问 Spring Initializer 网站（http://start.spring.io/），选择创建一个 Maven 项目并指定相应的 Group 和 Artifact，然后在添加的依赖中选择 Spring Web，点击生成即可。同样，完全可以基于 Maven 本身的功能特性和结构，来生成上图中的代码工程。而后，来、为这个代码工程添加一些支持 RESTful 风格的 HTTP 端点，在这里我们同样创建一个 CustomerController 类，如下所示：@RestController@RequestMapping(value=&quot;customers&quot;)public class CustomerController { @Autowired private CustomerTicketService customerTicketService; @PostMapping(value = &quot;/{accountId}/{orderNumber}&quot;) public CustomerTicket generateCustomerTicket( @PathVariable(&quot;accountId&quot;) Long accountId, @PathVariable(&quot;orderNumber&quot;) String orderNumber) { CustomerTicket customerTicket = customerTicketService.generateCustomerTicket(accountId, orderNumber); return customerTicket; }}RESTful 端点已经开发完成，然后需要对这个应用程序进行打包。基于 Spring Boot 和 Maven，当使用 mvn package 命令构建整个应用程序时，将得到一个 customerservice-0.0.1-SNAPSHOT.jar 文件，这个 jar 文件是可以直接运行的可执行文件，内置了 Tomcat Web 服务器。也就是说，通过如下命令直接运行这个 Spring Boot 应用程序：java –jar customerservice-0.0.1-SNAPSHOT.jar验证服务是否启动成功，以及 HTTP 请求是否得到正确响应呢？可以通过 Postman 访问“http://localhost:8083/customers/1”端点，可以得到如下图所示的HTTP响应结果，说明整个服务已经启动成功。 Postman 提供了强大的 Web API 和 HTTP 请求调试功能，界面简洁明晰，操作也比较方便快捷和人性化。Postman 能够发送任何类型的 HTTP 请求（如 GET、HEAD、POST、PUT 等），并能附带任何数量的参数和 HTTP 请求头（Header）。Spring Boot 中的配置体系在 Spring Boot 中，其核心设计理念是对配置信息的管理采用约定优于配置。这意味着需要设置的配置信息数量比使用传统 Spring 框架时还大大减少。当然，今天我们关注的主要是如何理解并使用 Spring Boot 中的配置信息组织方式，这里就需要引出一个核心的概念，即 Profile。配置文件与 ProfileProfile 本质上代表一种用于组织配置信息的维度，在不同场景下可以代表不同的含义。例如： 如果 Profile 代表的是一种状态，可以使用 open、halfopen、close 等值来分别代表全开、半开和关闭等 系统需要设置一系列的模板，每个模板中保存着一系列配置项，那么也可以针对这些模板分别创建 Profile 这里的状态或模版的定义完全由开发人员自主设计，我们可以根据需要自定义各种 Profile，这就是 Profile 的基本含义。另一方面，为了达到集中化管理的目的，Spring Boot 对配置文件的命名也做了一定的约定，分别使用 label 和 profile 概念来指定配置信息的版本以及运行环境，其中 label 表示配置版本控制信息，而 profile 则用来指定该配置文件所对应的环境。在 Spring Boot 中，配置文件同时支持 .properties 和 .yml 两种文件格式，结合 label 和 profile 概念，如下所示的配置文件命名都是常见和合法的：/{application}.yml/{application}-{profile}.yml/{label}/{application}-{profile}.yml/{application}-{profile}.properties/{label}/{application}-{profile}.properties如果采用 .yml 配置文件，配置信息将表示为如下的形式：server: port: 8081spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://119.3.52.175:3306/appointment username: root password: 1qazxsw2#edc 如果采用 .propertie 配置文件，配置信息将表示为如下的形式：spring.datasource.driverClassName=com.mysql.cj.jdbc.Driverspring.datasource.url=jdbc:mysql://127.0.0.1:3306/accountspring.datasource.username=root spring.datasource.password=root显然，类似这样的数据源通常会根据环境的不同而存在很多套配置。假设存在如下所示的配置文件集合： application-prod.yml application-test.yml application-uat.yml application.yml注意，这里有一个全局的 application.yml 配置文件以及多个局部的 profile 配置文件。在 Spring Boot 中，可以在主 application.properties 中使用如下的配置方式来激活当前所使用的 Profile：spring.profiles.active = test或spring:  profiles:    active: test也可以同时激活几个 Profile，这完全取决于对系统配置的需求和维度：spring:  profiles:    active: test, prod, uatSpring Boot 也支持把所有的 Profile 配置信息只保存在一个文件中而不是分散在多个配置文件中，需要做的事情只是对这些信息按 Profile 进行组织、分段，如下所示：spring: profiles: test #test 环境相关配置信息     spring: profiles: dev #prod 环境相关配置信息尽管上述方法是有效的，还是优先选择多个配置文件的组织方法管理各个 Profile 配置信息，这样才不容易混淆和出错。如果不希望在全局配置文件中指定所需要激活的 Profile，而是想把这个过程延迟到运行这个服务时，那可以直接在 java –jar 命令中添加“–spring.profiles.active”参数，如下所示：java –jar customerservice-0.0.1-SNAPSHOT.jar --spring.profiles.active=prod这种实现方案在通过脚本进行自动化打包和部署的场景下非常有用。代码控制与Profile在 Spring Boot 中，Profile 这一概念的应用场景还包括动态控制代码执行流程。为此，我们需要使用 @Profile 注解，先来看一个简单的示例。@Configurationpublic class DataSourceConfig { @Bean @Profile(&quot;dev&quot;) public DataSource devDataSource() {     // 创建 dev 环境下的 DataSource } @Bean() @Profile(&quot;prod&quot;) public DataSource prodDataSource(){     // 创建 prod 环境下的 DataSource }}我构建了一个 DataSourceConfig 配置类来专门管理各个环境所需的 DataSource。注意到这里使用 @Profile 注解来指定具体所需要执行的 DataSource 创建代码，通过这种方式，可以达到与使用配置文件相同的效果。更进一步，能够在代码中控制 JavaBean 的创建过程为我们根据各种条件动态执行代码流程提供了更大的可能性。例如，在日常开发过程中，一个常见的需求是根据不同的运行环境初始化数据，常见的做法是独立执行一段代码或脚本。基于 @Profile 注解，就可以将这一过程包含在代码中并做到自动化，如下所示：@Profile(&quot;dev&quot;)@Configurationpublic class DevDataInitConfig { @Bean public CommandLineRunner dataInit() { return new CommandLineRunner() { @Override public void run(String... args) throws Exception { // 执行 Dev 环境的数据初始化 }; } }  }这里用到了 Spring Boot 所提供了启动时任务接口 CommandLineRunner，实现了该接口的代码会在 Spring Boot 应用程序启动时自动进行执行。@Profile 注解的应用范围很广，可以将它添加到包含 @Configuration 和 @Component 注解的类及其方法，也就是说可以延伸到继承了 @Component 注解的 @Service、@Controller、@Repository 等各种注解中。" }, { "title": "Spring Boot 创建和管理自定义的配置信息", "url": "/posts/spring-boot-custom-config/", "categories": "Spring", "tags": "SpringBoot", "date": "2018-03-24 12:32:00 +0000", "snippet": "在应用程序中嵌入系统配置信息Spring Boot 通过自动配置机制内置了很多默认的配置信息，在这些配置信息中，有一部分系统配置信息也可以反过来作为配置项应用到我们自己的应用程序中。例如，获取当前应用程序名称，并作为一个配置项进行管理，很简单，直接通过 ${spring.application.name} 占位符就可以做到这一点，如下所示：myapplication.name: ${spring.application.name}通过 ${} 占位符同样可以引用配置文件中的其他配置项内容，如在下列配置项中，最终system.description配置项的值就是The system springcss is used for health。system.name=springcsssystem.domain=healthsystem.description=The system ${name} is used for ${domain}.假设使用 Maven 来构建应用程序，那么可以按如下所示的配置项来动态获取与系统构建过程相关的信息：info: app: encoding: @project.build.sourceEncoding@ java: source: @java.version@ target: @java.version@上述配置项的效果与如下所示的静态配置是一样的：info: app: encoding: UTF-8 java: source: 1.8.0_31 target: 1.8.0_31根据不同的需求，在应用程序中嵌入系统配置信息是很有用的，特别是在一些面向 DevOps 的应用场景中。创建和使用自定义配置信息面对纷繁复杂的应用场景，Spring Boot 所提供的内置配置信息并不一定能够完全满足开发的需求，这就需要创建并管理各种自定义的配置信息。例如，对于一个电商类应用场景，为了鼓励用户完成下单操作，希望每完成一个订单给就给到用户一定数量的积分。从系统扩展性上讲，这个积分应该是可以调整的，所以创建了一个自定义的配置项，如下所示：springcss.order.point = 10应用程序该获取这个配置项的内容呢,通常有两种方法。使用 @Value 注解使用 @Value 注解来注入配置项内容是一种传统的实现方法。针对前面给出的自定义配置项，可以构建一个 SpringCssConfig 类，如下所示：@Componentpublic class SpringCssConfig { @Value(&quot;${springcss.order.point}&quot;) private int point;}在 SpringCssConfig 类中，要做的就是在字段上添加 @Value 注解，并指向配置项的名称即可。使用 @ConfigurationProperties 注解相较 @Value 注解，更为现代的一种做法是使用 @ConfigurationProperties 注解。在使用该注解时，通常会设置一个“prefix”属性用来指定配置项的前缀，如下所示：@Component@ConfigurationProperties(prefix = &quot;springcss.order&quot;)public class SpringCsshConfig { privat int point; //省略 getter/setter}相比 @Value 注解只能用于指定具体某一个配置项，@ConfigurationProperties 可以用来批量提取配置内容。只要指定 prefix，就可以把该 prefix 下的所有配置项按照名称自动注入业务代码中。考虑一种更常见也更复杂的场景：假设用户根据下单操作获取的积分并不是固定的，而是根据每个不同类型的订单会有不同的积分，那么现在的配置项的内容，如果使用 Yaml 格式的话就应该是这样：springcss: points: orderType[1]: 10 orderType[2]: 20 orderType[3]: 30如果把这些配置项全部加载到业务代码中，使用 @ConfigurationProperties 注解同样也很容易实现。直接在配置类 SpringCssConfig 中定义一个 Map 对象，然后通过 Key-Value 对来保存这些配置数据，如下所示：@Component@ConfigurationProperties(prefix=&quot;springcss.points&quot;)public class SpringCssConfig {    private Map&amp;lt;String, Integer&amp;gt; orderType = new HashMap&amp;lt;&amp;gt;(); //省略 getter/setter}可以看到这里通过创建一个 HashMap 来保存这些 Key-Value 对。类似的，也可以实现常见的一些数据结构的自动嵌入。为自定义配置项添加提示功能对于管理自定义的配置信息，如何实现当输入某一个配置项的前缀时，IDE 就会自动弹出该前缀下的所有配置信息供你进行选择 ？当我们在 application.yml 配置文件中添加一个自定义配置项时，会注意到 IDE 会出现一个提示，说明这个配置项无法被 IDE 所识别。遇到这种提示时，是可以忽略的，因为它不会影响到任何执行效果。但为了达到自动提示效果，就需要生成配置元数据。生成元数据的方法也很简单，直接通过 IDE 的Create metadata for &#39;springcss.order.point&#39;按钮，就可以选择创建配置元数据文件，这个文件的名称为 additional-spring-configuration-metadata.json，文件内容如下所示：{ &quot;properties&quot;: [ { &quot;name&quot;: &quot;springcss.order.point&quot;, &quot;type&quot;: &quot;java.lang.String&quot;, &quot;description&quot;: &quot;A description for &#39;springcss.order.point&#39;&quot; } ]}通过这种方式，IDE 就会自动提示完整的配置项内容。另外，假设需要为 springcss.order.point 配置项指定一个默认值，可以通过在元数据中添加一个”defaultValue”项来实现，如下所示：{ &quot;properties&quot;: [ { &quot;name&quot;: &quot;springcss.order.point&quot;, &quot;type&quot;: &quot;java.lang.String&quot;, &quot;description&quot;: &quot;A description for &#39;springcss.order.point&#39;&quot;, &quot;defaultValue&quot;: 10 } ]}这时候，在 IDE 中设置这个配置项时，就会提出该配置项的默认值为 10。组织和整合配置信息Profile 可以认为是管理配置信息中的一种有效手段。除此之外，另一种组织和整合配置信息的方法，这种方法同样依赖于 @ConfigurationProperties 注解。使用 @PropertySources 注解在使用 @ConfigurationProperties 注解时，可以和 @PropertySource 注解一起进行使用，从而指定从哪个具体的配置文件中获取配置信息。例如，在下面这个示例中，通过 @PropertySource 注解指定了 @ConfigurationProperties 注解中所使用的配置信息是从当前类路径下的 application.properties 配置文件中进行读取。@Component@ConfigurationProperties(prefix = &quot;springcss.order&quot;)@PropertySource(value = &quot;classpath:application.properties&quot;)public class SpringCssConfig {}既然可以通过 @PropertySource 注解来指定一个配置文件的引用地址，那么显然也可以引入多个配置文件，这时候用到的是 @PropertySources 注解，使用方式如下所示：@PropertySources({ @PropertySource(&quot;classpath:application.properties &quot;), @PropertySource(&quot;classpath:redis.properties&quot;), @PropertySource(&quot;classpath:mq.properties&quot;)})public class SpringCssConfig {}这里，通过 @PropertySources 注解组合了多个 @PropertySource 注解中所指定的配置文件路径。SpringCssConfig 类可以同时引用所有这些配置文件中的配置项。另一方面，也可以通过配置 spring.config.location 来改变配置文件的默认加载位置，从而实现对多个配置文件的同时加载。例如，如下所示的执行脚本会在启动 customerservice-0.0.1-SNAPSHOT.jar 时加载 D 盘下的 application.properties 文件，以及位于当前类路径下 config 目录中的所有配置文件：java -jar customerservice-0.0.1-SNAPSHOT.jar --spring.config.location=file:///D:/application.properties, classpath:/config/配置文件的加载顺序通过前面的示例，看到可以把配置文件保存在多个路径，而这些路径在加载配置文件时具有一定的顺序。Spring Boot 在启动时会扫描以下位置的 application.properties 或者 application.yml 文件作为全局配置文件：–file:./config/–file:./–classpath:/config/–classpath:/它们的优先级，依次是： –classpath:/config/ –classpath:/ –file:./config/ –file:./Spring Boot 会全部扫描上图中的这四个位置，扫描规则是高优先级配置内容会覆盖低优先级配置内容。而如果高优先级的配置文件中存在与低优先级配置文件不冲突的属性，则会形成一种互补配置，也就是说会整合所有不冲突的属性。覆写内置的配置类如果不想使用这些配置 Spring Boot 内置了大量的自动配置，就需要对它们进行覆写。覆写的方法有很多，可以使用配置文件、Groovy 脚本以及 Java 代码。这里，以 Java 代码为例来简单演示覆写配置类的实现方法（参考 Spring Security 框架中，设置用户认证信息所依赖的配置类是 WebSecurityConfigurer 类，pring Security 提供了 WebSecurityConfigurerAdapter 这个适配器类来简化该配置类的使用方式，可以继承 WebSecurityConfigurerAdapter 类并且覆写其中的 configure() 的方法来完成自定义的用户认证配置工作的源码）。" }, { "title": "使用 Spring Data JPA 访问关系型数据库", "url": "/posts/spring-data-jpa-db/", "categories": "Spring", "tags": "SpringBoot, Spring Security", "date": "2018-03-12 09:32:00 +0000", "snippet": "引入 Spring Data JPA在应用程序中使用 Spring Data JPA，首先需要在 pom 文件中引入 spring-boot-starter-data-jpa 依赖，如下代码所示：&amp;lt;dependency&amp;gt;     &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;     &amp;lt;artifactId&amp;gt;spring-boot-starter-data-jpa&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;在使用这一组件的之前，有必要对 JPA 规范进行一定的了解。JPA 全称是 JPA Persistence API，即 Java 持久化 API，它是一个 Java 应用程序接口规范，用于充当面向对象的领域模型和关系数据库系统之间的桥梁，属于一种 ORM（Object Relational Mapping，对象关系映射）技术。JPA 规范中定义了一些既定的概念和约定，集中包含在 javax.persistence 包中，常见的如对实体（Entity）定义、实体标识定义、实体与实体之间的关联关系定义，以及 09 讲中介绍的 JPQL 定义等，关于这些定义及其使用方法，一会儿我们会详细展开说明。与 JDBC 规范一样，JPA 规范也有一大批实现工具和框架，极具代表性的如老牌的 Hibernate 及 Spring Data JPA。基于 Spring Data JPA 的整个开发过程，在 SpringCSS 案例中专门设计和实现一套独立的领域对象和 Repository。实体类注解order-service 中存在两个主要领域对象，即 Order 和 Goods。为了有所区分，分别命名为 JpaOrder 和 JpaGoods，它们就是 JPA 规范中的实体类。相对简单的 JpaGoods，JpaGoods 定义如下代码所示（把 JPA 规范的相关类的引用罗列在了一起）：@Entity@Table(name=&quot;goods&quot;)public class JpaGoods { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String goodsCode; private String goodsName; private Float price; public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getGoodsCode() { return goodsCode; } public void setGoodsCode(String goodsCode) { this.goodsCode = goodsCode; } public String getGoodsName() { return goodsName; } public void setGoodsName(String goodsName) { this.goodsName = goodsName; } public Float getPrice() { return price; } public void setPrice(Float price) { this.price = price; } }JpaGoods 中使用了 JPA 规范中用于定义实体的几个注解：最重要的 @Entity 注解、用于指定表名的 @Table 注解、用于标识主键的 @Id 注解，以及用于标识自增数据的 @GeneratedValue 注解，这些注解都比较直白，在实体类上直接使用即可。接下来，比较复杂的 JpaOrder，定义如下代码所示：@Entity@Table(name = &quot;`order`&quot;)@JsonIgnoreProperties(value = { &quot;hibernateLazyInitializer&quot;, &quot;handler&quot; })@NamedQueries({ @NamedQuery(name = &quot;getOrderByOrderNumberWithQuery&quot;, query = &quot;select o from JpaOrder o where o.orderNumber = ?1&quot;) })public class JpaOrder implements Serializable { private static final long serialVersionUID = 1L; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String orderNumber; private String deliveryAddress; @ManyToMany(targetEntity = JpaGoods.class) @JoinTable(name = &quot;order_goods&quot;, joinColumns = @JoinColumn(name = &quot;order_id&quot;, referencedColumnName = &quot;id&quot;), inverseJoinColumns = @JoinColumn(name = &quot;goods_id&quot;, referencedColumnName = &quot;id&quot;)) private List&amp;lt;JpaGoods&amp;gt; goods = new ArrayList&amp;lt;&amp;gt;(); public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getOrderNumber() { return orderNumber; } public void setOrderNumber(String orderNumber) { this.orderNumber = orderNumber; } public String getDeliveryAddress() { return deliveryAddress; } public void setDeliveryAddress(String deliveryAddress) { this.deliveryAddress = deliveryAddress; } public List&amp;lt;JpaGoods&amp;gt; getGoods() { return goods; } public void setGoods(List&amp;lt;JpaGoods&amp;gt; goods) { this.goods = goods; }}这里除了引入了常见的一些注解，还引入了 @ManyToMany 注解，它表示 order 表与 goods 表中数据的关联关系。在JPA 规范中，共提供了 one-to-one、one-to-many、many-to-one、many-to-many 这 4 种映射关系，它们分别用来处理一对一、一对多、多对一，以及多对多的关联场景。针对 order-service 这个业务场景，我设计了一张 order_goods 中间表存储 order 与 goods 表中的主键关系，且使用了 @ManyToMany 注解定义 many-to-many 这种关联关系，也使用了 @JoinTable 注解指定 order_goods 中间表，并通过 joinColumns 和 inverseJoinColumns 注解分别指定中间表中的字段名称以及引用两张主表中的外键名称。定义 Repository定义完实体对象后，再来提供 Repository 接口，这一步的操作非常简单，OrderJpaRepository 的定义如下代码所示：/** * 使用 Spring Data JPA 进行数据库访问 */@Repository(&quot;orderJpaRepository&quot;)public interface IOrderJpaRepositoryImpl extends JpaRepository&amp;lt;JpaOrder, Long&amp;gt;{}OrderJpaRepository 是一个继承了 JpaRepository 接口的空接口，此时 OrderJpaRepository 实际上已经具备了访问数据库的基本 CRUD 功能。使用 Spring Data JPA 访问数据库有了上面定义的 JpaOrder 和 JpaGoods 实体类，以及 OrderJpaRepository 接口，我们已经可以实现很多操作了。比如通过 Id 获取 Order 对象，首先可以通过构建一个 JpaOrderService 直接注入 OrderJpaRepository 接口，如下代码所示：public class JpaOrderService { @Autowired private IOrderJpaRepositoryImpl IOrderJpaRepositoryImpl; public JpaOrder getOrderById(Long orderId) { return IOrderJpaRepositoryImpl.getOne(orderId); }}然后，再通过构建一个 Controller 类嵌入上述方法，并通过 HTTP 请求查询 Id 为 1 的 JpaOrder 对象，获得的结果如下代码所示：{ &quot;id&quot;: 1,    &quot;orderNumber&quot;: &quot;Order10001&quot;,    &quot;deliveryAddress&quot;: &quot;test_address1&quot;,    &quot;goods&quot;: [        {            &quot;id&quot;: 1,            &quot;goodsCode&quot;: &quot;GoodsCode1&quot;,            &quot;goodsName&quot;: &quot;GoodsName1&quot;,            &quot;price&quot;: 100.0        },        {            &quot;id&quot;: 2,            &quot;goodsCode&quot;: &quot;GoodsCode2&quot;,            &quot;goodsName&quot;: &quot;GoodsName2&quot;,            &quot;price&quot;: 200.0        }    ]}请注意，这里不仅获取了 order 表中的订单基础数据，还同时获取了 goods 表中的商品数据，这种效果是如何实现的呢？是因为在 JpaOrder 对象中，添加了 @ManyToMany 注解，该注解会自动从 order_goods 表中获取商品主键信息，并从 goods 表中获取商品详细信息。了解了使用 Spring Data JPA 实现关系型数据库访问的过程，并对比使用 JdbcTemplate 访问关系型数据库，中发现使用 Spring Data JPA 更简单。在多样化查询实现过程中，不仅可以使用 JpaRepository 中默认集成的各种 CRUD 方法，还可以使用 @Query 注解、方法名衍生查询等。因此同时引入 QueryByExample 和 Specification 这两种机制来丰富多样化查询方式。使用 @Query 注解使用 @Query 注解实现查询的示例如下代码所示：@Repository(&quot;orderJpaRepository&quot;)public interface OrderJpaRepository extends JpaRepository&amp;lt;JpaOrder, Long&amp;gt;{    @Query(&quot;select o from JpaOrder o where o.orderNumber = ?1&quot;)    JpaOrder getOrderByOrderNumberWithQuery(String orderNumber);}这里，使用了 JPQL 根据 OrderNumber 查询订单信息。JPQL 的语法与 SQL 语句非常类似。说到 @Query 注解，JPA 中还提供了一个 @NamedQuery 注解对 @Query 注解中的语句进行命名。@NamedQuery 注解的使用方式如下代码所示：@Entity@Table(name = &quot;`order`&quot;)@NamedQueries({ @NamedQuery(name = &quot;getOrderByOrderNumberWithQuery&quot;, query = &quot;select o from JpaOrder o where o.orderNumber = ?1&quot;) })public class JpaOrder implements Serializable {}在上述示例中，在实体类 JpaOrder 上添加了一个 @NamedQueries 注解，该注解可以将一批 @NamedQuery 注解整合在一起使用。同时，我们还使用了 @NamedQuery 注解定义了一个“getOrderByOrderNumberWithQuery”查询，且指定了对应的 JPQL 语句。如果使用这个命名查询，在 OrderJpaRepository 中定义与该命名一致的方法即可。使用方法名衍生查询使用方法名衍生查询是最方便的一种自定义查询方式，在这过程中唯一需要做的就是在 JpaRepository 接口中定义一个符合查询语义的方法。比如希望通过 OrderNumber 查询订单信息，那么可以提供如下代码所示的接口定义：@Repository(&quot;orderJpaRepository&quot;)public interface OrderJpaRepository extends JpaRepository&amp;lt;JpaOrder, Long&amp;gt;{ JpaOrder getOrderByOrderNumber(String orderNumber);}通过 getOrderByOrderNumber 方法后，就可以自动根据 OrderNumber 获取订单详细信息了。使用 QueryByExample 机制一种强大的查询机制，即 QueryByExample（QBE）机制。针对 JpaOrder 对象，假如希望根据 OrderNumber 及 DeliveryAddress 中的一个或多个条件进行查询，按照方法名衍生查询的方式构建查询方法后，得到如下代码所示的方法定义：@Repository(&quot;orderJpaRepository&quot;)public interface OrderJpaRepository extends JpaRepository&amp;lt;JpaOrder, Long&amp;gt;{ List&amp;lt;JpaOrder&amp;gt; findByOrderNumberAndDeliveryAddress (String orderNumber, String deliveryAddress);}如果查询条件中使用的字段非常多，上面这个方法名可能非常长，且还需要设置一批参数，这种查询方法定义显然存在缺陷。因为不管查询条件有多少个，都需要把所有参数进行填充，哪怕部分参数并没有被用到。而且，如果将来需要再添加一个新的查询条件，该方法必须做调整，从扩展性上讲也存在设计缺陷。为了解决这些问题，便可以引入 QueryByExample 机制。QueryByExample 可以翻译为按示例查询，是一种用户友好的查询技术。它允许动态创建查询，且不需要编写包含字段名称的查询方法，也就是说按示例查询不需要使用特定的数据库查询语言来编写查询语句。从组成结构上讲，QueryByExample 包括 Probe、ExampleMatcher 和 Example 这三个基本组件。其中， Probe 包含对应字段的实例对象，ExampleMatcher 携带有关如何匹配特定字段的详细信息，相当于匹配条件，Example 则由 Probe 和 ExampleMatcher 组成，用于构建具体的查询操作。现在，基于 QueryByExample 机制重构根据 OrderNumber 查询订单的实现过程。首先，需要在 OrderJpaRepository 接口的定义中继承 QueryByExampleExecutor 接口，如下代码所示：@Repository(&quot;orderJpaRepository&quot;)public interface OrderJpaRepository extends JpaRepository&amp;lt;JpaOrder, Long&amp;gt;, QueryByExampleExecutor&amp;lt;JpaOrder&amp;gt; {然后，在 JpaOrderService 中实现如下代码所示的 getOrderByOrderNumberByExample 方法： public JpaOrder getOrderByOrderNumberByExample(String orderNumber) { JpaOrder order = new JpaOrder(); order.setOrderNumber(orderNumber); ExampleMatcher matcher = ExampleMatcher.matching().withIgnoreCase() .withMatcher(&quot;orderNumber&quot;, GenericPropertyMatchers.exact()).withIncludeNullValues(); Example&amp;lt;JpaOrder&amp;gt; example = Example.of(order, matcher); return IOrderJpaRepositoryImpl.findOne(example).orElse(new JpaOrder()); }上述代码中，首先构建了一个 ExampleMatcher 对象用于初始化匹配规则，然后通过传入一个 JpaOrder 对象实例和 ExampleMatcher 实例构建了一个 Example 对象，最后通过 QueryByExampleExecutor 接口中的 findOne() 方法实现了 QueryByExample 机制。使用 Specification 机制先考虑这样一种场景，比如需要查询某个实体，但是给定的查询条件不固定，此时该怎么办？这时通过动态构建相应的查询语句即可，而在 Spring Data JPA 中可以通过 JpaSpecificationExecutor 接口实现这类查询。相比使用 JPQL 而言，使用 Specification 机制的优势是类型安全。继承了 JpaSpecificationExecutor 的 OrderJpaRepository 定义如下代码所示：@Repository(&quot;orderJpaRepository&quot;)public interface OrderJpaRepository extends JpaRepository&amp;lt;JpaOrder, Long&amp;gt;,     JpaSpecificationExecutor&amp;lt;JpaOrder&amp;gt;{}对于 JpaSpecificationExecutor 接口而言，它背后使用的就是 Specification 接口，且 Specification 接口核心方法就一个，我们可以简单地理解该接口的作用就是构建查询条件，如下代码所示：Predicate toPredicate(Root&amp;lt;T&amp;gt; root, CriteriaQuery&amp;lt;?&amp;gt; query, CriteriaBuilder criteriaBuilder);其中 Root 对象代表所查询的根对象，可以通过 Root 获取实体的属性，CriteriaQuery 代表一个顶层查询对象，用来实现自定义查询，而 CriteriaBuilder 用来构建查询条件。基于 Specification 机制，同样对根据 OrderNumber 查询订单的实现过程进行重构，重构后的 getOrderByOrderNumberBySpecification 方法如下代码所示： public JpaOrder getOrderByOrderNumberBySpecification(String orderNumber) { JpaOrder order = new JpaOrder(); order.setOrderNumber(orderNumber); @SuppressWarnings(&quot;serial&quot;) Specification&amp;lt;JpaOrder&amp;gt; spec = new Specification&amp;lt;JpaOrder&amp;gt;() { @Override public Predicate toPredicate(Root&amp;lt;JpaOrder&amp;gt; root, CriteriaQuery&amp;lt;?&amp;gt; query, CriteriaBuilder cb) { Path&amp;lt;Object&amp;gt; orderNumberPath = root.get(&quot;orderNumber&quot;); Predicate predicate = cb.equal(orderNumberPath, orderNumber); return predicate; } }; return IOrderJpaRepositoryImpl.findOne(spec).orElse(new JpaOrder()); }从上面示例中可以看到，在 toPredicate 方法中，首先我们从 root 对象中获取了“orderNumber”属性，然后通过 cb.equal 方法将该属性与传入的 orderNumber 参数进行了比对，从而实现了查询条件的构建过程。总结Spring Data JPA 实现对关系型数据库访问，因为它不仅具有 ORM 框架的通用功能，同时还添加了 QueryByExample 和 Specification 机制等扩展性功能，应用上简单而高效。 在使用 Spring Data JPA 时，如何正确使用 QueryByExample 和 Specification 机制实现灵活的自定义查询？" }, { "title": "Spring 安全体系的整体架构", "url": "/posts/spring-safe-system/", "categories": "Spring", "tags": "SpringBoot, Spring Security", "date": "2018-03-07 09:32:00 +0000", "snippet": "在设计 Web 应用程序时： 一方面，因为缺乏对 Web 安全访问机制的认识，所以系统安全性是一个重要但又容易被忽略的话题； 另一方面，因为系统涉及的技术体系非常复杂，所以系统安全性又是一个非常综合的话题。在 Spring 家族中，Spring Security 专门提供了一个安全性开发框架。Web 应用程序的安全性需求在软件系统中，把需要访问的内容定义为一种资源（Resource），而安全性设计的核心目标是对这些资源进行保护，以此确保外部请求对它们的访问安全受控。在一个 Web 应用程序中，把对外暴露的 RESTful 端点理解为资源，关于如何对 HTTP 端点这些资源进行安全性访问，业界存在一些常见的技术体系。而安全领域中非常常见但又容易混淆的两个概念：认证（Authentication）和授权（Authorization）。 认证，首先需要明确你是谁这个问题，也就是说系统能针对每次访问请求判断出访问者是否具有合法的身份标识； 一旦明确了 “你是谁”，就能判断出“你能做什么”，这个步骤就是授权。一般来说，通用的授权模型都是基于权限管理体系，即对资源、权限、角色和用户的进行组合处理的一种方案。当把认证与授权结合起来后，即先判断资源访问者的有效身份，然后确定其对这个资源进行访问的合法权限，整个过程就形成了对系统进行安全性管理的一种常见解决方案，如下图所示：上图就是一种通用方案，而在不同的应用场景及技术体系下，系统可以衍生出很多具体的实现策略，比如 Web 应用系统中的认证和授权模型虽然与上图类似，但是在具体设计和实现过程中有其特殊性。在 Web 应用体系中， 认证这部分的需求相对比较明确，所以需要构建一套完整的存储体系来保存和维护用户信息，并且确保这些用户信息在处理请求的过程中能够得到合理利用。 授权的情况相对来说复杂些，比如：对某个特定的 Web 应用程序而言，面临的第一个问题是如何判断一个 HTTP 请求具备访问自己的权限。解决完这个第一个问题后，就算这个请求具备访问该应用程序的权限，并不意味着它能够访问其所具有的所有 HTTP 端点，比如业务上的某些核心功能还是需要具备较高的权限才能访问，这就涉及需要解决的第二个问题——如何对访问的权限进行精细化管理？如下图所示：在上图中，假设该请求具备对 Web 应用程序的访问权限，但不具备访问应用程序中端点 1 的权限，如果想实现这种效果，一般我们的做法是引入角色体系：首先对不同的用户设置不同等级的角色（即角色等级不同对应的访问权限也不同），再把每个请求绑定到某个角色（即该请求具备了访问权限）。接下来把认证和授权进行结合，梳理出了 Web 应用程序访问场景下的安全性实现方案，如下图所示：从上图可以看到，用户首先通过请求传递用户凭证完成用户认证，然后根据该用户信息中所具备的角色信息获取访问权限，最终完成对 HTTP 端点的访问授权。对一个 Web 应用程序进行安全性设计时： 首先需要考虑认证和授权，因为它们是核心考虑点； 在技术实现场景中，只要涉及用户认证，势必会涉及用户密码等敏感信息的加密； 针对用户密码的场景，主要使用单向散列加密算法对敏感信息进行加密。关于单向散列加密算法，它常用于生成消息摘要（Message Digest），主要特点为： 单向不可逆和密文长度固定，同时具备“碰撞”少的优点，即明文的微小差异会导致生成的密文完全不同。其中，常见的单向散列加密实现算法为 MD5（Message Digest 5）和 SHA（Secure Hash Algorithm）。而在 JDK 自带的 MessageDigest 类中，因为它已经包含了这些算法的默认实现，所以直接调用方法即可。在日常开发过程中，对于密码进行加密的典型操作时序图如下所示：上图中，引入了加盐（Salt）机制，进一步提升了加密数据的安全性。所谓加盐就是在初始化明文数据时，系统自动往明文中添加一些附加数据，然后再进行散列。目前，单向散列加密及加盐思想已被广泛用于系统登录过程中的密码生成和校验过程中，比如 Spring Security 框架。Spring Security 架构Spring Security 是 Spring 家族中历史比较悠久的一个框架，在 Spring Boot 出现之前，Spring Security 已经发展了很多年，尽管 Spring Security 的功能非常丰富，相比 Apache Shiro 这种轻量级的安全框架，它的优势就不那么明显了，加之应用程序中集成和配置 Spring Security 框架的过程比较复杂，因此它的发展过程并不是那么顺利。而正是随着 Spring Boot 的兴起，带动了 Spring Security 的发展。它专门针对 Spring Security 提供了一套完整的自动配置方案，可以零配置使用 Spring Security。Spring Security 中的过滤器链与业务中大多数处理 Web 请求的框架对比后，发现 Spring Security 中采用的是管道-过滤器（Pipe-Filter）架构模式，如下图所示：在上图中可以看到，处理业务逻辑的组件称为过滤器，而处理结果的相邻过滤器之间的连接件称为管道，它们构成了一组过滤器链，即 Spring Security 的核心。项目一旦启动，过滤器链将会实现自动配置，如下图所示：在上图中，看到了 BasicAuthenticationFilter、UsernamePasswordAuthenticationFilter 等几个常见的 Filter。这些类可以直接或间接实现 Servlet 类中的 Filter 接口，并完成某一项具体的认证机制。例如，上图中的 BasicAuthenticationFilter 用来认证用户的身份，而 UsernamePasswordAuthenticationFilter 用来检查输入的用户名和密码，并根据认证结果来判断是否将结果传递给下一个过滤器。这里请注意，整个 Spring Security 过滤器链的末端是一个 FilterSecurityInterceptor，本质上它也是一个 Filter，但它与其他用于完成认证操作的 Filter 不同，因为它的核心功能是用来实现权限控制，即判定该请求是否能够访问目标 HTTP 端点。因为我们可以把 FilterSecurityInterceptor 对权限控制的粒度划分到方法级别，所以它能够满足前面提到的精细化访问控制。通过上述分析，知道了在 Spring Security 中，认证和授权这两个安全性需求主要通过一系列的过滤器进行实现。基于过滤器链，Spring Security 的核心类结构：Spring Security 中的核心类先以最基础的 UsernamePasswordAuthenticationFilter 为例，该类的定义及核心方法 attemptAuthentication 如下代码所示：public class UsernamePasswordAuthenticationFilter extends AbstractAuthenticationProcessingFilter { public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException {        if (postOnly &amp;amp;&amp;amp; !request.getMethod().equals(&quot;POST&quot;)) {            throw new AuthenticationServiceException(&quot;Authentication method not supported: &quot; + request.getMethod());        }        String username = obtainUsername(request);        String password = obtainPassword(request);        if (username == null) {            username = &quot;&quot;;        }        if (password == null) {            password = &quot;&quot;;        }        username = username.trim();        UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken(username, password);        // Allow subclasses to set the &quot;details&quot; property        setDetails(request, authRequest);        return this.getAuthenticationManager().authenticate(authRequest); } …}围绕上述方法，通过翻阅 Spring Security 源代码，引出了该框架中一系列核心类，并梳理了它们之间的交互结构，如下图所示：上图中的很多类，通过名称就能明白它的含义和作用。以位于左下角的 SecurityContextHolder 为例，它是一个典型的 Holder 类，存储了应用的安全上下文对象 SecurityContext，包含系统请求中最近使用的认证信息。这里大胆猜想它的内部肯定使用了 ThreadLocal 来确保线程访问的安全性。而正如 UsernamePasswordAuthenticationFilter 中的代码所示，一个 HTTP 请求到达系统后，将通过一系列的 Filter 完成用户认证，然后具体的工作交由 AuthenticationManager 完成，AuthenticationManager 成功验证后会返回填充好的 Authentication 实例。AuthenticationManager 是一个接口，在其实现 ProviderManager 类时会进一步依赖 AuthenticationProvider 接口完成具体的认证工作。而在 Spring Security 中存在一大批 AuthenticationProvider 接口的实现类，分别完成各种认证操作。在执行具体的认证工作时，Spring Security 势必会使用用户详细信息，上图位于右边的 UserDetailsService 服务就是用来对用户详细信息实现管理。PS: 简要描述下在安全访问控制过程中，过滤器机制所发挥的作用是什么？ —— 实现认证和授权这两个安全性需求。过滤器本质上是对整个请求过程进行拦截" }, { "title": "基于 Spring Security 确保请求安全访问", "url": "/posts/spring-security-safe/", "categories": "Spring", "tags": "SpringBoot, Spring Security", "date": "2018-03-06 09:32:00 +0000", "snippet": "在日常开发过程中，需要对 Web 应用中的不同 HTTP 端点进行不同粒度的权限控制，并且希望这种控制方法足够灵活。而借助 Spring Security 框架，就可以对其进行简单实现。对 HTTP 端点进行访问授权管理在一个 Web 应用中，权限管理的对象是通过 Controller 层暴露的一个个 HTTP 端点，而这些 HTTP 端点就是需要授权访问的资源。开发人员使用 Spring Security 中提供的一系列丰富技术组件，即可通过简单的设置对权限进行灵活管理。使用配置方法实现访问授权的第一种方法是使用配置方法，关于配置方法的处理过程也是位于 WebSecurityConfigurerAdapter 类中，但使用的是 configure(HttpSecurity http) 方法，如下代码所示：protected void configure(HttpSecurity http) throws Exception {    http.authorizeRequests() .anyRequest() .authenticated() .and() .formLogin() .and() .httpBasic();}上述代码就是 Spring Security 中作用于访问授权的默认实现方法，这里用到了多个常见的配置方法。访问任何端点时，一旦在代码类路径中引入了 Spring Security 框架，就会弹出一个登录界面从而完成用户认证。因为认证是授权的前置流程，认证结束后就可以进入授权环节。结合这些配置方法的名称，实现这种默认的授权效果的具体步骤： 通过 HttpSecurity 类的 authorizeRequests() 方法，我们可以对所有访问 HTTP 端点的 HttpServletRequest 进行限制； anyRequest().authenticated() 语句指定了所有请求都需要执行认证，也就是说没有通过认证的用户无法访问任何端点； formLogin() 语句指定了用户需要使用表单进行登录，即会弹出一个登录界面； httpBasic() 语句使用 HTTP 协议中的 Basic Authentication 方法完成认证。当然，Spring Security 中还提供了很多其他有用的配置方法灵活使用，如下表，一起来看下: 配置方法 作用 anonymous() 允许匿名访问 authenticated() 允许认证用户访问 denyAll() 无条件禁止一切访问 hasAnyAuthority(String) 允许具有任意权限的用户进行访问 hasAnyRole(String) 允许具有任意角色的用户进行访问 hasAuthority(String) 允许具有特定权限的用户进行访问 haslpAddress(String) 允许来自特定 IP 地址的用户进行访问 hasRole(String) 允许具有特定角色的用户进行访问 permitAll() 无条件允许一切访问 rememberMe() 允许 remember-me 的用户进行访问 基于上表中的配置方法，就可以通过 HttpSecurity 实现自定义的授权策略。比方说，希望针对/orders根路径下的所有端点进行访问控制，且只允许认证通过的用户访问，那么可以创建一个继承了 WebSecurityConfigurerAdapter 类的 SpringCssSecurityConfig，并覆写其中的 configure(HttpSecurity http) 方法来实现，如下代码所示：@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter {    @Override    public void configure(HttpSecurity http) throws Exception {        http.authorizeRequests()            .antMatchers(&quot;/orders/**&quot;)            .authenticated();    }}请注意：虽然上表中的这些配置方法非常有用，但是由于我们无法基于一些来自环境和业务的参数灵活控制访问规则，也就存在一定的局限性。为此，Spring Security 还提供了一个 access() 方法，该方法允许传入一个表达式进行更细粒度的权限控制，这里，将引入 Spring 框架提供的一种动态表达式语言—— SpEL（Spring Expression Language 的简称）。只要 SpEL 表达式的返回值为 true，access() 方法就允许用户访问，如下代码所示：@Overridepublic void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .antMatchers(&quot;/orders&quot;) .access(&quot;hasRole(&#39;ROLE_USER&#39;)&quot;);}上述代码中，假设访问/orders端点的请求必须具备ROLE_USER角色，通过 access 方法中的 hasRole 方法，即可灵活地实现这个需求。当然，除了使用 hasRole 外，还可以使用 authentication、isAnonymous、isAuthenticated、permitAll 等表达式进行实现。因这些表达式的作用与前面介绍的配置方法一致。使用注解除了使用配置方法，Spring Security 提供了 @PreAuthorize 注解实现类似的效果，该注解定义如下代码所示：@Target({ ElementType.METHOD, ElementType.TYPE })@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface PreAuthorize {    //通过 SpEL 表达式设置访问控制    String value();}可以看到 @PreAuthorize 的原理与 access() 方法一样，即通过传入一个 SpEL 表达式设置访问控制，如下所示代码就是一个典型的使用示例：@RestController@RequestMapping(value=&quot;orders&quot;)public class OrderController { @PostMapping(value = &quot;/&quot;) @PreAuthorize(&quot;hasRole(ROLE_ADMIN)&quot;) public void addOrder(@RequestBody Order order) {     … }}从这个示例中可以看到，在/orders/这个 HTTP 端点上，添加了一个 @PreAuthorize 注解用来限制只有角色为ROLE_ADMIN的用户才能访问该端点。其实，Spring Security 中用于授权的注解还有 @PostAuthorize，它与 @PreAuthorize 注解是一组，主要用于请求结束之后检查权限。因这种情况比较少见，实现多维度访问授权方案HTTP 端点是 Web 应用程序的一种资源，而每个 Web 应用程序对于自身资源的保护粒度因服务而异。对于一般的 HTTP 端点，用户可能通过认证就可以访问；对于一些重要的 HTTP 端点，用户在已认证的基础上还会有一些附加要求。对资源进行保护的三种粒度级别： 用户级别： 该级别是最基本的资源保护级别，只要是认证用户就可能访问服务内的各种资源。 用户 + 角色级别： 该级别在认证用户级别的基础上，还要求用户属于某一个或多个特定角色。 用户 + 角色 + 操作级别： 该级别在认证用户 + 角色级别的基础上，对某些 HTTP 操作方法做了访问限制。基于配置方法和注解，可以轻松实现上述三种访问授权方案。使用用户级别保护服务访问因为 CustomerController 是 spring-css 案例中的核心入口，所以我们它的所有端点都应该受到保护。于是，在 customer-service 中，创建了一个 SpringCssSecurityConfig 类继承 WebSecurityConfigurerAdapter，如下代码所示：@Configurationpublic class SpringCssSecurityConfig extends WebSecurityConfigurerAdapter {    @Override    public void configure(HttpSecurity http) throws Exception {        http.authorizeRequests()            .anyRequest()            .authenticated();    }}位于 configure() 方法中的 .anyRequest().authenticated() 语句指定了访问 customer-service 下的所有端点的任何请求都需要进行验证。因此，当使用普通的 HTTP 请求访问 CustomerController 中的任何 URL（例如 http://localhost:8083/customers/1），将会得到如下图代码所示的错误信息，该错误信息明确指出资源的访问需要进行认证。{    &quot;error&quot;: &quot;access_denied&quot;,    &quot;error_description&quot;: &quot;Full authentication is required to access to this resource&quot;}覆写 WebSecurityConfigurerAdapter 的 config(AuthenticationManagerBuilder auth) 方法时提供了一个用户名springcss_user，现在就用这个用户名来添加用户认证信息并再次访问该端点。显然，因为此时传入的是有效的用户信息，所以可以满足认证要求。使用用户 + 角色级别保护服务访问对于某些安全性要求比较高的 HTTP 端点，通常需要限定访问的角色。例如，customer-service 服务中涉及客户工单管理等核心业务，认为不应该给所有的认证用户开放资源访问入口，而应该限定只有角色为ADMIN的管理员才开放。这时，就可以使用认证用户 + 角色保护服务的访问控制机制，具体的示例代码如下所示：@Configurationpublic class SpringCssSecurityConfig extends WebSecurityConfigurerAdapter {    @Override    public void configure(HttpSecurity http) throws Exception {        http.authorizeRequests()            .antMatchers(&quot;/customers/**&quot;)            .hasRole(&quot;ADMIN&quot;)            .anyRequest()            .authenticated();    }}在上述代码中可以看到，使用了HttpSecurity类中的antMatchers(&quot;/customer/&quot;)和hasRole(&quot;ADMIN&quot;)方法为访问/customers/的请求限定了角色，只有ADMIN 角色的认证用户才能访问以/customers/为根地址的所有 URL。如果使用了认证用户 + 角色的方式保护服务访问，使用角色为USER的认证用户springcss_user访问 customer-service 时就会出现如下所示的“access_denied”错误信息：{ &quot;error&quot;: &quot;access_denied&quot;, &quot;error_description&quot;: &quot;Access is denied&quot;}使用用户 + 角色+操作级别保护服务访问在认证用户+角色的基础上，我们需要再对具体的 HTTP 操作进行限制。在 customer-service 中，我们认为所有对客服工单的删除操作都很危险，因此可以使用 http.antMatchers(HttpMethod.DELETE, “/customers/**”) 方法对删除操作进行保护，示例代码如下：@Configurationpublic class SpringCssSecurityConfig extends WebSecurityConfigurerAdapter {    @Override    public void configure(HttpSecurity http) throws Exception{ http.authorizeRequests() .antMatchers(HttpMethod.DELETE, &quot;/customers/**&quot;) .hasRole(&quot;ADMIN&quot;) .anyRequest() .authenticated();    }}上述代码的效果在于对/customers端点执行删除操作时，需要使用具有ADMIN角色的springcss_admin用户，执行其他操作时不需要。因为如果使用springcss_user账户执行删除操作，还是会出现access_denied错误信息。" }, { "title": "Spring Boot 自动配置实现原理", "url": "/posts/spring-boot-auto-config/", "categories": "Spring", "tags": "SpringBoot, Spring Security", "date": "2018-03-05 09:32:00 +0000", "snippet": "Spring Boot 中的配置体系是一套强大而复杂的体系，其中最基础、最核心的要数自动配置（AutoConfiguration）机制了.@SpringBootApplication 注解@SpringBootApplication 注解位于 spring-boot-autoconfigure 工程的 org.springframework.boot.autoconfigure 包中，定义如下：//// Source code recreated from a .class file by IntelliJ IDEA// (powered by FernFlower decompiler)//@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = {TypeExcludeFilter.class}), @Filter(type = FilterType.CUSTOM, classes = {AutoConfigurationExcludeFilter.class})})public @interface SpringBootApplication { @AliasFor(annotation = EnableAutoConfiguration.class) Class&amp;lt;?&amp;gt;[] exclude() default {}; @AliasFor(annotation = EnableAutoConfiguration.class) String[] excludeName() default {}; @AliasFor(annotation = ComponentScan.class, attribute = &quot;basePackages&quot;) String[] scanBasePackages() default {}; @AliasFor(annotation = ComponentScan.class,attribute = &quot;basePackageClasses&quot;) Class&amp;lt;?&amp;gt;[] scanBasePackageClasses() default {}; @AliasFor(annotation = Configuration.class) boolean proxyBeanMethods() default true;}相较一般的注解，@SpringBootApplication 注解显得有点复杂。可以通过 exclude 和 excludeName 属性来配置不需要实现自动装配的类或类名，也可以通过 scanBasePackages 和 scanBasePackageClasses 属性来配置需要进行扫描的包路径和类路径。注意到 @SpringBootApplication 注解实际上是一个组合注解，它由三个注解组合而成，分别是 @SpringBootConfiguration、@EnableAutoConfiguration 和 @ComponentScan。@ComponentScan@ComponentScan 注解不是 Spring Boot 引入的新注解，而是属于 Spring 容器管理的内容。@ComponentScan 注解就是扫描基于 @Component 等注解所标注的类所在包下的所有需要注入的类，并把相关 Bean 定义批量加载到容器中。显然，Spring Boot 应用程序中同样需要这个功能。SpringBootConfiguration@SpringBootConfiguration 注解比较简单，事实上它是一个空注解，只是使用了 Spring 中的 @Configuration 注解。@Configuration 注解比较常见，提供了 JavaConfig 配置类实现。@EnableAutoConfiguration@EnableAutoConfiguration 注解是需要重点剖析的对象，该注解的定义如下代码所示：//// Source code recreated from a .class file by IntelliJ IDEA// (powered by FernFlower decompiler)//package org.springframework.boot.autoconfigure;import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Inherited;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;import org.springframework.context.annotation.Import;@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import({AutoConfigurationImportSelector.class})public @interface EnableAutoConfiguration { String ENABLED_OVERRIDE_PROPERTY = &quot;spring.boot.enableautoconfiguration&quot;; Class&amp;lt;?&amp;gt;[] exclude() default {}; String[] excludeName() default {};}这里关注两个新注解，@AutoConfigurationPackage 和 @Import(AutoConfigurationImportSelector.class)。@AutoConfigurationPackage 注解@AutoConfigurationPackage 注解定义如下：//// Source code recreated from a .class file by IntelliJ IDEA// (powered by FernFlower decompiler)//package org.springframework.boot.autoconfigure;import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Inherited;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;import org.springframework.boot.autoconfigure.AutoConfigurationPackages.Registrar;import org.springframework.context.annotation.Import;@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@Import({Registrar.class})public @interface AutoConfigurationPackage {}从命名上讲，该注解所在包下的类进行自动配置，而在实现方式上用到了 Spring 中的 @Import 注解。在使用 Spring Boot 时，@Import 也是一个非常常见的注解，可以用来动态创建 Bean。为了便于理解后续内容，有必要对 @Import 注解的运行机制做一些了解，该注解定义如下：//// Source code recreated from a .class file by IntelliJ IDEA// (powered by FernFlower decompiler)//package org.springframework.context.annotation;import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Import { Class&amp;lt;?&amp;gt;[] value();}在 @Import 注解的属性中可以设置需要引入的类名，例如 @AutoConfigurationPackage 注解上的 @Import(AutoConfigurationPackages.Registrar.class)。根据该类的不同类型，Spring 容器针对 @Import 注解有以下四种处理方式： 如果该类实现了 ImportSelector 接口，Spring 容器就会实例化该类，并且调用其 selectImports 方法； 如果该类实现了 DeferredImportSelector 接口，则 Spring 容器也会实例化该类并调用其 selectImports方法。DeferredImportSelector 继承了 ImportSelector，区别在于 DeferredImportSelector 实例的 selectImports 方法调用时机晚于 ImportSelector 的实例，要等到 @Configuration 注解中相关的业务全部都处理完了才会调用； 如果该类实现了 ImportBeanDefinitionRegistrar 接口，Spring 容器就会实例化该类，并且调用其 registerBeanDefinitions 方法； 如果该类没有实现上述三种接口中的任何一个，Spring 容器就会直接实例化该类。有了对 @Import 注解的基本理解，再来看 AutoConfigurationPackages.Registrar 类，定义如下：static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports { Registrar() { } public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { AutoConfigurationPackages.register(registry, (new AutoConfigurationPackages.PackageImport(metadata)).getPackageName()); } public Set&amp;lt;Object&amp;gt; determineImports(AnnotationMetadata metadata) { return Collections.singleton(new AutoConfigurationPackages.PackageImport(metadata)); }}可以看到这个 Registrar 类实现了前面第三种情况中提到的 ImportBeanDefinitionRegistrar 接口并重写了 registerBeanDefinitions 方法，该方法中调用 AutoConfigurationPackages 自身的 register 方法：public static void register(BeanDefinitionRegistry registry, String... packageNames) { if (registry.containsBeanDefinition(BEAN)) { BeanDefinition beanDefinition = registry.getBeanDefinition(BEAN); ConstructorArgumentValues constructorArguments = beanDefinition.getConstructorArgumentValues(); constructorArguments.addIndexedArgumentValue(0, addBasePackages(constructorArguments, packageNames)); } else { GenericBeanDefinition beanDefinition = new GenericBeanDefinition(); beanDefinition.setBeanClass(AutoConfigurationPackages.BasePackages.class); beanDefinition.getConstructorArgumentValues().addIndexedArgumentValue(0, packageNames); beanDefinition.setRole(2); registry.registerBeanDefinition(BEAN, beanDefinition); }}这个方法的逻辑是先判断整个 Bean 有没有被注册，如果已经注册则获取 Bean 的定义，通过 Bean 获取构造函数的参数并添加参数值；如果没有，则创建一个新的 Bean 的定义，设置 Bean 的类型为 AutoConfigurationPackages 类型并进行 Bean 的注册。AutoConfigurationImportSelector.class@EnableAutoConfiguration 注解中的 @Import(AutoConfigurationImportSelector.class) 部分，首先明确 AutoConfigurationImportSelector 类实现了 @Import 注解第二种情况中的 DeferredImportSelector 接口，所以会执行如下所示的 selectImports 方法： public String[] selectImports(AnnotationMetadata annotationMetadata) { if (!this.isEnabled(annotationMetadata)) { return NO_IMPORTS; } else { AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader.loadMetadata(this.beanClassLoader); AutoConfigurationImportSelector.AutoConfigurationEntry autoConfigurationEntry = this.getAutoConfigurationEntry(autoConfigurationMetadata, annotationMetadata); return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations()); } } protected AutoConfigurationImportSelector.AutoConfigurationEntry getAutoConfigurationEntry(AutoConfigurationMetadata autoConfigurationMetadata, AnnotationMetadata annotationMetadata) { if (!this.isEnabled(annotationMetadata)) { return EMPTY_ENTRY; } else { AnnotationAttributes attributes = this.getAttributes(annotationMetadata); // 获取 configurations 集合 List&amp;lt;String&amp;gt; configurations = this.getCandidateConfigurations(annotationMetadata, attributes); configurations = this.removeDuplicates(configurations); Set&amp;lt;String&amp;gt; exclusions = this.getExclusions(annotationMetadata, attributes); this.checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = this.filter(configurations, autoConfigurationMetadata); this.fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationImportSelector.AutoConfigurationEntry(configurations, exclusions); } }}这段代码的核心是通过 getCandidateConfigurations 方法获取 configurations 集合并进行过滤。getCandidateConfigurations 方法如下所示：protected List&amp;lt;String&amp;gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { List&amp;lt;String&amp;gt; configurations = SpringFactoriesLoader.loadFactoryNames(this.getSpringFactoriesLoaderFactoryClass(), this.getBeanClassLoader()); Assert.notEmpty(configurations, &quot;No auto configuration classes found in META-INF/spring.factories. If you are using a custom packaging, make sure that file is correct.&quot;); return configurations;}这段代码中可以先关注 Assert 校验，该校验是一个非空校验，会提示 在 META-INF/spring.factories 中没有找到自动配置类 这个异常信息。看到这里，不得不提到 JDK 中的 SPI 机制，因为无论从 SpringFactoriesLoader 这个类的命名上，还是 META-INF/spring.factories 这个文件目录，两者之间都存在很大的相通性。从类名上看，AutoConfigurationImportSelector 类是一种选择器，负责从各种配置项中找到需要导入的具体配置类。该类的结构如下图所示：SPI 机制和 SpringFactoriesLoader要想理解 SpringFactoriesLoader 类，首先需要了解 JDK 中 SPI（Service Provider Interface，服务提供者接口）机制。JDK 中的 SPI 机制JDK 提供了用于服务查找的一个工具类 java.util.ServiceLoader 来实现 SPI 机制。当服务提供者提供了服务接口的一种实现之后，可以在 jar 包的 META-INF/services/ 目录下创建一个以服务接口命名的文件，该文件里配置着一组 Key-Value，用于指定服务接口与实现该服务接口具体实现类的映射关系。而当外部程序装配这个 jar 包时，就能通过该 jar 包 META-INF/services/目录中的配置文件找到具体的实现类名，并装载实例化，从而完成模块的注入。SPI 提供了一种约定，基于该约定就能很好地找到服务接口的实现类，而不需要在代码里硬编码指定。JDK 中 SPI 机制开发流程如下： 设计一个服务接口，并提供对应实现类，可以根据扩展需求提供多种实现类； 在 META-INF/services/ 目录中创建一个以服务接口命名的文件，配置实现该服务接口的具体实现类； 外部程序通过 META-INF/services/ 目录下的配置文件找到具体的实现类名并实例化SpringFactoriesLoaderSpringFactoriesLoader 类似这种 SPI 机制，只不过以服务接口命名的文件是放在 META-INF/spring.factories 文件夹下，对应的 Key 为 EnableAutoConfiguration。SpringFactoriesLoader 会查找所有 META-INF/spring.factories 文件夹中的配置文件，并把 Key 为 EnableAutoConfiguration 所对应的配置项通过反射实例化为配置类并加载到容器中。这一点我们可以在 SpringFactoriesLoader 的 loadSpringFactories 方法中进行印证：private static Map&amp;lt;String, List&amp;lt;String&amp;gt;&amp;gt; loadSpringFactories(@Nullable ClassLoader classLoader) { MultiValueMap&amp;lt;String, String&amp;gt; result = (MultiValueMap)cache.get(classLoader); if (result != null) { return result; } else { try { Enumeration&amp;lt;URL&amp;gt; urls = classLoader != null ? classLoader.getResources(&quot;META-INF/spring.factories&quot;) : ClassLoader.getSystemResources(&quot;META-INF/spring.factories&quot;); LinkedMultiValueMap result = new LinkedMultiValueMap(); while(urls.hasMoreElements()) { URL url = (URL)urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); Iterator var6 = properties.entrySet().iterator(); while(var6.hasNext()) { Entry&amp;lt;?, ?&amp;gt; entry = (Entry)var6.next(); String factoryTypeName = ((String)entry.getKey()).trim(); String[] var9 = StringUtils.commaDelimitedListToStringArray((String)entry.getValue()); int var10 = var9.length; for(int var11 = 0; var11 &amp;lt; var10; ++var11) { String factoryImplementationName = var9[var11]; result.add(factoryTypeName, factoryImplementationName.trim()); } } } cache.put(classLoader, result); return result; } catch (IOException var13) { throw new IllegalArgumentException(&quot;Unable to load factories from location [META-INF/spring.factories]&quot;, var13); } }}以下就是 spring-boot-autoconfigure 工程中所使用的 spring.factories 配置文件片段，可以看到 EnableAutoConfiguration 项中包含了各式各样的配置项，这些配置项在 Spring Boot 启动过程中都能够通过 SpringFactoriesLoader 加载到运行时环境，从而实现自动化配置：# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\org.springframework.boot.autoconfigure.MessageSourceAutoConfiguration,\\org.springframework.boot.autoconfigure.PropertyPlaceholderAutoConfiguration,\\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\org.springframework.boot.autoconfigure.cloud.CloudAutoConfiguration,\\org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\\…以上就是 Spring Boot 中基于 @SpringBootApplication 注解实现自动配置的基本过程和原理。当然，@SpringBootApplication 注解也可以基于外部配置文件加载配置信息。基于约定优于配置思想，Spring Boot 在加载外部配置文件的过程中大量使用了默认配置。@ConditionalOn 系列条件注解Spring Boot 默认提供了 100 多个 AutoConfiguration 类，显然我们不可能会全部引入。所以在自动装配时，系统会去类路径下寻找是否有对应的配置类。如果有对应的配置类，则按条件进行判断，决定是否需要装配。这里就引出了在阅读 Spring Boot 代码时经常会碰到的另一批注解，即 @ConditionalOn 系列条件注解。@ConditionalOn 系列条件注解的示例先通过一个简单的示例来了解 @ConditionalOn 系列条件注解的使用方式，例如以下代码就是这类注解的一种典型应用，该代码位于 Spring Cloud Config 的客户端代码工程 spring-cloud-config-client 中：@Bean@ConditionalOnMissingBean(ConfigServicePropertySourceLocator.class)@ConditionalOnProperty(value = &quot;spring.cloud.config.enabled&quot;, matchIfMissing = true)public ConfigServicePropertySourceLocator configServicePropertySource(ConfigClientProperties properties) {        ConfigServicePropertySourceLocator locator = new ConfigServicePropertySourceLocator(properties);        return locator;}可以看到，这里运用了两个 @ConditionalOn 注解，一个是 @ConditionalOnMissingBean，一个是@ConditionalOnProperty。再比如在 Spring Cloud Config 的服务器端代码工程 spring-cloud-config-server 中，存在如下 ConfigServerAutoConfiguration 自动配置类:@Configuration@ConditionalOnBean(ConfigServerConfiguration.Marker.class)@EnableConfigurationProperties(ConfigServerProperties.class)@Import({ EnvironmentRepositoryConfiguration.class, CompositeConfiguration.class, ResourceRepositoryConfiguration.class,        ConfigServerEncryptionConfiguration.class, ConfigServerMvcConfiguration.class })public class ConfigServerAutoConfiguration {}这里我运用了@ConditionalOnBean 注解。实际上，Spring Boot 中提供了一系列的条件注解，常见的包括： @ConditionalOnProperty：只有当所提供的属性属于 true 时才会实例化 Bean @ConditionalOnBean：只有在当前上下文中存在某个对象时才会实例化 Bean @ConditionalOnClass：只有当某个 Class 位于类路径上时才会实例化 Bean @ConditionalOnExpression：只有当表达式为 true 的时候才会实例化 Bean @ConditionalOnMissingBean：只有在当前上下文中不存在某个对象时才会实例化 Bean @ConditionalOnMissingClass：只有当某个 Class 在类路径上不存在的时候才会实例化 Bean @ConditionalOnNotWebApplication：只有当不是 Web 应用时才会实例化 Bean当然 Spring Boot 还提供了一些不大常用的 @ConditionalOnXXX 注解，这些注解都定义在 org.springframework.boot.autoconfigure.condition 包中。显然上述 ConfigServicePropertySourceLocator 类中只有在 spring.cloud.config.enabled 属性为 true（通过 matchIfMissing 配置项表示默认即为 true）以及类路径上不存在 ConfigServicePropertySourceLocator 时才会进行实例化。而 ConfigServerAutoConfiguration 只有在类路径上存在 ConfigServerConfiguration.Marker 类时才会进行实例化，这是一种常用的自动配置控制技巧。@ConditionalOn 系列条件注解的实现原理@ConditionalOn 系列条件注解非常多，无意对所有这些组件进行展开。事实上这些注解的实现原理也大致相同，只需要深入了解其中一个就能做到触类旁通。这里挑选 @ConditionalOnClass 注解进行展开，该注解定义如下：@Target({ ElementType.TYPE, ElementType.METHOD })@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(OnClassCondition.class)public @interface ConditionalOnClass { Class&amp;lt;?&amp;gt;[] value() default {}; String[] name() default {};}可以看到， @ConditionalOnClass 注解本身带有两个属性，一个 Class 类型的 value，一个 String 类型的 name，所以我们可以采用这两种方式中的任意一种来使用该注解。同时 ConditionalOnClass 注解本身还带了一个 @Conditional(OnClassCondition.class) 注解。所以， ConditionalOnClass 注解的判断条件其实就包含在 OnClassCondition 这个类中。OnClassCondition 是 SpringBootCondition 的子类，而 SpringBootCondition 又实现了 Condition 接口。Condition 接口只有一个 matches 方法，如下所示：public interface Condition { boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata);}SpringBootCondition 中的 matches 方法实现如下：@Overridepublic final boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) { String classOrMethodName = getClassOrMethodName(metadata); try { ConditionOutcome outcome = getMatchOutcome(context, metadata); logOutcome(classOrMethodName, outcome); recordEvaluation(context, classOrMethodName, outcome); return outcome.isMatch(); } //省略其他方法}这里的 getClassOrMethodName 方法获取被添加了@ConditionalOnClass 注解的类或者方法的名称，而 getMatchOutcome 方法用于获取匹配的输出。我们看到 getMatchOutcome 方法实际上是一个抽象方法，需要交由 SpringBootCondition 的各个子类完成实现，这里的子类就是 OnClassCondition 类。在理解 OnClassCondition 时，我们需要明白在 Spring Boot 中，@ConditionalOnClass 或者 @ConditionalOnMissingClass 注解对应的条件类都是 OnClassCondition，所以在 OnClassCondition 的 getMatchOutcome 中会同时处理两种情况。这里我们挑选处理 @ConditionalOnClass 注解的代码，核心逻辑如下所示：ClassLoader classLoader = context.getClassLoader();ConditionMessage matchMessage = ConditionMessage.empty();List&amp;lt;String&amp;gt; onClasses = getCandidates(metadata, ConditionalOnClass.class);if (onClasses != null) { List&amp;lt;String&amp;gt; missing = getMatches(onClasses, MatchType.MISSING, classLoader); if (!missing.isEmpty()) { return ConditionOutcome.noMatch( ConditionMessage.forCondition(ConditionalOnClass.class).didNotFind(&quot;required class&quot;, &quot;required classes&quot;).items(Style.QUOTE, missing) );    } matchMessage = matchMessage .andCondition(ConditionalOnClass.class) .found(&quot;required class&quot;, &quot;required classes&quot;) .items(Style.QUOTE, getMatches(onClasses, MatchType.PRESENT, classLoader));}这里有两个方法值得注意，一个是 getCandidates 方法，一个是 getMatches 方法。首先通过 getCandidates 方法获取了 ConditionalOnClass 的 name 属性和 value 属性。然后通过 getMatches 方法将这些属性值进行比对，得到这些属性所指定的但在类加载器中不存在的类。如果发现类加载器中应该存在但事实上又不存在的类，则返回一个匹配失败的 Condition；反之，如果类加载器中存在对应类的话，则把匹配信息进行记录并返回一个 ConditionOutcome。ps：在日常开发过程中，诸如 SPI 机制和 @ConditionalOn 系列条件注解也都可以直接应用到我们自身的系统设计和开发中，从而提供高扩展性的架构实现方案。" }, { "title": "基于 Spring Security 构建用户认证体系", "url": "/posts/spring-security-user-auth/", "categories": "Spring", "tags": "SpringBoot, Spring Security", "date": "2018-03-04 09:32:00 +0000", "snippet": "在 Spring Boot 中整合 Spring Security 框架的方式非常简单，我们只需要在 pom 文件中引入 spring-boot-starter-security 依赖即可，这与以往需要提供很多配置才能与 Spring Security 完成集成的开发过程不同，如下代码所示：&amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;spring-boot-starter-security&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;请注意，只要在代码工程中添加了上述依赖，包含在该工程中的所有 HTTP 端点都将被保护起来。例如，在 spring-css 案例的 account-service 中，存在一个 AccountController ，且它暴露了一个“accounts/ /{accountId}”端点。现在，启动 account-service 服务并访问上述端点，弹出了如下图所示的界面内容：同时，在系统的启动控制台日志中，有如下所示的日志信息:Using generated security password: 17bbf7c4-456a-48f5-a12e-a680066c8f80可以看到，Spring Security 为自动生成了一个密码，可以基于user这个账号及上述密码登录这个界面。如果使用 Postman 可视化 HTTP 请求工具，可以设置授权类型为Basic Auth并输入对应的用户名和密码完成对 HTTP 端点的访问。事实上，在引入 spring-boot-starter-security 依赖之后，Spring Security 会默认创建一个用户名为“user”的账号。很显然，每次启动应用时，通过 Spring Security 自动生成的密码都会有所变化，因此它不适合作为一种正式的应用方法。如果想设置登录账号和密码，最简单的方式是通过配置文件。例如，可以在 account-service 的 application.yml 文件中添加如下代码所示的配置项：spring:  security:    user:      name: springcss      password: springcss_password重启 account-service 服务后，就可以使用上述用户名和密码完成登录。虽然基于配置文件的用户信息存储方案简单且直接，但是显然缺乏灵活性，因此 Spring Security 为我供了多种存储和管理用户认证信息的方案。配置 Spring Security在 SpringSecurity 中，初始化用户信息所依赖的配置类是 WebSecurityConfigurer 接口，该接口实际上是一个空接口，继承了更为基础的 SecurityConfigurer 接口。在日常开发中，不需要自己实现这个接口，而是使用 ~=WebSecurityConfigurerAdapter 类简化该配置类的使用方式。比如我们可以通过继承 WebSecurityConfigurerAdapter 类并且覆写其中的 configure(AuthenticationManagerBuilder auth) 的方法完成配置工作。关于 WebSecurityConfigurer 配置类，首先我们需要明确配置的内容。实际上，初始化所使用的用户信息非常简单，只需要指定用户名（Username）、密码（Password）和角色（Role）这三项数据即可。在 WebSecurityConfigurer 类中，使用 AuthenticationManagerBuilder 类创建一个 AuthenticationManager 就能够轻松实现基于内存、LADP 和 JDBC 的验证。使用基于内存的用户信息存储方案使用 AuthenticationManagerBuilder 完成基于内存的用户信息存储方案。实现方法是调用 AuthenticationManagerBuilder 的 inMemoryAuthentication 方法，示例代码如下所示：@Overrideprotected void configure(AuthenticationManagerBuilder builder) throws Exception { builder.inMemoryAuthentication() .withUser(&quot;springcss_user&quot;) .password(&quot;password1&quot;) .roles(&quot;USER&quot;) .and() .withUser(&quot;springcss_admin&quot;) .password(&quot;password2&quot;) .roles(&quot;USER&quot;, &quot;ADMIN&quot;);}从上面的代码中，我们看到系统中存在”springcss _user”和”springcss _admin”这两个用户，其密码分别是”password1”和”password2”，分别代表着普通用户 USER 及管理员 ADMIN 这两个角色。在 AuthenticationManagerBuilder 中，上述 inMemoryAuthentication 的方法的实现过程如下代码所示：public InMemoryUserDetailsManagerConfigurer&amp;lt;AuthenticationManagerBuilder&amp;gt; inMemoryAuthentication() throws Exception { return apply(new InMemoryUserDetailsManagerConfigurer&amp;lt;&amp;gt;());}这里的 InMemoryUserDetailsManagerConfigurer 内部又使用到了 InMemoryUserDetailsManager 对象。通过深入该类，可以获取 Spring Security 中与用户认证相关的一大批核心对象，它们之间的关系如下图所示：首先，来看上图中代表用户详细信息的 UserDetails 接口，如下代码所示：public interface UserDetails extends Serializable {    //获取该用户的权限信息    Collection&amp;lt;? extends GrantedAuthority&amp;gt; getAuthorities();    //获取密码 String getPassword(); //获取用户名    String getUsername(); //判断该账户是否已失效    boolean isAccountNonExpired();    //判断该账户是否已被锁定    boolean isAccountNonLocked();    //判断该账户的凭证信息是否已失效    boolean isCredentialsNonExpired();    //判断该用户是否可用    boolean isEnabled();}在上述代码中，发现 UserDetails 存在一个子接口 MutableUserDetails，从命名上不难看出，后者是一个可变的 UserDetails，而可变的内容就是密码。关于 MutableUserDetails 接口的定义如下代码所示：interface MutableUserDetails extends UserDetails {    //设置密码    void setPassword(String password);}在 Spring Security 中，针对 UserDetails 还存在一个专门的 UserDetailsService，该接口专门用来管理 UserDetails，它的定义如下代码所示：public interface UserDetailsService {    //根据用户名获取用户信息    UserDetails loadUserByUsername(String username) throws UsernameNotFoundException;}而 UserDetailsManager 继承了UserDetailsService，并提供了一批针对 UserDetails 的操作接口，如下代码所示：public interface UserDetailsManager extends UserDetailsService {    //创建用户    void createUser(UserDetails user);    //更新用户    void updateUser(UserDetails user);    //删除用户    void deleteUser(String username);    //修改密码    void changePassword(String oldPassword, String newPassword);    //判断指定用户名的用户是否存在    boolean userExists(String username);}介绍完 UserDetailsManager 后再回到 InMemoryUserDetailsManager 类，它实现了 UserDetailsManager 接口中的所有方法，这些方法主要用来对用户信息进行维护，从而形成一条代码支线。为了完成用户信息的配置，还存在另外一条代码支线，即 UserDetailsManagerConfigurer。该类维护了一个 UserDetails 列表，并提供了一组 withUser 方法完成用户信息的初始化，如下代码所示：private final List&amp;lt;UserDetails&amp;gt; users = new ArrayList&amp;lt;&amp;gt;();public final C withUser(UserDetails userDetails) { this.users.add(userDetails); return (C) this;}从上述代码中，看到 withUser 方法返回的是一个 UserDetailsBuilder 对象，通过该对象可以实现类似 .withUser(&quot;springcss_user&quot;).password(&quot;password1&quot;).roles(&quot;USER&quot;) 这样的链式语法，从而完成用户信息的设置。请注意，这里的 .roles() 方法实际上是 .authorities() 方法的一种简写，因为 Spring Security 会在每个角色名称前自动添加ROLE_前缀，，可以通过如下所示的代码实现同样的功能：@Overrideprotected void configure(AuthenticationManagerBuilder builder) throws Exception { builder.inMemoryAuthentication() .withUser(&quot;springcss_user&quot;) .password(&quot;password1&quot;) .authorities(&quot;ROLE_USER&quot;) .and() .withUser(&quot;springcss_admin&quot;) .password(&quot;password2&quot;) .authorities(&quot;ROLE_USER&quot;, &quot;ROLE_ADMIN&quot;);}可以看到，基于内存的用户信息存储方案也比较简单，但是由于用户信息写死在代码中，因此同样缺乏灵活性。使用基于数据库的用户信息存储方案既然是将用户信息存储在数据库中，我们势必需要创建表结构。因此，在 Spring Security 的源文件中，我们可以找到对应的 SQL 语句，如下代码所示：CREATE TABLE users ( username varchar_ignorecase ( 50 ) NOT NULL PRIMARY KEY, PASSWORD varchar_ignorecase ( 500 ) NOT NULL, enabled boolean NOT NULL );CREATE TABLE authorities ( username varchar_ignorecase ( 50 ) NOT NULL,authority varchar_ignorecase ( 50 ) NOT NULL,CONSTRAINT fk_authorities_users FOREIGN KEY ( username ) REFERENCES users ( username ));CREATE UNIQUE INDEX ix_auth_username ON authorities ( username, authority );一旦在自己的数据库中创建了这两张表，且添加了相应数据，就可以直接注入一个 DataSource 对象查询用户数据，如下代码所示：@AutowiredDataSource dataSource;@Overrideprotected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.jdbcAuthentication().dataSource(dataSource) .usersByUsernameQuery(&quot;select username, password, enabled from Users &quot; + &quot;where username=?&quot;) .authoritiesByUsernameQuery(&quot;select username, authority from UserAuthorities &quot; + &quot;where username=?&quot;) .passwordEncoder(new BCryptPasswordEncoder());}这里使用了 AuthenticationManagerBuilder 的 jdbcAuthentication 方法配置数据库认证方式，而内部则使用了 JdbcUserDetailsManager 工具类。围绕 JdbcUserDetailsManager 整条代码链路的类层结构与 InMemoryUserDetailsManager 非常一致，在该类中定义了各种用户数据库查询的 SQL 语句，以及使用 JdbcTemplate 完成数据库访问的具体实现方法。可以对照 InMemoryUserDetailsManager 类层结构图进行分析。注意，在上述方法中，通过 jdbcAuthentication() 方法验证用户信息时，必须集成加密机制，即使用 passwordEncoder() 方法嵌入一个 PasswordEncoder 接口的实现类。在 Spring Security 中，PasswordEncoder 接口代表一种密码编码器，定义如下代码所示：public interface PasswordEncoder {    //对原始密码进行编码    String encode(CharSequence rawPassword);    //对提交的原始密码与库中存储的加密密码进行比对    boolean matches(CharSequence rawPassword, String encodedPassword);    //判断加密密码是否需要再次进行加密，默认返回false    default boolean upgradeEncoding(String encodedPassword) {        return false;    }}Spring Security 中内置了一大批 PasswordEncoder 接口的实现类，如下图所示：上图中，比较常用的算法如SHA-256 算法的 StandardPasswordEncoder、bcrypt 强哈希算法的 BCryptPasswordEncoder等。在实际案例中，我使用的是 BCryptPasswordEncoder，它的 encode 方法如下代码所示：public String encode(CharSequence rawPassword) { String salt; if (random != null) { salt = BCrypt.gensalt(version.getVersion(), strength, random); } else { salt = BCrypt.gensalt(version.getVersion(), strength); } return BCrypt.hashpw(rawPassword.toString(), salt);}可以看到，上述 encode 方法执行了两个步骤： 第一步是生成盐； 第二步是根据盐和明文密码生成最终的密文密码。实现定制化用户认证方案自此，明确了用户信息存储的实现过程实际上是完全可定制化，而 Spring Security 所做的工作只是把常见、符合一般业务场景的实现方式嵌入框架中。如果存在特殊的场景，完全可以通过自定义用户信息存储方案进行实现。UserDetails 接口代表用户详细信息，而 UserDetailsService 接口负责对 UserDetails 进行各种操作 。因此，实现定制化用户认证方案的关键是实现 UserDetails 和 UserDetailsService 这两个接口。扩展 UserDetails扩展 UserDetails 的方法的实质是直接实现该接口，例如可以构建如下所示的 SpringCssUser 类：public class SpringCssUser implements UserDetails { private static final long serialVersionUID = 1L; private Long id; private final String username; private final String password; private final String phoneNumber; // 省略getter/setter    @Override    public String getUsername() { return username;    }    @Override    public String getPassword() { return password;    } @Override public Collection&amp;lt;? extends GrantedAuthority&amp;gt; getAuthorities() { return Arrays.asList(new SimpleGrantedAuthority(&quot;ROLE_USER&quot;)); } @Override public boolean isAccountNonExpired() { return true; } @Override public boolean isAccountNonLocked() { return true; } @Override public boolean isCredentialsNonExpired() { return true; } @Override public boolean isEnabled() { return true; }}扩展 UserDetailsService实现 UserDetailsService 接口，如下代码所示：@Servicepublic class SpringCssUserDetailsService implements UserDetailsService { @Autowired private SpringCssUserRepository repository; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException { SpringCssUser user = repository.findByUsername(username); if (user != null) { return user; } else { throw new UsernameNotFoundException(&quot;spring-css User &#39;&quot; + username + &quot;&#39; not found&quot;); }  }}在 UserDetailsService 接口中，只需要实现 loadUserByUsername 方法就行。因此，可以基于 SpringCssUserRepository 的 findByUsername 方法，再根据用户名从数据库中查询数据。整合定制化配置使用自定义的 SpringCssUserDetailsService 完成用户信息的存储和查询，此时只需要对配置策略做一些调整，调整后的完整 SpringCssSecurityConfig 类如下代码所示：@Configurationpublic class SpringCssSecurityConfig extends WebSecurityConfigurerAdapter {    @Autowired    SpringCssUserDetailsService springCssUserDetailsService;    @Override    protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.userDetailsService(springCssUserDetailsService); }}注入了 SpringCssUserDetailsService，并将其添加到 AuthenticationManagerBuilder 中，这样 AuthenticationManagerBuilder 将基于自定义的 SpringCssUserDetailsService 完成 UserDetails 的创建和管理。" }, { "title": "Spring Data 对数据访问过程的统一抽象", "url": "/posts/spring-data/", "categories": "Spring", "tags": "SpringBoot, Spring Data", "date": "2018-03-02 09:32:00 +0000", "snippet": "事实上，JdbcTemplate 是相对偏底层的一个工具类，作为系统开发最重要的基础功能之一，数据访问层组件的开发方式在 Spring Boot 中也得到了进一步简化，并充分发挥了 Spring 家族中另一个重要成员 Spring Data 的作用。Spring Data 是 Spring 家族中专门用于数据访问的开源框架，其核心理念是对所有存储媒介支持资源配置从而实现数据访问。众所周知，数据访问需要完成领域对象与存储数据之间的映射，并对外提供访问入口，Spring Data 基于 Repository 架构模式抽象出一套实现该模式的统一数据访问方式。Spring Data 对数据访问过程的抽象主要体现在两个方面： 提供了一套 Repository 接口定义及实现； 实现了各种多样化的查询支持。Repository 接口及实现Repository 接口是 Spring Data 中对数据访问的最高层抽象，接口定义如下所示：public interface Repository&amp;lt;T, ID&amp;gt; {}在以上代码中，看到 Repository 接口只是一个空接口，通过泛型指定了领域实体对象的类型和 ID。在 Spring Data 中，存在一大批 Repository 接口的子接口和实现类，该接口的部分类层结构如下所示：可以看到 CrudRepository 接口是对 Repository 接口的最常见扩展，添加了对领域实体的 CRUD 操作功能，具体定义如下代码所示：//// Source code recreated from a .class file by IntelliJ IDEA// (powered by FernFlower decompiler)//package org.springframework.data.repository;import java.util.Optional;@NoRepositoryBeanpublic interface CrudRepository&amp;lt;T, ID&amp;gt; extends Repository&amp;lt;T, ID&amp;gt; { &amp;lt;S extends T&amp;gt; S save(S var1); &amp;lt;S extends T&amp;gt; Iterable&amp;lt;S&amp;gt; saveAll(Iterable&amp;lt;S&amp;gt; var1); Optional&amp;lt;T&amp;gt; findById(ID var1); boolean existsById(ID var1); Iterable&amp;lt;T&amp;gt; findAll(); Iterable&amp;lt;T&amp;gt; findAllById(Iterable&amp;lt;ID&amp;gt; var1); long count(); void deleteById(ID var1); void delete(T var1); void deleteAll(Iterable&amp;lt;? extends T&amp;gt; var1); void deleteAll();}这些方法都是自解释的，可以看到 CrudRepository 接口提供了保存单个实体、保存集合、根据 id 查找实体、根据 id 判断实体是否存在、查询所有实体、查询实体数量、根据 id 删除实体 、删除一个实体的集合以及删除所有实体等常见操作，其中几个方法的实现过程。在实现过程中，首先需要关注最基础的 save 方法。通过查看 CrudRepository 的类层结构，找到它的一个实现类 SimpleJpaRepository，这个类显然是基于 JPA 规范所实现的针对关系型数据库的数据访问类。save 方法如下代码所示：private final JpaEntityInformation&amp;lt;T, ?&amp;gt; entityInformation;private final EntityManager em;@Transactionalpublic &amp;lt;S extends T&amp;gt; S save(S entity) { if (this.entityInformation.isNew(entity)) { this.em.persist(entity); return entity; } else { return this.em.merge(entity); }}显然，上述 save 方法依赖于 JPA 规范中的 EntityManager，当它发现所传入的实体为一个新对象时，就会调用 EntityManager 的 persist 方法，反之使用该对象进行 merge。接着看一下用于根据 id 查询实体的 findById 方法，如下代码所示：public Optional&amp;lt;T&amp;gt; findById(ID id) { Assert.notNull(id, &quot;The given id must not be null!&quot;); Class&amp;lt;T&amp;gt; domainType = this.getDomainClass(); if (this.metadata == null) { return Optional.ofNullable(this.em.find(domainType, id)); } else { LockModeType type = this.metadata.getLockModeType(); Map&amp;lt;String, Object&amp;gt; hints = this.getQueryHints().withFetchGraphs(this.em).asMap(); return Optional.ofNullable(type == null ? this.em.find(domainType, id, hints) : this.em.find(domainType, id, type, hints)); }}在执行查询过程中，findById 方法会根据领域实体的类型调用 EntityManager 的 find 方法来查找目标对象。需要注意的是，这里也会用到一些元数据 Metadata，以及涉及改变正常 SQL 执行效果的 Hint 机制的使用。多样化查询支持在日常开发过程中，数据查询的操作次数要远高于数据新增、数据删除和数据修改，因此在 Spring Data 中，除了对领域对象提供默认的 CRUD 操作外，我们还需要对查询场景高度抽象。而在现实的业务场景中，最典型的查询操作是 @Query 注解和方法名衍生查询机制。@Query 注解可以通过 @Query 注解直接在代码中嵌入查询语句和条件，从而提供类似 ORM 框架所具有的强大功能。下面就是使用 @Query 注解进行查询的典型例子：@Repositorypublic interface AccountRepository extends JpaRepository&amp;lt;Account, Long&amp;gt; { @Query(&quot;select a from Account a where a.accountName = ?1&quot;) Account findAccountByAccountName(String accountName);}注意到这里的 @Query 注解使用的是类似 SQL 语句的语法，它能自动完成领域对象 Account 与数据库数据之间的相互映射。因我们使用的是 JpaRepository，所以这种类似 SQL 语句的语法实际上是一种 JPA 查询语言，也就是所谓的 JPQL（Java Persistence Query Language）。JPQL 的基本语法如下所示：SELECT 子句 FROM 子句 [WHERE 子句] [GROUP BY 子句][HAVING 子句] [ORDER BY 子句]JPQL 语句是不是和原生的 SQL 语句非常类似，唯一的区别就是 JPQL FROM 语句后面跟的是对象，而原生 SQL 语句中对应的是数据表中的字段。@Query 注解定义，这个注解位于 org.springframework.data.jpa.repository 包中，如下所示：package org.springframework.data.jpa.repository;import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;import org.springframework.data.annotation.QueryAnnotation;@Retention(RetentionPolicy.RUNTIME)@Target({ElementType.METHOD, ElementType.ANNOTATION_TYPE})@QueryAnnotation@Documentedpublic @interface Query { String value() default &quot;&quot;; String countQuery() default &quot;&quot;; String countProjection() default &quot;&quot;; boolean nativeQuery() default false; String name() default &quot;&quot;; String countName() default &quot;&quot;;}@Query 注解中最常用的就是 value 属性，在前面示例中 JPQL 语句有使用到 。如果将 nativeQuery 设置为 true，那么 value 属性则需要指定具体的原生 SQL 语句。请注意，在 Spring Data 中存在一批 @Query 注解，分别针对不同的持久化媒介。例如 MongoDB 中存在一个 @Query 注解，但该注解位于 org.springframework.data.mongodb.repository 包中，定义如下：package org.springframework.data.mongodb.repository;public @interface Query {    String value() default &quot;&quot;;    String fields() default &quot;&quot;;    boolean count() default false;    boolean exists() default false;    boolean delete() default false;}与面向 JPA 的 @Query 注解不同的是，MongoDB 中 @Query 注解的 value 值是一串 JSON 字符串，用于指定需要查询的对象条件。方法名衍生查询方法名衍生查询也是 Spring Data 的查询特色之一，通过在方法命名上直接使用查询字段和参数，Spring Data 就能自动识别相应的查询条件并组装对应的查询语句。典型的示例如下所示：@Repositorypublic interface AccountRepository extends JpaRepository&amp;lt;Account, Long&amp;gt; { List&amp;lt;Account&amp;gt; findByFirstNameAndLastName(String firstName, String lastName);}在上面的例子中，通过 findByFirstNameAndLastname 这样符合普通语义的方法名，并在参数列表中按照方法名中参数的顺序和名称（即第一个参数是 fistName，第二个参数 lastName）传入相应的参数，Spring Data 就能自动组装 SQL 语句从而实现衍生查询。很神奇！而想要使用方法名实现衍生查询，需要对 Repository 中定义的方法名进行一定约束。首先需要指定一些查询关键字，常见的关键字如下表所示：有了这些查询关键字后，在方法命名上还需要指定查询字段和一些限制性条件。例如，在前面的示例中，只是基于“fistName”和“lastName”这两个字段做查询。事实上，可以查询的内容非常多，下表列出了更多的方法名衍生查询示例，如下：在 Spring Data 中，方法名衍生查询的功能非常强大，上表中罗列的这些也只是全部功能中的一小部分而已。如果在一个 Repository 中同时指定了 @Query 注解和方法名衍生查询，那么 Spring Data 会具体执行哪一个呢？要想回答这个问题，就需要对查询策略有一定的了解。在 Spring Data 中，查询策略定义在 QueryLookupStrategy 中，如下代码所示：public interface QueryLookupStrategy {    public static enum Key {        CREATE, USE_DECLARED_QUERY, CREATE_IF_NOT_FOUND;        public static Key create(String xml) {            if (!StringUtils.hasText(xml)) {                return null;            }            return valueOf(xml.toUpperCase(Locale.US).replace(&quot;-&quot;, &quot;_&quot;));        }    }    RepositoryQuery resolveQuery(Method method, RepositoryMetadata metadata, ProjectionFactory factory, NamedQueries namedQueries);}从以上代码中，看到 QueryLookupStrategy 分为三种，即 CREATE、USE_DECLARED_QUERY **和 **CREATE_IF_NOT_FOUND。这里的 CREATE 策略指的是直接根据方法名创建的查询策略，也就是使用前面介绍的方法名衍生查询。而 USE_DECLARED_QUERY 指的是声明方式，主要使用 @Query 注解，如果没有 @Query 注解系统就会抛出异常。而最后一种 CREATE_IF_NOT_FOUND 可以理解为是 @Query 注解和方法名衍生查询两者的兼容版。请注意，Spring Data 默认使用的是 CREATE_IF_NOT_FOUND 策略，也就是说系统会先查找 @Query 注解，如果查到没有，会再去找与方法名相匹配的查询。Spring Data 中的组件Spring Data 支持对多种数据存储媒介进行数据访问，表现为提供了一系列默认的 Repository，包括针对关系型数据库的 JPA/JDBC Repository，针对 MongoDB、Neo4j、Redis 等 NoSQL 对应的 Repository，支持 Hadoop 的大数据访问的 Repository，甚至包括 Spring Batch 和 Spring Integration 在内的系统集成的 Repository。在 Spring Data 的官方网站https://spring.io/projects/spring-data 中，列出了其提供的所有组件，可以在这里看到。根据官网介绍，Spring Data 中的组件可以分成四大类： 核心模块（Main modules）； 社区模块（Community modules）； 关联模块（Related modules）和正在孵化的模块（Modules in Incubation）。例如，前面介绍的 Respository 和多样化查询功能就在核心模块 Spring Data Commons 组件中。这里，特别想注意的是正在孵化的模块，它目前只包含一个组件，即 Spring Data R2DBC。 R2DBC 是 Reactive Relational Database Connectivity 的简写，代表响应式关系型数据库连接，相当于是响应式数据访问领域的 JDBC 规范。ps：不是 Mybatis 的替代品、另一种实现策略，是 Java 领域的 ORM 标准规范 ！！！ Spring Data 时，针对查询操作可以使用哪些高效的实现方法 ?" }, { "title": "Spring 技术体系", "url": "/posts/spring-technology-system/", "categories": "Spring", "tags": "Spring", "date": "2018-03-02 09:32:00 +0000", "snippet": "Spring 框架自 2003 年由 Rod Johnson 设计并实现以来，经历了多个重大版本的发展和演进，已经形成了一个庞大的家族式技术生态圈。目前，Spring 已经是 Java EE 领域最流行的开发框架，在全球各大企业中都得到了广泛应用。Spring 家族技术生态全景图通过来自 Spring 官网主页中的图可以看到，Spring 框架的七大核心技术体系： 微服务 响应式编程 云原生 Web 应用 Serverless 架构 事件驱动 批处理这些技术体系虽各自独立，但也有一定交集，例如： 微服务架构往往会与基于 Spring Cloud 的云原生技术结合在一起使用，并且微服务架构的构建过程也需要依赖于能够提供 RESTful 风格的 Web 应用程序等；另一方面，在具备特定的技术特点之外，这些技术体系也各有其应用场景。例如： 实现日常报表等轻量级的批处理任务，而又不想引入 Hadoop 这套庞大的离线处理平台时，使用基于 Spring Batch 的批处理框架是一个不错的选择； 实现与 Kafka、RabbitMQ 等各种主流消息中间件之间的集成，但又希望开发人员不需要了解这些中间件在使用上的差别，那么使用基于 Spring Cloud Stream 的事件驱动架构是首选，因为这个框架对外提供了统一的 API，从而屏蔽了内部各个中间件在实现上的差异性。这里不对 Spring 中的所有七大技术体系做全面的展开。在日常开发过程中： 如果构建单体 Web 服务，可以采用 Spring Boot； 如果想要开发微服务架构，则采用基于 Spring Boot 的 Spring Cloud，而 Spring Cloud 同样内置了基于 Spring Cloud Stream 的事件驱动架构。 同时，响应式编程是 Spring 5 引入的最大创新，代表了一种系统架构设计和实现的技术方向。现在的 Spring 家族技术体系都是在 Spring Framework 基础上逐步演进而来的。Spring Framework 的整体架构，如下图所示：Spring 从诞生之初就被认为是一种容器，上图中的 “核心容器” 部分就包含了一个容器所应该具备的核心功能，包括： 基于依赖注入机制的 JavaBean 处理； 面向切面 AOP； 上下文 Context； Spring 自身所提供的表达式工具等一些辅助功能。图中最上面的两个框就是构建应用程序所需要的最核心的两大功能组件，也是日常开发中最常用的组件，即数据访问和** Web 服务**。这两大部分功能组件中包含的内容非常多，而且充分体现了 Spring Framework 的集成性，也就是说，框架内部整合了业界主流的数据库驱动、消息中间件、ORM 框架等各种工具，我们根据需要灵活地替换和调整自己想要使用的工具。从开发语言上讲，虽然 Spring 应用最广泛的是在 Java EE 领域，但在当前的版本中，也支持 Kotlin、Groovy 以及各种动态开发语言。Spring Boot 与 Web 应用程序Spring Boot 构建在 Spring Framework 基础之上，是新一代的 Web 应用程序开发框架。可以通过下面这张图来了解 Spring Boot 的全貌：通过浏览 Spring 的官方网站，可以看到 Spring Boot 已经成为 Spring 中顶级的子项目。自 2014 年 4 月发布 1.0.0 版本以来，Spring Boot 俨然已经发展为 Java EE 领域开发 Web 应用程序的首选框架。Spring Cloud 与微服务架构Spring Cloud 构建在 Spring Boot 基础之上，它的整体架构图如下所示：技术组件的完备性是 Spring Cloud 框架的主要优势，它集成了业界一大批知名的微服务开发组件。例如： 服务治理：Spring Cloud Netfilx Eureka 服务容错：Spring Cloud Circuit Breaker 服务网关：Spring Cloud Gateway 配置中心：Spring Cloud Config 事件驱动：Spring Cloud Stream 服务安全：Spring Cloud Security 链路跟踪：Spring Cloud Sleuth 服务测试：Spring Cloud Contract 等等…..基于 Spring Boot 的开发便利性，Spring Cloud 巧妙地简化了微服务系统基础设施的开发过程，Spring Cloud 包含上图中所展示的服务发现注册、API 网关、配置中心、消息总线、负载均衡、熔断器、数据监控等。Spring 5 与响应式编程随着 Spring 5 的正式发布，我们迎来了响应式编程（Reactive Programming）的全新发展时期。Spring 5 中内嵌了与数据管理相关的响应式数据访问、与系统集成相关的响应式消息通信以及与 Web 服务相关的响应式 Web 框架等多种响应式组件，从而极大地简化了响应式应用程序的开发过程和开发难度。下图展示了响应式编程的技术栈与传统的 Servlet 技术栈之间的对比：从上图可以看到，上图左侧为基于 Spring WebFlux 的技术栈，右侧为基于 Spring MVC 的技术栈。传统的 Spring MVC 构建在 Java EE 的 Servlet 标准之上，该标准本身就是阻塞式和同步的，而 Spring WebFlux 基于响应式流，因此可以用来构建异步非阻塞的服务。在 Spring 5 中，选取了 Project Reactor 作为响应式流的实现库。由于响应式编程的特性，Spring WebFlux 和 Project Reactor 的运行需要依赖于诸如 Netty 和 Undertow 等支持异步机制的容器。同时也可以选择使用较新版本的 Tomcat 和 Jetty 作为运行环境，因为它们支持异步 I/O 的 Servlet 3.1。下图更加明显地展示了 Spring MVC 和 Spring WebFlux 之间的区别和联系：" }, { "title": "剖析一个 Spring Web 应用", "url": "/posts/spring-web-application/", "categories": "Spring", "tags": "SpringBoot, Spring MVC", "date": "2018-03-02 09:32:00 +0000", "snippet": "在典型的 Web 应用程序中，前后端通常采用基于 HTTP 协议完成请求和响应。HTTP 请求响应过程如下： HTTP 请求； URL 地址映射； HHTP 请求的参数构建； 对象（数据）序列化； 各个服务自身内部的业务逻辑处理； 对象（数据）反序列化； HTTP 响应基于 Spring MVC 完成上述开发流程所需要的开发步骤如下： 使用 web.xml 定义 Spring 的 DispatcherServlet； 完成启动 Spring MVC 的配置文件； 编写响应 HTTP 请求的 Controller； 将服务部署到 Tomcat Web 服务器事实上，基于传统的 Spring MVC 框架开发 Web 应用逐渐暴露出一些问题，比较典型的就是配置工作过于复杂和繁重，以及缺少必要的应用程序管理和监控机制。优化这一套开发过程，有几个点值得去挖掘，比方说减少不必要的配置工作、启动依赖项的自动管理、简化部署并提供应用监控等。这些优化点恰巧推动了以 Spring Boot 为代表的新一代开发框架的诞生，基于 Spring Boot 的开发流程见下： 使用 @SpringBootApplication 注解创建服务启动类； 编写响应 HTTP 请求的 Controller； 脱离服务器独立运行服务并启动服务监控作为 Spring 家族新的一员，Spring Boot 提供了令人兴奋的特性，这些特性的核心价值在于确保了开发过程的简单性，具体体现在编码、配置、部署、监控等多个方面。优点如下: 使编码更简单。只需在 Maven 中添加一项依赖并实现一个方法就可以提供微服务架构中所推崇的 RESTful 风格接口； 使配置更简单。它把 Spring 中基于 XML 的功能配置方式转换为 Java Config，同时提供了 .yml 文件来优化原有基于 .properties 和 .xml 文件的配置方案，.yml 文件对配置信息的组织更为直观方便，语义也更为强大。同时，基于 Spring Boot 的自动配置特性，对常见的各种工具和框架均提供了默认的 starter 组件来简化配置； 相较于传统模式下的 war 包，Spring Boot 部署包既包含了业务代码和各种第三方类库，同时也内嵌了 HTTP 容器。这种包结构支持 java –jar application.jar 方式的一键启动，不需要部署独立的应用服务器，通过默认内嵌 Tomcat 就可以运行整个应用程序。 基于 Spring Boot 新提供的 Actuator 组件，可以通过 RESTful 接口获取应用程序的当前运行时状态并对这些状态背后的度量指标进行监控和报警。例如： 可以通过/env/{name}端点获取系统环境变量； 通过/mapping端点获取所有 RESTful 服务； 通过/dump端点获取线程工作状态； 通过/metrics/{name}端点获取 JVM 性能指标等 剖析一个 Spring Web 应用程序针对一个基于 Spring Boot 开发的 Web 应用程序，其代码组织方式需要遵循一定的项目结构(我主要使用 Maven 来管理项目工程中的结构和包依赖)，一个典型的 Web 应用程序的项目结构如下：在上图中，有几个地方需要特别注意，我也在图中做了专门的标注，分别是包依赖、启动类、控制器类以及配置文件。包依赖Spring Boot 提供了一系列 starter 工程来简化各种组件之间的依赖关系。以开发 Web 服务为例，我们需要引入 spring-boot-starter-web 这个工程，而这个工程中并没有具体的代码，只是包含了一些 pom 依赖，如下： org.springframework.boot:spring-boot-starter org.springframework.boot:spring-boot-starter-tomcat org.springframework.boot:spring-boot-starter-validation com.fasterxml.jackson.core:jackson-databind org.springframework:spring-web org.springframework:spring-webmvc这里包括了传统 Spring MVC 应用程序中会使用到的 spring-web 和 spring-webmvc 组件，因此 Spring Boot 在底层实现上还是基于这两个组件完成对 Web 请求响应流程的构建。如果使用 Spring Boot 2.2.4 版本，你会发现它所依赖的 Spring 组件都升级到了 5.X 版本。在应用程序中引入 spring-boot-starter-web 组件就像引入一个普通的 Maven 依赖一样，如下所示。&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;一旦 spring-boot-starter-web 组件引入完毕，就可以充分利用 Spring Boot 提供的自动配置机制开发 Web 应用程序。启动类使用 Spring Boot 的最重要的一个步骤是创建一个 Bootstrap 启动类。Bootstrap 类结构简单且比较固化，如下所示：package cn.happymaya.customer;@SpringBootApplicationpublic class CustomerApplication { public static void main(String[] args) { SpringApplication.run(CustomerApplication.class, args); }}控制器类Bootstrap 类提供了 Spring Boot 应用程序的入口，相当于应用程序已经有了最基本的骨架。接下来我们就可以添加 HTTP 请求的访问入口，表现在 Spring Boot 中也就是一系列的 Controller 类。这里的 Controller 与 Spring MVC 中的 Controller 在概念上是一致的，一个典型的 Controller 类如下所示：@RestController@RequestMapping(value=&quot;customers&quot;)public class CustomerController { @Autowired private CustomerTicketService customerTicketService; @PostMapping(value = &quot;/{accountId}/{orderNumber}&quot;) public CustomerTicket generateCustomerTicket( @PathVariable(&quot;accountId&quot;) Long accountId, @PathVariable(&quot;orderNumber&quot;) String orderNumber) { CustomerTicket customerTicket = customerTicketService.generateCustomerTicket(accountId, orderNumber); return customerTicket; }}请注意，以上代码中包含了 @RestController；用于指定请求地址的映射关系 @RequestMapping；等同于指定了 GET 请求的 @RequestMapping 注解 @GetMapping 这三个注解：是传统 Spring MVC 中所提供的 @Controller 注解的升级版，相当于就是 @Controller 和 @ResponseEntity 注解的结合体，会自动使用 JSON 实现序列化/反序列化操作配置文件在 src/main/resources 目录下存在一个 application.yml 文件，这就是 Spring Boot 中的主配置文件。例如，可以将如下所示的端口、服务名称以及数据库访问等配置信息添加到这个配置文件中：server: port: 8081spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://119.3.52.175:3306/appointment username: root password: 1qazxsw2#edc 事实上，Spring Boot 提供了强大的自动配置机制，如果没有特殊的配置需求，完全可以基于 Spring Boot 内置的配置体系完成诸如数据库访问相关配置信息的自动集成。" }, { "title": "设计模式空间", "url": "/posts/design-pattern-space/", "categories": "Design Pattern", "tags": "设计模式, Design Pattern", "date": "2018-02-24 13:11:22 +0000", "snippet": "   目的       创建型 结构型 行为型 类 Factory Method* Adapter* Interpreter（解释器）       Template Method（模板方法） 对象 Abstract Factory* Adapter Chain of Responsibility（责任链）   Builder Bridge Command（命令）   Prototype Composite* Iterator（迭代器模式）   Singleton* Decorator* Mediator（中间者模式）     Facade Memento（备忘录模式）     Flayweight Obeserver（观察者模式）*     Proxy State（状态模式）       Strategy（策略模式）       Visitor（访问者模式） 模式依据其目的分为： 创建型（creational） 与对象的创建有关 结构型（structural） 处理类或对象的组合 行为型（behavioral），主要的关注点是对象内部算法以及对象之间的职责和分别，比如，具体实现算法、选择策略、状态变化等抽象概念 类行为型模式，使用继承的方式关联不同类之间的行为（描述算法和控制流） 对象行为型模式，使用组合或聚合方式来分配不同类之间的行为。 模式依据其范围，指定模式主要是用于类还是用于对象 类模式：处理类和子类之间的关系，这些关系通过继承建立、是静态的、在编译时便确定下来了； 对象模式：处理对象间的关系，这些关系在运行时是可以变化的，更具有动态性。从某种意义上，几乎所有模式都使用继承机制，因而“类模式”只指那些集中于处理类间关系的模式。大部分模式都属于“对象模式”的范畴。创建型类模式将对象的部分创建工作延迟到子类，创建型对象模式则将它延迟到另一个对象中；结构型类模式使用继承机制来组合类，结构型对象模式描述了对象的组装方式；学习设计模式真正的好处并不在于学会“如何使用”它们，而是在于通过分析学到“如何找到变化，如何封装变化”的思想精髓，并最终通过实践融合到实际编程中，对实际编码设计有帮助。创建型设计模式 单例模式（Singleton），类似于一种技巧，是工厂模式一种数量上的特例，相当于强制实现了有限、唯一对象的生产。 建造者模式（Builder），侧重点在于如何实现对象创建过程的自由组合，避免在代码中出现大量 new 式的硬编码。当对象结构发生改变时，能灵活增删步骤节点，还能避免对程序中大量分散 new 语句的修改。换句话说，它实现了对象创建过程的多态。 抽象工厂模式（Abstract Factory），重点是创建一组实现统一抽象产品的工厂对象族（同一个逻辑层级），本质上是为了寻找正确的抽象产品。它可以很好地保证被创建对象的工厂之间的一致性，常常用来解决跨平台的设计问题。 工厂方法模式（Factory Method），有效解决了创建对象时的不确定性。使用的办法就是将创建对象的时机延迟到了每一个具体的创建工厂中，让具体工厂自行解决对象的复杂创建过程，并通过统一的定义接口来保证创建对象时的可任意替换性。换句话说，它实现了对象创建时的多态。 原型模式（Prototype），就是一种将对象生成的责任代理给自己的模式，也就是“复制自我”。通过复制能快速建立运行的对象副本，最大的作用在于动态扩展运行时的对象能力。换句话说，它实现了对象拷贝的多态。总结成一句话就是，当软件系统中需要对象的创建、组合或聚集时，就可以考虑使用创建型模式“家族”来帮助你提升代码的灵活性。结构型设计模式 适配器模式，实现了不同接口功能之间的转换，为组件的快速复用提供了直接的解决办法。 桥接模式，实现了抽象实体和抽象行为之间的永久绑定，可以理解为在基于已有的构件上设计可能会发生变化的行为。它往往与抽象工厂模式共同用于跨平台设计的场景。 组合模式，用于表达整体和部分的关系，可以忽略单个对象和合成对象之间的差别。它实际采用的是一种树状结构。 装饰模式，与继承不同，通过代理方式实现了接口功能的多态，避免了大量子类的派生。它适用于链状和树状的结构，但容易造成对象与装饰器之间耦合度过高。 门面模式，用于对子系统提供统一的接口。 享元模式，用于解决大对象重复创建损耗资源的问题，通过共享对象池来复用对象。 代理模式，模式的结构与装饰模式非常相似，但侧重点不同，一种是修改对象的行为，另一种是控制访问。总体来说，这七种结构型模式重点关注的都是对象与对象之间的结构关系，以及如何更好地组合以扩展代码整体结构的灵活性。行为型设计模式行为型设计模式共有 11 种，每一种的要点可简单提炼和总结为如下： 访问者模式，在对象级别中实际为树型结构，与抽象工厂模式类似。它给使用者提供了一种统一访问树结构中数据节点的方式，因此具备灵活扩展的特性。 模板方法模式，定义一个算法模板，并将具体的执行步骤延迟到子类中实现。 策略模式，将多个不同的算法封装成策略，让它们可以互相替换，适合应用于对计算效率有一定要求的系统。策略模式通常会和工厂方法模式配合使用，为使用者提供一组使用策略。 状态模式，最常用的实现方式是状态机，大量应用于需要控制状态流转的系统中。常用在游戏、工作流引擎、购物流程等系统开发中。 观察者模式，是经典 MVC 模式的变形，与中介者模式的结构很类似，在结构上都是星形结构，但侧重点不同。观察者模式侧重于将观察者和被观察者代码解耦，中介者则侧重于充当两个对象之间的新媒介。观察者模式的应用场景非常广泛，比如，邮件订阅、公众号推送、RSS、消息中间件等。 备忘录模式，也叫快照模式，通常用于捕获一个对象的内部状态，比如保存、打开、关闭等状态，并在执行对象之外保存一个副本状态，方便用于之后恢复对象到某一个时间状态。 中介者模式，最大的作用在于解耦对象之间的直接引用，在结构上体现为将网状的结构变成以中介者为中心的星形结构，从而保证了对象行为上的稳定性，即不会因为新对象的引入造成大量类之间引用的修改。它的设计思想和分层思想很像，通过引入一个中间层，将层与层之间的多对多关系变为一对多关系。不过要注意，中间层不能设计得过于复杂而变成另一种过度依赖的层。 迭代器模式，大量应用于基础类库中，对重复遍历操作进行封装。现在大部分编程语言都提供了现成的迭代器可以使用，我们不需要从零开始开发。 解释器模式，为某个语言（编程语言也是语言）定义它的语法表示，比如 if-else 语法，并定义一个解释器用来处理这个语法。 命令模式，将某个命令（函数方法）封装成对象进行传递，关注的维度是命令，比如，打开、关闭文件的命令。用于处理多个命令调用和使用远程服务的场景。另外，它还会与备忘录模式结合在一起用于撤销和重做等场景。 责任链模式，用于链条状结构，将处理请求沿链条进行传递，动态指定职责的承担对象，由各自对象实现对应职责。比如，一个请求先经过 A 拦截器处理，然后再把请求传递给 B 拦截器，B 拦截器处理完后再传递给 C 拦截器，以此类推，形成一个链条，因此也叫拦截器模式。" }, { "title": "访问者模式：实现对象级别的矩阵结构", "url": "/posts/visitor/", "categories": "Design Pattern, behavioral", "tags": "设计模式, Design Pattern, Visitor, 访问者模式, 对象行为型模式", "date": "2018-02-24 13:11:22 +0000", "snippet": "访问者模式。一个原理看似很简单，但是理解起来有一定难度，使用场景相对较少的行为型模式：模式原理原始定义是：允许在运行时将一个或多个操作应用于一组对象，将操作与对象结构分离。这个定义非常的抽象，但是依然要找出其关键点，有两个： 运行时使用一组对象的一个或多个操作，比如，对不同类型的文件（.pdf、.xml、.properties）进行扫描； 分离对象的操作和对象本身的结构，比如，扫描多个文件夹下的多个文件，对于文件来说，扫描是额外的业务操作，如果在每个文件对象上都加一个扫描操作，太过于冗余，而扫描操作具有统一性，非常适合访问者模式。由此可见，访问者模式核心关注点是：分离一组对象结构和对象的操作，对象结构可以各不相同，但必须以某一个或一组操作作为连接的中心点。换句话说，访问者模式是以行为（某一个操作）作为扩展对象功能的出发点，在不改变已有类的功能的前提下进行批量扩展。访问者模式的 UML 图：从上图中，访问者模式包含的关键角色有四个： 访问者类（Visitor）：一个接口或抽象类，定义声明所有可访问类的访问操作； 访问者实现类（VisitorBehavior）：实现在访问者类中声明的所有访问方法； 访问角色类（Element）：定义一个可以获取访问操作的接口，这是使客户端对象能够“访问”的入口点； 访问角色实现类（Element A 等）：实现访问角色类接口的具体实现类，将访问者对象传递给此对象作为参数。也就是说，访问者模式可以在不改变各来访类的前提下定义作用于这些来访类的新操作。比如： 没有二维码时，旅游景区检票大门提供的访问者实现类就是人工检票，无论哪里来的游客只有购票、检票后才能入园； 有了二维码时，园区新增一个检票机，将二维码作为一种新的进景点的操作（对应访问者类），那么各类游客（对应访问角色类）可能用支付宝扫二维码，也可能使用微信扫二维码，这时二维码就是新的一个访问点。园区没有改变来访者，但是提供了一种新的操作，对于园区来说，不管是什么身份的人，对他们来说都只是一个访问者而已。UML 对应代码实现：public interface Visitor { void visitA(ElementA elementA); void visitB(ElementB elementB); //... //void visitN(ElementN elementN);}public class VisitorBehavior implements Visitor { @Override public void visitA(ElementA elementA) { int x = elementA.getAState(); x++; System.out.println(&quot;=== 当前A的state为：&quot;+x); elementA.setAState(x); } @Override public void visitB(ElementB elementB) { double x = elementB.getBState(); x++; System.out.println(&quot;=== 当前B的state为：&quot;+x); elementB.setBState(x); }}public interface Element { void accept(Visitor v);}public class ElementA implements Element { private int stateForA = 0; public void accept(Visitor v) { System.out.println(&quot;=== 开始访问元素 A......&quot;); v.visitA(this); } public int getAState(){ return stateForA; } public void setAState(int value){ stateForA = value; }}// 单元测试public class Demo { public static void main(String[] args) { List&amp;lt;Element&amp;gt; elementList = new ArrayList&amp;lt;&amp;gt;(); ElementA elementA = new ElementA(); elementA.setAState(11); ElementB elementB = new ElementB(); elementA.setAState(12); elementList.add(elementA); elementList.add(elementB); for (Element element :elementList) { element.accept(new VisitorBehavior()); } }}这段代码实现的逻辑比较简单，在单元测试中： 先建立了一个来访类的列表； 而后新建操作访问角色实现类 A 和访问角色实现类 B，将两个对象都加入来访类的列表中，创建相同的访问者实现类 VisitorBehavior，这样就完成了一个访问者模式。使用访问者模式的原因使用访问者模式的原因，主要有以下三个： 解决编程部分语言不支持动态双分派的能力。比如：Java 是静态多分派、动态单分派的语言。什么叫双分派？所谓双分派技术就是在选择一个方法的时候，不仅要根据消息接收者的运行时来判断，还要根据参数的运行时判断。与之对应的就是单分派，在选择一个方法的时候，只根据消息接收者的运行时来判断; 实际上，大多数时候我们都无法提前预测所有程序运行的行为，需要在运行时动态传入参数来改变程序的行为，对于 Java 这类语言就需要通过设计模式来弥补这部分功能。 需要动态绑定不同对象和对象操作。 比如，对不同类型的文件进行扫描、复制并转换新的文件、翻译不同语言文字等，在这类场景中，只是希望在程序运行过程中进行操作绑定，用完以后就释放。如果按照传统的方式，每个新操作都需要在类上增加方法，不仅得频繁修改代码，而且还会编译打包运行，这些都会非常耗时，这时使用访问者模式就能很好地解决这个问题； 通过行为与对象结构的分离，实现对象的职责分离，提高代码复用性。访问者模式能够在对象结构复杂的情况下动态地为对象添加操作，这就做到了对象的职责分离，尤其对于一些老旧的系统来说，能够快速地扩展功能，提高代码复用性。 使用场景分析通过不同路由访问不同操作系统的例子来说明。假设一家路由器软件的生产商，接了很多家不同硬件品牌的路由器的软件需求（比如，D-Link、TP-Link），这时需要针对不同的操作系统（比如，Linux 和 Windows）做路由器发送数据的兼容功能。首先，先定义了路由器的访问角色类 Router，如下代码：public interface Router { void sendData(char[] data); void accept(RouterVisitor v);}然后，再分别针对不同型号的路由器具体实现对应的功能。public class DLinkRouter implements Router{ @Override public void sendData(char[] data) {} @Override public void accept(RouterVisitor v) { v.visit(this); }}public class TPLinkRouter implements Router { @Override public void sendData(char[] data) {} @Override public void accept(RouterVisitor v) { v.visit(this); }}接下来，再配置一下访问者类 RouterVisitor、访问者实现类 LinuxRouterVisitor 和 WindowsRouterVisitor，用于给不同的路由器提供访问的入口点。public interface RouterVisitor { void visit(DLinkRouter router); void visit(TPLinkRouter router);}public class LinuxRouterVisitor implements RouterVisitor{ @Override public void visit(DLinkRouter router) { System.out.println(&quot;=== DLinkRouter Linux visit success!&quot;); } @Override public void visit(TPLinkRouter router) { System.out.println(&quot;=== TPLinkRouter Linux visit success!&quot;); }}public class WindowsRouterVisitor implements RouterVisitor{ @Override public void visit(DLinkRouter router) { System.out.println(&quot;=== DLinkRouter Windows visit success!&quot;); } @Override public void visit(TPLinkRouter router) { System.out.println(&quot;=== DLinkRouter Windows visit success!&quot;); }}到此就完成了所有配置，最后再运行一下测试：public class Client { public static void main(String[] args) { LinuxRouterVisitor linuxRouterVisitor = new LinuxRouterVisitor(); WindowsRouterVisitor windowsRouterVisitor = new WindowsRouterVisitor(); DLinkRouter dLinkRouter = new DLinkRouter(); dLinkRouter.accept(linuxRouterVisitor); dLinkRouter.accept(windowsRouterVisitor); TPLinkRouter tpLinkRouter = new TPLinkRouter(); tpLinkRouter.accept(linuxRouterVisitor); tpLinkRouter.accept(windowsRouterVisitor); }}// 输出结果// === DLinkRouter Linux visit success!// === DLinkRouter Windows visit success!// === TPLinkRouter Linux visit success!// === DLinkRouter Windows visit success!可以发现，不同型号的路由器可以在运行时动态添加（第一次分派），对于不同的操作系统来说，路由器可以动态地选择适配（第二次分派），整个过程完成了两次动态绑定。这也就引出了访问者模式常用的场景，大致总结为如下三类： 当对象的数据结构相对稳定，而操作却经常变化的时候。 比如，上面例子中路由器本身的内部构造（也就是数据结构）不会怎么变化，但是在不同操作系统下的操作可能会经常变化，比如，发送数据、接收数据等； 需要将数据结构与不常用的操作进行分离的时候。 比如，扫描文件内容这个动作通常不是文件常用的操作，但是对于文件夹和文件来说，和数据结构本身没有太大关系（树形结构的遍历操作），扫描是一个额外的动作，如果给每个文件都添加一个扫描操作会太过于重复，这时采用访问者模式是非常合适的，能够很好分离文件自身的遍历操作和外部的扫描操作； 需要在运行时动态决定使用哪些对象和方法的时候。 比如，对于监控系统来说，很多时候需要监控运行时的程序状态，但大多数时候又无法预知对象编译时的状态和参数，这时使用访问者模式就可以动态增加监控行为。所以说，访问者模式重点关注不同类型对象在运行时动态进行绑定，以及对多个对象增加统一操作的场景。优缺点访问者模式主要有以下优点： 简化客户端操作。比如，扫描文件时，对于客户端来说只需要执行扫描，而不需要关心不同类型的文件该怎么读取，也不用知道文件该如何被读取； 增加新的访问操作和访问者会非常便捷。在每次新增操作时，都是将对象视为统一的访问者，只需要关注操作是否正确地在对象上执行，而不需要关注对象如何被构建，这样使操作变得更加容易； 满足开闭原则。由于访问者模式没有对原有对象进行修改，只是新增了外部的统一操作，扩展新类不影响旧类，满足开闭原则； 满足单一职责原则。每一个行为都是一个单一的行为操作，能够组合相关类的功能逻辑，到代码内聚度更高； 通过行为能够快速组合一组复杂的对象结构。比如，访问角色类可能是树形结构，增加一个新操作后，对每一个对象都是进行的统一操作，这时新增一个其他结构的对象也能按照统一操作进行，便能将多个不同结构的对象进行自由组合。当然，访问者模式同样不是万能的，它也有一些缺点： 增加新的数据结构困难。因为新增结构，又需要新增操作，这时就让结构和操作发生关联，也就破坏了原有的模式，并且可能造成原有结构的不可用； 具体元素在变更时需要修改代码，容易引入问题。虽然访问者模式分离了操作和对象，但是当访问者对象本身发生变化时，依然需要修改代码，这时可能会对操作的方法造成影响。总结访问者模式能够通过添加新的行为来封装不同类型的对象，并隐藏不同对象各自的变化。这里所说的隐藏的变化主要包括： 允许添加新行为到一组对象里； 行为的实现和数量； 在运行时动态给对象添加额外行为； 访问者类和访问角色类之间的调用关系。访问者模式的原理理解起来不像之前结构型和创建型模式那么容易，主要原因在于行为的动态多变性，涉及很多编译原理和操作系统的知识。说实话，访问者模式在真实的应用中算得上是比较难的模式之一了。" }, { "title": "模板方法模式：实现同一模板框架下的算法扩展", "url": "/posts/template-method/", "categories": "Design Pattern, behavioral", "tags": "设计模式, Design Pattern, Template Method, 模板方法, 类行为型模式", "date": "2018-02-24 13:11:22 +0000", "snippet": "模板方法模式的原理和代码实现都比较简单，也被广泛应用，但是因为使用继承机制，副作用往往盖过了主要作用，所以在使用时尤其要小心谨慎。原理模板方法模式原始定义是：在操作中定义算法的框架，将一些步骤推迟到子类中。模板方法让子类在不改变算法结构的情况下重新定义算法的某些步骤。关键：解决算法框架这类特定的问题，同时明确表示需要使用继承的结构。UML 图：模板方法模式包含的关键角色有两个： 抽象父类：定义一个算法所包含的所有步骤，并提供一些通用的方法逻辑； 具体子类：继承自抽象父类，根据需要重写父类提供的算法步骤中的某些步骤。UML 对应代码实现：public abstract class AbstractClassTemplate { void step1(String key){ //dosomthing System.out.println(&quot;=== 在模板类里 执行步骤 1&quot;); if (step2(key)) { step3(); } else { step4(); } step5(); } boolean step2(String key){ System.out.println(&quot;=== 在模板类里 执行步骤 2&quot;); if (&quot;x&quot;.equals(key)) { return true; } return false; } abstract void step3(); abstract void step4(); void step5() { System.out.println(&quot;=== 在模板类里 执行步骤 5&quot;); } void run(String key){ step1(key); }}public class ConcreteClassA extends AbstractClassTemplate { @Override void step3() { System.out.println(&quot;===在子类 A 中 执行：步骤3&quot;); } @Override void step4() { System.out.println(&quot;===在子类 A 中 执行：步骤4&quot;); }}public class ConcreteClassB extends AbstractClassTemplate { @Override void step3() { System.out.println(&quot;===在子类 B 中 执行：步骤3&quot;); } @Override void step4() { System.out.println(&quot;===在子类 B 中 执行：步骤4&quot;); }}public class Demo { public static void main(String[] args) { AbstractClassTemplate concreteClassA = new ConcreteClassA(); concreteClassA.run(&quot;&quot;); System.out.println(&quot;===========&quot;); AbstractClassTemplate concreteClassB = new ConcreteClassB(); concreteClassB.run(&quot;x&quot;); }}// 输出结果：// === 在模板类里 执行步骤 1// === 在模板类里 执行步骤 2// ===在子类 A 中 执行：步骤4// === 在模板类里 执行步骤 5// ===========// === 在模板类里 执行步骤 1// === 在模板类里 执行步骤 2// ===在子类 B 中 执行：步骤3// === 在模板类里 执行步骤 5模板方法模式的实现原理很简单，就是一个父类下面的子类通过继承父类而使用通用的逻辑，同时根据各自需要优化其中某些步骤。使用模板方法的理由使用模板方法模式的原因主要有两个： 期望在一个通用的算法或流程框架下进行自定义开发。 比如，在使用 Jenkins 的持续集成发布系统中，可以定制一个固定的 Jenkins Job 任务，将打包、发布、部署的流程作为一个通用的流程。对于不同的系统来说，只需要根据自身的需求增加步骤或删除步骤，就能轻松定制自己的持续发布流程； 避免同样的代码逻辑进行重复编码。比如，当调用 HTTP 接口时，我们会使用 HttpClient、OkHttp 等工具类进行二次开发，不过我们经常会遇见这样的情况，明明有人已经封装过同样的功能，但还是忍不住想要对这些工具类进行二次封装开发，实际上最终实现的功能都只是调用 HTTP 接口，这样“重复造轮子”会非常浪费开发时间。这时如果使用模板方法模式定义个统一的调用 HTTP 接口的逻辑，就能很好地避免重复编码。使用场景模板方法模式的使用场景一般有： 多个类有相同的方法并且逻辑可以共用时； 将通用的算法或固定流程设计为模板，在每一个具体的子类中再继续优化算法步骤或流程步骤时； 重构超长代码时，发现某一个经常使用的公有方法。假设设计一个简单的持续集成发布系统——研发部开发的代码放在 GitLab 上，并使用一个固定的发布流程来进行程序的上线发布，使用模板方法模式来定义一系列规范的流程。首先，定义一个通用的流程框架类 DeployFlow，定义的步骤有六步，分别是： 从 GitLab 上拉取代码、编译打包、部署测试环境、测试、上传包到线上环境以及启动程序。其中，从 GitLab 上拉取代码、编译打包、部署测试环境和测试这四个步骤，需要子类来进行实现，代码如下：public abstract class DeployFlow { /* 使用final关键字来约束步骤不能轻易修改 */ public final void buildFlow() { /* 从GitLab上拉取代码 */ pullCodeFromGitlab(); /* 编译打包 */ compileAndPackage(); /* 部署测试环境 */ copyToTestServer(); /* 测试 */ testing(); /* 上传包到线上环境 */ copyToRemoteServer(); /* 启动程序 */ startApp(); } public abstract void pullCodeFromGitlab(); public abstract void compileAndPackage(); public abstract void copyToTestServer(); public abstract void testing(); private void copyToRemoteServer() { System.out.println(&quot;统一自动上传 启动App包到对应线上服务器&quot;); } private void startApp() { System.out.println(&quot;统一自动 启动线上App&quot;); }}然后，分别实现两个子类： 实现本地的打包编译和上传以及实现全自动化的持续集成式的发布。/* 实现全自动化的持续集成式的发布 */public class CicdDeployFlow extends DeployFlow { @Override public void pullCodeFromGitlab() { System.out.println(&quot;持续集成服务器将代码拉取到节点服务器上......&quot;); } @Override public void compileAndPackage() { System.out.println(&quot;自动进行编译&amp;amp;打包......&quot;); } @Override public void copyToTestServer() { System.out.println(&quot;自动将包拷贝到测试环境服务器......&quot;); } @Override public void testing() { System.out.println(&quot;执行自动化测试......&quot;); }}/* 实现本地的打包编译和上传 */public class LocalDeployFlow extends DeployFlow{ @Override public void pullCodeFromGitlab() { System.out.println(&quot;手动将代码拉取到本地电脑......&quot;); } @Override public void compileAndPackage() { System.out.println(&quot;在本地电脑上手动执行编译打包......&quot;); } @Override public void copyToTestServer() { System.out.println(&quot;手动通过 SSH 上传包到本地的测试服务......&quot;); } @Override public void testing() { System.out.println(&quot;执行手工测试......&quot;); }}最后，运行一个单元测试，测试一下本地发布 LocalDeployFlow 和持续集成发布 CicdDeployFlow：package cn.happymaya.ndp.template_method.example;public class Client { public static void main(String[] args) { System.out.println(&quot;开始本地手动发布流程======&quot;); DeployFlow localDeployFlow = new LocalDeployFlow(); localDeployFlow.buildFlow(); System.out.println(&quot;********************&quot;); System.out.println(&quot;开始 CICD 发布流程======&quot;); DeployFlow cicdDeployFlow = new CicdDeployFlow(); cicdDeployFlow.buildFlow(); }}// 输出结果// 开始本地手动发布流程======// 手动将代码拉取到本地电脑......// 在本地电脑上手动执行编译打包......// 手动通过 SSH 上传包到本地的测试服务......// 执行手工测试......// 统一自动上传 启动App包到对应线上服务器// 统一自动 启动线上App// ********************// 开始 CICD 发布流程======// 持续集成服务器将代码拉取到节点服务器上......// 自动进行编译&amp;amp;打包......// 自动将包拷贝到测试环境服务器......// 执行自动化测试......// 统一自动上传 启动App包到对应线上服务器// 统一自动 启动线上App模板方法模式应用场景的最大特征在于，通常是对算法的特定步骤进行优化，而不是对整个算法进行修改。一旦整体的算法框架被定义完成，子类便无法进行直接修改，因为子类的使用场景直接受到了父类场景的影响。优缺点模板方法模式主要有以下两个优点。 有效去除重复代码。 模板方法模式的父类保存通用的代码逻辑，这样可以让子类不再需要重复处理公用逻辑，只用关注特定的逻辑，从而起到去除子类中重复代码的目的。 有助于找到更通用的模板。 由于子类间重复的代码逻辑都会被抽取到父类中，父类也就慢慢变成了更通用的模板，这样有助于积累更多通用的模板，提升代码复用性和扩展性。模板方法模式的缺点： 不符合开闭原则。 一个父类调用子类实现操作，通过子类扩展增加新的行为，但是子类执行的结果便会受到父类的影响，不符合开闭原则的“对修改关闭”； 增加代码阅读的难度。 由于父类的某些步骤或方法被延迟到子类执行，那么需要跳转不同的子类阅读代码逻辑，如果子类的数量很多的话，跳转会很多，不方便联系上下文逻辑线索。而且模板方法中的步骤越多，其维护工作就可能会越困难。 违反里氏替换原则。 虽然模板方法模式中的父类会提供通用的实现方法，但是延迟到子类的操作便会变成某种定制化的操作，一旦替换子类，可能会导致父类不可用或整体逻辑发生变化。" }, { "title": "策略模式：解决不同活动策略营销推荐场景", "url": "/posts/strategy/", "categories": "Design Pattern, behavioral", "tags": "设计模式, Design Pattern, 策略模式, Strategy, 对象行为型模式", "date": "2018-02-23 14:18:32 +0000", "snippet": "模板方法模式能够进行公有方法的抽取，起到快速复用和扩展的作用。另一种快速复用和扩展代码的行为型模式：策略模式。策略模式在实际的开发中很常用，最常见的应用场景是利用它来替换过多的 if-else 嵌套的逻辑判断。除此之外，还能结合工厂模式给客户端提供非常灵活的使用体验。原理策略模式的原始定义是：定义一系列算法，封装每个算法，并使它们可以互换。策略让算法独立于使用它的客户端而变化。在这个定义中，策略模式明确表示应当由客户端自己决定在什么样的情况下使用哪些具体的策略。也就是说，服务端是作为一个策略的整体调控者，具体选择运行哪些策略其实是要交给客户端来决定的。比如，压缩文件的时候，提供一系列的不同压缩策略，像 gzip、zip 等，至于客户端在什么时候使用 gzip，由客户端自行去决定。同时，gzip 还可以被替换为其他的压缩策略。策略模式 UML 图的结构：策略模式包含了三个关键角色： 上下文信息类（Context）：用于存放和执行需要使用的具体策略类以及客户端调用的逻辑； 抽象策略类（Strategy）：定义策略的共有方法； 具体策略类（StrategyA 等）：实现抽象策略类定义的共有方法。UML 对应的代码实现：public interface IStrategy { void operation();}public class Context { public void request(IStrategy s) { s.operation(); }}public class StrategyA implements IStrategy{ @Override public void operation() { System.out.println(&quot;=== 执行策略 A ......&quot;); }}public class StrategyB implements IStrategy{ @Override public void operation() { System.out.println(&quot;=== 执行策略 B ......&quot;); }}策略模式的本质就是通过上下文信息类来作为中心控制单元，对不同的策略进行调度分配。使用场景策略模式常见的使用场景有以下几种： 系统中需要动态切换几种算法的场景； 使用多重的条件选择语句来实现的业务场景； 只希望客户端选择已经封装好的算法场景而不用关心算法实现细节； 分离使用策略和创建策略的场景。使用策略模式的基本流程：首先， 定义策略 API——PromotionStrategy，每一种促销策略的算法都要实现该接口。该接口有一个 recommand 方法，接收并返回一个 int 对象，返回的就是推荐后可以参加的促销活动。实际上，推荐返回的可能是一个活动对象，这里用简单的数字代替：public interface IPromotionStrategy { // 返回1 代表 可以参加 满减活动 // 返回2 代表 可以参加 N折优惠活动 // 返回3 代表 可以参加 M元秒杀活动 int recommand(String skuId);}而后，定义三个类实现了 PromotionStrategy 接口，分别代表满减策略、N 折扣优惠活动策略和 M 元秒杀活动策略，类分别是 FullReduceStrategy、NPriceDiscountStrategy 和 MSpikeStrategy：public class FullReduceStrategyImpl implements IPromotionStrategy { @Override public int recommand(String skuId) { System.out.println(&quot;=== 执行 满减活动&quot;); //推荐算法和逻辑写这里 return 1; }}public class MSpikeStrategyImpl implements IPromotionStrategy { @Override public int recommand(String skuId) { System.out.println(&quot;=== 执行 M 元秒杀活动&quot;); //推荐算法和逻辑写这里 return 3; }}public class NPriceDiscountStrategyImpl implements IPromotionStrategy { @Override public int recommand(String skuId) { System.out.println(&quot;=== 执行 N 折扣优惠活动&quot;); //推荐算法和逻辑写这里 return 2; }}接着，实现促销推荐的上下文信息类 Promotional，这里是存储和使用策略的地方。类中有一个 recommand 方法，用于执行推荐策略。它的构造函数有一个 PromotionStrategy 参数，可以在运行期间使用该参数决定使用哪种促销策略：public class Promotional { private final IPromotionStrategy strategy; public Promotional(IPromotionStrategy strategy) { this.strategy = strategy; } public void recommand(String skuId) { strategy.recommand(skuId); }}最后，简单的单元测试代码来看下具体的运行结果：public class Client { public static void main(String[] args) { Promotional fullReducePromotional = new Promotional(new FullReduceStrategyImpl()); fullReducePromotional.recommand(&quot;1122334455&quot;); Promotional nPriceDiscountPromotional = new Promotional(new NPriceDiscountStrategyImpl()); nPriceDiscountPromotional.recommand(&quot;6677889900&quot;); Promotional mSpikePromotional = new Promotional(new MSpikeStrategyImpl()); mSpikePromotional.recommand(&quot;11335577&quot;); }}// 输出结果// === 执行 满减活动// === 执行 N 折扣优惠活动// === 执行 M 元秒杀活动使用策略模式的理由使用策略模式的原因，主要有以下三个： 提升代码的可维护性。在实际开发中，有许多算法可以实现某一功能，如查找、排序等，通过 if-else 等条件判断语句来进行选择非常方便。但是这就会带来一个问题：当在这个算法类中封装了大量查找算法时，该类的代码就会变得非常复杂，维护也会突然就变得非常困难。虽然策略模式看上去比较笨重，但实际上在每一次新增策略时都通过新增类来进行隔离，短期虽然不如直接写 if-else 来得效率高，但长期来看，维护单一的简单类耗费的时间其实远远低于维护一个超大的复杂类； 动态快速地替换更多的算法。策略模式最大的作用在于分离使用算法的逻辑和算法自身实现的逻辑，这样就意味着当想要优化算法自身的实现逻辑时就变得非常便捷，一方面可以采用最新的算法实现逻辑，另一方面可以直接弃用旧算法而采用新算法。使用策略模式能够很方便地进行替换； 应对需要频繁更换策略的场景。 比如，用户推荐类场景。特别是对于一些 C 端产品来说，在获取了用户的反馈数据后，会根据用户的特性制定不同的运营策略，这时如果采用 if-else 的方式编码，那么每一次的策略变化都会导致系统代码的修改，从运营的角度看是不可接受的，而采用策略模式就能很容易地解决这个问题。 优缺点使用策略模式主要有以下几个优点： 提供良好的代码扩展性。 每一个策略都是对应生成一个新的具体策略类，满足开闭原则，同时满足里氏替换原则，可以任意替换相同的策略，这样用户可以在不修改原有系统的基础上选择算法或行为，同时也可以灵活地增加新的算法或行为； 提供了一种管理多个不同算法策略的办法。 策略模式提供了一种很好的思路，可以将算法的实现和使用算法的代码隔离开来，这样就能很好地管理不同的算法； 提供使用组合替换继承的办法。 策略模式使用组合的方式来替代继承，避免了子类出现异常而影响父类； 降低使用多重条件（if-else）嵌套语句的理解难度。 在实际的开发中，使用 if-else 是非常常见的编程方法，但是随着业务逻辑变得越来越复杂，如果一味地增加 if-else，会让代码变得非常难以理解和维护，使用策略模式则能避免这些问题的出现； 在运行时动态切换算法，提升代码灵活性。 由于策略模式将算法的选择权交给了客户端，那么客户端可以根据自身的需求灵活地切换算法。缺点： 客户端的学习成本变高。 虽然策略模式让客户端自行决定使用哪一个策略，看上去很自由，但实际上隐含着客户必须要知道所有的策略才能做选择的事实。一旦新增或修改策略，客户端都需要知道； 具体策略类的数量会剧增，增加维护成本。 由于每一个策略都对应一个具体策略类，所以当策略比较庞大时，需要维护的类数量也会激增； 不如函数式编程简洁。 现在有很多编程语言都支持函数式——允许在一组匿名函数中实现不同版本的算法。对于一些小型的策略来说，使用函数式编程就能解决问题，但使用策略模式反而过于复杂。总结策略模式最大的用处是能在运行时改变代码的算法行为，同时给使用者提供一种可以根据情况来选择算法的途径。虽然策略模式是一个比较容易理解和使用的设计模式，但是却增加了使用者的难度，因为可能需要在了解了所有的策略后才能做出决策。即便是类似排序这样简单的算法，不同使用者的选择也可能完全不同，如果交给使用者来选择，就意味着使用者需要了解不同排序算法的优劣，才能更好地做出选择。不过，策略模式对算法起到了很好的封装作用，通过使用算法和创建算法的分离，将算法实现的复杂性放到了子类去解决。同时，策略模式还可以随时进行替换，对于一些老旧的算法，可以很方便地进行替换和升级。" }, { "title": "状态模式：通过有限状态机监控功能的“状态变化”", "url": "/posts/state/", "categories": "Design Pattern, behavioral", "tags": "设计模式, Design Pattern, State, 状态模式, 对象行为型模式", "date": "2018-02-22 13:08:22 +0000", "snippet": "状态模式的应用场景非常广泛，比如：线上购物订单、手机支付、音乐播放器、游戏、工作流引擎等场景。状态模式设计的初衷是应对同一个对象里不同状态变化时的不同行为的变化。模式原理原始定义是：允许一个对象在其内部状态改变时改变它的行为，对象看起来似乎修改了自己的类一样。这个定义确实有些抽象，简而言之，状态模式就是让一个对象通过定义一系列状态的变化来控制行为的变化。比如，给购买的物品定义几个包裹运送状态，已下单、运送中、已签收等，当“已下单”状态变为“运送中”状态时，物流货车会把包装好的包裹运送到指定地址，也就是说，当包裹的状态发生改变时，就会触发相应的外部操作。状态模式的标准 UML 图：从这个 UML 图中，能看出状态模式包含的关键角色有三个。 上下文信息类（Context）：实际上就是存储当前状态的类，对外提供更新状态的操作。 抽象状态类（State）：可以是一个接口或抽象类，用于定义声明状态更新的操作方法有哪些。 具体状态类（StateA 等）：实现抽象状态类定义的方法，根据具体的场景来指定对应状态改变后的代码实现逻辑。UML 对应代码实现：package cn.happymaya.ndp.behavioral.state;public class Context { private State currentState; public Context(State currentState) { this.currentState = currentState; if (null == currentState) { this.currentState = StateA.instance(); } } public State getCurrentState() { return currentState; } public void setCurrentState(State currentState) { this.currentState = currentState; } public void request() { currentState.handle(this); }}public interface State { void handle(Context context);}public class StateA implements State{ private static StateA instance = new StateA(); private StateA() {} public static StateA instance() { return instance; } @Override public void handle(Context context) { System.out.println(&quot;=== 状态 A&quot;); }}public class StateB implements State{ private static StateB instance = new StateB(); private StateB() {} public static StateB instance() { return instance; } @Override public void handle(Context context) { System.out.println(&quot;=== 状态 B&quot;); }}在该代码实现中： 状态 A 和状态 B 被封装为具体的状态类 StateA 和 StateB，均实现了同一个抽象状态类 State 接口； 上下文信息类 Context 存储一个全局变量 currentState，用以保存当前状态对象，具体状态类通过将 Context 对象作为参数输入，就能获取访问全局的当前状态，以完成状态切换。因此，状态模式设计的核心点在于找到合适的抽象状态以及状态之间的转移关系，通过改变状态来达到改变行为的目的。使用场景一般来讲，状态模式常见使用场景有这样几种： 对象根据自身状态的变化来进行不同行为的操作时， 比如，购物订单状态； 对象需要根据自身变量的当前值改变行为，不期望使用大量 if-else 语句时， 比如，商品库存状态。 对于某些确定的状态和行为，不想使用重复代码时， 比如，某一个会员当天的购物浏览记录。举一个现实中的例子，可以通过按电视遥控器上的按钮来改变电视的显示状态。如果电视处于打开状态，我们可以将其设置为关闭、静音或更换频道。但如果电视是关闭状态，当我们按下切换频道的按钮时是不会有任何作用的。因为这时对于关闭状态的电视来说，只有设置为打开状态才有效。再比如，状态模式在程序中的另一个例子就是 Java 线程状态，一个线程的生命周期里有五种状态，只有在获得当前状态后才能确定下一个状态。通过一个简单的例子来演示一下。在线上购物的过程中，当我们选定好了商品并提交订单后，装有商品的包裹就会开始进行运送。这里定义了 6 种简单的包裹运送状态：已下单、仓库处理中、运输中、派送中、待取件和已签收。如下图所示：首先，定义包裹的状态 PackageState，在接口中声明一个更新状态的方法 updateState()，该方法接收包裹上下文信息类 PackageContext 作为参数。/** * 包裹状态 */public interface PackageState { /** * 定义了 6 中状态 * 1 - 已下单 * 2 - 仓库处理中 * 3 - 运输中 * 4 - 派送中 * 5 - 待取件 * 6 - 已签收 */ void updateState(PackageContext ctx);}然后，定义详细上下文信息类 PackageContext，其中包含一个当前状态 PackageState 和一个包裹的 id。/** * 包裹上下文 */public class PackageContext { private PackageState currentState; private String packageId; public PackageContext(PackageState currentState, String packageId) { this.currentState = currentState; this.packageId = packageId; if (currentState == null) { this.currentState = Ackonwledged.getInstance(); } } public PackageState getCurrentState() { return currentState; } public void setCurrentState(PackageState currentState) { this.currentState = currentState; } public void setPackageId(String packageId) { this.packageId = packageId; } public String getPackageId() { return packageId; } public void update() { currentState.updateState(this); }}接着，依次定义具体状态类： 已下单（Acknowledged） 仓库处理中（WarehouseProcessing） 运输中（InTransition） 派送中（Delivering） 待取件（WaitForPickUp）、已签收（Received）。每一个类都会实现 updateState() 方法，同时使用单例模式模拟状态的唯一性。public class Ackonwledged implements PackageState{ // Singleton private static Ackonwledged instance = new Ackonwledged(); private Ackonwledged(){} public static Ackonwledged getInstance() { return instance; } @Override public void updateState(PackageContext ctx) { System.out.println(&quot;=== state start...&quot;); System.out.println(&quot;1 - Package is acknowledged !!&quot;); ctx.setCurrentState(WarehouseProcessing.getInstance()); }}public class WarehouseProcessing implements PackageState { //Singleton private static WarehouseProcessing instance = new WarehouseProcessing(); private WarehouseProcessing() {} public static WarehouseProcessing getInstance() { return instance; } @Override public void updateState(PackageContext ctx) { System.out.println(&quot;2 - Package is WarehouseProcessing&quot;); ctx.setCurrentState(InTransition.getInstance()); }}public class InTransition implements PackageState { //Singleton private static InTransition instance = new InTransition(); private InTransition() {} public static InTransition getInstance() { return instance; } @Override public void updateState(PackageContext ctx) { System.out.println(&quot;3 - Package is in transition !!&quot;); ctx.setCurrentState(Delivering.getInstance()); }}public class Delivering implements PackageState{ //Singleton private static Delivering instance = new Delivering(); private Delivering() {} public static Delivering getInstance() { return instance; } @Override public void updateState(PackageContext ctx) { System.out.println(&quot;4 - Package is Delivering !!&quot;); ctx.setCurrentState(WaitForPickUp.getInstance()); }}public class Received implements PackageState { //Singleton private static Received instance = new Received(); private Received() {} public static Received getInstance() { return instance; } @Override public void updateState(PackageContext ctx) { System.out.println(&quot;6 - Package is Received !!&quot;); System.out.println(&quot;=== state end &quot;); }}最后，运行一个单元测试，通过执行上下文信息类的更新操作了变更状态。package cn.happymaya.ndp.behavioral.state;public class Client { public static void main(String[] args) { PackageContext ctx = new PackageContext(null, &quot;Test123&quot;); ctx.update(); ctx.update(); ctx.update(); ctx.update(); ctx.update(); ctx.update(); }}从单元测试的结果中，能直观地看到：执行一次状态更新，状态会变为下一个状态，直至状态结束。使用状态模式的原因使用状态模式的原因，主要有以下两个： 当要设计的业务具有复杂状态变迁时，期望通过状态变化来快速进行变更操作，并降低代码耦合性。 在上面使用场景的例子中，大致看到了一个包裹的状态变化流程，实际上的购物订单的状态变化远比这个要复杂。对于状态变化引起行为变化的情况，使用状态模式就能够很好地解决。 一方面因为状态是提前进行分析整理的，这样能减少代码实现的难度； 另一方面是因为状态与状态之间做了天然的隔离，能够将相关的行为聚合到一起，提高类的内聚度，降低耦合性； 避免增加代码的复杂性。在通常的设计中，每次新增状态时，需要添加大量的条件判断语句。最典型的就是 if-else 不断嵌套处理，这样的代码发展到后期，逻辑会变得异常复杂，进而导致代码可维护性和灵活性变差； 使用状态模式能够很好地从状态的维度来进行逻辑的关联，状态与状态之间只有切换的动作，至于状态本身如何进行复杂处理，对于另一个状态来说，其实并不关心，这样就能很好地避免对象间的调用关系变得复杂。 优缺点通过上述分析，可以得出使用状态模式主要有以下优点： 提前定好可能的状态，降低代码实现复杂度。 状态模式通常需要提前设计好状态的转移，这样就需要提前设计好状态之间的转移关系，在实现代码时就变得容易很多； 快速理解状态和行为之间的关系。 由于将操作和状态相关联，那么所有与某个状态相关的对象都会被聚合在一起，这样可以很方便地增加和删除操作，而不会影响其他状态的功能； 避免写大量的 if-else 条件语句。 比如，要判断订单商品到达配送站的状态，需要判断商品是在运送中还是已送达，到了以后还要再判断发往哪里，等等，这样的 if-else 条件语句会随着场景的增多而不断增加。如果建立起状态之间的转移关系，订单商品到达配送站会触发状态变换，然后进行对应状态下的对应操作，这样就能够有效减少直接的条件语句判断。 可以让多个环境对象共享一个状态对象，从而减少重复代码。 状态模式通常用于整体的流程控制和状态变更，这样对于多环境的应用程序来说，只需要共享一整个状态，而不需要每个环境都各自实现自己的状态对象。当然，状态模式也有一些缺点： 造成很多零散类。 状态模式因为需要对每一个状态定义一个具体状态类，所以势必会增加系统类和对象的个数； 状态切换关系越复杂，代码实现难度越高。 随着状态的不断扩展，状态的结构与实现就会变得复杂，比如，Ａ 状态切换到 Ｂ，Ｂ 切换到 Ｃ，Ｃ 异常回退 Ａ，Ａ 再走 Ｄ 异常状态，等等。如果使用不当，就会造成维护代码的人需要花费大量时间来梳理状态转移关系； 不满足开闭原则。 状态模式虽然降低了状态与状态之间的耦合性，但是新增和修改状态都会涉及前一个状态和后一个状态的代码修改，增大了引入代码问题的概率。总结 状态模式描述了对象状态的变化以及对象如何在每一种状态下表现出不同的行为； 适合场景是：对象本身具备很多状态变化，同时不同变化需要不同的行为来处理；** 状态模式虽然可以让代码条理清楚，容易阅读，但是实际上对开闭原则的支持并不友好，新增状态可能会影响原有的状态，在使用时要注意！ 要想用好状态模式，关键点在于寻找好的状态以及状态与状态之间的关系，而不是急着去实现状态模式。状态确定好以后，状态模式本身代码实现是非常容易的。" }, { "title": "观察者模式：发送消息变化的通知", "url": "/posts/oberver/", "categories": "Design Pattern, behavioral", "tags": "设计模式, Design Pattern, Oberver, 观察者模式, 对象行为型模式", "date": "2018-02-21 13:08:22 +0000", "snippet": "观察者模式是一种非常流行的设计模式，也常被叫作订阅-发布模式。观察者模式在现代的软件开发中应用非常广泛，比如，商品系统、物流系统、监控系统、运营数据分析系统等。常说的基于事件驱动的架构，其实也是观察者模式的一种最佳实践。当观察某一个对象时，对象传递出的每一个行为都被看成是一个事件，观察者通过处理每一个事件来完成自身的操作处理。原理原始定义是：定义对象之间的一对多依赖关系，这样当一个对象改变状态时，它的所有依赖项都会自动得到通知和更新。这个定义中包含了两个前提条件： 被依赖的对象叫作被观察者，依赖的对象叫作观察者； 观察者观察被观察者的状态变化。不过，这听上去还是有点太抽象，更容易理解下面这些不同的对于观察者模式的叫法： 发布者-订阅者； 生产者-消费者； 事件发布-事件监听。不管怎么叫，这些模式在本质上都是观察者模式。观察者模式 UML 图如下：从该 UML 图中，观察者模式包含的四个关键角色： 发布者（Publisher）：也被叫作主题、被订阅者、被观察者等，通常是指观察者关心的相关对象集合，比如，将 GitLab 上的 Git 库作为发布者，我们关心 Git 库的代码发生了变更后做特定的操作； 具体发布者（PublisherImpl）：实现了发布者定义的方法的具体实现类。； 订阅者（Observer）：也叫作观察者，它会存储一个注册列表，用于存放订阅者。当发布者发布消息或事件时，会通知到订阅者进行处理； 具体订阅者（ObserverImpl）：实现具体定义的方法操作。UML 对应的代码实现，是一种经典的实现方式，如下：public interface IPublisher { void addObserver(IObserver o); void removeObserver(IObserver o); void notify(double amt);}public interface IObserver { void notify(String acct, double amt);}public class ObserverImpl implements IObserver { @Override public void notify(String acct, double amt) { System.out.println(&quot;=== 接收到通知：账户：&quot;+acct + &quot; 账单：&quot;+amt); }}public class PublisherImpl implements IPublisher { private String acct; private double balance; private List&amp;lt;IObserver&amp;gt; myObservers; public PublisherImpl(String anAcct, double aBalance){ this.acct = anAcct; this.balance = aBalance; this.myObservers = new ArrayList&amp;lt;&amp;gt;(); } @Override public void addObserver(IObserver o) { myObservers.add(o); } @Override public void removeObserver(IObserver o) { myObservers.remove(o); } @Override public void notify(double amt) { balance -= amt; if (balance &amp;lt; 0) { overdrawn(); } } private void overdrawn() { for (IObserver o : myObservers) { o.notify(acct, balance); } }}public class Demo { public static void main(String[] args) { IPublisher account = new PublisherImpl(&quot;TEST123&quot;, 10.00); IObserver bill = new ObserverImpl(); account.addObserver(bill); account.notify(11.00); }}// 输出结果// === 接收到通知：账户：TEST123 账单：-1.0使用场景观察者模式常见的使用场景有以下几种： 当一个对象状态的改变需要改变其他对象时。比如，商品库存数量发生变化时，需要通知商品详情页、购物车等系统改变数量。 一个对象发生改变时只想要发送通知，而不需要知道接收者是谁。比如，订阅微信公众号的文章，发送者通过公众号发送，订阅者并不知道哪些用户订阅了公众号。 需要创建一种链式触发机制时。比如，在系统中创建一个触发链，A 对象的行为将影响 B 对象，B 对象的行为将影响 C 对象……这样通过观察者模式能够很好地实现。 微博或微信朋友圈发送的场景。这是观察者模式的典型应用场景，一个人发微博或朋友圈，只要是关联的朋友都会收到通知；一旦取消关注，此人以后将不会收到相关通知。 需要建立基于事件触发的场景。比如，基于 Java UI 的编程，所有键盘和鼠标事件都由它的侦听器对象和指定函数处理。当用户单击鼠标时，订阅鼠标单击事件的函数将被调用，并将所有上下文数据作为方法参数传递给它。栗子实现一个简单的消息订阅通知的功能。首先，定义观察者 IMessageObserver。它只有一个 update 方法，当有消息 Message 发送过来时就会调用该方法：public interface IMessageObserver { void update(Message m);}然后，定义一个被观察者 ISubject ，它定义了三个方法 增加观察者方法 attach； 删除观察者方法 detach ； 更新通知方法 notifyUpdate：```javapublic interface ISubject { /* 增加观察者 / void attach(IMessageObserver o); / 删除观察者 / void detach(IMessageObserver o); / 更新通知 */ void notifyUpdate(Message m); } 接着，定义消息的具体数据结构 Message，这里使用一个 content 消息内容： ```javapublic class Message { final String content; public Message (String m) { this.content = m; } public String getContent() { return content; } }再接下来，实现 ISubject 的具体发布者类 MessagePublisher，它持有一组观察者 MessageObserver 实例，可以通过 attach 和 detach 接口方法新增和删除观察者：public class MessagePublisher implements ISubject { private List&amp;lt;IMessageObserver&amp;gt; observers = new ArrayList&amp;lt;&amp;gt;(); @Override public void attach(IMessageObserver o) { observers.add(o); } @Override public void detach(IMessageObserver o) { observers.remove(o); } @Override public void notifyUpdate(Message m) { observers.forEach(x-&amp;gt;x.update(m)); }}最后，实现三个具体订阅者类，它们都实现了 MessageObserver 接口，分别代表不同的消息接收后的对应处理操作：public class MessageSubscriber1 implements IMessageObserver{ @Override public void update(Message m) { System.out.println(&quot;MessageSubscriber1 :: &quot; + m.getContent()); }}public class MessageSubscriber2 implements IMessageObserver{ @Override public void update(Message m) { System.out.println(&quot;MessageSubscriber2 :: &quot; + m.getContent()); }}public class MessageSubscriber3 implements IMessageObserver{ @Override public void update(Message m) { System.out.println(&quot;MessageSubscriber3 :: &quot; + m.getContent()); }}运行一段单元测试，具体运行结果：public class Client { public static void main(String[] args) { IMessageObserver s1 = new MessageSubscriber1(); IMessageObserver s2 = new MessageSubscriber2(); IMessageObserver s3 = new MessageSubscriber3(); ISubject p = new MessagePublisher(); p.attach(s1); p.attach(s2); /* s1 和 s2 会收到消息通知 */ p.notifyUpdate(new Message(&quot;First Message&quot;)); p.detach(s1); p.attach(s3); /* s2 和 s3 会收到消息通知 */ p.notifyUpdate(new Message(&quot;Second Message&quot;)); }}// 输出结果// MessageSubscriber1 :: First Message// MessageSubscriber2 :: First Message// MessageSubscriber2 :: Second Message// MessageSubscriber3 :: Second Message观察者模式使用场景的特点在于找到合适的被观察者，定义一个通知列表，将需要通知的对象放到这个通知列表中，当被观察者需要发起通知时，就会通知这个列表中的所有“人”。使用观察者模式的理由使用观察者模式的原因，主要有以下两个： 为了方便捕获观察对象的变化并及时做出相应的操作。 观察者模式对于对象自身属性或行为发生变化后，需要快速应对的应用场景尤其有效。购物包裹的运送。因为商品变成包裹的形式后会出现各种各样的状态变化（配送中、接收中、退货中等），这些状态变化非常快，并且每一个状态都会对应一系列连锁的操作，这时使用观察者模式就非常方便，能够通知需要知道这些状态变化的系统，并做相应的处理，就像，物流监控、性能监控、运营系统等； 为了提升代码扩展性。 如果不使用观察者模式来捕获一个被观察对象的属性变化，那么就需要在被观察对象执行代码逻辑中加入调用通知某个对象进行变更的逻辑，这样不仅增加了代码的耦合性，也让代码扩展变得非常困难，因为要想新增一个新的观察对象，就需要修改被观察对象的代码，这样的扩展性非常低。相反，使用观察者模式则只需要将观察者对象注册到被观察者存储的观察者列表中就能完成代码扩展。优缺点观察者模式主要有以下三个优点： 降低系统与系统之间的耦合性。 比如，建立以事件驱动的系统，能创建更多观察者与被观察者，对象之间相互的关系比较清晰，可以随时独立添加或删除观察者； 提升代码扩展性。 由于观察者和被观察之间是抽象耦合，所以增删具体类并不影响抽象类之间的关系，这样不仅满足开闭原则，也满足里氏替换原则，能够很好地提升代码扩展性； 可以建立一套基于目标对象特定操作或数据的触发机制。 比如，基于消息的通知系统、性能监控系统、用户数据跟踪系统等，观察者通过特定的事件触发或捕获特定的操作来完成一系列的操作。观察者模式的缺点如下： **增加代码的理解难度。 **由于使用组合关系，被观察者和观察者之间属于松散关系，所以在代码逻辑的理解上，就需要提前搞清楚上下文中有哪些中间类或调用逻辑关系，这样才能正确理解逻辑； **降低了系统性能。 **观察者模式通常需要事件触发，当观察者对象越多时，被观察者需要通知观察者所花费的时间也会越长，这样会在某种程度上影响程序的效率。总结观察者模式在原理上是一个比较抽象的模式，但是业界根据不同的应用场景和需求总结出了很多不同的实现方式，这让观察者模式变得易于使用。在观察者模式中，被观察者通常会维护一个观察者列表。当被观察者的状态发生改变时，就会通知观察者。观察者模式现在被大量应用在分布式系统的开发中。从本质上来讲，观察者模式描述了如何建立对象与对象之间一种简单的依赖关系。在现实中，最为常见的观察者模式的例子就是订阅微信公众号，当公众号发布一篇新文章时，能在微信上收到消息，然后选择合适的时间打开阅读。因此，在使用观察者模式时，要重点关注哪些变化是需要从被观察对象那里捕获（观察到）并进行下一步处理的，一方面不能盲目捕获所有的变化，另一方面也不能遗漏重要的变化。换句话说，找到合适的变化并进行正确的处理才是使用观察者模式的正确打开方式。" }, { "title": "备忘录模式：在聊天会话中记录历史消息", "url": "/posts/memento/", "categories": "Design Pattern, behavioral", "tags": "设计模式, Design Pattern, 备忘录模式, Memento, 对象行为型模式", "date": "2018-02-20 13:08:22 +0000", "snippet": "相较于其他设计模式，备忘录模式不算太常用，但好在这个模式理解、掌握起来并不难，代码实现也比较简单，应用场景更是比较明确和有限。一般应用于编辑器或会话上下文中防丢失、撤销、恢复等场景中。模式原理分析原始定义是：捕获并外部化对象的内部状态，以便以后可以恢复，所有这些都不会违反封装。这个定义是非常简单的，备忘录模式对应的 UML 图：从上面 UML 图中，备忘录模式包含两个关键角色： 原始对象（Originator）： 创建自身所需要的属性和业务逻辑; 提供方法 create() 和 restore(memento) 来保存和恢复对象副本。 备忘录（Memento）：用于保存原始对象所有属性状态，以便在未来进行撤销操作。UML 对应代码实现：首先，创建原始对象 Originator，对象中有四个属性，分别： state 用于显示当前对象状态； id、name、phone 用来模拟业务属性； 并添加 get、set 方法、create() 方法用于创建备份对象，restore(memento) 用于恢复对象状态。public class Originator { private String state = &quot;原始对象&quot;; private String id; private String name; private String phone; public Originator(){} public Memento create() { return new Memento(id, name, phone); } public void restore(Memento m) { this.state = m.getState(); this.id = m.getId(); this.name = m.getName(); this.phone = m.getPhone(); } @Override public String toString() { return &quot;Originator{&quot; + &quot;state=&#39;&quot; + state + &#39;\\&#39;&#39; + &quot;, id=&#39;&quot; + id + &#39;\\&#39;&#39; + &quot;, name=&#39;&quot; + name + &#39;\\&#39;&#39; + &quot;, phone=&#39;&quot; + phone + &#39;\\&#39;&#39; + &#39;}&#39;; }}然后，再来创建备忘录对象 Memento，备忘录对象几乎就和原始对象的属性一模一样。public class Memento { private String state = &quot;从备份对象恢复为原始对象&quot;; // 打印当前状态 private String id; private String name; private String phone; public Memento(String id, String name, String phone) { this.state = state; this.id = id; this.name = name; this.phone = phone; } @Override public String toString() { return &quot;Memento{&quot; + &quot;state=&#39;&quot; + state + &#39;\\&#39;&#39; + &quot;, id=&#39;&quot; + id + &#39;\\&#39;&#39; + &quot;, name=&#39;&quot; + name + &#39;\\&#39;&#39; + &quot;, phone=&#39;&quot; + phone + &#39;\\&#39;&#39; + &#39;}&#39;; }}接着运行一个单元测试，如下代码：package cn.happymaya.ndp.behavioral.memento;public class Demo { public static void main(String[] args) { Originator originator = new Originator(); originator.setId(&quot;1&quot;); originator.setName(&quot;mickjoust&quot;); originator.setPhone(&quot;12345678900&quot;); System.out.println(originator); Memento memento = originator.create(); originator.setName(&quot;修改&quot;); System.out.println(originator); originator.restore(memento); System.out.println(originator); }}// 输出结果// Originator{state=&#39;原始对象&#39;, id=&#39;1&#39;, name=&#39;mickjoust&#39;, phone=&#39;12345678900&#39;}// Originator{state=&#39;原始对象&#39;, id=&#39;1&#39;, name=&#39;修改&#39;, phone=&#39;12345678900&#39;}// Originator{state=&#39;从备份对象恢复为原始对象&#39;, id=&#39;1&#39;, name=&#39;mickjoust&#39;, phone=&#39;12345678900&#39;}从上面的代码实现和最后输出的结果可以看出，备忘录模式代码实现非常简单的，关键点就在于要能保证原始对象在某一个时刻的对象状态被完整记录下来。使用场景备忘录模式经常使用的场景以下两种： 需要保存一个对象在某一个时刻状态时。比如，在线编辑器中编写文字时断网，需要恢复为断网前的状态； 不希望外界直接访问对象的内部状态时。比如，在包裹配送的过程中，如果从仓库运送到配送站，只需要显示“在运行中”，而具体使用汽车还是飞机，这个用户并不需要知道。通过一个栗子演示一下，假设现在正在管理一个博客系统，经常需要创建 Blog 对象，但是有一些作者在写 Blog 的过程中可能会出现断网的情况，这时就需要保存 Blog 对象在这个时刻的状态信息，后续等作者重新联网后，能够及时地恢复其断网前的状态。首先，创建一个 Blog 对象，该对象中包含 id、title 和 content，分别代表了 Blog 的唯一编号、标题和内容；并提供创建备忘录的 createMemento() 和 restore(BlogMemento m) 方法，分别用于创建备忘录和通过备忘录来恢复原始的 Blog 对象。public class Blog { private Long id; private String title; private String content; public Blog(Long id, String title) { this.id = id; this.title = title; } // 省略 getter/setter public BlogMemento createMemento() { return new BlogMemento(id, title, content); } public void restore(BlogMemento m) { this.id = m.getId(); this.title = m.getTitle(); this.content = m.getContent(); } @Override public String toString() { return &quot;Blog{&quot; + &quot;id=&quot; + id + &quot;, title=&#39;&quot; + title + &#39;\\&#39;&#39; + &quot;, content=&#39;&quot; + content + &#39;\\&#39;&#39; + &#39;}&#39;; }}然后，创建一个 Blog 的备忘录对象 BlogMemento，同样是复制 Blog 所需要的所有属性内容。public class BlogMemento { private final long id; private final String title; private final String content; public BlogMemento(long id, String title, String content) { this.id = id; this.title = title; this.content = content; } public long getId() { return id; } public String getTitle() { return title; } public String getContent() { return content; }}这样基于 Blog 对象的备忘录就创建好了。最后，依然运行一段单元测试代码来看看运行结果。package cn.happymaya.ndp.behavioral.memento;public class Client { public static void main(String[] args) { Blog blog = new Blog(1L, &quot;My Blog&quot;); blog.setContent(&quot;ABC&quot;); // 原始文章 System.out.println(blog); BlogMemento memento = blog.createMemento(); // 创建 blog 的备忘录 blog.setContent(&quot;123&quot;); // 改变内容 System.out.println(blog); blog.restore(memento); // 撤销操作 System.out.println(blog); // 这时显示原始的内容 }}// 输出结果Blog{id=1, title=&#39;My Blog&#39;, content=&#39;ABC&#39;}Blog{id=1, title=&#39;My Blog&#39;, content=&#39;123&#39;}Blog{id=1, title=&#39;My Blog&#39;, content=&#39;ABC&#39;}从运行结果中，可以发现使用备忘录模式能非常方便地进行撤销操作。编辑文章内容时，其实就是在修改 content 内容，这时备忘录会记录特定时间点里的对象状态，如果这时需要撤销修改，那么就会恢复到原来的对象状态。所以说，备忘录模式在频繁需要撤销与恢复的场景中能够发挥很好的作用。备忘录模式使用备忘录模式的原因，可总结为以下两个。 为了记录多个时间点的备份数据。与传统备份不同的是，备忘录模式更多是用来记录多个时间点的对象状态数据，比如编辑器、聊天会话中会涉及多次操作和多次交互对话，一方面是为了记录某个时间点数据以便以后运营用来做数据分析，另一方面是为了能够通过多次数据快照，防止客户端篡改数据； 需要快速撤销当前操作并恢复到某对象状态。 比如，微信中的撤回功能其实就是备忘录模式的一种体现，用户发错信息后，需要立即恢复到未发送状态。优缺点通过上述分析，备忘录模式，主要有以下优点： 能够快速撤消对对象状态的更改。 比如，在编辑器中不小心覆盖了一段重要的文字，使用 undo 操作能够快速恢复这段文字； 能够帮助缓存记录历史对象状态。比如，在客服会话聊天中，对于某一些重要的对话（退换货、价保等），我们会记录这些对象数据，传统的做法是调用一次服务接口，一旦服务出现故障就很容易导致聊天回复速度变慢；而使用备忘录模式则能够记录这些重要的数据信息（用户提供的订单数据），而不需要反复查询接口，这样能提升回复客户的效率。 能够提升代码的扩展性。 备忘录模式是通过增加外部对象来保存原始对象的状态，而不是在原始对象中新增状态记录，当不再需要保存对象状态时就能很方便地取消这个对象。同理，新增备忘录对象也非常容易。同样，备忘录模式也有一些缺点： 备忘录会破坏封装性。 因为当备忘录在进行恢复的过程中遇见错误时，可能会恢复错误的状态。比如，备份的对象状态中有需要调用数据库等外部服务时，在恢复过程中如果遇见数据库宕机，那么可能恢复的对象数据就会存在错误； 备忘录的对象数据很大时，读取数据可能出现内存用尽的情况。 比如，在编辑器中加入高清的图片，如果直接记录图片本身可能会导致内存被用尽，进而导致系统出现崩溃的情况。总结备忘录模式也叫快照模式。具体来说，就是通过捕获对象在某一个时刻对象状态，再将其保存到外部对象，以便在需要的时候恢复对象到指定时刻下的状态。备忘录模式的应用场景比较局限，主要是用来备份、撤销、恢复等，这与平时常说的“备份”看上去很相似，但实际上两者的差异是很大的： 备忘录模式更侧重于代码设计和实现，支持简单场景中的应用，比如记录 Web 请求中的 header 信息等； 备份更多是以解决方案的形式出现，比如异地容灾备份、数据库主从备份等所支持的是更复杂的业务场景，备份的数据量往往也更大。因此，在使用备忘录模式时，莫要认为它就是万能的备份模式，要合理评估对象所使用的内存空间，再确定是否使用备忘录模式。" }, { "title": "中介者模式：通过中间层来解决耦合过多的问题", "url": "/posts/mediator/", "categories": "Design Pattern, behavioral", "tags": "设计模式, Design Pattern, Mediator, 中介者模式, 对象行为型模式", "date": "2018-02-19 13:18:32 +0000", "snippet": "中介者模式理解起来不难，代码实现简单，学习难度也很小，只要合理充分地应用这个模式，往往就能够解决一些意想不到的问题。模式原理分析中介者模式的原始定义是：中介者对象封装了一组对象之间的交互，这组对象会将它们的交互委托给中介者对象，而不是直接交互。可以看到，这个定义是难得简单和明确，中介者对象就是用于处理对象与对象之间的直接交互，封装了多个对象之间的交互细节。中介者模式的 UML 图：从这个 UML 图中，我们能看出中介者模式包含了四个关键角色: 抽象中介者（Mediator）：定义中介者需要执行的方法操作; 具体中介者（MediatorImpl）：实现抽象中介者定义的方法操作，同时可以包含更多逻辑; 抽象组件类（Component）：定义组件需要执行的方法操作; 具体组件类（ComponentA、ComponentB）：继承自抽象组件类，实现具体的组件业务逻辑。UML 对应的代码实现：public interface IMediator { void apply(String key);}public class MediatorImpl implements IMediator { @Override public void apply(String key) { System.out.println(&quot;最终中介执行操作, key 为: &quot; + key); }}public abstract class Component { private IMediator mediator; public Component(IMediator mediator) { this.mediator = mediator; } public abstract void exec(String key); public IMediator getMediator() { return mediator; }}public class ComponentA extends Component { public ComponentA(IMediator mediator) { super(mediator); } @Override public void exec(String key){ System.out.println(&quot;===在组件 A 中, 通过中介者执行&quot;); getMediator().apply(key); }}public class ComponentB extends Component { public ComponentB(IMediator mediator) { super(mediator); } @Override public void exec(String key){ System.out.println(&quot;===在组件 B 中, 通过中介者执行&quot;); getMediator().apply(key); }}public class Demo { public static void main(String[] args) { IMediator mediator = new MediatorImpl(); Component componentA = new ComponentA(mediator); componentA.exec(&quot;key-A&quot;); Component componentB = new ComponentA(mediator); componentA.exec(&quot;key-B&quot;); }}// ===在组件 A 中, 通过中介者执行// 最终中介执行操作, key 为: key-A// ===在组件 A 中, 通过中介者执行// 最终中介执行操作, key 为: key-B从上面的代码实现中现中介者模式的关键点就在于在组件与组件之间加入一个中间对象来进行间接通信。虽然多了“一层”会更烦琐些，但是这样就可以在中介者里进行其他的一些操作。使用场景分析中介者模式常见的使用场景有以下几种： 系统中对象之间存在复杂的引用关系时，比如，聊天系统； 通过一个中间对象来封装多个类中的共有行为时，比如，在分层架构中的 DAO 层和数据库 DB 层中间再引入一个读写分离和读写均衡的中间层； 不想生成太多的子类时.假设要设计一个可以让多人参与进去的聊天室。首先，定义聊天室的接口 ChatRoom，其中包含两个方法 sendMessage 和 addUser，分别代表发送消息和新增用户。这里的 ChatRoom 就是抽象的中介者类。public interface ChatRoom { void sendMessage(String msg, String userId); void addUser(User user);}然后，创建一个 ChatRoom 的实现类ChatRoomImpl，使用 addUser 来添加需要聊天的用户对象，同时这里再使用一个Map来保存添加时需要用来进行通信的对象列表，在发送消息 sendMessage 的方法中，我们通过 userId 指定某个对象来接收消息。public class ChatRoomImpl implements IChatRoom{ private Map&amp;lt;String, User&amp;gt; userMap = new HashMap&amp;lt;&amp;gt;(); @Override public void sendMessage(String msg, String userId) { User user = userMap.get(userId); user.receive(msg); } @Override public void addUser(User user) { this.userMap.put(user.getId(), user); } }接下来, 再定义一个抽象组件类 User，如下所示：public abstract class User { private IChatRoom mediator; private String id; private String name; public User(IChatRoom mediator, String id, String name) { this.mediator = mediator; this.id = id; this.name = name; } public abstract void send(String msg, String userId); public abstract void receive(String msg); public IChatRoom getMediator() { return mediator; } public String getId() { return id; } public String getName() { return name; } }继承 User 实现一个具体的组件类 ChatUser，并实现发送消息 send 和接收消息 receive 的方法。public class ChatUser extends User{ public ChatUser(IChatRoom mediator, String id, String name) { super(mediator, id, name); } @Override public void send(String msg, String userId) { System.out.println(this.getName() + &quot; :: Sending Message : &quot; + msg); getMediator().sendMessage(msg, userId); } @Override public void receive(String msg) { System.out.println(this.getName() + &quot; :: Received Message : &quot; + msg); } }最后，运行一段单元测试代码：public class Client { public static void main(String[] args) { IChatRoom chatroom = new ChatRoomImpl(); User user1 = new ChatUser(chatroom,&quot;1&quot;, &quot;Spike&quot;); User user2 = new ChatUser(chatroom,&quot;2&quot;, &quot;Mia&quot;); User user3 = new ChatUser(chatroom,&quot;3&quot;, &quot;Max&quot;); User user4 = new ChatUser(chatroom,&quot;4&quot;, &quot;Mick&quot;); chatroom.addUser(user1); chatroom.addUser(user2); chatroom.addUser(user3); chatroom.addUser(user4); user1.send(&quot;Hello man&quot;, &quot;2&quot;); user2.send(&quot;Hey&quot;, &quot;1&quot;); }}// 输出结果// Spike :: Sending Message : Hello man// Mia :: Received Message : Hello man// Mia :: Sending Message : Hey// Spike :: Received Message : He到此，就完成了一个简单的聊天室程序。从代码实现中能看出，中介者在使用时需要知道对象之间的交互关系，然后通过封装这些交互关系的变化让对象在使用中介者时变得更简单。使用中介者模式的原因使用中介者模式的原因，可总结为以下三个： 解决对象之间直接耦合的问题，避免“一处修改多处”的连锁反应出现。比如，在上面聊天室的例子中，如果用户与用户之间是直连通信的，那么任何一个用户对象发生变化都会影响到直接聊天的那个用户，而那个用户可能又在跟别的用户聊天，以此类推，用户之间的关系会变得越来越复杂。这样，当再修改代码时，就很容易造成“一处修改又引起多处修改”的连锁反应。而使用中介者模式时，用户会通过聊天室和别的用户通信，避免了直接与对方通信，这样修改某个用户对象时并不会影响到其他对象； 在结构上作为中转，解耦两个服务或系统之间的直接耦合关系。在分层架构中，都知道视图层一般不会直接使用 DAO 层，因为一旦直接使用，DAO 层任何一个微小的变动都可能引起视图层的变化，这时通常会引入 Service 层作为中介者来进行请求的转发，以达到解耦的目的，避免了相互之间的直接影响，同时也能在中间层里加入一些特定的逻辑，如性能监控、埋点数据记录等； 为了更便捷地统一协同对象之间的通信。在对远程服务器进行调用时，协调网络通信是一件异常复杂和烦琐的事情，这时如果有一个中介者来统一协调，则会大大提升效率，比如，Dubbo 一类的 RPC 框架就是一个完整的中介者模式的体现。对于所有的 Java RPC 调用来说，只需要通过这个中间层来进行通信即可，而不需要知道对方服务器地址以及如何发起网络调用。优缺点中介者模式主要有这样几个优点： 减少对象之间的直接交互，间接解耦过多依赖。比如，Maven 就是 Java 中引用 jar 包时的中介者，如果我们手动直接引用 jar 包，会容易造成非常混乱的引用关系，而使用 Maven 则能很方便地减少代码直接依赖 jar 包的问题； 减少子类的创建数量。比如，在多个用户的会话请求中，我们可以使用一个通用的上下文的中介者来保存会话中一些不变的静态数据，这样就不需要每新增一个会话都需要新增一些静态数据； 简化各系统的设计和实现。由于中介者能够处理一些共用的通信逻辑，所以其他对象在进行自身业务的处理时可以不用关心共用的通信逻辑，这样就大大减少了系统的实现逻辑； 通过新建中间层快速扩展新功能，提升代码扩展性。比如，对象通过中间层调用时，我们可以在中间层加入对每一次请求或方法调用的耗时统计，这样就能快速扩展功能。同样，中介者模式的缺点如下： 中介者类中的交互逻辑可能变得非常复杂且难以维护。当中介者类中包含了太多对象之间的交互细节后，中介者就变成了新的复杂对象，使得系统维护成本变高； 中介者变成了新的重度依赖对象。一旦中介者对象变得复杂后，势必会增加与其他对象之间的耦合度，而这时如果中介者对象发生故障，则依赖的相关对象也会受到影响，修改中介者也会影响关联对象； 中介者需要知道所有对象交互的逻辑。由于中介者对象承担了交互对象的传输渠道，所以就需要知道对象交互的详细细节，这样无疑增加了中介者对象的学习成本。总结虽然中介者模式的原理和实现都非常简单，但是中介者在系统中承担的责任却是非常重要的。中介者通常会承担两方面的职责： 中转作用（结构性）。不同对象通过中介者进行中转就意味着不再需要显式直接引用对象，比如聊天中的两个用户。当用户需要和另一个对象进行通信时，只需要通过中介者，而不需要直接和对方建立联系。这样从结构上不再会形成网状结构，而是以某个中介者为中心的星型结构，这样能极大地降低对象的结构耦合性； 协调作用（行为性）。中介者会在自身内部分装协调逻辑，并对同类型的对象请求进行统一的处理。总之，中介者模式提供了一种减少对象之间耦合度的思路。对于一些维护性的旧项目来说，直接修改已有代码通常都会导致系统出现问题，而通过引入中间层，能够起到过渡的作用，同时还能够逐渐解耦原有的强耦合关系，让系统的扩展性变得更强。不过，中介者也可能因此变得异常复杂，一旦中介者出现问题，就会导致所有系统都出现问题，所以在使用时也需要注意设计的度。" }, { "title": "迭代器模式：实现遍历数据时的职责分离", "url": "/posts/iterator/", "categories": "Design Pattern, behavioral", "tags": "设计模式, Design Pattern, Iterator, 迭代器模式, 对象行为型模式", "date": "2018-02-18 13:18:32 +0000", "snippet": "迭代器模式是一个设计时很少用到，但编码实现时却经常使用到的行为型设计模式。在绝大多数编程语言中，迭代器已经成为一个基础的类库，直接用来遍历集合对象。在平时开发中，更多的是直接使用它，很少会从零去实现一个迭代器。模式原理迭代器模式又叫游标（Cursor）模式，它的原始定义是：迭代器提供一种对容器对象中的各个元素进行访问的方法，而又不需要暴露该对象的内部细节。可以看到，该定义很明确地指出，迭代器模式就是为了提供一种通用的访问对象的方式，它的 UML 图：从该 UML 图中，迭代器模式有四个关键角色： 抽象集合类（Aggregate）：创建和抽象迭代器类相关联的方法，同时可以添加其他集合类需要的方法； 具体集合类（ConcreteAggregate）：实现抽象集合类声明的所有方法，在具体使用集合类时会创建对应具体的迭代器类； 抽象迭代器类（Iterator）：定义统一的迭代器方法 hasNext() 和 next()，用于判断当前集合中是否还有对象以及按顺序读取集合中的当前对象； 具体迭代器类（ConcreteIterator）：实现了抽象迭代器类声明的方法，处理具体集合中对对象位置的偏移以及具体对象数据的传输。UML 图对应的代码实现如下：public interface Iterator { Object next(); boolean hasNext();}public class ConcreteIterator implements Iterator{ private Object[] objects; private int position; public ConcreteIterator(Object[] objects) { this.objects = objects; } @Override public Object next() { return objects[position++]; } @Override public boolean hasNext() { return position &amp;lt; objects.length; }}public interface Aggregate { Iterator createIterator();}public class ConcreteAggregate implements Aggregate{ private Object[] objects; public ConcreteAggregate(Object[] objects) { this.objects = objects; } @Override public Iterator createIterator() { return new ConcreteIterator(objects); } }public class Demo { public static void main(String[] args) { Object[] objects = new Object[2]; objects[0] = new Object(); objects[1] = new Object(); Aggregate aggregate = new ConcreteAggregate(objects); Iterator iterator = aggregate.createIterator(); while (iterator.hasNext()) { Object currentObject = iterator.next(); System.out.println(currentObject.toString()); } }}从最后的结果可以看出，通过实现迭代器模式最终打印了对象数组中的两个不同对象。从上面的代码实现也能看出，迭代器的实现原理非常简单，就是通过为集合对象创建统一的迭代器 Iterator 来统一对集合里的对象进行访问。使用场景分析常见的使用场景有以下两种： 希望对客户端隐藏其遍历算法复杂性时。有时可能出于使用便利性或安全性的考虑，只想让客户端使用遍历某个集合的对象，而不告诉客户端具体的遍历算法； 需要简化重复的循环遍历逻辑时。比如，读取数组里的数据，遍历二叉树等树形结构。先来创建抽象迭代器 IteratorIterator（这里为了和 Java 中的 Iterator 接口区别开），声明为泛型接口，接收类型参数E，同时声明四个方法：reset()、next()、currentItem() 和 hasNext()。public interface IteratorIterator&amp;lt;E&amp;gt; { void reset(); //重置为第一个元素 E next(); //获取下一个元素 E currentItem(); //检索当前元素 boolean hasNext(); //判断是否还有下一个元素存在.}接下来，我们再来定义抽象集合 ListList（同样为了和 Java 中的 List 接口区别开），也声明为泛型接口，接收类型参数 E，声明一个创建迭代器 IteratorIterator 的方法 iterator()。public interface ListList&amp;lt;E&amp;gt; { IteratorIterator&amp;lt;E&amp;gt; iterator();}然后，构造一个对象 Topic，对象中只包含 name 属性以及其构造函数和 get、set 方法。public class Topic { private String name; public Topic(String name) { super(); this.name = name; } public String getName() { return name; } public void setName(String name) { this.name = name; }}再接着实现一个具体的迭代器类 TopicIterator，其中包含属性为 Topic 的数组和一个记录对象存储位置的对象 position。当执行 next() 方法时，会获取当前记录位置的对象，至于 reset() 则会重置对象在数组中的位置为 0，currentItem() 方法则会返回当前位置下的对象，hasNext() 则判断当前位置是否越界。public class TopicIterator implements IteratorIterator&amp;lt;Topic&amp;gt; { private Topic[] topics; private int position; public TopicIterator(Topic[] topics) { this.topics = topics; position = 0; } @Override public void reset() { position = 0; } @Override public Topic next() { return topics[position++]; } @Override public Topic currentItem() { return topics[position]; } @Override public boolean hasNext() { if(position &amp;gt;= topics.length) { return false; } return true; }}同样，还需要实现一个具体的集合类 TopicList，该类中只实现一个创建迭代器的方法，返回对应具体迭代器的类方法。public class TopicList implements ListList&amp;lt;Topic&amp;gt;{ private Topic[] topics; public TopicList(Topic[] topics) { this.topics = topics; } @Override public IteratorIterator&amp;lt;Topic&amp;gt; iterator() { return new TopicIterator(topics); }}最后，再来运行一段单元测试：public class Client { public static void main(String[] args) { Topic[] topics = new Topic[5]; topics[0] = new Topic(&quot;topic1&quot;); topics[1] = new Topic(&quot;topic2&quot;); topics[2] = new Topic(&quot;topic3&quot;); topics[3] = new Topic(&quot;topic4&quot;); topics[4] = new Topic(&quot;topic5&quot;); ListList&amp;lt;Topic&amp;gt; list = new TopicList(topics); IteratorIterator&amp;lt;Topic&amp;gt; iterator = list.iterator(); while(iterator.hasNext()) { Topic currentTopic = iterator.next(); System.out.println(currentTopic.getName()); } }}// 输出结果// topic1// topic2// topic3// topic4// topic5上面的代码实现非常简单，如果你对 Java 非常熟悉的话，就能在 JDK 的类库中找到 List 的源码实现。正是因为迭代器模式的使用场景非常明确，所以在实际的开发中，它的应用非常广泛，几乎所有涉及集合的遍历时都会使用到迭代器模式。使用迭代器模式的原因使用迭代器模式的原因，可总结为以下两个: 减少程序中重复的遍历代码。我们都知道，对于放入一个集合容器中的多个对象来说，访问必然涉及遍历算法。如果不将遍历算法封装到容器里（比如，List、Set、Map 等），那么就需要使用容器的人自行去实现遍历算法，这样容易造成很多重复的循环和条件判断语句出现，不利于代码的复用和扩展，同时还会暴露不同容器的内部结构。而使用迭代器模式是将遍历算法作为容器对象自身的一种“属性方法”来使用，能够有效地避免写很多重复的代码，同时又不会暴露内部结构; 为了隐藏统一遍历集合的方法逻辑。迭代器模式把对不同集合类的访问逻辑抽象出来，这样在不用暴露集合内部结构的情况下，可以隐藏不同集合遍历需要使用的算法，同时还能够对外提供更为简便的访问算法接口。优缺点迭代器模式有以下优点： 满足单一职责原则。由于迭代器模式是将遍历算法代码统一抽取封装为独立的类，这个类的职责便只有一个——遍历查询所有数据，所以这很符合单一职责原则； 满足开闭原则。当需要对新的对象集合进行扩展时，只需要新增具体的对象迭代器和具体的集合类便能方便地进行扩展； 可以并行遍历同一集合。因为每个对象都有自身的遍历器对象，那么可以同时使用这个遍历器来进行遍历，而无须等待； 可以减少直接使用 for 循环的重复代码问题。直接使用 for 循环的缺点在于必须事先知道集合的数据结构，一旦需要更换一种对象集合的话，则可能需要实现相同的循环逻辑。同时，代码会因此变成了一种硬编码形式，每次都需要修改代码才能进行新的结构的遍历。而使用迭代器模式则可以很好地来避免这个问题。同样，迭代器模式也有一些缺点： 增加子类数量。当新增某种集合类型的迭代器时，还得新增对应类型的迭代器和集合对象，这会增加很多不同的子类； 增加系统复杂性。由于分离了更为抽象的遍历算法逻辑，所以对于那些不了解设计模式的维护者来说，相当于变相地增加新的复杂性。总结迭代器模式的实现原理非常简单，关键思想是将访问和遍历的职责从集合对象中分离出来，放入标准的协议对象中。这样既能对客户端隐藏复杂结构的遍历访问方式，也能提供减少重复遍历的代码实现。现在几乎所有编程语言都会实现迭代器模式，主要被实现为类库来使用，可以说使用非常普遍。其实，除了编程语言之外，很多组件也有应用，比如 Redis 中的 rehash() 操作，就是迭代器模式的体现，而且 Redis 更进一步地还区分了安全迭代器和非安全迭代器。所以说，在遇见需要使用迭代器模式的场景时，回想一下迭代器模式的基本原理，然后“依葫芦画瓢”来构建的迭代器。" }, { "title": "解释器模式：实现自定义配置规则功能", "url": "/posts/interpreter/", "categories": "Design Pattern, behavioral", "tags": "设计模式, Design Pattern, Interpreter, 解释器模式, 类行为型模式", "date": "2018-02-17 13:18:32 +0000", "snippet": "解释器模式使用频率不算高，通常用来描述如何构建一个简单“语言”的语法解释器。它只在一些非常特定的领域被用到，比如： 编译器； 规则引擎； 正则表达式； SQL 解析等。不过，了解它的实现原理，可以帮助思考如何通过更简洁的规则来表示复杂的逻辑。模式原理分析解释器模式的原始定义是：用于定义语言的语法规则表示，并提供解释器来处理句子中的语法。语法也称文法，在语言学中指任意自然语言中句子、短语以及词等语法单位的语法结构与语法意义的规律： 比如：在编程语言中，if-else 用作条件判断的语法，for 用于循环语句的语法标识； 再比如，“我爱中国”是一个中文句子，可以用名词、动词、形容词等语法规则来直观地描述句子。解释器模式的 UML 图：从该 UML 图中，可以看出解释器模式包含四个关键角色： 抽象表达式（AbstractExpression）：定义一个解释器有哪些操作，可以是抽象类或接口，同时说明只要继承或实现的子节点都需要实现这些操作方法； 终结符表达式（TerminalExpression）：用于解释所有终结符表达式； 非终结符表达式（NonterminalExpression）：用于解释所有非终结符表达式； 上下文（Context）：包含解释器全局的信息。解释器模式 UML 对应的代码实现如下：/** * 抽象表达式类 */public interface AbstractExpression { boolean interpreter(Context context);}/** * 上下文信息类 */public class Context { private String data; public Context(String data) { this.data = data; } public String getData() { return data; } public void setData(String data) { this.data = data; } }/** * 终结符表达式类 */public class TerminalExpression implements AbstractExpression { private final String data; public TerminalExpression(String data) { this.data = data; } @Override public boolean interpreter(Context context) { return context.getData().contains(data); } }/** * 非终结符表达式类 */public class NonTerminalExpression implements AbstractExpression{ AbstractExpression expr1; AbstractExpression expr2; public NonTerminalExpression(AbstractExpression expr1, AbstractExpression expr2) { this.expr1 = expr1; this.expr2 = expr2; } @Override public boolean interpreter(Context context) { return expr1.interpreter(context) &amp;amp;&amp;amp; expr2.interpreter(context); }}/** * 单元测试类 */public class Demo { public static void main(String[] args) { Context context1 = new Context(&quot;mick,mia&quot;); Context context2 = new Context(&quot;mick,mock&quot;); Context context3 = new Context(&quot;spike&quot;); AbstractExpression person1 = new TerminalExpression(&quot;mick&quot;); AbstractExpression person2 = new TerminalExpression(&quot;mia&quot;); AbstractExpression isSingle = new NonTerminalExpression(person1, person2); System.out.println(isSingle.interpreter(context1)); System.out.println(isSingle.interpreter(context2)); System.out.println(isSingle.interpreter(context3)); }}//输出结果// true// false// false在上面的代码实现中，NonterminalExpression 用于判断两个表达式是否都存在，存在则在解释器判断时输出 true，如果只有一个则会输出 false。也就是说，表达式解释器的解析逻辑放在了不同的表达式子节点中，这样就能通过增加不同的节点来解析上下文。所以说，解释器模式原理的本质就是对语法配备解释器，通过解释器来执行更详细的操作。使用场景分析一般来讲，解释器模式常见的使用场景有这样几种： 当语言的语法较为简单并且对执行效率要求不高时。比如，通过正则表达式来寻找 IP 地址，就不需要对四个网段都进行 0~255 的判断，而是满足 IP 地址格式的都能被找出来； 当问题重复出现，且可以用一种简单的语言来进行表达时。比如，使用 if-else 来做条件判断语句，当代码中出现 if-else 的语句块时都统一解释为条件语句而不需要每次都重新定义和解释； 当一个语言需要解释执行时。如 XML 文档中&amp;lt;&amp;gt;括号表示的不同的节点含义。创建一个逻辑与的解释器例子。简单来说，就是通过字符串名字来判断表达式是否同时存在，存在则打印 true，存在一个或不存在都打印 false。在下面的代码中，会创建一个接口 Expression 和实现 Expression 接口的具体类，并定义一个终结符表达式类 TerminalExpression 作为主解释器，再定义非终结符表达式类，这里 OrExpression、AndExpression 分别是处理不同逻辑的非终结符表达式。public interface IExpression { boolean interpreter(String con);}public class TerminalExpressionImpl implements IExpression{ String data; public TerminalExpressionImpl(String data) { this.data = data; } @Override public boolean interpreter(String conn) { return conn.contains(data); }}public class AndExpressionImpl implements IExpression{ IExpression expr1; IExpression expr2; public AndExpressionImpl(IExpression expr1, IExpression expr2) { this.expr1 = expr1; this.expr2 = expr2; } @Override public boolean interpreter(String con) { return expr1.interpreter(con) &amp;amp;&amp;amp; expr2.interpreter(con); } }public class QrExpressionImpl implements IExpression{ IExpression expr1; IExpression expr2; public QrExpressionImpl(IExpression expr1, IExpression expr2) { this.expr1 = expr1; this.expr2 = expr2; } @Override public boolean interpreter(String con) { return expr1.interpreter(con) || expr2.interpreter(con); } }public class Client { public static void main(String[] args) { IExpression person1 = new TerminalExpressionImpl(&quot;mick&quot;); IExpression person2 = new TerminalExpressionImpl(&quot;mia&quot;); IExpression isSingle = new QrExpressionImpl(person1, person2); IExpression spike = new TerminalExpressionImpl(&quot;spike&quot;); IExpression mock = new TerminalExpressionImpl(&quot;mock&quot;); IExpression isCommitted = new AndExpressionImpl(spike, mock); System.out.println(isSingle.interpreter(&quot;mick&quot;)); System.out.println(isSingle.interpreter(&quot;mia&quot;)); System.out.println(isSingle.interpreter(&quot;max&quot;)); System.out.println(isCommitted.interpreter(&quot;mock, spike&quot;)); System.out.println(isCommitted.interpreter(&quot;Single, mock&quot;)); }}在最终单元测试的结果中，可以看到：在表达式范围内的单词能获得 true 的返回，没有在表达式范围内的单词则会获得 false 的返回。使用解释器模式的原因使用解释器模式的原因，可总结为以下两个： 将领域语言（即问题表征）定义为简单的语言语法。这样做的目的是通过多个不同规则的简单组合来映射复杂的模型。比如，在中文语法中会定义主谓宾这样的语法规则，当我们写了一段文字后，其实是可以通过主谓宾这个规则来进行匹配的。如果只是一个汉字一个汉字地解析，解析效率会非常低，而且容易出错。同理，在开发中我们可以使用正则表达式来快速匹配IP地址，而不是将所有可能的情况都用 if-else 来进行编写； 更便捷地提升解释数学公式这一类场景的计算效率。我们都知道，计算机在计算加减乘除一类的数学运算时，和人类计算的方式是完全不同的，需要通过一定的规则运算才能得出最后的结果。比如，3+2-（4 X 5)，如果我们不告诉计算机先要运算括号中的表达式，计算机则只会按照顺序进行计算，这显然是错误的。而使用解释器模式，则能很好地通过预置的规则来进行判断和解释。优缺点使用解释器模式主要有以下优点： 很容易改变和扩展语法逻辑。由于在模式中是使用类来表示语法规则的，因此当我们需要新增或修改规则时，只需要新增或修改类即可。同时，还可以使用继承或组合方式来扩展语法； 更容易实现语法。我们可以定义节点的类型，并编写通用的规则来实现这些节点类，或者还可以使用编译器来自动生成它们。同样，解释器模式也不是万能的，也有一些缺点： 维护成本很高。语法中的每个规则至少要定义一个类，因此，语法规则越多，类越难管理和维护； 执行效率较低。由于解释器模式会使用到树的数据结构，那么就会使用大量的循环和递归调用来访问不同的节点，当需要解释的句子语法比较复杂时，会执行大量的循环语句，性能很低； 应用场景单一，复用性不高。在开发中，除了要发明一种新的编程语言或对某些新硬件进行解释外，解释器模式的应用实例其实非常少，加上特定的数据结构，扩展性很低。总结在实际的业务开发中，解释器模式很少使用，主要应用于 SQL 解析、符号处理引擎等场景中。在解释器模式中通常会使用树的结构，有点类似于组合模式中定义的树结构，终端表达式对象是叶对象，非终端表达式是组合对象。虽然解释器模式很灵活，能够使用语法规则解析很多复杂的句子，比如，编程语法。但是稍不留神就很容易把解释逻辑写在一个类中，进而导致后期难以维护。除此之外，把解析逻辑拆分为单个的子节点后，又会因为类数量的暴增，导致代码的理解难度倍增。不过，解释器模式能够通过一些简短的规则来解决复杂的数据匹配问题，比如，正则表达式 [0-9] 就能匹配数字字符串。所以说，理解解释器模式的原理还是很有必要的。" }, { "title": "命令模式：在一次请求中封装多个参数", "url": "/posts/command/", "categories": "Design Pattern, behavioral", "tags": "设计模式, Design Pattern, Command, 命令模式, 对象行为型模式", "date": "2018-02-16 13:18:32 +0000", "snippet": "命令模式使用频率不算太高。如果熟悉函数式编程的话，会发现命令模式完全没有使用的必要，甚至在业务开发的场景中也很少使用到。不过对于想要找到正确抽象的设计者来说，命令模式的设计思想却非常值得借鉴。 查询，获取一个不可变的结果； 命令，改变状态，不一定获取结果 模式原理命令模式的原始定义是：将一个请求封装为一个对象，从而可以参数化具有不同请求、队列或日志请求的其他对象，并支持可撤销的操作。从这个定义能看出，命令模式是为了将一组操作封装在对象中而设计，简单说，就是为了将函数方法封装为对象以方便传输。但是，实际的大部分编程语言中，函数不能作为参数进行传递，比如，Java 直到 8 以后才真正支持将函数作为参数传递。所以说，在实际开发中，如果用到的编程语言不支持函数作为参数来传递，就可以借助命令模式将函数封装为对象来使用。命令模式的 UML 图：从上图，命令模式包含五个关键角色： 抽象命令类（Command）。声明需要做的操作有哪些； 具体命令类（Command1、2等）：实现 Command 接口，其中存储一个接收者类，并在 execute 调用具体命令时，委托给接收者来执行具体方法； 调用者（Invoker）：客户端通过与调用者交互，操作不同命令对象； 抽象接收者（Receiver）：声明需要执行的命令操作，同时提供给客户端使用； 具体接收者（Receiver1、2等）：实现抽象接收者，用于接收命令并执行真实的代码逻辑。这个 UML 图对应的代码实现：public interface Command { void excute();}public class Command1 implements Command{ private final Receiver receiver; public Command1(Receiver receiver) { this.receiver = receiver; } @Override public void excute() { receiver.operationA(); }}public class Command2 implements Command{ private final Receiver receiver; public Command2(Receiver receiver) { this.receiver = receiver; } @Override public void excute() { receiver.operationB(); }}public class Command3 implements Command{ private final Receiver receiver; public Command3(Receiver receiver) { this.receiver = receiver; } @Override public void excute() { receiver.operationC(); }}public interface Receiver { void operationA(); void operationB(); void operationC();}public class Receiver1 implements Receiver{ @Override public void operationA() { System.out.println(&quot;操作 A&quot;); } @Override public void operationB() { System.out.println(&quot;操作 B&quot;); } @Override public void operationC() { System.out.println(&quot;操作 C&quot;); }}public class Demo { public static void main(String[] args) { Receiver receiver1 = new Receiver1(); Invoker invoker = new Invoker(); invoker.setCommands(new Command1(receiver1)); invoker.setCommands(new Command2(receiver1)); invoker.setCommands(new OperationA(receiver1)); invoker.run(); }}//输出结果// 操作 A// 操作 B// 操作 C上面的代码实现非常简单： 其中 Command 就和通常理解的“命令”相一致，比如，点击按钮打开文件、点击按钮关闭窗口等命令，只不过这个命令是告诉计算机的； 接收者则类似不同的处理对象，比如，浏览器可以是一个接收者来接收打开关闭文件的命令，文件编辑器也可以是接收者来接收打开关闭文件的命令。命令模式的核心关键点就在于围绕着命令来展开，通过抽象不同的命令，并封装到对象，让不同的接收者针对同一个命令都能做出自身的反应。使用场景命令模式常见的使用场景有以下几种情况： 需要通过操作来参数化对象时。比如，当将鼠标移动到网页上的下拉菜单时，获取下拉列表的同时还能点菜单项； 想要将操作放入队列、按顺序执行脚本操作或者执行一些远程操作命令时。比如，先 SSH 登录远程服务器，再 tail 查询某个目录下的日志文件，并将日志打印回显到网页的某个窗口中； 实现操作回滚功能的场景时。虽然备忘录模式也能够实现，但是命令模式能够更好地记录命令操作的顺序和相关的上下文信息。对于命令模式的使用场景，一个经典的类比例子就是 Shell 脚本。一个 Shell 脚本其实就是这里的 Invoker 调用者，脚本里各式各样的 ps、cat、sed 等命令就是 Command，而 bash shell 或 z shell 就是作为接收者来具体实现执行命令的。当然，命令模式并不仅限于操作系统的命令，在实际的业务开发中，可能是对应的一组复杂的代码调用逻辑，比如：触发数据统计、日志记录、链路跟踪等。假设正在开发一款网页的文字编辑器，对于文本格式期望支持 HTML 和 Markdown 的格式，编辑器需要有打开、保存和关闭的功能。于是，先来创建一个抽象命令类 Command，其中定义一个无返回的方法 execute。public interface Command { void excute();}再来依次实现打开（Open）、保存（Save）、关闭（Close）三个操作，每个操作中都存有一个 Editor（抽象接收者类），在实现方法 execute 时，会调用 Editor 对应的 open、save 和 close 方法。public class Open implements Command{ private Editor editor; public Open(Editor editor) { this.editor = editor; } @Override public void excute() { editor.open(); } }public class Save implements Command{ private Editor editor; public Save(Editor editor) { this.editor = editor; } @Override public void excute() { editor.save(); } }public class Close implements Command{ private Editor editor; public Close(Editor editor) { this.editor = editor; } @Override public void excute() { editor.close(); } }然后，定义 Editor 的三个操作方法：打开、保存和关闭。public interface Editor { void open(); void save(); void close(); }接着，再来实现支持 HTML5 的编辑器 Html5Editor（具体接收者），分别实现打开、保存和关闭三个方法，这里具体只是打印了三种不同的操作。public class HtmlEditor implements Editor{ @Override public void open() { System.out.println(&quot;=== Html5Editor 执行 open 操作&quot;); } @Override public void save() { System.out.println(&quot;=== Html5Editor 执行 save 操作&quot;); } @Override public void close() { System.out.println(&quot;=== Html5Editor 执行 close 操作&quot;); }}同样，再实现支持 Markdown 的编辑器 MarkDownEditor，功能和 Html5Editor 相同。public class MarkDownEditor implements Editor{ @Override public void open() { System.out.println(&quot;=== MarkDownEditor 执行 open 操作&quot;); } @Override public void save() { System.out.println(&quot;=== MarkDownEditor 执行 save 操作&quot;); } @Override public void close() { System.out.println(&quot;=== MarkDownEditor 执行 close 操作&quot;); }}最后，定义一个 Web 编辑器来模拟执行编辑器的操作，并自定义一个简单的编辑流程来模拟运行编辑器的使用。public class WebEditFlow { private final List&amp;lt;Command&amp;gt; commands; public WebEditFlow() { this.commands = new ArrayList&amp;lt;&amp;gt;(); } public void setCommands(Command command) { commands.add(command); } public void run() { commands.forEach(Command::excute); }}public class Client { public static void main(String[] args) { HtmlEditor htmlEditor = new HtmlEditor(); Open htmlOpen = new Open(htmlEditor); Save htmlSave = new Save(htmlEditor); Close htmlClose = new Close(htmlEditor); MarkDownEditor markDownEditor = new MarkDownEditor(); Open markDownOpen = new Open(markDownEditor); Save markDownSave = new Save(markDownEditor); Close marDownClose = new Close(markDownEditor); WebEditFlow webEditFlow = new WebEditFlow(); webEditFlow.setCommands(htmlOpen); webEditFlow.setCommands(htmlSave); webEditFlow.setCommands(htmlClose); webEditFlow.setCommands(markDownOpen); webEditFlow.setCommands(markDownSave); webEditFlow.setCommands(marDownClose); webEditFlow.run(); }}//输出结果// === Html5Editor 执行 open 操作// === Html5Editor 执行 save 操作// === Html5Editor 执行 close 操作// === MarkDownEditor 执行 open 操作// === MarkDownEditor 执行 save 操作// === MarkDownEditor 执行 close 操作从上面的例子中可以看出，命令模式的使用还是非常简单的，使用关键点就在于按照顺序来组合不同的命令。因此，命令模式的使用场景也非常局限，只能针对命令顺序执行的场景，而对于需要多种组合的场景来说，命令模式并不是很适合。使用命令模式的原因使用命令模式的原因，可总结为以下三个： 只关心具体的命令和动作，不想知道具体的接收者是谁以及如何操作。在实际的开发中，有时我们经常需要向某些对象发送请求，但又不知道请求的接收者是谁。比如，在 Linux 下查看操作系统的日志打印，通常我们知道执行 tail 命令就可以查看日志打印，但是对于 tail 命令如何将文件内容转化为窗口中的显示，实际上我们并不关心。命令模式通过将发送者和接收者解耦开，也就去除了它们之间的直接引用的关系，就能让发送者只提供命令而不必知道命令到底是如何完成的； 为了围绕命令的维度来构建功能。比如，可以构建上传、下载、打开、关闭这样的命令，这样更符合人类自然的思考逻辑，同时避免了使用者需要了解大量的代码实现逻辑，起到隐藏代码逻辑的作用。同时，还能自由组合相关的命令，完成一系列的组合功能，比如，登录到服务器自动下载日志文件，并保存关键数据到数据库； 为了方便统计跟踪行为操作。比如，对于数据的排序、序列化、跟踪、日志记录等操作。使用命令模式能够便捷地记录相关操作，例如，执行撤销和重做操作时，可以从执行的命令列表中快速找到相关操作进行重置，弥补了备忘录模式的缺点。像一些需要读取大量数据的场景中，使用命令模式来读取上下文信息，还能避免内存溢出的风险。优缺点通过上述分析，使用命令模式的优点： 降低对象之间的耦合度。比如，发出请求的对象和执行请求的对象是通过命令对象来间接耦合的，避免了直接引用； 扩展更容易。由于命令模式是以命令作为统一的使用维度，新增逻辑可以放入一个新的命令中来执行。同时，对于同一个命令，还可以使用不同的接收者来进行实现； 可以快捷地设计组合命令。对于调用系统命令或远程命令时，使用命令模式能够方便地进行自由组合。比如，登录到远程服务器执行一组命令操作。命令模式的缺点： 不同的接收者需要实现重复的命令。比如，Markdown 编辑器需要实现编辑器里的打开、保存、关闭等操作，新增一个富文本编辑器时也需要实现打开、保存、关闭等操作； 当命令中涉及对象状态变化时，可能导致不同的结果出现。比如，在执行某条命令时调用异步方法获取结果，这时如果异步执行未完成，那么接下来执行的命令可能就会出现与预期不符的情况； 增加了代码的数量和修改难度。每新增一个命令都需要在对应的接收者那里新增命令的实现，代码量会增加很多。同样，一旦代码发生修改，也会需要修改所有接收者的实现，不利于代码的维护。总结命令模式将一个或一组命令封装为一个对象，从而能够将函数方法作为参数进行传输，同时还能够解耦客户端和服务端的直接耦合，适用场景有：做简单的请求排队，记录请求日志，以及支持可撤销的操作。简单来说，命令模式的本质是对命令进行封装，将发出命令的责任和执行命令的责任分离开。命令模式的原理稍微有点复杂，在使用时也容易区分不开接收者和命令，好在适用的场景比较有限，大多数场景，并不是高频使用的模式。" }, { "title": "责任链模式", "url": "/posts/chain-of-responsobility/", "categories": "Design Pattern, behavioral", "tags": "设计模式, Design Pattern, Chain of Responsibility, 责任链模式, 对象行为型模式", "date": "2018-02-15 13:18:32 +0000", "snippet": "相较而言，责任链模式（Chain of Responsibility）是一个使用频率很高的模式，它属于一个行为型对象设计模式。模式原理责任链模式的原始定义是：通过为多个对象提供处理请求的机会，避免将请求的发送者与其接收者耦合。链接接收对象并沿着链传递请求，直到对象处理它。这个定义读起来抽象难懂，实际上它只说了一个关键点：通过构建一个处理流水线来对一次请求进行多次的处理。就像网购一样：当你收到了购的商品后，发现商品有质量问题，于是打电话询问客服关于退货流程，客服接到电话后，会先打开订单系统查询提供的订单信息并确认是否正确，确认后再使用物流系统通知快递小哥上门取件，快递小哥取件后会返回商品让仓储系统进行确认，并通知商品系统……这样的一个过程就是责任链模式的真实应用。责任链模式的 UML 图：从该 UML 图中，可以看出责任链模式其实只有两个关键角色。 处理类（Handler）：可以是一个接口，用于接收请求并将请求分派到处理程序链条中（实际上就是一个数组链表），其中，会先将链中的第一个处理程序放入开头来处理。 具体处理类（HandlerA、B、C）：按照链条顺序对请求进行具体处理。该 UML 对应的代码实现：public interface Handler { void setNext(Handler handler); void handle(Request request);}public class HandlerA implements Handler { private Handler next; @Override public void setNext(Handler handler) { this.next = handler; } @Override public void handle(Request request) { System.out.println(&quot;HandlerA 执行 代码逻辑, 处理: &quot; + request.getData()); request.setData(request.getData().replace(&quot;AB&quot;, &quot;&quot;)); if (null != next) { next.handle(request); } else { System.err.println(&quot;执行中止!&quot;);; } }}public class HandlerB implements Handler{ private Handler next; public HandlerB() { } @Override public void setNext(Handler handler) { this.next = handler; } @Override public void handle(Request request) { System.out.println(&quot;HandlerB 执行 代码逻辑, 处理: &quot; + request.getData()); request.setData(request.getData().replace(&quot;CD&quot;, &quot;&quot;)); if (null != next) { next.handle(request); } else { System.err.println(&quot;执行中止!&quot;);; } }}public class HandlerC implements Handler{ private Handler next; public HandlerC() { } @Override public void setNext(Handler handler) { this.next = handler; } @Override public void handle(Request request) { System.out.println(&quot;HandlerC 执行 代码逻辑, 处理: &quot; + request.getData()); if (null != next) { next.handle(request); } else { System.err.println(&quot;执行中止!&quot;);; } }}public class Request { private String data; public String getData() { return data; } public void setData(String data) { this.data = data; } }从这段代码实现可以看出，责任链模式的实现非常简单，每一个具体的处理类都会保存在它之后的下一个处理类。当处理完成后，就会调用设置好的下一个处理类，直到最后一个处理类不再设置下一个处理类，这时处理链条全部完成。在代码示例中，HandlerA 删除掉字符串 ABCDE 中的 AB，并交给 HandlerB 处理；HandlerB 删除掉 CDE 中的 CD，并交给 HandlerC；HandlerC 处理完后，整个执行过程中止。使用场景责任链模式常见的使用场景有以下几种情况。 在运行时需要动态使用多个关联对象来处理同一次请求时。比如：请假流程、员工入职流程、编译打包发布上线流程等。 不想让使用者知道具体的处理逻辑时。比如，做权限校验的登录拦截器。 需要动态更换处理对象时。比如，工单处理系统、网关 API 过滤规则系统等。下面通过一个简单的例子演示。这里创建一个获取数字并判断正负或零的程序，程序接收一个数字的请求，在链条上进行处理并打印对应的处理结果。先创建一个链条 Chain，并设置起始的处理类，如下代码所示：public class Chain { Excutor chain; public Chain() { buildChain(); } private void buildChain() { Excutor e1 = new NegativeExcutor(); Excutor e2 = new ZeroExcutor(); Excutor e3 = new PositiveExcutor(); e1.setNext(e2); e2.setNext(e3); this.chain = e1; } public void process(Integer num) { chain.handle(num); }}接下来创建抽象的处理类 Excutor，声明两个方法： setNext 用于设置下一个处理类； handle 是具体的业务逻辑。public interface Excutor { /** * 用于设置下一个处理类 * @param excutor */ void setNext(Excutor excutor); /** * 具体业务逻辑 * @param num */ void handle(Integer num);}NegativeExcutor、PositiveExcutor 和 ZeroExcutor 分别代表处理负数、正数和零。/** * 处理负数 */public class NegativeExcutor implements Excutor{ private Excutor next; @Override public void setNext(Excutor excutor) { this.next = excutor; } @Override public void handle(Integer num) { if (null != num &amp;amp;&amp;amp; num &amp;lt; 0) { System.out.println(&quot;NegativeExcutor 获取数字: &quot; + num + &quot;, 处理完成! &quot;); } else { if (null != next) { System.out.println(&quot;=== 经过 NegativeExcutor&quot;); next.handle(num); } else { System.out.println(&quot;处理终止! - NegativeExcutor&quot;); } } }}/** * 处理正数 */public class PositiveExcutor implements Excutor{ private Excutor next; @Override public void setNext(Excutor excutor) { this.next = excutor; } @Override public void handle(Integer num) { if (null != num &amp;amp;&amp;amp; num &amp;gt; 0) { System.out.println(&quot;PositiveExcutor 获取数字: &quot; + num + &quot;, 处理完成! &quot;); } else { if (null != next) { System.out.println(&quot;=== 经过 PositiveExcutor&quot;); next.handle(num); } else { System.out.println(&quot;处理终止! - PositiveExcutor&quot;); } } }}/** * 处理零 */public class ZeroExcutor implements Excutor{ private Excutor next; @Override public void setNext(Excutor excutor) { this.next = excutor; } @Override public void handle(Integer num) { if (null != num &amp;amp;&amp;amp; num == 0) { System.out.println(&quot;ZeroExcutor 获取数字: &quot; + num + &quot;, 处理完成! &quot;); } else { if (null != next) { System.out.println(&quot;=== 经过 ZeroExcutor&quot;); next.handle(num); } else { System.out.println(&quot;处理终止! - ZeroExcutor&quot;); } } }}最后，运行一个单元测试：public class Client { public static void main(String[] args) { Chain chain = new Chain(); chain.process(99); System.out.println(&quot;---------&quot;); chain.process(-11); System.out.println(&quot;---------&quot;); chain.process(0); System.out.println(&quot;---------&quot;); chain.process(null); }}// 输出结果// === 经过 NegativeExcutor// === 经过 ZeroExcutor// PositiveExcutor 获取数字: 99, 处理完成! // ---------// NegativeExcutor 获取数字: -11, 处理完成! // ---------// === 经过 NegativeExcutor// ZeroExcutor 获取数字: 0, 处理完成! // ---------// === 经过 NegativeExcutor// === 经过 ZeroExcutor// 处理终止! - PositiveExcutor从最后结果可以看到，当输入不同的数时，都会经过一整个链条的流转，直到最终的处理对象完成处理。所以说，责任链模式就像工厂的流水线作业一样，按照某一个标准化的流程来执行，用于规则过滤、Web 请求协议解析等具备链条式的场景中，通过拆分不同的处理节点来完成整个流程的处理。使用责任链模式的原因使用责任链模式的原因，可总结为以下三个： 解耦使用者和后台庞大流程化处理。在线购物订单里包含了物流、商品、支付、会员等多个系统的处理逻辑，如果让使用者一一和它们对接，势必会造成使用困难、系统之间调用混乱的情况发生，而通过订单建立一个订单的状态变更流程，就能将这些系统很好地串联在一起，这不仅能够让使用者只需要关注订单流程这一个入口，同时还能够让不同的系统按照各自的职责来发挥作用。比如，订单在未完成支付前，商品系统是不会通知物流系统进行商品发货的； 为了动态更换流程处理中的处理对象。比如，在请假流程中，申请人一般会提交申请给直接领导审批，但有时直接领导可能无法进行审批操作，这时系统就可以更换审批人到其他审批人，这样就不会阻塞请假流程的审批； 为了处理一些需要递归遍历的对象列表。比如，权限的规则过滤。对于不同部门不同级别人员的权限，就可以采用一个过滤链条来进行权限的管控。优缺点通过上述分析，责任链模式的优点： 降低客户端对象与处理链条上对象之间的耦合度。比如，提交上线审核，提交人只知道最开始申请的处理人是谁，而后续是否需要别的审核人其实是由处理链条来控制的； 提升系统扩展性。对于需要多次处理的同一个请求，可以在链条上增加新的具体处理类，满足开闭原则，能极大地提升系统扩展性； 增强了具体处理类的职责独立性。即便链条上的工作流程发生了变化，也可以动态地改变具体处理类的调用次序和增加类的新的职责。每个类只需要处理自己该处理的工作，不该处理的就传递给下一个对象完成，明确各类的责任范围，同时也符合类的单一职责原则； 简化了对象之间前后关联处理的复杂性。每个对象只需存储一个指向后继者的引用，不需保持其他所有处理者的引用，这避免了使用众多的 if 或者 if···else 语句。同样，责任链模式的缺点： 降低性能。由于每一个请求都需要经历一次完整的链条上具体处理类的处理，系统性能势必会受到一定影响，比如，依赖更多的代码行或依赖更复杂的代码逻辑； 调试难度增大。调试代码需要验证每个具体处理者是否都能接收到请求，一旦出现错误，排查与修改也变得更加麻烦； 容易出现死锁异常。一旦某一个对象设置后继者出现错误，就会出现循环调用，进而导致堆栈溢出的错误。总结在实际软件开发中，责任链模式应用非常广泛，可以说只要是与流程相关的软件系统都能够使用责任链模式来构建： 用在代码中实现松散耦合； 动态增删子处理流程。责任链模式的原理和实现虽然都非常简单，但是在实际使用中需要注意： 维护上下文关系的正确性，一旦出现循环调用，很容易死锁而导致程序崩溃； 控制责任链中的处理对象数量。如果处理对象的数量过多，比如超过 20 个，容易让代码变得难以维护，这时还是应该尽可能减少处理对象的数量，将其合并到相类似的处理对象中去。" }, { "title": "代理模式：控制和管理对象访问", "url": "/posts/proxy-pattern/", "categories": "Design Pattern, creational", "tags": "设计模式, Design Pattern, Proxy, 代理模式, 对象结构型模式", "date": "2018-02-13 12:45:00 +0000", "snippet": "代理模式原理非常简单，和装饰模式很类似，都是在不改变同一个接口功能的前提下，对原有接口功能做扩展。但是代理模式的应用却比装饰模式更为广泛，因为代理模式并不执着于链式结构，而是采用更为灵活的单一结构，在很多框架和组件的设计里都能看到代理模式的身影，比如： JDK 的动态代理机制； Spring 的 AOP 机制； Dubbo 框架等。 模式原理分析 原始定义是：够提供对象的替代品或其占位符。代理控制着对于原对象的访问，允许将请求提交给对象前后进行一些处理。 从这个定义中能看出，代理模式是作为对象之间的一种中间结构来使用的，通过构建一个代理对象来对原始的功能进行委托处理，其中有一个很重要的功能就是控制对象的访问。拿现实中的例子来说，假设你有一套房子要卖，可你却正好在外地出差，你不能亲自处理带人看房、过户等购房手续，这时你就可以找一个房产中介来作为你的代理人，委托他来帮你带人看房、处理过户手续。这就是现实中经典的代理模式的例子。代理模式的 UML 图：从这个 UML 图中，代理模式中有三个关键角色： 抽象主题类（RealObject）：声明公用的方法，定义可供客户端使用的统一功能； 主题实现类（RealObjectImpl）：实现了抽象主题类的所有方法； 代理类（Proxy）：实现抽象主题类方法，并隐藏在代理后面可能其他类的实现。在图中这三个角色都有相互依赖的关系: 代理类采用继承方式，获取抽象主题类中定义的公共方法； 在代理类内，可进行相关扩展操作，但最终还是需执行主题实现类的方法。和适配器模式有很不同的地方，适配器模式是转换为新的接口，而代理模式不会改变原有接口。UML 对应的代码实现：public interface RealObject { void doSomething();}public class RealObjectImpl implements RealObject{ @Override public void doSomething() { System.out.println(&quot;=== 真实对象输出打印&quot;); }}public class Proxy extends RealObjectImpl{ @Override public void doSomething() { // 这里做一些代理操作或额外的操作 System.out.println(&quot;== 通过代理类来执行真实对象&quot;); super.doSomething(); }}public class Demo { public static void main(String[] args) { RealObject realObjectProxy = new Proxy(); realObjectProxy.doSomething(); }}// 输出// == 通过代理类来执行真实对象// === 真实对象输出打印在上面代码实现中，RealObjectImpl 实现了接口 RealObject 的功能 doSomething，这时又创建了一个代理对象 Proxy，它继承了 RealObjectImpl，目的是在使用 RealObject 时可以做一些额外的操作。代码实现虽然简单，但是展现了代理模式的基本理念——作为一个外包装的中间层，享有控制住访问对象的权利，同时也能扩展一些功能。使用代理模式的原因虽然代理模式的基本原理非常简单，但是为什么不直接使用已有对象的功能，而非要在中间加一层代理呢？这有主要有三个原因： 客户端有时无法直接操作某些对象。 比如：在分布式应用中，需要调用的对象可能是运行在另外一台服务器上，访问它时，必须要通过网络才能访问。如果让客户端直接去调用，那么意味着客户端需要处理网络服务，包括连接、打包、传包、解包等复杂操作； 如果使用代理模式，在客户端和远程服务端之间建立一个网络代理对象，那么客户端只需要调用代理对象就能跟远程对象建立联系，甚至就像调用本地对象一样。这其实就是常说的 RPC 服务的基本原理，本质上就是代理模式。 客户端执行某些耗时操作容易造成服务端阻塞。 比如：在类似有道、石墨、语雀这样的云编辑器里进行文案编写时，拷贝多张图片可能就是一件非常耗时的操作，使用者并不希望在执行拷贝图片的操作后，打字就无法正常操作甚至无法查看其他页面。这时，对于软件设计者来说，图片加载就可以通过代理模式来解决：标示图片所在位置，然后使用代理对象去读取图片资源，这样就不会影响其他客户端与服务端之间的操作了。 服务端需要控制客户端的访问权限。 代理模式除了扩展功能外，另一个更为重要的功能是做权限控制。比如，某一项业务由于安全原因只能让一部分特定的用户去访问，如果在原有功能基础上再增加权限过滤功能就会增加代码的耦合性，并且也不方便组件的复用。其实，这时做一个代理类就可以解决该问题，对于特定的接口来说，只需要指定所有请求必须通过该代理类，然后由该代理类做权限判断即可。 使用场景不难发现，代理模式的核心能力在于对某一个具体的功能进行增强和补充。结合一个简单的例子来说明。在 Spring 中，我们经常会使用 @Transactional 注解来做事务控制，代码如下所示：@Servicepublic class TestInfo { @Autowired private TestService testService; @Transactional public TestData create(String case) { System.out.println(testService.getClass().getName()); return testService.create(author); }}在 TestInfo 类中，@Transactional 注解了 create 方法，这个注解表示 Spring 会使用事务的方式来执行代码中的 create 方法。 事务，简单来说，事务是指程序中一系列关联的逻辑操作，所有操作必须全部完成后才算一次操作成功，否则所有的操作变更都会被撤销。在上面的代码中，为了完成一次事务，Spring 便采用了代理模式的方式来解决，也就是 @Transactional 注解的实现。 @Transactional 注解实现的原理： 在一次请求调用的事务开始后，Transactional 的注解实现类会生成一个代理对象 connection； 通过 AOP 机制将其放入 Spring 的 Bean 资源池中与 DataSource、DataSourceTransactionManager 相关的对象容器中； 在接下来的整个事务中的代码，都会通过该 connection 对象连接数据库，执行所有数据库命令； 事务结束时，回到最初的代理 connection 对象上返回操作，并关闭代理对象 connection。显然，@Transactional 注解的实现过程其实就是代理模式的一种最佳实践。 代理模式应用非常广泛，目前的使用场景大致可总结为以下五大类： 虚拟代理，适用于延迟初始化，用小对象表示大对象的场景：“大对象”通常包含大量 IO 资源，比如图片、大文件、模型文件等。显然很占用内存空间，一直保持其运行会很消耗系统资源，这时就可以使用代理模式； 解决方法，可以先创建一个消耗相对较小的对象来代理这个大对象的创建，而实际上真实的大对象只会在真正需要时才会被创建，这样的代理方式就被称为虚拟代理。比如，在 Java 中的 CopyOnWriteArrayList 数组对象的实现就是使用了虚拟代理的方式，目的就是要让操作延迟，只有对象被真正用到的时候才会被克隆; 保护代理，适用于服务端对客户端的访问控制场景。代理模式有一个非常重要的应用场景就是控制一个对象对另一个对象的访问与使用权限。当客户端通过代理对象访问服务端的原始对象时，代理对象会根据具体的规则来判断客户端是否有访问权限。比如，防火墙其实就是一种保护代理的具体实践； 远程代理，适用于需要本地执行远程服务代码的场景。 在这种场景中，代理对象会隐藏处理所有与网络相关的复杂细节。随着微服务架构的流行，越来越多的程序应用部署在多台服务器上，各自服务都更专注于各自的业务，当需要使用其他服务时就会频繁进行远程服务调用，但不可能所有的业务都要自己实现网络调用，于是就出现了的远程代理框架，比如，gRpc、Dubbo 等； 日志记录代理，适用于需要保存请求对象历史记录的场景，比如，日志监控。客户端在调用请求时，并不会感知到日志记录，这是因为代理对象在原始对象周围添加了监控功能； 缓存代理，适用于缓存客户请求结果并对缓存生命周期进行管理的场景。比如，商品详情页通常包含大量图片和文字介绍，代理对象可以对重复请求相同的结果进行缓存。优缺点优点主要有以下： 作为接口的特定中间层，能够降低对象间的直接耦合。代理对象很好地解耦了客户端与服务端之间的调用关系，即使客户端在使用服务端对象还未准备好或不存在时，代理也可以正常工作； 虚拟代理通过延迟加载以及使用小对象代表大对象的方式，帮助减少系统资源的损耗，提升系统运行速度； 保护代理可以控制客户端对服务端的访问权限； 远程代理帮助客户端快速访问分布式机器上的对象，分布式服务器通常可以提供集群负载均衡、故障容错和高性能的计算能力； 日志记录代理能记录每次操作的信息，对于用户使用轨迹跟踪、数据统计、定位问题等都有重要作用； 缓存代理能够提供各式各样的缓存结果，对于需要高频读取重复数据的系统来说，能极大地提升系统性能。代理模式的缺点有如下几个： 在客户端和服务端之间增加了代理对象，所以也增加了系统的复杂度； 实现了代理模式的服务，如果处理请求的时间过长，就容易造成多个服务调用阻塞而影响整体系统的处理速度； 对于一些偏操作系统或标准协议等底层的代理服务而言，代码实现可能很复杂。总结代理模式试图解决的问题： 有无法直接调用某些对象； 耗时的操作； 某个接口可能需要外部额外操作，如日志记录、权限管控、重复操作等； 一直保存大对象； 需要控制访问权限。对于这些问题，代理模式通过建立一个代理对象，在原始对象的外围来封装这些问题可能引起的代码变化。代理模式的原理虽然简单，但是应用却非常广泛，特别是在中间件领域随处可见代理模式，比如 Dubbo、gRPC 都是代理模式的优秀实践。" }, { "title": "享元模式：通过共享对象减少内存加载消耗", "url": "/posts/flyweight/", "categories": "Design Pattern, structural", "tags": "设计模式, Design Pattern, 享元模式, Flyweight, 对象结构型模式", "date": "2018-02-12 14:18:32 +0000", "snippet": "享元模式的原理和实现都很简单，但是应用场景却相对狭窄，它和缓存模式、池化模式有所联系，却又有不同。模式原理原始定义是：摒弃在每个对象中保存所有数据的方式，通过共享多个对象所共有的相同状态，从而让我们能在有限的内存容量中载入更多对象。从定义中可以发现，享元模式要解决的核心问题就是节约内存空间，使用的办法是找出相似对象之间的共有特征，然后复用这些特征。UML 图是如何表示享元模式的，如下图：从这个 UML 图中，享元模式包含的关键角色有四个： 享元类（Flyweight）：定义了享元对象需要实现的公共操作方法。在该方法中会使用一个状态作为输入参数，也叫外部状态，由客户端保存，在运行时改变； 享元工厂类（Flyweight Factory）：管理一个享元对象类的缓存池。它会存储享元对象之间需要传递的共有状态，比如，按照大写英文字母来作为状态标识，这种只在享元对象之间传递的方式就叫内部状态。同时，它还提供了一个通用方法 getFlyweight()，主要通过内部状态标识来获取享元对象； 可共享的具体享元类（ConcreteFlyweight）：能够复用享元工厂内部状态并实现享元类公共操作的具体实现类； 非共享的具体享元类（UnsharedConcreteFlyweight）：不复用享元工厂内部状态，但实现享元类的具体实现类。 内部状态：不会随环境改变而改变的状态，俗称不可变对象。比如，在 Java 中 Integer 对象初始化就是缓存 -127 到 128 的值，无论怎么使用 Integer，这些值都不会变化。 外部状态：随环境改变而改变的状态。通常是某个对象所独有的，不能被共享，因此，也只能由客户端保存。之所以需要外部状态就是因为客户端需要不同的定制化操作。 UML 对应的代码实现：/** * 享元类 */public interface FlyWeight { void operation(int state);}/** * 享元工厂类 */public class FlyWeightFactory { // 定义一个池容器 public Map&amp;lt;String, FlyWeight&amp;gt; pool = new HashMap&amp;lt;&amp;gt;(); public FlyWeightFactory() { // 将对应的内部状态添加进去 pool.put(&quot;A&quot;, new ConcreteFlyweight(&quot;A&quot;)); pool.put(&quot;B&quot;, new ConcreteFlyweight(&quot;B&quot;)); pool.put(&quot;C&quot;, new ConcreteFlyweight(&quot;C&quot;)); } // 根据内部状态来查找值 public FlyWeight getFlyWeight(String key) { if (pool.containsKey(key)) { System.out.println(&quot;==== 享元池中有，直接复用，key：&quot; + key); return pool.get(key); } else { System.out.println(&quot;==== 享元池中没有，重新创建并复用，key：&quot; + key); FlyWeight flyWeightNew = new ConcreteFlyweight(key); pool.put(key, flyWeightNew); return flyWeightNew ; } } public FlyWeight getUnSharedFlyWeight(String key) { FlyWeight flyWeight = new UnSharedConcreteFlyweight(key); System.out.println(&quot;=== 创建不共享的对象，key: &quot; + key); return flyWeight; }}public class UnSharedConcreteFlyweight implements FlyWeight{ private String uniqueKey; public UnSharedConcreteFlyweight(String uniqueKey) { this.uniqueKey = uniqueKey; } @Override public void operation(int state) { System.out.println(&quot;=== 使用不共享的对象，内部状态：&quot;+uniqueKey+&quot;，外部状态：&quot;+state); }}/** * 享元工厂类 */public class FlyWeightFactory { // 定义一个池容器 public Map&amp;lt;String, FlyWeight&amp;gt; pool = new HashMap&amp;lt;&amp;gt;(); public FlyWeightFactory() { // 将对应的内部状态添加进去 pool.put(&quot;A&quot;, new ConcreteFlyweight(&quot;A&quot;)); pool.put(&quot;B&quot;, new ConcreteFlyweight(&quot;B&quot;)); pool.put(&quot;C&quot;, new ConcreteFlyweight(&quot;C&quot;)); } // 根据内部状态来查找值 public FlyWeight getFlyWeight(String key) { if (pool.containsKey(key)) { System.out.println(&quot;==== 享元池中有，直接复用，key：&quot; + key); return pool.get(key); } else { System.out.println(&quot;==== 享元池中没有，重新创建并复用，key：&quot; + key); FlyWeight flyWeightNew = new ConcreteFlyweight(key); pool.put(key, flyWeightNew); return flyWeightNew ; } } public FlyWeight getUnSharedFlyWeight(String key) { FlyWeight flyWeight = new UnSharedConcreteFlyweight(key); System.out.println(&quot;=== 创建不共享的对象，key: &quot; + key); return flyWeight; }}这段代码实现非常简单，不过这里你可能会联想到缓存模式，比如，LRU 缓存模式。但这两者是完全不同的设计意图，它们的本质区别在于：享元模式要解决的问题是节约内存的空间大小，而缓存模式本质上是为了节省时间。回到上面的代码分析中，能看出享元模式封装的变化有： 对象内部状态的定义规则，比如，是通过字母共享状态，还是通过固定的数字来共享状态； 具体享元对象所实现的公共操作的逻辑。所以说，享元模式本质上是通过创建更多的可复用对象的共有特征来尽可能地减少创建重复对象的内存消耗。使用场景一般来讲，享元模式常用的使用场景有以下几种： 系统中存在大量重复创建的对象。比如，同一个商品的展示图片、详情介绍、文字介绍等，当自营商家系统调用或第三方商家调用时，商品系统可以使用同一个对象来节省内存空间； 可以使用外部特定的状态来控制使用的对象。比如，使用常用的中文汉字作为读取的标识，读取享元池中共享的多个中文汉字对象； 相关性很高并且可以复用的对象。比如，公司的组织结构人员基本信息、网站的分类信息列表等。在现实中，享元模式最常使用的场景是在编辑器软件中，比如，在一个文档中多次出现相同的图片，则只需要创建一个图片对象，通过在应用程序中设置该图片出现的位置，就可以实现该图片在不同地方多次重复显示的效果。在 Java 中，享元模式一个常用的场景就是，使用数据类的包装类对象的 valueOf() 方法。比如，使用 Integer.valueOf() 方法时，实际的代码实现中有一个叫 IntegerCache 的静态类，它就是一直缓存了 -127 到 128 范围内的数值，如下代码所示，可以在 Java JDK 中的 Integer 类的源码中找到这段代码。public static Integer valueOf(int i) { if (i &amp;gt;= IntegerCache.low &amp;amp;&amp;amp; i &amp;lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);}享元模式本质上在使用时就是找到不可变的特征，并缓存起来，当类似对象使用时从缓存中读取，以达到节省内存空间的目的。比如，在需要承接大流量的系统中使用图片，都知道高清图片即便是压缩后占用的内存空间也很大，那么在使用图片时节省内存空间就是首要考虑的设计因素，而享元模式可以很好地帮助解决这类问题场景。使用享元模式的原因使用享元模式的原因，主要有以下两个： 减少内存消耗，节省服务器成本。 比如，当大量商家的商品图片、固定文字（如商品介绍、商品属性）在不同的网页进行展示时，通常不需要重复创建对象，而是可以使用同一个对象，以避免重复存储而浪费内存空间。由于通过享元模式构建的对象是共享的，所以当程序在运行时不仅不用重复创建，还能减少程序与操作系统的 IO 交互次数，大大提升了读写性能； 聚合同一类的不可变对象，提高对象复用性。 比如，Java 中的 Number 对象族类（Integet、Long、Double 等）都是使用了享元模式例子，通过缓存不同范围数值来重复使用相同的数值。优缺点通过上面的分析，我们可以得出使用享元模式主要有以下优点。 可以极大减少内存中对象的数量，使得相同对象或相似对象在内存中只保存一份。 通过封装内存特有的运行状态，达到共享对象之间高效复用的目的。同样，享元模式也不是万能的，它也有一些缺点： 以时间换空间，间接增加了系统的实现复杂度。比如，需要分离出内部状态和外部状态，其中，外部状态这个定义比较模糊，也很难统一，而内部状态除了一些常用的常量容易被找到以外，那些更通用的组件对象在不同的业务系统中其实是不容易被找到的，因为不同的人对相似对象的理解并不一致，这就需要对程序员的代码设计实现思维有一定的要求。 运行时间更长，对于一些需要快速响应的系统并不合适。享元模式的目的是节省空间，而没有说需要提供更短的时间，这适合数据类项目的使用，而不适合一些有高并发要求的系统。总结享元模式为共享对象定义了一个很好的结构范例。不过，用好享元模式的关键在于找到不可变对象，比如，常用数字、字符等。之所以做对象共享而不是对象复制的一个很重要的原因，就在于节省对象占用的内存空间大小。缓存模式和享元模式最大的区别就是： 享元模式强调的是空间效率，比如，一个很大的数据模型对象如何尽量少占用内存并提供可复用的能力； 缓存模式强调的是时间效率，比如：缓存秒杀的活动数据和库存数据等，数据可能会占用很多内存和磁盘空间，但是得保证在大促活动开始时要能及时响应用户的购买需求。也就是说，两者本质上解决的问题类型是不同的。虽然享元模式应用不如缓存模式多，但是对于超大型数据模式来说，它却是非常有效的优化方法之一。特别是对于现在越来越多的数据系统来说，共享变得更加重要，因为复制虽然时间效率更高，但是空间上可能完全不够。、" }, { "title": "门面模式：实现 API 网关的高可用性", "url": "/posts/facade/", "categories": "Design Pattern, structural", "tags": "设计模式, Design Pattern, 门面(外观)模式, Facade, 对象结构型模式", "date": "2018-02-11 14:18:32 +0000", "snippet": "门面模式的原理非常容易理解，使用也非常灵活，因此，它的应用非常广泛。门面模式和代理模式经常容易搞混。比如，业务 API 网关和 Nginx 网关是不是差不多？实际上这两种模式的本质原理是不同的。模式原理原始定义是：为子系统中的一组接口提供统一的接口。它定义了一个更高级别的接口，使子系统更易于使用。这个定义告诉门面模式本质就是：统一多个接口的功能。换句话说，当需要用更统一的标准方式来与系统交互时，就可以采用门面模式。比如： 使用 Slf4j 日志框架来统一 log4j、log4j2、CommonLog 等日志框架； 支付时通过扫描二维码来使用支付系统。对于用户来说，他们并不关心后台系统实现有多么复杂，只关心最终能否支付成功。不过，这里要注意的是，“定义更高级别的接口”不代表只能定义一个接口，这也是和代理模式根本的不同之处。也就是说，门面模式可能代理的是多个接口，而代理模式通常只是代理某一个接口。门面模式的 UML 图：可以看到，门面模式包含的关键角色有两个： 门面系统，负责处理依赖子系统的请求，并将请求代理给适当的子系统进行处理； 子系统，代表某个领域内的功能实现，比如，订单、用户、支付等，专门处理由门面系统指派的任务。平时最常见的电脑开机按钮就是一个门面模式，点击按钮电脑就会启动，再点击按钮电脑就会关闭，至于电脑如何运行 CPU、启动内存、读取硬盘、点亮显示器，我们并不关心，只关心使用视角下的电脑，而不关心电脑本身是如何运行的。门面模式封装的变化主要是子系统的一切变化（自身复杂性、可能出现的问题等），并且随着子系统的独立演化，子系统可能变得越来越复杂，但是只要和门面系统的交互不发生改变，那么就并不会影响门面系统。就好比，虽然后端研发团队不断地迭代更新 API 版本，但只要接口定义不变，那么前端团队就不需要进行任何修改。所以说，门面模式的原理本质就是简化外部系统使用内部多个子系统的使用方式。使用场景门面模式常用的使用场景有以下几种： 简化复杂系统。 比如，开发了一整套的电商系统后（包括订单、商品、支付、会员等系统），不能让用户依次使用这些系统后才能完成商品的购买，而是需要一个门户网站或手机 App 这样简化过的门面系统来提供在线的购物功能； 减少客户端处理的系统数量。 比如，在 Web 应用中，系统与系统之间的调用可能需要处理 Database 数据库、Model 业务对象等，其中使用 Database 对象就需要处理打开数据库、关闭连接等操作，然后转换为 Model 业务对象，实在是太麻烦了。如果能够创建一个数据库使用的门面（其实就是常说的 DAO 层），那么实现以上过程将变得容易很多； 让一个系统（或对象）为多个系统（或对象）工作。 比如，线程池 ThreadPool 就是一个门面模式，它为系统提供了统一的线程对象的创建、销毁、使用等； 联合更多的系统来扩展原有系统。 当电商系统中需要一些新功能时，比如，人脸识别，可以不需要自行研发，而是购买别家公司的系统来提供服务，这时通过门面系统就能方便快速地进行扩展； 作为一个简洁的中间层。 门面模式还可以用来隐藏或者封装系统中的分层结构，同时作为一个简化的中间层来使用。比如，在秒杀、库存、钱包等场景中，需要共享有状态的数据时（如商品库存、账户里的钱），在不改变原有系统的前提下，通过一个中间的共享层（如将秒杀活动的商品库存总数统一放在 Redis 里），就能统一进行各种服务（如，秒杀详情页、商品详情页、购物车等）的调用。总结来说，门面模式在使用的时候能够提供一个简单的概览视图，让使用者能够很方便地去使用。实际上，这一视图对于绝大多数用户来说已经足够了。比如，点击添加商品到购物车、结算、支付、等待收货……虽然对于电商系统本身来说需要考虑各种各样的情况，但是对于用户来说，购物网站这个门面就已经足够他们使用了。这也是为什么手机上的 App 虽然只有一个，但是实际上这个 App 背后需要的可能是成百上千的“一个人”在运营和研发这个 App。所以说，门面模式本身并不是一个代码实现的模式，而是组合更多的其他模式来使用的一种通用解决方案。使用门面模式的原因使用门面模式的原因，主要有以下两个： 为了解决遗留系统重构的问题遗留系统通常是指承担着当前多个系统处理流程中重要的一环，但可能因为开发时间太过久远或初始维护团队发生重大调整，进而导致遗留下来的代码难以维护而得名； 在重构遗留系统的过程中，内部子系统通常可能是非常复杂的，一般新维护的团队基本上不会修改原有系统的接口，但是新功能又必须要上线，这时就会采用门面模式先统一重要的接口，然后再逐渐地更新与迁移功能到新系统上，完成遗留系统的重构。这样既能保证原有线上系统不受影响，也能不断更新老旧的代码，让代码更易维护； 结合经验来看，门面模式在设计上就是为了兼容更多不同的系统，非常适合重构遗留系统。 为了解决分层架构中的扩展问题。 不管是单体应用还是分布式应用，使用分层架构时，容易出现各个层次的入口和出口被滥用的现象，比如，视图层滥用 DAO 层来访问数据存储。 实际上，对于更高层的使用者来说，有时其实并不关心底层的实现逻辑。如果底层提供了太丰富的功能而又没有做限制时，就容易让高层的使用者混淆。而使用门面模式能够清晰地定义一类操作，比如，数据的增删改查门面系统，任务调度的门面系统。 一方面，门面模式能够充当权限管控的角色（按类区分），让上层的访问能够汇聚到一个门面系统中，方便使用； 另一方面，门面模式也能简化下层不同子系统的依赖关系（按分类关系聚合），避免滥用。这样在每一层里进行系统扩展时，就不会影响到其他系统。 优缺点门面模式主要有以下几个优点： 对使用者屏蔽子系统的细节，因而减少了使用者处理的对象数目，让整个系统使用起来更加方便。比如，API 网关对外只有一个调用点，而后端服务可以用成百上千的服务系统连接网关； 实现了子系统与使用者之间的松散耦合关系。比如，活动时用户只需要点击抢购按钮就能实现一键下单并送货，用户不用知道商品系统是如何扣减商品的，也不用知道物流系统如何调度送货的； 有助于建立层次结构系统，并简化层与层之间的依赖关系。比如，视图层要访问存储层数据时，如果直接使用数据访问接口会造成依赖混乱，而按照某一个分类的服务建立一个门面服务层，比如，动态路由读写数据门面系统、数据埋点采集门面系统等，能够提供更简化统一的操作； 能够消除复杂的循环依赖。比如，为多个外部系统提供统一的 SDK 应用包，统一定义不同的接口方法，指定对应的子系统进行使用； 有利于系统在不同平台之间的移植和重构。 比如，系统早期通过 C# 实现，但现在团队期望通过 Java 进行重构，这时门面系统就可以充当中间的协议统一者，提供 HTTP 协议接口，然后逐渐迁移代码功能，使用过渡与升级同时并存的方式，完成系统的切换与升级。当然，门面模式也有一些缺点： 降低了可靠性。从架构模式上看，可能会出现过多的子系统依赖一个门面系统的情况，常见的网关模式就是这样，只要网关挂掉，所有子系统可能就无法使用了； 容易导致子系统越来越复杂。有了简化的门面系统后，子系统可能就会不受约束地自由扩展。如果子系统在进行跨版本的升级而不通知门面系统时，就可能会造成系统的不可用。总结 门面模式的本质是：简化调用，统一操作； 门面模式最典型的一种应用就是 API 网关，特别是随着微服务的不断流行，API 网关充当着越来越重要的角色： API 网关要解决的一个主要问题就是：高可用性。但实际会发现，从模式的原理上很难解决高可用的问题，因为只要依赖的系统越多，网关出现了问题，那么所有系统都变成了不可用状态。这也是有时不使用 API 网关的原因； 现在更多的是通过其他手段来保护网关，比如，降级、限流、熔断等操作，或者通过不断扩展网关的吞吐量处理能力来提高网关的可用性。 虽然门面模式满足最小知识原则，但这仅仅是对于用户而言的，而对于使用系统的维护者来说，庞大的子系统依然需要大量的业务知识； 虽然门面模式有很多优势，如统一对外的服务管控，但是同样劣势也很明显，如造成系统单点故障而使所有系统不可用。所以说，门面模式更多是为了简化操作而提出的一种优化方法，不要过于迷信它，而是应该辩证地看待它。" }, { "title": "装饰器模式：在基础组件上扩展新功能", "url": "/posts/decorator/", "categories": "Design Pattern, structural", "tags": "设计模式, Design Pattern, 装饰器模式, Decorator, 对象结构型模式", "date": "2018-02-10 14:18:32 +0000", "snippet": "装饰器模式看上去和适配器模式、桥接模式很相似，都是使用组合方式来扩展原有类的，但其实本质上却相差甚远呢。简单来说，适配器模式侧重于转换，而装饰模式侧重于动态扩展；桥接模式侧重于横向宽度的扩展，而装饰模式侧重于纵向深度的扩展。原理装饰模式的原始定义是：允许动态地向一个现有的对象添加新的功能，同时又不改变其结构，相当于对现有的对象进行了一个包装。这个定义非常清晰易懂，因为不能直接修改原有对象的功能，只能在外层进行功能的添加，所以装饰器模式又叫包装器模式。装饰器模式的 UML 图：装饰模式的四个关键角色： 组件：作为装饰器类包装的目标类； 具体组件：实现组件的基础子类。 装饰器：一个抽象类，其中包含对组件的引用，并且还重写了组件接口方法。 具体装饰器：继承扩展了装饰器，并重写组件接口方法，同时可以添加附加功能。代码实现如下：/* 组件 */public interface IComponent { void excute();}/* 具体组件 */public class BaseComponent implements IComponent{ @Override public void excute() { // TODO: 2022/6/6 do something }}/* 装饰器 */public class BaseDecorator implements IComponent{ private IComponent wrapper; public BaseDecorator(IComponent wrapper) { this.wrapper = wrapper; } @Override public void excute() { wrapper.excute(); }}/* 具体装饰器 A */public class DecoratorA extends BaseDecorator{ public DecoratorA(IComponent wrapper) { super(wrapper); } @Override public void excute() { super.excute(); }}/* 具体装饰器 B */public class DecoratorB extends BaseDecorator{ public DecoratorB(IComponent wrapper) { super(wrapper); } @Override public void excute() { super.excute(); } }代码实现比较简单： 组件 Component 定义了组件具备的基本功能； 具体组件 BaseComponent 是对组件（接口）的一种基础功能的实现； 装饰器 BaseDecorator 中包含 Component 的抽象实例对象，作为装饰器装饰的目标对象； 具体装饰器 DecoratorA 和 DecoratorB 继承装饰器 BaseDecorator 来进行具体附加功能的沿用与扩展。所以说，装饰器模式本质上就是给已有不可修改的类附加新的功能，同时还能很方便地撤销。使用的场景装饰模式常用的使用场景有以下几种： 快速动态扩展和撤销一个类的功能场景。 比如，有的场景下对 API 接口的安全性要求较高，那么就可以使用装饰模式对传输的字符串数据进行压缩或加密。如果安全性要求不高，则可以不使用； 可以通过顺序组合包装的方式来附加扩张功能的场景。 比如，加解密的装饰器外层可以包装压缩解压缩的装饰器，而压缩解压缩装饰器外层又可以包装特殊字符的筛选过滤的装饰器等； 不支持继承扩展类的场景。 比如，使用 final 关键字的类，或者系统中存在大量通过继承产生的子类。在现实中有一个很形象的关于装饰器使用场景的例子，那就是单反相机镜头前的滤镜。不加滤镜其实不会影响拍照，而滤镜实际上就是一个装饰器，滤镜上又可以加滤镜，这样就做到了不改变镜头而又给镜头增加了附加功能。假设要创建一个文件读写器程序，能够读字符串，又能将读入的字符串写入文件，如下： 创建抽象的文件读取接口 DataLoader，代码如下： /* 抽象的文件读取接口 */public interface IDataLoader { String read(); void write(String data);} 创建具体组件 BaseFileDataLoader，重写组件 DataLoader 的读写方法： /* 具体组件 */public class BaseFileDataLoader implements IDataLoader { private String filePath; public BaseFileDataLoader(String filePath) { this.filePath = filePath; } @Override public String read() { char[] buffer = null; File file = new File(filePath); try (FileReader reader = new FileReader(file)) { buffer = new char[(int) file.length()]; reader.read(buffer); } catch (IOException e) { e.printStackTrace(); } return new String(buffer); } @Override public void write(String data) { File file = new File(filePath); try (OutputStream fos = new FileOutputStream(file)) { fos.write(data.getBytes(), 0, data.length()); } catch (IOException e) { e.printStackTrace(); } } } 创建装饰器 DataLoaderDecorator，这里要包含一个引用 DataLoader 的对象实例 wrapper，同样是重写 DataLoader 方法，不过这里使用 wrapper 来读写： public class DataLoaderDecorator implements IDataLoader{ private IDataLoader wrapper; public DataLoaderDecorator(IDataLoader wrapper) { this.wrapper = wrapper; } @Override public String read() { return wrapper.read(); } @Override public void write(String data) { wrapper.write(data); } } 创建在读写时有加解密功能的具体装饰器 EncryptionDataDecorator，它继承了装饰器 DataLoaderDecorator 重写读写方法。 不过，需要注意的是，这里新建了 encode 和 dcode 方法来包装它的父类 DataLoaderDecorator 的读写方法，实现在读文件时进行解密、写文件时进行加密的功能。 /* 有加解密功能的具体装饰器 */public class EncryptionDataDecorator extends DataLoaderDecorator{ public EncryptionDataDecorator(IDataLoader wrapper) { super(wrapper); } @Override public String read() { return decode(super.read()); } @Override public void write(String data) { super.write(encode(data)); } private String encode(String data) { byte[] result = data.getBytes(); for (int i = 0; i &amp;lt; result.length; i++) { result[i] += (byte) 1; } return Base64.getEncoder().encodeToString(result); } private String decode(String data) { byte[] result = Base64.getDecoder().decode(data); for (int i = 0; i &amp;lt; result.length; i++) { result[i] -= (byte) 1; } return new String(result); } } 再然后，创建一个*压缩和解压的具体装饰器类* CompressionDataDecorator，新建 compress 和 decompress 方法用来包装父类 DataLoaderDecorator 的读写方法，也就是在读取时解压、写入时压缩 public class CompressionDataDecorator extends DataLoaderDecorator{ public CompressionDataDecorator(IDataLoader wrapper) { super(wrapper); } @Override public String read() { return decompress(super.read()); } @Override public void write(String data) { super.write(compress(data)); } private String compress(String stringData) { byte[] data = stringData.getBytes(); try { ByteArrayOutputStream bout = new ByteArrayOutputStream(512); DeflaterOutputStream dos = new DeflaterOutputStream(bout, new Deflater()); dos.write(data); dos.close(); bout.close(); return Base64.getEncoder().encodeToString(bout.toByteArray()); } catch (IOException e) { e.printStackTrace(); return null; } } private String decompress(String stringData) { byte[] data = Base64.getDecoder().decode(stringData); try { InputStream in = new ByteArrayInputStream(data); InflaterInputStream iin = new InflaterInputStream(in); ByteArrayOutputStream bout = new ByteArrayOutputStream(512); int b; while ((b = iin.read()) != -1) { bout.write(b); } in.close(); iin.close(); bout.close(); return new String(bout.toByteArray()); } catch (IOException e) { e.printStackTrace(); return null; } } } 运行一个单元测试：创建一个具体装饰器，写入的时候先加密再压缩，然后通过普通组件类和具体装饰器类读取进行对比 : public class Demo { public static void main(String[] args) { String testInfo = &quot;Name, testInfo\\nMia, 10000\\nMax, 9100&quot;; DataLoaderDecorator encoded = new CompressionDataDecorator( new EncryptionDataDecorator( new BaseFileDataLoader(&quot;demo.txt&quot;))); encoded.write(testInfo); IDataLoader plain = new BaseFileDataLoader(&quot;demo.txt&quot;); System.out.println(&quot;- 输入 ----------------&quot;); System.out.println(testInfo); System.out.println(&quot;- 加密 + 压缩 写入文件--------------&quot;); System.out.println(plain.read()); System.out.println(&quot;- 解密 + 解压 --------------&quot;); System.out.println(encoded.read()); }} 总之，装饰模式适用于一个通用功能需要做扩展而又不想继承原有类的场景，同时还适合一些通过顺序排列组合就能完成扩展的场景。使用装饰模式的理由主要有以下两个： 为了快速动态扩展类功能，降低开发的时间成本。 比如，一个类 A，有子类 A01、A02，然后 A01 又有子类 A001，以此类推，A0001、A00001……这样的设计会带来一个严重的问题，那就是：当需要扩展 A01 时，所有 A01 的子类和父类都会受到影响。但是，如果这时我们使用装饰器 B01、B02、C01、C02，那么扩展 A01 就会变为 A01B01C01、A01B02C02 这样的组合。这样就能快速地扩展类功能，同时还可以按需来任意组合，极大地节省了开发时间; 希望通过继承的方式扩展老旧功能。 比如，前面说的，当类标识有 final 关键字时，要想复用这个类就只能通过重新复制代码的方式，不过通常这样的类又处于需要对外提供功能的状态，不能轻易修改，而梳理上下文逻辑又费时费力，那么采用装饰模式就是一个很好的选择。因为装饰器是在外层进行扩展，即使功能不合适，也能及时地撤销而不影响原有的功能。所以说，在一些维护系统的升级或重构场景中，使用装饰模式来重构代码，在短期内都能达到快速解耦的效果。优缺点使用装饰模式主要有以下四个大的优点： 快速扩展对象的功能。 对于一些独立且无法修改的类来说，当需要在短期内扩展功能时，采用装饰模式能快速有效地扩展功能，同时也不会影响原有的功能； 可以动态增删对象实例的功能。 比如，在上面文件读写器的例子中，我们可以在创建对象的时候再决定是一起使用压缩装饰器和加密装饰器，还是分开使用，或者只是用基本的读写功能； 可以在统一行为上组合几种行为。 装饰模式是对某一个接口行为进行的组合扩展，通过包装的方式不断扩展代码的行为，从而实现了更多行为的组合； 满足单一职责原则。 每一个具体装饰器类只实现一个组件的具体行为，即便附加了新的功能也是围绕着组件的职责而做扩展，保证了职责的单一性。装饰器模式的缺点： 在调用链中删除某个装饰器时需要修改代码。 装饰模式的最大弊端在于，当在某个组件上附加了太多装饰器后，想要删除其中的某个装饰器时，就需要修改前后的装饰器的引用位置，这样容易导致上下文中代码都需要修改的情况，大大增加了出错的可能性； 容易导致产生很多装饰对象，增加代码理解难度。 由于使用了组合方式，并且在调用时使用了链式结构，这样间接增加了很多装饰器对象，而一旦不了解装饰模式的特性，就很容易误解为多个对象的参数调用，增加了代码的理解难度； 增加问题定位和后期代码维护成本。 虽然装饰模式使用的组合方式比继承更加灵活，但同时也会增加代码的复杂性，在维护代码时会增加问题定位难度，同时调试时也需要逐级排查，比较烦琐，增加了后期代码维护成本。总结装饰模式就像是送人礼物时的“包装盒”，可以选择各种各样的包装盒，还可以在包装盒里嵌套包装盒。装饰模式在结构上体现为链式结构，通过在外层不断地添加具体装饰器类来对原有的组件类进行扩展，这样在保证原有功能的情况下，还能额外附加新的功能。这也是学习和理解装饰模式的核心所在。虽然装饰模式的原理和使用都很简单，但是有时链式结构本身会让代码调用链条变得很长，变成了一种对原有组件接口的定制化开发。因此，一般情况下不建议装饰器超过 10 个，如果超过还是要考虑重构组件功能。除此之外，对于没有上下逻辑的装饰器，也要尽量避免使用装饰模式。" }, { "title": "组合模式：实现抽象协议与不同实现的绑定", "url": "/posts/composite/", "categories": "Design Pattern, structural", "tags": "设计模式, Design Pattern, 组合模式, Composite, 对象结构型模式", "date": "2018-02-09 14:18:32 +0000", "snippet": "单纯从字面上来看，很容易将“组合模式”和“组合关系”搞混。组合模式最初只是用于解决树形结构的场景，更多的是处理对象组织结构之间的问题。组合关系则是通过将不同对象封装起来完成一个统一功能。虽然组合模式并不常用，但是学习它的原理能够获得更多复杂结构上的思考。比如： MySQL 的索引设计中就是用了 B+ 树算法的组合模式设计，极大地提升了数据查询时的性能。组合模式的原理很容易理解，但在代码实现上却是反直觉的。原理组合模式的定义是：将对象组合成树形结构以表示整个部分的层次结构。组合模式可以让用户统一对待单个对象和对象的组合。两个关键点： 一个是用树形结构来分层； 另一个是通过统一对待来简化操作。之所以要使用树形结构，其实就是为了能够在某种层次上进行分类，并且能够通过统一的操作来对待复杂的结构。比如，常见的公司统计多个维度的人员的工资信息，如果一个一个统计单人的工资信息会比较费时费力，但如果将人员工资信息按照组织结构构建成一棵“树”，那么按照一定的分类标示（比如，部门、岗位），就能快速找到相关人员的工资信息，而不是每次都要查找完所有人的数据后再做筛选。组合场景的 UML 如下图所示：组合模式中包含了三个关键角色： 抽象组件：定义需要实现的统一操作； 组合节点：代表一个可以包含多个节点的复合对象，意味着在它下面还可以有其他组合节点或叶子节点； 叶子节点：代表一个原子对象，意味着在它下面不会有其他节点了。组合模式最常见的结构就是树形结构，通过上面的三个角色就可以很方便地构建树形结构。可以结合现实中的例子来理解，比如，一个公司中有总经理，在总经理之下有经理、秘书、副经理等，而在经理之下则有组长、开发人员等，其结构图大致如下：除了树形结构以外，组合模式中还有环形结构和双向结构（如下图），其中，环形结构和数据结构中的单向链表很相似，而双向结构其实就是 Spring 中 Bean 常用的结构。组合模式对应的 UML 的代码实现：/* 抽象组件 */public abstract class Component { public abstract void operation();}/* 叶子节点 */public class Leaf extends Component{ @Override public void operation() { /* 叶子节点的操作放在这里 */ }}/* 组合节点 */public class Node extends Component{ /* 存放子节点列表 */ private List&amp;lt;Component&amp;gt; childrenList; @Override public void operation() { for (Component component : childrenList) { component.operation(); } }}这里有一个经常容易被搞混淆的地方，就是 Node 中 operation() 方法下的 for 循环。很多时候，以为这个 for 循环是一个固定实现的代码，但其实这个理解是错的。实际上，这里的 for 循环想要表达的意思是，遍历组合节点下的所有子节点时才需要用到循环，而不是这里只能用循环。从上面的分析中，组合模式封装了如下变化： 叶子节点和组合节点之间的区别； 真实的数据结构（树、环、网等）； 遍历真实结构的算法（比如，广度优先，还是深度优先等）； 结构所使用的策略（比如，用来汇总数据，还是寻找最佳/最坏的节点等）。所以说，组合模式本质上封装了复杂结构的内在变化，让使用者通过一个统一的整体来使用对象之间的结构。使用场景组合模式的使用场景有： 处理一个树形结构，比如，公司人员组织架构、订单信息等； 跨越多个层次结构聚合数据，比如，统计文件夹下文件总数； 统一处理一个结构中的多个对象，比如，遍历文件夹下所有 XML 类型文件内容。以”订单信息”为例。假设一个新的商品订单系统（如下图），如何计算每个订单的总费用呢？从上面的简图可以看到，一个订单中可能通常会包含各类商品、发票等信息。在现实中，每个商品都会被放到一个快递盒中，然后小的盒子又可以被放入另一个更大的盒子中，以此类推，整个结构看上去像是一棵“倒过来的树”。过去，计算价格时的做法是：打开所有快递盒，找到每件商品，然后计算总价。但在程序中，会发现这不是使用简单的 for 循环语句（一次打开盒子的动作）就能完成的，此外，还需要知道：对应的商品类别有哪些，这个订单使用了哪些商品，价格是多少，赠送的商品有哪些，要抵消多少价格……而组合模式就是专门为需要反复计算或统计的场景而生的。比如生成树形对象功能的具体实现： 定义一个抽象组件 AbstractNode，其中定义节点可以做的操作有：判断是否为根节点、获取节点 id、获取节点关联父节点 id、设置 id、设置父 id、增加、删除节点和获取子节点： public abstract class AbstractNode { public abstract boolean isRoot(); public abstract int getId(); public abstract int getParentId(); public abstract void setId(int id); public abstract void setParentId(int parentId); public abstract void add(AbstractNode abstractNode); public abstract void remove(AbstractNode abstractNode); public abstract AbstractNode getChild(int i);} 创建组合节点 Node，继承自 AbstractNode 实现定义的 8 种接口方法，其中 List 对象 children 用于存放子节点列表： public class ComponentNode extends AbstractNode{ private List&amp;lt;AbstractNode&amp;gt; children; private int id; private int pid; public ComponentNode() { children = new ArrayList&amp;lt;AbstractNode&amp;gt;(); } @Override public boolean isRoot() { return -1 == pid; } @Override public int getId() { return id;} @Override public int getParentId() { return pid; } @Override public void setId(int id) { this.id = id;} @Override public void setParentId(int parentId) { this.pid = parentId; } @Override public void add(AbstractNode abstractNode) { abstractNode.setParentId(this.pid + children.size()); abstractNode.setId(abstractNode.getParentId() + 1); children.add(abstractNode); } @Override public void remove(AbstractNode abstractNode) { children.remove(abstractNode); } @Override public AbstractNode getChild(int i) { return children.get(i); }} 再创建叶子节点 Leaf，同样继承自 AbstractNode，重写 8 种接口方法，不过，因为叶子节点不能新增和删除节点，所以添加和删除方法不支持，并且获取子节点方法也应该为空： public class ComponentLeaf extends AbstractNode{ private int id; private int pid; @Override public boolean isRoot() { return false; } @Override public int getId() { return this.id; } @Override public int getParentId() { return this.pid; } @Override public void setId(int id) { this.id = id; } @Override public void setParentId(int parentId) { this.pid = parentId; } @Override public void add(AbstractNode abstractNode) { throw new UnsupportedOperationException(&quot;这个是叶子节点，无法增加...&quot;); } @Override public void remove(AbstractNode abstractNode) { throw new UnsupportedOperationException(&quot;这个是叶子节点，无法删除&quot;); } @Override public AbstractNode getChild(int i) { return null; }} 最后，运行一个单元测试。创建一个根节点，再将一个有两个叶子节点的组合节点添加到根节点上，并打印组合节点的 id 值l public class Demo { public static void main(String[] args) { AbstractNode rootNode = new ComponentNode(); rootNode.setId(0); rootNode.setParentId(-1); AbstractNode node = new ComponentNode(); node.add(new ComponentLeaf()); node.add(new ComponentLeaf()); rootNode.add(new ComponentLeaf()); rootNode.add(new ComponentLeaf()); rootNode.add(node); System.out.println(node.getId()); }} 通过单元测试代码的运行，实现了建立一个简单的树形结构。从上面所有的代码实现中，定义根节点比较重要，通过根节点能够不断找到相关的节点，而且使用的操作都是相同的。总之，在面向对象编程中，组合模式能够很好地适用于解决树形结构的应用场景。使用组合模式的理由使用组合模式的三个主要原因： 希望一组对象按照某种层级结构进行管理，比如，管理文件夹和文件，管理订单下的商品等。树形结构天然有一种层次的划分特性，能够让我们自然地理解多个对象间的结构； 需要按照统一的行为来处理复杂结构中的对象，比如，创建文件，删除文件，移动文件等。在使用文件时，我们其实并不关心文件夹和文件是如何被组织和存储的，只要我们能够正确操作文件即可，这时组合模式就能够很好地帮助我们组织复杂的结构，同时按照定义的统一行为进行操作； 能够快速扩展对象组合。 比如，订单下的手机商品节点可以自由挂接不同分类的手机（品牌类的，如华为、苹果），并且还可以按照商品的特性（如，价格、图片、商家、功能等）再自由地挂接新的节点组合，而查找时可以从手机开始查找，不断增加节点类型，直到找到合适的手机商品。优缺点使用组合模式主要有以下三大优点： 清晰定义分层结构。组合模式在实现树形结构时，能够非常清楚地定义层次，并且能让使用者忽略层次的差异，以方便对整个层次结构进行控制。 简化使用者使用复杂结构数据的代码。由于组合模式中的对象只有组合节点和叶子节点两种类型，而节点的使用操作是一样的，比如，查找子节点，查找父节点等，那么对于使用者来说，就能使用一致的行为，而不用不关心当前处理的是单个对象还是整个结构，间接简化了使用者的代码。 快速新增节点，提升组合灵活性。 在组合模式中，新增节点会很方便，而无须对现有代码进行任何修改，符合“开闭原则”；同时还能够在局部节点上按照相关性再进行自由的组合，大大提升了对象结构的灵活性。组合模式的缺点。 难以限制节点类型。 比如，在上面的订单例子中，订单树中除了组合商品类的节点外，实际上只要不约束，就可以组合任意类型的节点，因为它们都来自同一个根节点，都属于节点类型，只不过节点里包含的信息各不相同罢了，所以，在使用组合模式时，通常需要在设计时从逻辑层面上进行一定的约束。 要增加很多运行时的检查，增加了代码复杂度。 一旦对象类型不能做限制后，就必须通过运行时来进行类型检查，而这个实现过程比较复杂，会增加很多额外的代码耦合性，同时还会增加代码的理解难度。 错误的遍历算法可能会影响系统性能。 组合模式实现树形结构虽然好用，但是一旦使用了错误的遍历算法，就会在数据量剧增的情况下拖慢系统速度，比如，当使用简单多层 for 循环嵌套来查找全量的数据时，算法的时间复杂度可能是 m 次方 O(n^m)，会造成外部服务阻塞等待，这样很可能会直接导致其他服务因为长时间等待而出现超时错误。因此，使用组合模式时一定要谨慎选择遍历算法。" }, { "title": "桥接模式：实现抽象协议与不同实现的绑定", "url": "/posts/bridge/", "categories": "Design Pattern, structural", "tags": "设计模式, Design Pattern, 桥接模式, Bridge, 对象结构型模式", "date": "2018-02-08 13:18:32 +0000", "snippet": "桥接模式。桥接模式的原理非常简单，但是使用起来会有一定的难度，所以相对于适配器模式来说，在理解桥接模式时，重点是能跳出局部，多从整体结构上去思考。模式原理定义是：将抽象部分与它的实现部分分离，使它们都可以独立地变化。不过，这里的抽象常常容易被理解为抽象类，并将实现理解为继承后的“派生类”，但是这样理解存在局限性，因为 GoF 的本意是想表达“从对象与对象间的关系去看，做抽象实体与抽象行为的分离”，所以使用抽象实体和抽象行为来描述更为准确。桥接模式的 UML 下图所示：从该图中，桥接模式主要包含了以下四个关键角色： 抽象实体：定义的一种抽象分类。比如，电脑中的 CPU、内存、摄像头、显示屏等； 具体实体：继承抽象实体的子类实体。比如，Intel i7 CPU、三星内存、徕卡摄像头、京东方显示屏幕等； 抽象行为：定义抽象实体中具备的多种行为。比如，CPU 逻辑运算、内存读写存储、摄像头拍照、屏幕显示图像等； 具体行为：实现抽象行为的具体算法。比如，Intel 使用 X64 架构实现 CPU 逻辑运算，Mac M1 芯片使用 ARM 架构实现 CPU 逻辑运算，等等。在我看来，桥接模式原理的核心是抽象与抽象之间的分离，这样分离的好处就在于，具体的实现类依赖抽象而不是依赖具体，满足 DIP 原则，很好地完成了对象结构间的解耦。换句话说，抽象的分离间接完成了具体类与具体类之间的解耦，它们之间使用抽象来进行组合或聚合，而不再使用继承。桥接模式对应 UML 图的代码实现，具体如下：package cn.happymaya.ndp.creational.bridge.uml;public abstract class AbstractEntity { /** * 行为对象 */ protected AbstractBehvior myBehavior; /** * 实体与行为的关联 */ public AbstractEntity(AbstractBehvior aBehvior) { myBehavior = aBehvior; } /** * 子类需要实现的方法 */ public abstract void request();}public class DetailEntityA extends AbstractEntity{ public DetailEntityA(AbstractBehvior aBehvior) { super(aBehvior); } @Override public void request() { super.myBehavior.operation1(); }}public class DetailEntityB extends AbstractEntity{ public DetailEntityB(AbstractBehvior aBehvior) { super(aBehvior); } @Override public void request() { super.myBehavior.operation2(); }}public abstract class AbstractBehvior { public abstract void operation1(); public abstract void operation2();}public class DetailBehaviorA extends AbstractBehvior{ @Override public void operation1() { System.out.println(&quot;op-1 form DetailBehaviorA&quot;); } @Override public void operation2() { System.out.println(&quot;op-2 form DetailBehaviorA&quot;); }}public class DetailBehaviorB extends AbstractBehvior{ @Override public void operation1() { System.out.println(&quot;op-1 form DetailBehaviorB&quot;); } @Override public void operation2() { System.out.println(&quot;op-2 form DetailBehaviorB&quot;); }}从上面的代码实现你会很容易发现，桥接模式封装了如下变化： 实体变化； 行为变化； 两种变化之间的关系； 变化引起的变化。桥接模式封装变化的本质上是对事物进行分类（实体），并对实体中的功能性（行为）再划分的一种解决方案。比如，电子产品可以被分为手机、电脑等，其中手机隐藏了手机一类相关的变化；同样，手机和电脑都具备使用 App 软件的功能，它们各自隐藏了如何使用 App 的具体方式。在面向对象软件开发中，我们通常是使用接口或抽象类来作为抽象实体和具体实体，使用具体对象实例和实现接口的对象作为抽象行为和具体行为。所以说，桥接模式的本质是通过对一个对象进行实体与行为的分离，来将需要使用多层继承的场景转换为使用组合或聚合的方式，进而解耦对象间的强耦合关系，达到对象与对象之间的动态绑定的效果，提升代码结构的扩展性。使用场景桥接模式常用场景有如下几种： 需要提供平台独立性的应用程序时。 比如，不同数据库的 JDBC 驱动程序、硬盘驱动程序等； 需要在某种统一协议下增加更多组件时。 比如，在支付场景中，我们期望支持微信、支付宝、各大银行的支付组件等。这里的统一协议是收款、支付、扣款，而组件就是微信、支付宝等； 基于消息驱动的场景。 虽然消息的行为比较统一，主要包括发送、接收、处理和回执，但其实具体客户端的实现通常却各不相同，比如，手机短信、邮件消息、QQ 消息、微信消息等； 拆分复杂的类对象时。 当一个类中包含大量对象和方法时，既不方便阅读，也不方便修改； 希望从多个独立维度上扩展时。 比如，系统功能性和非功能性角度，业务或技术角度等； 需要在运行时切换不同实现方法时。 比如，通过门面模式调用外部 RPC 服务。接下来，通过一个不同操作系统下的文件上传例子来快速理解桥接模式的使用场景。首先，创建一个抽象实体类 FileUploader，它包含了两个抽象行为：上传（upload）和检查（check）。package cn.happymaya.ndp.creational.bridge.example;public interface FileUploader { Object upload(String path); boolean check(Object object); }然后，再建立一个具体实体类 FileUploaderImpl，其中包含了抽象行为类 FileUploadExcutor（文件上传执行器），实现了抽象行为 upload 和 check。public class FileUploaderImpl implements FileUploader{ private FileUploadExcutor excutor = null; public FileUploaderImpl(FileUploadExcutor excutor) { this.excutor = excutor; } @Override public Object upload(String path) { return excutor.uploadFile(path); } @Override public boolean check(Object object) { return excutor.checkFile(object); }}public interface FileUploadExcutor { Object uploadFile(String path); boolean checkFile(Object object);}接下来，在 Linux 平台上实现文件上传执行器 LinuxFileUpLoadExcutor，在 Windows 上实现文件上传执行器 WindowsFileUpLoadExcutor，具体代码如下所示：public class LinuxFileUploadExcutor implements FileUploadExcutor{ @Override public Object uploadFile(String path) { return null; } @Override public boolean checkFile(Object object) { return false; }}public class WindowsUploadExcutor implements FileUploadExcutor{ @Override public Object uploadFile(String path) { return null; } @Override public boolean checkFile(Object object) { return false; }}从上面代码中，可以发现：通过将文件上传执行器和文件上传行为进行分离，就能实现实体和行为的灵活演化。比如： 当你想要实现一个新的上传到云存储的文件上传执行器时，可以先新建一个叫 OSSFileUploaderImpl 的具体实现类，然后建立对应的云存储文件执行器，接着再分别实现华为云、阿里云、腾讯云等各种不同云存储的文件上传执行器； 如果你还想要在执行器里加入新的行为，比如删除，这时平台上的执行器并不需要调用“删除”这个接口，这样就做到了实体和行为的解耦，极大地提升了代码的扩展性。当做了实体和行为的分离后，还可以结合更多的模式来扩展桥接模式。比如，这里简单扩展了一下桥接模式的 UML 图：在实现抽象行为时，可以使用适配器模式来扩展功能，也可以使用门面模式来扩展更多外部的服务功能。总体来说，桥接模式的使用场景非常灵活，侧重于实体和行为的分离，然后再基于这两个维度进行独立的演化。使用桥接模式的原因使用桥接模式的原因，主要有以下三个： 为了灵活扩展代码结构。 上面使用了适配器模式和门面模式的桥接模式就是一个很好的思考方向，与通过硬编码直接调用 API 的形式相比，“通过模式来扩展”会更容易控制代码行数和逻辑结构。就经验来看，在很多大规模代码系统中，有结构的代码可维护性会更好。因为是人来维护代码的，而人的特性是天生对结构型的东西更“敏感”，并且灵活的结构在后期进行代码重构时也能更好地替换与修改； 为了更好地解决跨平台兼容性问题。 桥接模式之所以能很好地解决跨平台的兼容性问题，就是因为桥接模式通过抽象层次上结构的分离，让相关的分类能够聚合到各自相关的层次逻辑中，而不同的平台对于同一个 API 在具体的代码实现上是不同的，这样反而符合不同操作系统按照各自维度演化的特性； 为了在运行时组合不同的组件。 无论是框架还是外部服务，都需要基于一个统一的协议进行协同工作，但是通过静态的继承方法很难做到在程序运行时进行方法或组件的动态更换。而使用桥接模式和门面模式就可以很方便地进行替换，比如，在上面文件上传执行器的案例中，可以使用一个统一的 API 网关调用不同的云服务来完成文件上传。优缺点桥接模式主要有以下四个大优点： 分离实体与行为，提升各自维度的演化效率。 比如： 订单中的会员信息可以理解为抽象实体，普通会员和 plus 会员就是不同的具体实体； 会员中的积分累计就是抽象行为，不同会员按照各自的积分计算轨迹进行计算就是具体行为的体现，那么，会员可以再继续增加更多会员类别，而积分计算规则也可以不断更新； 符合开闭原则，提升代码复用性。 每一个维度的类都以组合或聚合关系进行合作，新增类或修改类都在各自类内部进行，不影响其他类； 用组合关系替代了多重继承，提升了代码结构的演化灵活性。 多继承违背了单一职责原则，虽然关联性更强，但复用性很差； 组合关系的优势就在于可以在任意阶段进行升级与替换，并且可以按需进行组合与撤销，这对于需求快速变化的开发场景而言很适用，能够极大地提升代码结构的灵活性。 符合表达原则，提升代码的可理解性。由于桥接模式从抽象层次就进行了分离，不同的类别会按照各自的特点进行演化，所以不管是在结构上还是代码内在含义上，都更聚焦，这样在阅读代码时也就能更容易理解。缺点： 增加了维护成本。 桥接模式因为需要做很多实体和行为的分离，所以会间接地要增加不少代码行数。再加上使用组合和聚合关系不像继承关系那样容易找到对象之间的调用关系，稍不注意就会影响到其他对象，这样大大增加了代码修改维护的成本。 导致性能下降。 组合或聚合关系在面向对象编程中使用的是委托的实现方式，简单理解就是调用的对象变多了，自然也就影响到了程序的性能。 增加设计难度。 桥接模式更重视聚合而非继承关系，那么就需要建立更多的抽象层，要求开发者针对抽象层进行设计与编程。我们都知道，找到正确的抽象层有时是一件相当困难的事情，虽然现在有很多优秀的设计能够作为借鉴，但在一些新的领域里依然会有一定的设计难度。总结 桥接模式可以说是 DIP 原则的具体实践。在软件开发中，一个对象可以从实体和行为两个角度来进行分离，其实就是将依赖从一个大而全的对象变换到依赖两个可以独立变化的维度，控制也就发生了反转； 桥接模式因为重视组合和聚合，从而有效避免了多重继承带来的问题。也就是说，通过抽象实体与抽象行为的关联，将静态的继承关系转换为了动态的组合关系，从而使得系统结构更加灵活； 在实际开发中，应该将桥接模式和更多的模式结合起来使用，将不同模式或服务作为某一个独立的维度来进行演化； 多在实践中寻找可以做实体和行为分离的场景，并尝试使用桥接模式来解决，这是学习桥接模式最好的办法。 如果加入的新组合是一种很common，那没什么问题，但如果是 special，只针对某一个特定场景，后续的继承维护可能就可能造成代码冗余，维护越来越难。 一旦开始 special定制化，代码耦合性就会陡增。即便用了设计模式也不能避免修改代码带来的连带影响。所以，明白了设计原理的前提下，还要能批判性的使用设计模式，而不能表面上大行其道的用设计模式，私底下还是在做硬编码。" }, { "title": "适配器模式：处理不同 API 接口的兼容性", "url": "/posts/adapter/", "categories": "Design Pattern, structural", "tags": "设计模式, Design Pattern, 适配器模式, Adapter, 类结构型模式", "date": "2018-02-07 08:18:32 +0000", "snippet": "适配器模式的原始定义是：将类的接口转换为客户期望的另一个接口，适配器可以让不兼容的两个类一起协同工作。该定义明确说明了适配器模式的关键点就在于转换，而转换时要在已有的接口基础上做好兼容。适配器模式的 UML 图，如下所示：适配器模式中包含三个关键角色： 目标类， 适配器类即将要进行适配的抽象类或接口； 适配器类， 可以是类或接口，是作为具体适配者类的中间类来使用； 具体适配者类， 可以是内部的类或服务，也可以是外部对象或服务。UML 图对应的代码实现，如下：public abstract class TargetAbstraction { public abstract String filter(String str);}public class TargetAbstractionImpl extends TargetAbstraction{ @Override public String filter(String str) { return str.replaceAll(&quot;a&quot;, &quot;A&quot;); }}public class Adapter extends TargetAbstraction{ private OtherClass otherClass; public Adapter() { otherClass = new OtherClass(); } @Override public String filter(String str) { otherClass.preCheck(str); return otherClass.replace(str); } }public class OtherClass { public OtherClass() {} public String replace(String str) { return str.replaceAll(&quot;&amp;lt;&quot;, &quot;[&quot;); } public void preCheck(String str) {} }代码实现中的 Adapter 类充当了一个中间者的角色，Adapter 类继承目标类 TargetAbstraction 并实现接 filter，同时在 fliter 中加入新的扩展功能，扩展功能使用具体适配者类 OtherClass 来实现，这样在保留原有 filter 功能的同时，也增加了新的功能。适配器模式封装了三个重要事实： 具体适配者类可以有不同的接口； 用户在使用适配器类时实际上使用了多个接口； 适配器类和具体适配者类引入了变化。如下简图所示，适配器模式的类实际上是作为中间者来封装变化的。所以说，适配器模式的核心原理就是在原有的接口或类的外层封装一个新的适配器层，以实现扩展对象结构的效果，并且这种扩展可以无限扩展下去。使用场景适配器模式的使用场景主要有这两大类： 第一类，原有接口功能不满足现有要求，需要在兼容老接口的同时做适当的扩展，具体如下： 原有接口无法修改时； 原有接口功能太老旧时； 过渡升级旧接口时； 第二类，有相似性的多个不同接口之间做功能的统一，具体如下： 统一多个类的接口设计时； 需要依赖外部系统时； 适配不同数据格式时； 不同接口协议转换时。 适配器模式的使用场景侧重于将不适用的功能转换到期望可用的功能。使用适配器模式的理由第一，原有接口无法修改但又必须快速兼容部分新功能。 有时某些接口会因为一些因素而无法修改，比如，已交接的系统、跨团队、外部公用接口等，但这种情况下又需要适当扩展现有接口的功能，该怎么办呢？能想到的第一个办法就是使用适配器模式进行扩展。适配器模式也被称为“最好用打补丁模式”，就是因为只要是一个接口，都可以用它来进行适配。不过，要注意的是适配的新接口和目标接口差异不大时，扩展才更有效，不要被“适配器就是万能接口”的思维所误导，这就像你非要适配 10 年前的软盘接口一样不现实，也没有必要。第二，需要使用外部组件组合成新组件来提供功能，而又不想重复开发部分功能。 比如，构建自然语言识别功能时，不想从零开始训练庞大的中文语义模型来实现 NLP 接口，这时就可以选择使用外部第三方公共平台提供的 NLP 接口，然后组合实现自己的 NLP 接口，形成新的组件。虽然这样效率很高，但是依赖外部系统的风险同样突出（如果外部功能变更或下线，则组件可能不可用），只是作为短期的过渡方案，适配器模式可以说是绝佳选择。第三，不同数据格式、不同协议需要转换。 比如，API 网关中经常需要对 iOS、安卓、H5 等不同的客户端进行数据和通信协议上的适配转换，这时网关就是一个是适配器，适配客户端的同时适配服务端。适配器模式的优点适配器模式主要有以下五个大的优点： 将目标类和具体适配者类解耦。 通过引入一个适配器类来兼容现有的目标类，重用原有类功能的同时扩展新功能，而无须修改原有目标类代码，这样很好地避免了具体适配者类和目标类的直接耦合。 增加了类的透明性。 具体的适配者类中新增功能只影响适配者类，而对于使用目标类的客户端类来说是透明的（使用目标类接口），客户端的调用逻辑不会受到影响； 满足里氏替换原则。 具体适配者类通过适配器类与目标类进行交互，那么适配器类只要不影响目标类的接口功能，具体适配者类无论使用什么样的新功能，都能很方便快速地进行替换。 符合开闭原则。 由于具体适配者类要么是适配器类的子类，要么和适配器类是组合关系，所以对目标类没有修改，满足开闭原则。 统一多个类或接口。 一个适配器类可以把多个不同的具体适配者类和子类，都适配到同一个目标类上，如果这个目标类是一个新类，那么就是间接实现了统一多个类或接口的功能。适配器模式的缺点： 一次只能适配一个抽象类或接口。 像 Java、C# 等编程语言是不支持多重继承的，那么在进行适配时，一次最多只能适配一个适配者类。另外，目标类只能为抽象类或接口，不能为具体实例类，这样会在适配时增加很多类文件和代码量，如果适配的类或接口比较多，那么就会增加代码的理解难度。 过度嵌套会导致接口臃肿。 适配器有一个最大的弊端就是，一旦不停地在同一个目标类上增加适配器，就会很容易让接口变得越来越臃肿。你见过一个接口被适配 20 次的情景吗？我前不久在工作中就见过，其实这也是开闭原则极端副作用的某种体现。因为不想去修改原有接口，所以就不断使用新接口适配，而维护接口的人又在不断变化，于是就继续按照这个不修改的思路维护下去，表面上的确符合开闭原则，但实际上只不过是将风险不断隐藏罢了。一旦原始接口（目标类）功能下线后，这个适配链条造成的影响会非常大。 目标接口依赖太多适配接口，修改目标接口会导致所有适配接口都需要定制修改。 本来适配器模式是为了解耦，但是如果适配太多接口，就会演变为另一种定制化的开发。比如，上游平台商家提供的接口变更，导致下游使用方频繁变更接口。再比如，消息组件接口的变更导致所有引用消息组件的适配器全部都需要修改。" }, { "title": "单例设计模式：有效进行程序初始化", "url": "/posts/singleton/", "categories": "Design Pattern, creational", "tags": "设计模式, Design Pattern, Serven implementation methods of singleton, 单例设计模式, 对象创建型模式", "date": "2018-02-05 08:45:00 +0000", "snippet": "单例模式分析在 GoF 中，单例模式最早的定义如下： 单例模式（Singleton）允许存在一个和仅存在一个给定类的实例。它提供一种机制让任何实体都可以访问该实例。对应的 UML 图，如下上图中，单例模式（Singleton）类声明了一个名为 instance 的静态对象和名为 get­Instance() 的静态方法，静态对象用来存储对象自身的属性和方法，静态方法用来返回其所属类的一个相同实例。以单例模式经典的懒汉式初始化方式为例，其代码实现如下：package cn.happymaya.ndp.creational.singleton;/* final 不允许被继承 */public class LazyManSingleton { /* 在定义实例对象的时候直接初始化 */ private static LazyManSingleton instance = null; /* 私有构造方法，不允许外部 new */ private LazyManSingleton(){} private static LazyManSingleton getInstance() { if (null == instance) { instance = new LazyManSingleton(); } return instance; }}由此，可以得出单例模式包含三个要点： 一个单例类只能有一个实例； 单例类必须自行创建这个实例； 单例类必须保证全局其他对象都能唯一访问到它。显然，这三个要点是单例模式需要应对的变化，就是： 对象实例数量受到限制的事实； 对象实例的构造与销毁； 需要保证对象实例成为“线程安全”的某种机制。从上面那段示例代码还可以看出，单例模式的对象职责有两个： 保证一个类只有一个实例； 为该实例提供一个全局访问节点。单例类的默认构造函数和静态对象都是内部调用，之所以将默认构造函数设为私有，是为了防止其他对象使用单例类的 new 运算符。然后，提供一个对外的公共方法来获取唯一的对象实例。在我看来，单例模式就类似于全局变量或全局函数的角色，可以使用它来代替全局变量。常见场景和解决方案单例模式更多是在程序一开始进行初始化时使用的。常见单例模式应用和使用的解决方案有： 饿汉式初始化 懒汉式初始化 同步信号 双重锁定 使用 ThreadLocal看下 ThreadLocal 的方式，比如，下面这个 AppContext 代码示例：package cn.happymaya.ndp.creational.singleton;import java.util.HashMap;import java.util.Map;public class AppContext { private static final ThreadLocal&amp;lt;AppContext&amp;gt; local = new ThreadLocal&amp;lt;&amp;gt;(); private Map&amp;lt;String, Object&amp;gt; data = new HashMap&amp;lt;&amp;gt;(); public Map&amp;lt;String, Object&amp;gt; getData() { return getAppContext().data; } // 批量存储数据 public void setData(Map&amp;lt;String, Object&amp;gt; data) { getAppContext().data.putAll(data); } // 存数据 public void set(String key, String value) { getAppContext().data.put(key, value); } // 取数据 private void get(String key) { getAppContext().data.get(key); } // 初始化的实现方法 private static AppContext init() { AppContext context = new AppContext(); local.set(context); return context; } // 做延迟初始化 public static AppContext getAppContext() { AppContext context = local.get(); if (null == context) { context = init(); } return context; } // 删除实例 public static void remove() { local.remove(); }}上面的代码实现，实际上是懒汉式初始化的扩展，只不过用 ThreadLocal 替换静态对象来存储唯一对象实例。之所会选择 ThreadLocal，就是因为 ThreadLocal 相比传统的线程同步机制更有优势。在传统的同步机制中，通常会通过对象的锁机制来保证同一时间只有一个线程访问单例类。这时该类是多个线程共享的，我们都知道使用同步机制时，什么时候对类进行读写、什么时候锁定和释放对象是有很烦琐要求的，这对于一般的程序员来说，设计和编写难度相对较大。而 ThreadLocal 则会为每一个线程提供一个独立的对象副本，从而解决了多个线程对数据的访问冲突的问题。正因为每一个线程都拥有自己的对象副本，也就省去了线程之间的同步操作。所以说，现在绝大多数单例模式的实现基本上都是采用的 ThreadLocal 这一种实现方式。为什么使用单例模式 系统某些资源有限。比如: 控制某些共享资源（像数据库或文件这些）的访问权限。资源有限会带来访问冲突的问题，如果不限制实例的数量，有限的资源就会很快耗尽，同时造成大量对象处于等待资源中; 同时读写一个超大的 AI 模型文件，或使用外部进程使服务，如果不适用单例模式，随着用户进程数开启越多，系统原有进程资源就会变得越少，这不仅会导致操作系统处理速度变慢，还会造成用户进行自身处理速度缓慢; 需要表示为全局唯一的对象。比如: 系统要求提供一个唯一的序列号生成器。客户调用类的单个实例只允许使用一个公共访问点，除了该公共访问点，不能通过其他途径访问该实例。在一个系统中要求一个类只有一个实例时才应当使用单例模式; 反过来，如果一个类可以有几个实例共存，就需要对单例模式进行改进，使之成为多例模式。 优缺点优点： 对有限资源的合理利用，保护有限的资源，防止资源重复竞抢； 更高内聚的代码组件，能提升代码复用性； 具备全局唯一访问点的权限控制，方便按照统一规则管控权限； 从负载均衡角度考虑，可以轻松地将 Singleton 扩展成两个、三个或更多个实例。由于封装了基数问题，所以在适当的时候可以自由更改实例的数量。缺点： 作为全局变量使用时，引用的对象越多，代码修改影响的范围也就越大； 作为全局变量，在全局变量中使用状态变量，会造成加/解锁的性能损耗； 即便能扩展多实例，耦合性依然很高，因为屏蔽了不同对象之间的调用关系； 不支持有参数的构造函数。 Spring 单例 Bean 与单例模式的区别： 单例模式，是指在一个 JVM 进程中有且仅有一个实例；\\ 单例 Bean，是指在一个 Spring 容器中有且仅有一个实例。 " }, { "title": "单例设计模式的七种实现方式", "url": "/posts/singleton-seven-implementation-methods/", "categories": "Design Pattern, creational", "tags": "设计模式, Design Pattern, Serven implementation methods of singleton, 单例设计模式, 对象创建型模式", "date": "2018-02-05 02:33:00 +0000", "snippet": "单例设计模式时 GoF23 种设计模式中最常用的设计模式之一，无论是三方库，还是日常开发中，几乎都可以看到单例的影子。单例设计模式提供了一种在多线程情况下保证实例唯一性的解决方案。虽然单例设计模式的实现非常简单，但实现方式却多种多样，从三个维度对其进行评估：线程安全、高性能、懒加载。饿汉式/* final 不允许被继承 */public final class HungryManSingleton { /* 实例变量 */ private byte[] data = new byte[1024]; /* 在定义实例对象的时候直接初始化 */ private static HungryManSingleton instance = new HungryManSingleton(); /* 私有构造方法，不允许外部 new */ private HungryManSingleton(){} private static HungryManSingleton getInstance() { return instance; }}关键点：instance 作为类变量，直接得到了初始化。如果主动使用 Singleton 类，那么 instance 实例将会直接完成创建，包括其中的实例变量都会得到初始化，例如 1K 空间的 data 将会同时被创建。instance 作为类变量在类初始化的过程中，会被收集进 &amp;lt;clinit&amp;gt;() 方法中，该方法能够百分之百保证同步，也就是说在多线程的情况下可能被实例化两次，但是 instance 被 ClassLoader 加载后可能很长一段时间才被使用，意味着 instance 实例所开辟的堆内存会驻留更久时间。倘若类中的成员属性比较少，并且占用内存资源不多，饿汉式也未尝不可，相反，倘若类中的成员都是比较重的资源，那么这种方式就不太妥。总之，饿汉式的单例模式可以保证多线程下的唯一实例，getInstance 方法性能也比较高，但是无法进行懒加载。懒汉式/* final 不允许被继承 */public class LazyManSingleton { /* 实例变量 */ private byte[] data = new byte[1024]; /* 在定义实例对象的时候直接初始化 */ private static LazyManSingleton instance = null; /* 私有构造方法，不允许外部 new */ private LazyManSingleton(){} private static LazyManSingleton getInstance() { if (null == instance) { instance = new LazyManSingleton(); } return instance; }}关键点：在使用类实例的时候，再去创建（用时创建），这样可以避免类在初始化时提前创建。Singleton 的类变量 instance = null，因此当 Singleton.class 被初始化的时候 instance 并不会被实例化；在 getInstance 方法，会先判断 instance 实例是否被实例化，看起来没有什么问题，但是将 getInstance 方法放在多线程环境下进行分析，则会导致 instance 被实例化一次以上，不能保证单例的唯一性，如下图。总之，懒汉式的单例模式可以保证实例的懒加载，但无法保证实例的唯一性。懒汉式 + 数据同步方法在多线程的情况下，instance 成为共享资源（数据），当多个线程对其访问使用时，需要保证数据的同步性，只要对懒汉式的代码上稍加修改，增加同步的月约束就可以，修改后的代码如下：/* final 不允许被继承 */public class LazyManSyncSingleton { /* 实例变量 */ private byte[] data = new byte[1024]; /* 在定义实例对象的时候直接初始化 */ private static LazyManSyncSingleton instance = null; /* 私有构造方法，不允许外部 new */ private LazyManSyncSingleton(){} /* 向 getInstance 方法加入同步控制，每次只能有一个线程能够进入 */ private static synchronized LazyManSyncSingleton getInstance() { if (null == instance) { instance = new LazyManSyncSingleton(); } return instance; }}懒汉式 + 数据同步的方式，既满足懒加载，又能够百分之百地保证 instance 实例的唯一性，又由于 synchronized 关键字天生的排他性，导致 getInstance() 方法只能在同一时刻被一个线程所访问，性能低下。Double-Check/* final 不允许被继承 */public class DoubleCheckSingleton { /* 实例变量 */ private byte[] data = new byte[1024]; /* 在定义实例对象的时候直接初始化 */ private static DoubleCheckSingleton instance = null; Connection connection; Socket socket; /* 私有构造方法，不允许外部 new */ private DoubleCheckSingleton() { this.connection = null; // 初始化 connection this.socket = null; // 初始化 socket } private static DoubleCheckSingleton getInstance() { // 当 instance 为 null 时，进入同步代码块 // 同时该判断避免了每次都需要进入同步代码块，可以提高些效率 if (null == instance) { // 只有一个线程能够获取 DoubleCheckSingleton.class 关联的 monitor synchronized (DoubleCheckSingleton.class) { // 判断如果 instance 为 null ，则创建 if (null == instance) { instance = new DoubleCheckSingleton(); } } } return instance; }}Double-Check ，提供了一种高效的数据同步策略，就是首次初始化时加锁，之后允许多个线程同时进行 getInstance 方法的调用来获得类的实例。当两个线程发现 null=instance 成立时，只有一个线程有资格进人同步代码块，完成对 instance 的实例化；随后的线程发现 null=instance 不成立则无须进行任何动作，以后对 getInstance 的访问就不需要数据同步的保护了。这种方式看起来是那么的完美和巧妙，既满足了懒加载，又保证了 instance 实例的唯一性，Double-Check 的方式提供了高效的数据同步策略，可以允许多个线程同时对 getInstance 进行访问，但是这种方式在多线程的情况下有可能会引起空指针异常，原因是：在 Singleton 的构造函数中，需要分别实例化 conn 和 socket 两个资源，还有 Singleton 自身，根据 JVM 运行时指令重排序和 Happens–Before 规则，这三者之间的实例化顺序并无前后关系的约束，那么极有可能是 instance 最先被实例化，而 conn 和 socket 并未完成实例化，未完成初始化的实例调用其方法将会抛出空指针异常，如下：Volatile + Double-CheckDouble-Check 还有可能引起类成员变量的实例化 conn 和 socket 发生在 instance 实例化之后，这一切都是由 JVM 在运行时指令重排序所导致的。volatile 关键字则可以防止这种重排序的发生，因此稍作修改就可以满足多线程下的单例：/* final 不允许被继承 */public class VolatileDoubleCheckSingleton { /* 实例变量 */ private byte[] data = new byte[1024]; /* 在定义实例对象的时候直接初始化 */ private volatile static VolatileDoubleCheckSingleton instance = null; Connection connection; Socket socket; /* 私有构造方法，不允许外部 new */ private VolatileDoubleCheckSingleton() { this.connection = null; // 初始化 connection this.socket = null; // 初始化 socket } private static VolatileDoubleCheckSingleton getInstance() { // 当 instance 为 null 时，进入同步代码块 // 同时该判断避免了每次都需要进入同步代码块，可以提高些效率 if (null == instance) { // 只有一个线程能够获取 DoubleCheckSingleton.class 关联的 monitor synchronized (DoubleCheckSingleton.class) { // 判断如果 instance 为 null ，则创建 if (null == instance) { instance = new VolatileDoubleCheckSingleton(); } } } return instance; }}Holder 方式关键点： 借助了类加载的特点。/* final 不允许被继承 */public class HolderSingleton { /* 实例变量 */ private byte[] data = new byte[1024]; /* 私有构造方法，不允许外部 new */ private HolderSingleton(){} /* 在静态内部类中持有 Singleton 的实例，并且可能直接初始化 */ private static class Holder { private static HolderSingleton instance = new HolderSingleton(); } /* 调用 getInstance 方法，事实上是获得 Holder 的 instance 静态属性 */ public static HolderSingleton getInstance() { return Holder.instance; }}在 HolderSingleton 类中，没有 instance 的静态成员，而是将其放到了静态内部类 Holder 之中，因此在HolderSingleton 类的初始化过程中并不会创建 Singleton 的实例。Holder 类中定义了 HolderSingleton 的静态变量，并且直接进行了实例化，当 Holder 被主动引用的时候则会创建 HolderSingleton 的实例，Singleton 实例的创建过程在 Java 程序编译时期收集至&amp;lt;clinit&amp;gt;O方法中，该方法又是同步方法，因此可以保证内存的可见性、JVM 指令的顺序性和原子性。Holder方式的单例设计是最好的设计之一，也是目前使用比较广的设计之一。枚举方式使用枚举的方式实现单例模式是《Effective Java》中力推的方式，在很多优秀的开源代码中经常可以看到使用枚举方式实现单例模式的（身影)。枚举类型不允许被继承，同样是线程安全的且只能被实例化一次，但是枚举类型不能够懒加载，对 Singleton 主动使用，比如调用其中的静态方法则 INSTANCE 会立即得到实例化./* 枚举类型本身是 final 的，不允许被继承 */public enum EnumSingleton { INSTANCE; // 实例变量 private byte[] data = new byte[1024]; EnumSingleton(){ System.out.println(&quot;INSTANCE will be initialized immediately...&quot;); } public static void method() { // 调用该方法，则会主动使用 Singleton，INSTANCE 将会被实例化 } public static EnumSingleton getInstance() { return INSTANCE; }}在对它进行改造，增加懒加载的特性，类似于 Holder  的方式，改进后的代码如下：/* 枚举类型本身是 final 的，不允许被继承 */public class EnumLazySingleton { /* 实例变量 */ private byte[] data = new byte[1024]; private EnumLazySingleton(){} /* 使用枚举充当 holder */ private enum EnumHolder{ INSTANCE; private EnumLazySingleton instance; EnumHolder() { this.instance = new EnumLazySingleton(); } private EnumLazySingleton getEnumLazySingleton() { return instance; } } public static EnumLazySingleton getInstance() { return EnumHolder.INSTANCE.getEnumLazySingleton(); }}总结虽然单例设计模式简单，但在多线程的情况下，设计单例程序未必就能满足单实例、懒加载以及高性能。优先 Holder 和枚举方式来设计单例。" }, { "title": "原型模式：对象拷贝", "url": "/posts/prototype/", "categories": "Design Pattern, creational", "tags": "设计模式, Design Pattern, Prototype, 原型模式, 对象创建型模式", "date": "2018-02-04 14:24:55 +0000", "snippet": "原型模式最早出现于 1963 年的一个叫 Sketchpad 的系统中，说起 Sketchpad 或许并不熟悉，但是说起 CAD（计算机辅助设计），现在在工程设计领域几乎无人不知，其实 Sketchpad 就被认为是现代 CAD 程序的鼻祖，主要思想是拥有一张可以实例化成许多副本的原图，如果用户更改了主工程图，则所有实例也会更改。这便是原型模式最初的思维模型。不过在面向对象编程中，对象的原型在变化时通常不会影响新实例对象。实际上，原型模式不仅在 Java、C++ 等主要基于类的编程语言中有广泛应用，而且还在一开始就是基于原型的 JavaScript 等编程语言中得到了发扬光大。原型模式的原理与使用其实很简单，但是要想用好它，除了要了解它的优点以外，更要重点注意它带来的问题以及掌握如何规避这些问题。模式原理定义是：使用原型实例指定创建对象的种类，然后通过拷贝这些原型来创建新的对象。定义中清晰地指出了两个关键点: 建立原型; 基于原型做拷贝。原理很简单，在现代编程中，经常会用到的 Ctrl+C 加 Ctrl+V 编程，可以说就是最直接的原型模式的实践之一。原型模式的 UML 图：从这个 UML 图中，可以看到原型模式中的关键角色有三个： 使用者； 原型； 新实例。使用者需要建立一个原型，才能基于原型拷贝出新实例。除此之外，使用者还需要决策什么时候使用原型、什么时候使用新实例，以及从原型到新实例之间的拷贝应该采用什么样的算法策略，这些都是使用者来进行控制和决定的。只不过通常会使用一个通用的抽象拷贝接口来对外提供拷贝。UML 图对应的代码该实现呢可参考下面这段基于 Java Cloneable 接口的代码实现：public interface PrototypeInterface extends Cloneable { PrototypeInterface clone() throws CloneNotSupportedException;}public class ProtypeA implements PrototypeInterface{ @Override public ProtypeA clone() throws CloneNotSupportedException { System.out.println(&quot;Cloning new objecct: A&quot;); return (ProtypeA)super.clone(); }}public class ProtypeB implements PrototypeInterface{ @Override public ProtypeB clone() throws CloneNotSupportedException { System.out.println(&quot;Cloning new objecct: B&quot;); return (ProtypeB)super.clone(); }}public class Demo { public static void main(String[] args) throws CloneNotSupportedException { ProtypeA source = new ProtypeA(); System.out.println(source); ProtypeA newInstanceA = source.clone(); System.out.println(newInstanceA); }}代码中： 定义的 PrototypeInterface 接口通过继承 Cloneable 接口并重写 clone() 方法来实现对象的拷贝； ProtypeA 和 ProtypeB 都可以在建立自己的原型对象后，调用 clone() 方法来创建新对象； 需要注意的是，Cloneable 接口本身是空方法，调用的 clone() 方法其实是 Object.clone() 方法。从以上内容会发现，原型模式封装了如下变化： 原始对象的构造方式； 对象的属性与其他对象间的依赖关系； 对象运行时状态的获取方式； 对象拷贝的具体实现策略。所以说，原型模式从建立原型到拷贝原型生成新实例，都是对用户透明的，一旦中间任何一个小细节出现问题，你可能获取的就是一个错误的对象。使用场景原型模式常见的使用场景有以下六种： 资源优化场景。当进行对象初始化需要使用很多外部资源时，比如，IO 资源、数据文件、CPU、网络和内存等； 复杂的依赖场景。 比如，F 对象的创建依赖 A，A 又依赖 B，B 又依赖 C……于是创建过程是一连串对象的 get 和 set； 性能和安全要求的场景。 比如，同一个用户在一个会话周期里，可能会反复登录平台或使用某些受限的功能，每一次访问请求都会访问授权服务器进行授权，但如果每次都通过 new 产生一个对象会非常烦琐，这时则可以使用原型模式； 同一个对象可能被多个修改者使用的场景。 比如，一个商品对象需要提供给物流、会员、订单等多个服务访问，而且各个调用者可能都需要修改其值时，就可以考虑使用原型模式； 需要保存原始对象状态的场景。 比如，记录历史操作的场景中，就可以通过原型模式快速保存记录； 结合工厂模式来使用。 在实际项目中，原型模式除了单独基于对象使用外，还可以结合工厂方法模式一起使用，通过定义统一的复制接口，比如 clone、copy。使用一个工厂来统一进行拷贝和新对象创建， 然后由工厂方法提供给调用者使用。在实际的一些类库和组件中都有原型模式的应用，比如： Spring 中使用@Scope(&quot;prototype&quot;)注解来使得注入的 Java Bean 使用原型模式； Fastjson 中的 JSON.parseObject() 也是一种原型模式的实践; JDK 中，使用 cloneable 接口的都能实现原型模式。接下来还是通过一个具体的实例理解原型模式的使用。假设正在构建一个家庭的知识管理系统，系统会很频繁地使用电子书和电影类的实例对象，但不想每次创建对象时都等待很长的时间（像一部电影的大小通常都在 1GB 以上），于是决定使用原型模式来快速拷贝创建对象。首先，创建一个继承接口 Cloneable 的接口 IPrototype，如下所示：public interface IPrototype extends Cloneable{ // 继承 Cloneable 接口，重写 clone() 方法，以便能使用父类的 Object.clone() 负值方法 // 也可以直接实现 Cloneable 接口，效果是一样的； // 为了统一业务接口层级，子类都要实现 IPrototype IPrototype clone() throws CloneNotSupportedException;}然后，再分别让电影类 Movie 和电子书类 EBook 实现 IPrototype 接口的拷贝方法。public class Movie implements IPrototype{ /** * 打印并拷贝对象 * @return movice object * @throws CloneNotSupportedException */ @Override public Movie clone() throws CloneNotSupportedException { System.out.println(&quot;Cloning Movie object&quot;); return (Movie)super.clone(); } /** * 方便结果展示 * @return String */ @Override public String toString() { return &quot;Movice: {}&quot;; }}public class EBook implements IPrototype{ /** * 打印并拷贝对象 * @return EBook object * @throws CloneNotSupportedException */ @Override public EBook clone() throws CloneNotSupportedException { System.out.println(&quot;Cloning Book object...&quot;); return (EBook) super.clone(); } /** * 方便结果展示 * @return String */ @Override public String toString() { return &quot;EBook: {}&quot;; }}接下来，使用工厂模式来根据不同的对象类型进行对象的拷贝创建。public enum ModeType { MOVICE(&quot;movice&quot;), EBOOK(&quot;eBook&quot;); private String name; ModeType(String name) { this.name = name; } public String getName() { return name; }}public class PrototypeFactory { /** * 充当注册表的作用，用于存放原始对象，作为对象拷贝的基础 */ private static Map&amp;lt;String, IPrototype&amp;gt; prototypes = new HashMap&amp;lt;&amp;gt;(); /** * 初始化时，就将原始对象放入注册表中 */ static { prototypes.put(ModeType.MOVICE.getName(), new Movie()); prototypes.put(ModeType.EBOOK.getName(), new EBook()); } /** * 获取对象，使用 names 来进行对象拷贝 */ public static IPrototype getInstance(final String s) throws CloneNotSupportedException { return prototypes.get(s).clone(); }}准备完毕后，写一段简单的单元测试来调用原型工厂创建对象。public class Clinet { public static void main(String[] args) { try { String moviePrototype = PrototypeFactory.getInstance(ModeType.MOVICE.getName()).toString(); System.out.println(moviePrototype); String eBookPrototype = PrototypeFactory.getInstance(ModeType.EBOOK.getName()).toString(); System.out.println(eBookPrototype); } catch (CloneNotSupportedException e) { e.printStackTrace(); } }}// 输出结果如下：// Cloning Movie object// Movice: {}// Cloning Book object...// EBook: {}在上面的代码实现中，我们通过实现 Cloneable 接口并使用 clone() 方法来进行对象的拷贝。电影类 Movie 和电子书 EBook 分别实现了各自的拷贝逻辑，当通过原型工厂 PrototypeFactory 获取指定类型的对象时，我们其实获得的对象就是原始电影类或电子书类的对象副本。综合以上分析，你会发现，原型模式的适用场景通常都是对已有的复杂对象或大型对象的创建上，在这样的场景中，创建对象通常是一件非常烦琐的事情，通过拷贝对象能快速地创建对象。其实这里还涉及一个扩展知识点：浅拷贝与深拷贝。当我们在做对象拷贝时，需要在浅拷贝和深拷贝之间做取舍。如果类仅包含原始字段和不可变字段，可以使用浅拷贝；如果类还包含有可变字段的引用（比如，对象中包含对象），那么我们就应该使用深拷贝。关于浅拷贝与深拷贝的话题这里我就不做更多的展开讲述了，你若感兴趣的话可以自行学习和研究。使用原型模式的原因使用原型的模式原因主要有以下四个： 减少每次创建对象的资源消耗。当类初始化消耗资源特别多时，原型模式特别有用。比如，在 AI 系统中，我们经常需要频繁使用大量不同分类的数据模型文件，在对这一类文件建立对象模型时，不仅会长时间占用 IO 读写资源，还会消耗大量 CPU 运算资源，如果频繁创建模型对象，就会很容易造成服务器 CPU 被打满而导致系统宕机。通过原型模式我们可以很容易地解决这个问题，当我们完成对象的第一次初始化后，新创建的对象便使用对象拷贝（在内存中进行二进制流的拷贝），虽然拷贝也会消耗一定资源，但是相比初始化的外部读写和运算来说，内存拷贝消耗会小很多，而且速度快很多； 降低对象创建的时间消耗。 比如，需要查询数据库来创建对象时，如果数据库正好繁忙或锁表中，那么这个创建过程就可能出现长时间等待的情况。在很多高并发场景中，稍微长时间的等待可能都是致命的，因为大量的数据和请求如洪水一般涌入服务器，很容易引起雪崩效应。这时使用原型模式就是相当于对对象创建的过程进行了一次缓存读取，而不必一直阻塞程序的执行； 快速复制对象运行时状态。原型模式相比于传统的使用 new 关键字创建对象还有一个巨大的优势，那就是当构造函数中包含大量属性或定制化业务逻辑时，不用完全了解创建过程也能快速创建对象。比如，当一个对象类有 30 个以上的属性或方法时（属性字段可能为另一个对象），如果你都通过 get 和 set 方法来创建对象，你会发现复制粘贴都是一件痛苦的事，因为你可能都忘记了哪些字段是必选、哪些又是有数据的。这也是我们在接收 HTTP 和 RPC 传输的 JSON 数据时，更愿意采用反序列化（也是一种原型模式的实践）到对象的方式，而不是 new 一个新对象再赋值的原因； 能保存原始对象的副本。 在某些需要保存历史状态的场景中，比如，聊天消息、上线发布流程、需要撤销操作的程序等，原型模式能快速地复制现有对象的状态并留存副本，方便快速地回滚到上一次保存或最初的状态，避免因网络延迟、误操作等原因而造成数据的不可恢复。优缺点主要有以下三个大的优点： 原型并不基于继承，因此没有继承的缺点。原型模式是对对象的直接复制，当新对象发生变化时，并不会对原始对象有任何影响，而继承的对象一旦发生了修改则会影响到父类； 复制大对象时，性能更优。比如，Java 使用的原型模式是基于内存二进制流的拷贝，而直接 new 一个大对象是 JVM 进行堆内分配并可能触发 Full GC，相比之下，使用 new 关键字时所做的操作实际上更多，而使用内存拷贝的方式在复制的性能上会更优； 可以快速扩展运行时对象的属性和方法。原型模式一方面简化了对象的创建过程，另一方面能够保留原始的对象状态，这样的优势是：在程序运行过程中，如果想要动态扩展对象的功能（增减方法或属性值），可以在不影响原有对象的情况下，动态扩展对象的功能。比如，结合 AOP 切面编程可以实现录制业务调用轨迹，加入应用性能监控，做动态数据埋点等操作。原型模式也不是十全十美的，它也有一些缺点： 虽然不基于继承，但原型需要一个被初始化过的正确对象。如果被复制的对象在进行复杂的初始化时失败或出现错误的初始化后，那么复制的对象也可能是错误的； 复制大对象时，可能出现内存溢出的 OOM 错误。虽然复制对象有诸多优点，但是不要忘记内存的大小是有限制的，如果你想要复制的对象已经占用了 80% 的内存空间，那么复制时大概率会导致内存溢出，而这时的解决办法要么是增加内存，要么是拆分对象； 动态扩展对象功能时可能会掩盖新的风险。虽然原型模式能够在运行时帮助快速扩展功能，但同时也可能使新对象的负荷更重。比如，埋点服务中通常会拷贝一份对象在某个时间节点的数据，并添加一些追踪数据后再推送给埋点服务，这样就可能增加过多的内存消耗，影响原有功能执行的性能，有时还可能引起 OOM，导致系统宕机。切记，如果没有充分验证过动态扩展功能的话，不要轻易使用动态扩展，因为加入额外的新功能，大概率是会影响原有功能的。总结原型模式可以说是创建型模式中灵活性最高的模式，不过在带来灵活性的同时，也带来了更大的风险，这对我们的设计与实现间接提出了更高的要求。使用原型模式时可能需要对 IO 流、内存和 JVM 等一些底层的原理有更加深入的理解才行，虽然对象的拷贝看上去很容易，如果一旦使用不当，很容易就导致系统直接崩溃，这也是不愿意使用原型模式的原因之一。但是在很多类库和框架中，随处可见原型模式的身影，比如，JDK、Netty、Spring 等。还有 JavaScript、TypeScript 更是会时常用到原型模式。在我看来，原型模式的实现——浅拷贝和深拷贝，本质上都是基于性能优化角度来更好地实现拷贝功能，只不过实现方式和使用场景有所不同而已。决定了要采用原型模式后，再考虑使用哪种方式会更加合适。况且很多开源框架和组件里都有相关实现，并不一定非要从零去实现浅拷贝或深拷贝，比如： Apache Common 包中的 deepCopy； Spring 中的 BenUtils.copyProperties 等，你若感兴趣的话可以去研究相关源码，这里就不赘述了。" }, { "title": "工厂方法模式：解决生成对象时的不确定性", "url": "/posts/factory-method/", "categories": "Design Pattern, creational", "tags": "设计模式, Design Pattern, Factory Method, 工厂方法模式, 对象创建型模式", "date": "2018-02-03 14:45:00 +0000", "snippet": "和抽象工厂模式很类似，但工厂方法模式因为只围绕着一类接口来进行对象的创建与使用，使用场景更简单和单一，在实际的项目中使用频率反而比抽象工厂模式更高。原理工厂方法模式的原始定义是：定义一个创建对象的接口，但让实现这个接口的类来决定实例化哪个类。工厂方法模式的目的很简单，就是封装对象创建的过程，提升创建对象方法的可复用性。工厂方法模式的 UML 图：工厂方法模式包含三个关键角色： 抽象接口（也叫抽象产品）； 核心工厂； 具体产品（也可以是具体工厂）。其中，核心工厂通常作为父类负责定义创建对象的抽象接口以及使用哪些具体产品，具体产品可以是一个具体的类，也可以是一个具体工厂类，负责生成具体的对象实例。于是，工厂方法模式便将对象的实例化操作延迟到了具体产品子类中去完成。不同于抽象工厂模式，工厂方法模式侧重于直接对具体产品的实现进行封装和调用，通过统一的接口定义来约束程序的对外行为。换句话说，用户通过使用核心工厂来获得具体实例对象，再通过对象的统一接口来使用对象功能。工厂方法模式对应 UML 图的代码实现如下：/* 抽象产品 */public interface IProduct { void apply();}/* 核心工厂类 */public class ProductFactory { public static IProduct getProduct(String name){ if (&quot;a&quot;.equals(name)) { return new ProductAImpl(); } return new ProductBImpl(); }}/* 具体产品实现 A */public class ProductAImpl implements IProduct { @Override public void apply() { System.out.println(&quot;use A product now&quot;); }}/* 具体产品实现 B */public class ProductBImpl implements IProduct{ @Override public void apply() { System.out.println(&quot;use B product now&quot;); }}/* client 使用者 */public class Client { public static void main(String[] args) { IProduct iProductb = ProductFactory.getProduct(&quot;&quot;); iProductb.apply(); IProduct iProducta = ProductFactory.getProduct(&quot;a&quot;); iProducta.apply(); }}总体来说，工厂方法模式是围绕着特定的抽象产品（一般是接口）来封装对象的创建过程，客户端只需要通过工厂类来创建对象并使用特定接口的功能。使用场景工厂方法模式有以下几个使用场景： 需要使用很多重复代码创建对象时，比如，DAO 层的数据对象、API 层的 VO 对象等; 创建对象要访问外部信息或资源时，比如，读取数据库字段，获取访问授权 token 信息，配置文件等； 创建需要统一管理生命周期的对象时，比如，会话信息、用户网页浏览轨迹对象等； 创建池化对象时，比如，连接池对象、线程池对象、日志对象等。这些对象的特性是：有限、可重用，使用工厂方法模式可以有效节约资源。 希望隐藏对象的真实类型时，比如，不希望使用者知道对象的真实构造函数参数等。一段经典的源码实现 —— MyBatis 实现的 Log 日志功能——来学习。代码如下：public final class LogFactory { public static final String MARKER = &quot;MYBATIS&quot;; private static Constructor&amp;lt;? extends Log&amp;gt; logConstructor; private LogFactory() {} public static Log getLog(Class&amp;lt;?&amp;gt; clazz) { return getLog(clazz.getName()); } public static Log getLog(String logger) { try { return (Log)logConstructor.newInstance(logger); } catch (Throwable var2) { throw new LogException(&quot;Error creating logger for logger &quot; + logger + &quot;. Cause: &quot; + var2, var2); } } ...省略具体工厂实现类... private static void tryImplementation(Runnable runnable) { if (logConstructor == null) { try { runnable.run(); } catch (Throwable var2) { } } } private static void setImplementation(Class&amp;lt;? extends Log&amp;gt; implClass) { try { Constructor&amp;lt;? extends Log&amp;gt; candidate = implClass.getConstructor(String.class); Log log = (Log)candidate.newInstance(LogFactory.class.getName()); if (log.isDebugEnabled()) { log.debug(&quot;Logging initialized using &#39;&quot; + implClass + &quot;&#39; adapter.&quot;); } logConstructor = candidate; } catch (Throwable var3) { throw new LogException(&quot;Error setting Log implementation. Cause: &quot; + var3, var3); } } static { tryImplementation(LogFactory::useSlf4jLogging); tryImplementation(LogFactory::useCommonsLogging); tryImplementation(LogFactory::useLog4J2Logging); tryImplementation(LogFactory::useLog4JLogging); tryImplementation(LogFactory::useJdkLogging); tryImplementation(LogFactory::useNoLogging); }}这段代码的实现很简单，却充分体现了工厂方法模式使用场景的本质：尽可能地封装对象创建过程中所遇见的所有可能变化。这里 LogFactory 的职责就是核心工厂的创建职责，所需要创建的具体产品就是实现 Log 这个接口的特定实现，比如，Slf4j、Log4J 等。Log 接口代码如下所示：public interface Log { boolean isDebugEnabled(); boolean isTraceEnabled(); void error(String var1, Throwable var2); void error(String var1); void debug(String var1); void trace(String var1); void warn(String var1);}这些具体类的代码实现，各自的代码风格可能完全不同，但最终实现的日志功能却是一样的。其中，Slf4jImpl 甚至还为不同的 Slf4j 版本 API 接口做了兼容性处理，如果想要扩展一个新的日志实现，那么新增一个实现类并在核心工厂类里加入调用代码即可。这里的关键其实是 Log 这个接口，这个接口设计得非常好，也就是抽象产品设计得特别好，不仅满足接口隔离原则，而且还是找到了正确抽象的典型代表。每一个操作几乎都是日志相关的原子操作，即便具体类的实现不同，但只要使用 LogFactory 就能获得满足要求的日志功能。使用工厂模式的理由使用工厂方法模式的原因，主要有以下三个： 为了把对象的创建和使用过程分开，降低代码耦合性。 这是使用工厂方法模式最直接的理由之一。在实际的软件开发中，你可能更喜欢使用 new 来创建对象，同时紧接着便开始使用新创建的对象，这看上去并没有什么问题，但是随着创建对象数量的增多，你会发现，当你想要重构、修改已有的对象属性和方法时，你几乎不敢轻易修改，因为你早已记不清哪些对象在哪里被创建和使用，以及跟哪些对象发生了关联和交互。而使用工厂方法模式，就能很好地避免这个问题，创建的过程始终在工厂内部管理，只要对外使用的方法不发生变化，那么就不会对创建对象造成影响； 减少重复代码。 对于要写代码的程序员或架构师来说，面对成千上万相同的数据对象进行增删改查时，如果每次都使用 new 来创建对象的话，那么 80% 的时间都会浪费在同样属性的 get 与 set 上。这时要是使用的对象之间还有相互引用的话（A 引用 B，B 又引用 C……），重复的代码就会剧增。而对于多个相同对象的构建过程，除了使用建造者模式以外，还可以使用工厂方法模式来避免出现过多的重复代码，将相同的创建规则统一放在一起 统一管理创建对象的不同实现逻辑。 比如，当一个业务对象发生业务逻辑变化时，使用工厂方法模式后，你不需要找到所有创建对象的地方去修改，而只需要在工厂里修改即可。即便这时你想要扩展对象为新的子类，也不需要把所有调用父类的地方都改成子类，只需要在工厂中修改其生产的对象为新的子类。同时，还隐藏了具体的创建过程，减少了使用者误用逻辑而导致未知错误出现的概率。优缺点优点： 能根据用户的需求定制化地创建对象。 工厂方法模式是基于某一个抽象产品角色来进行具体的实现工厂的设计。这样的好处就在于具体工厂可以根据自己的需求来决定创建什么样的具体产品，同时，还能把不同的算法细节完全封装在具体的工厂内部 隐藏了具体使用哪种产品来创建对象。 由于工厂方法模式对外使用统一的抽象接口，这样就向用户隐藏了具体正在使用的产品实例，让用户只需要关心抽象接口即可，无须关心创建细节，甚至都不用知道具体产品类的真实类名； 实现同一抽象父类的多态性，满足“里氏替换原则（LSP）”。 在使用工厂方法模式时，因为是围绕着统一的抽象接口来实现具体的功能，那么就能很便捷地使用不同的算法策略来实现同一功能，所以这样更好地实现了不同具体产品之间的可替换性； 满足“开闭原则”。 当你想要在系统中加入新的具体对象时，不用再修改抽象接口和核心工厂，也不用修改客户端，更不用修改其他具体工厂和具体产品，而只需要新增一个具体工厂和具体产品就可以了。这样系统的可扩展性也就变得非常好，完全符合“开闭原则”。缺点： 抽象接口新增方法时，会增加开发成本。 当统一的抽象接口中新增方法时，相应的每个具体工厂都需要新增实现。不管具体工厂是否需要这个方法，都必须要新写代码，这样在一定程度上增加了开发工作量，因为修改后就需要编译、运行和测试，自然增加了开发成本。 具体工厂实现逻辑不统一，增加代码理解难度。 虽然核心工厂已经保证了部分共有逻辑的实现，但是具体产品依然是由具体工厂封装实现的，一旦具体工厂采用非通用的实现策略，那么对于维护的人员来说，就需要耗费大量的精力和时间去学习和理解。如果对象的属性数量并不多，并且创建过程也不复杂的话，那么用不着使用工厂方法模式来创建对象，毕竟工厂方法模式强调使用继承来实现对象的创建，会引入继承相关的副作用。" }, { "title": "建造者模式：创建不同形式复杂对象", "url": "/posts/builder/", "categories": "Design Pattern, behavioral", "tags": "设计模式, Design Pattern, Builder, 建造者模式, 对象创建型模式", "date": "2018-02-02 13:11:22 +0000", "snippet": "创建型设计模式——Builder 模式，中文一般叫建造者模式或生成器模式。建造者模式的代码实现非常简单，原理掌握起来也不难，而难点就在于什么时候采用它。比如，经常会遇到的以下两个问题： 为什么直接使用构造函数或者使用 set 方法来创建对象不方便？ 为什么一定需要建造者模式来创建？建造者模式在 GoF 的书中，建造者模式的定义是这样的： 将复杂对象的构造与其表示分离，以便同一构造过程可以创建不同的表示。建造者模式的通用 UML 图：从上图中，可以看到，建造者模式主要包含四个角色: Product：代表最终构建的对象，比如，汽车类; Builder：代表建造者的抽象基类（可以使用接口来代替）。它定义了构建 Product 的步骤，它的子类（或接口实现类）需要实现这些步骤。同时，它还需要包含一个用来返回最终对象的方法 getProduct(); ConcreteBuilder：代表 Builder 类的具体实现类; Director：代表需要建造最终对象的某种算法。这里通过使用构造函数 Construct(Builder builder) 来调用 Builder 的创建方法创建对象，等创建完成后，再通过 getProduct() 方法来获取最终的完整对象。为了更好理解建造者模式，创建一个 Product 所需要的时序图，如下图所示：可以看到，这个创建原理还是很简单的。总结来说，就是先创建一个建造者，然后给建造者指定一个构建算法，建造者按照算法中的步骤分步完成对象的构建，最后获取最终对象。UML 图对应的实现代码：package cn.happymaya.ndp.creational.builder;public class Product { private int partA; private String partB; private int partC; public Product(int partA, String partB, int partC) { this.partA = partA; this.partB = partB; this.partC = partC; } @Override public String toString() { return &quot;Product{&quot; + &quot;partA=&quot; + partA + &quot;, partB=&quot; + partB + &quot;, partC=&quot; + partC + &#39;}&#39;; }}public interface Builder { void buildPartA(int partA); void buildPartB(String partB); void buildPartC(int partC); Product getResult();}public class ConcreteBuilder implements Builder{ private int partA; private String partB; private int partC; @Override public void buildPartA(int partA) { this.partA = partA; } @Override public void buildPartB(String partB) { this.partB = partB; } @Override public void buildPartC(int partC) { this.partC = partC; } @Override public Product getResult() { return new Product(partA,partB,partC); }}public class Director { public void construct(Builder builder) { builder.buildPartA(1); builder.buildPartB(&quot;test-test&quot;); builder.buildPartC(2); }}运行一下单元测试：public class Demo { public static void main(String[] args) { Director director = new Director(); Builder builder = new ConcreteBuilder(); director.construct(builder); System.out.println(builder.getResult()); }}输出结果如下：Product{partA=1, partB=test-test, partC=2}从代码实现中，可以分析出建造者模式封装（信息隐藏）了如下变化： 每个具体建造器的构建步骤； 当前正在使用哪一个建造器； 现有建造器的数量； 一个建造器里可以创建多个属性的特性。你会发现，使用建造者模式后对象的职责是保证按照正确的步骤进行自由的组合。常用场景在以下四种情况下可以使用建造者模式。 需要生成的对象包含多个成员属性。 需要生成的对象的属性相互依赖，需要指定其生成顺序。 对象的创建过程独立于创建该对象的类。 需要隔离复杂对象的创建和使用，并使得相同的创建过程可以创建不同的产品。接下来通过“没有使用建造者模式”和“使用了建造者模式”这两个对比的例子来帮助你理解建造者模式的常用场景，如下代码所示，创建一个打工人的类，每个打工人都需要包含姓名、年龄、电话和性别。public class MigrantWorkerOld { private String name; // 姓名 private int age; // 年龄 private String phone; // 电话 private String gender; // 性别 public MigrantWorkerOld(String name, int age, String phone, String gender) { this.name = name; this.age = age; this.phone = phone; this.gender = gender; } public MigrantWorkerOld(String name, int age, String phone) { this.name = name; this.age = age; this.phone = phone; } public MigrantWorkerOld(String name, int age) { this.name = name; this.age = age; } }上面这段代码没有使用建造者模式，所以需要使用传统的 getter、setter 方法，并指定不同的入参来构造对象。而使用建造者模式后的类，功能却发生了完全不一样的变化，如下所示：package cn.happymaya.ndp.creational.builder;public class MigrantWorker { // 所有属性 private String name; // 姓名 private int age; // 年龄 private String phone; // 电话 private String gender; // 性别 public MigrantWorker(){} public static MigrantWorker builder() { return new MigrantWorker(); } // 将属性作为步骤 public MigrantWorker name(String name) { this.name = name; return this; } // 将属性作为步骤 public MigrantWorker age(int age) { this.age = age; return this; } // 将属性作为步骤 public MigrantWorker phone(String phone) { this.phone = phone; return this; } // 将属性作为步骤 public MigrantWorker gender(String gender) { this.gender = gender; return this; } public MigrantWorker build() { validateObject(this); return this; } private void validateObject(MigrantWorker migrantWorker) { // TODO: 2018/7/18 可以做基础校验，或自定义校验 } @Override public String toString() { return &quot;MigrantWorker{&quot; + &quot;name=&#39;&quot; + name + &#39;\\&#39;&#39; + &quot;, age=&quot; + age + &quot;, phone=&#39;&quot; + phone + &#39;\\&#39;&#39; + &quot;, gender=&#39;&quot; + gender + &#39;\\&#39;&#39; + &#39;}&#39;; }}再写一个简单的单元测试：package cn.happymaya.ndp.creational.builder;public class Demo { public static void main(String[] args) { MigrantWorker migrantWorker1 = MigrantWorker.builder() .name(&quot;Spike&quot;) .age(27) .phone(&quot;1810000111&quot;) .gender(&quot;男&quot;) .build(); System.out.println(migrantWorker1); MigrantWorker migrantWorker2 = MigrantWorker.builder() .name(&quot;Mia&quot;) .age(17) .phone(&quot;1810000111&quot;) // 没有性别 .build(); System.out.println(migrantWorker2); MigrantWorker migrantWorker3 = MigrantWorker.builder() .name(&quot;Mia&quot;) // 没有年龄 // 没有电话 // 没有性别 .build(); System.out.println(migrantWorker2); }}得到的输出结果如下：MigrantWorker{name=&#39;Spike&#39;, age=27, phone=&#39;1810000111&#39;, gender=&#39;男&#39;}MigrantWorker{name=&#39;Mia&#39;, age=17, phone=&#39;1810000111&#39;, gender=&#39;null&#39;}MigrantWorker{name=&#39;Mia&#39;, age=17, phone=&#39;1810000111&#39;, gender=&#39;null&#39;}从输出结果中发现，虽然在创建 MigrantWorker 对象实例时只是指定了不同的属性构建步骤，但却构建出了完全不同的对象实例，而使用传统的 getter、setter 方法，则需要写很多不同的构造函数来应对变化。所以说，使用建造者模式能更方便地帮助按需进行对象的实例化，避免写很多不同参数的构造函数，同时还能解决同一类型参数只能写一个构造函数的弊端。使用理由虽然上面案例的实现比较简单，但是也充分演示了如何使用建造者模式。在实际的使用中，通常可以直接使用 lombok 的 @Builder 注解实现类自身的建造者模式，或者使用案例中的将自身类作为建造者的方法来实现。实际上，所有 JDK 类库中的 Appendable 接口都是实现了建造者模式的优秀范例。那么问题来了，为什么要使用建造者模式来创建类？在我看来，有以下两点原因： 分阶段、分步骤的方法更适合多次运算结果类创建场景。在面向对象软件开发中，很多时候创建类所需要的参数并不是一次都能准备好的，比如，计算订单优惠价格、查询库存状态等，有的参数可能需要通过调用多个服务运算后才能得出，这时我们可以根据已知参数预先对类进行创建，等有合适的参数时再设置类的属性，而不是等到所有结果都齐备后才去创建类； 不需要关心特定类型的建造者的具体算法实现。 比如，在使用 StringBuilder 时，并不太关心它的具体代码实现，而是关心它提供给的使用功能。这在某些需要快速复用的场景中，能起到提升编码效率的作用。而换个角度来看，当你需要给别人提供一个建造者来创建类时，你就需要严格地设计你的建造者，并保证你的建造者类能够创建符合预期的类。优缺点优点有以下三点： 分离创建与使用。 在建造者模式中， 使用方不必知道你的内部实现算法（步骤）的细节，通过统一方法接口的调用，可以自由组合出不同的对象实例； 满足开闭原则。 每一个建造者都相对独立，因此能方便地进行替换或新增，这就大大提升了代码的可扩展性； 自由地组合对象的创建过程。由于建造者模式将复杂的创建步骤拆分为单个独立的创建步骤，这不仅使得代码的可读性更高，也使得在创建过程中，使用者可以根据自己的需要灵活创建。缺点也有以下三点： 使用范围有限。 建造者模式所创建的对象一般都需要有很多的共同点，如果对象实例之间的差异性很大，则不适合使用建造者模式； 容易引起超大的类。我们都知道一辆汽车内部构造其实很复杂，作为开发者的你其实更关心的是像发动机、轮胎这样具备重用性的组件。一旦过度定制化对象创建的过程步骤，那么随着创建对象新需求的出现或变化，新的创建步骤就会被加进来，这会造成代码量的急剧膨胀，最终形成一个庞大的超大类； 增加代码行数。 虽然建造者模式能够提高代码的可阅读性，但也会以增加代码行数来作为代价。总结在现实中，经常会遇见很多使用建造者模式的软件，比如： Maven、Ant 之类的自动化构建系统； Jenkins 等持续集成系统它们实际上都是使用了建造者模式的具体例子。建造者模式的主要优点：在于客户端不必知道对象内部组成的细节，将创建与使用进行了很好的解耦，使得可以使用相同的创建过程创建不同的对象，因此符合“开闭原则”，能够极大地提升代码的复用性。同时，因为每一个对象属性的创建步骤都被独立出来，所以还可以更加精细地控制对象的创建过程。不过，缺点也同样突出。当使用建造者模式创建对象时，需要对象具备更多的共同点才能抽象出更适合的步骤，因此使用范围会受到很大的限制，一旦产品内部开始变得复杂，可能就会导致需要定义很多定制化的建造者类来适应这种变化，从而导致代码变得非常庞大。应用建造者模式的关键就在于抓住拆分步骤，这是与工厂模式最大的区别所在。如果把建造者模式比作汽车整体组装工厂，那么工厂模式就是汽车配件组装工厂，前者侧重于把对象按照特定步骤组装完整，后者侧重于把组成对象的每一个属性或方法做得更通用后再组装。一定不要以为只要叫“工厂”就是指我们通常认为的统一组装模式。参考： [] 《Effective Java》 设计模式 图说" }, { "title": "抽象工厂模式：统一不同代码风格下代码级别", "url": "/posts/abstract-factory/", "categories": "Design Pattern, creational", "tags": "设计模式, Design Pattern, Abstract factory, 抽象工厂模式, 对象创建型模式", "date": "2018-02-01 12:45:00 +0000", "snippet": "在 GoF 的 《设计模式》一书中，工厂模式被分为三种： 简单工厂 工厂方法 抽象工厂其中，在书中作者将简单工厂模式看作是工厂方法模式的一种特例。在实际工作中，用的比较多的是工厂方法模式和抽象工厂模式这两类。下面先总结一下抽象工厂模式。 抽象工厂模式真正的重点和难点在于：如何找到正确的抽象。虽然抽象工厂模式很容易实现，但更重要的是意识到-“正确的抽象往往都很简单，也很底层”。比如： 数据库的增删查改操作， 日志的 debug、info、warn、error 级别， JVM 内存模型，等等。 抽象工厂模式想要告诉不只是在写代码时随便建个抽象类就够了，而是当对一类功能进行抽象分析时有没有找到足够简单而又正确的抽象模式原理分析抽象工厂模式的定义：提供了一个用于创建相关对象或对象族的接口，而不需要指定其具体类显然，上面的定义是给使用者说的。因为从使用者的角度来看，他有时可能只关心某一个抽象的大类，例如，当去租车时，对店员说：“想要租一辆小型轿车”，但具体品牌和型号并不在意。但是站在创建者的角度看，他需要关注的是如何找到这个正确的抽象大类，就好比在前面的租车场景中，你需要从普通的汽车消费者变成汽车厂厂长一样，必须关注最后具体的型号该怎么落地。因此，当在创建抽象工厂模式的时候，最终还是会涉及指定具体的实现类。由此可见，抽象工厂模式的定义只是说了抽象工厂模式应该朝着分析共性规律的方向走，而具体操作时需要仔细分析具体实现类应该怎么实现才可以。下面是抽象工厂模式原始的 UML 图：在上面的 UML 图中，能看出抽象工厂模式中包含了四个关键节点： 抽象工厂 抽象产品（通用的一类对象或接口） 具体工厂 具体产品（继承通用对象或接口后扩展特有属性）为了更好的理解这四个角色，常常使用家具厂来类比。比如说，抽象工厂生产的抽象产品是椅子、桌子、沙发一类的家具，那具体工厂可能就在生产具体的产品：椅子设计成现代简约风格或欧洲宫廷风格，使用的材质有木质或铝制，等等。本质上椅子的特性没有发生重大改变，但在外观上，不同的具体工厂生产的椅子尺寸、材质、外观各不相同。其中最为关键的角色并不是抽象工厂本身，而是抽象产品。抽象产品的好坏才是直接决定了抽象工厂和具体工厂能否发挥最大作用的关键所在。这也是多次提到的“找到正确的抽象很重要”的原因。明白了上诉道理，下面是 UML 图的代码实现，会发现思路特别清晰：使用场景分析实际工作中，抽象工厂模式在现实中有很多的应用。比如，一个应用程序中支持多个操作系统时，就会用到抽象工厂模式这样的机制，需要为使用抽象工厂模式的原因使用抽象工厂模式的原因主要有一下三点： 对于不同产品系列有比较多共性特征时，使用抽象工厂 模式，有助于提升组件的复用性。 当需要提升代码的扩展性并降低维护成本时，把对象的创建和使用过程分开，能够有效地将代码统一到一个级别上。 解决跨平台带来的兼容性问题。优缺点抽象工厂模式的优点，我总结为以下五点： 符合开闭原则。当需要添加新的工厂类，只用新继承一个类，不用修改抽象工厂和其他具体类。 保证同一工厂生成的产品符合预期。抽象工厂定义了统一的抽象产品功能，只要是继承了这个抽象工厂，本质的抽象产品功能是不会发生改变的。 使用的和创建的代码进行解耦。在具体的代码实现中，使用者只用关心如何使用具体的功能，而不再需要关心这个对象的创建过程。这样使得对象与对象之间的耦合关系变得单一，降低了过多应用带来耦合风险。 单一职责原则。由于将产品的实现代码放到同一层级里，并继承同一个抽象工厂类，所以说，即便具体的代码风格有所不同，也不影响最终提供功能的统一性，使得代码的可维护性大大提高。 容易扩展（增加新的产品系列）。因为有了抽象工厂作为参考模板，那么再新增的具体工厂时非常容易，不需要修改其他具体工厂，并且各自的工厂可以朝着自己的演化方向发展。当然，抽象工厂也有一些缺点。主要有以下三点： 增加代码量。抽象工厂模式很好的进行了职责分离，同时也增加了更多的类文件和代码，使得开发时间长。并且随着子类越来越多，当一个抽象工厂需要改动时，影响的代码范围会很大。 增加学习成本。抽象工厂模式是自顶向下的设计，在最开始就找到正确的抽象是一件非常困难的事情，这就需要大量的实践，并不断总结归纳。 变更产品的结构困难。抽象工厂模式最大的缺点在于，一旦定义了某种产品结构后，要想修改就得修改所有的具体工厂和抽象工厂。总结抽象工厂模式的使用和创建很简单。这个模式的重点和难点在于抓住抽象工厂模式的本质——找到正确的抽象，只有这样，才能更好的发挥抽象工厂模式的作用。当没有找到正确的抽象产品，不要着急使用抽象工厂模式，如果只是想要封装对象创建过程，使用工厂模式绰绰有余。参考 设计模式 图说" } ]
