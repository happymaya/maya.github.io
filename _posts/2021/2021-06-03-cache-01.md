---
title: 缓存
author:
  name: superhsc
  link: https://github.com/happymaya
date: 2020-06-03 24:33:22 +0800
categories: [Database, Cache]
tags:  [Cache]
math: true
mermaid: true
---

![cache-01](https://images.happymaya.cn/assert/\cache\image\cache-01.png)

## 定义

- 侠义缓存的定义（缓存最初的含义），用于**==加速 CPU 数据交换的 存储器 RAM（随机存取存储器==）**，通常这种存储器使用昂贵但快速的静态 RAM（SRAM）技术，用以对 DRAM进 行加速。这是一个狭义缓存的定义。
- 广义缓存的定义，更宽泛，任何可以**==用于数据高速交换的存储介质都是缓存，可以是硬件也可以是软件。==**



## 意义

通过开辟一个新的数据交换缓冲区，来解决原始数据获取代价太大的问题，让数据得到更快的访问！



## 基本思想

利用时间局限性原理，通过空间换时间来达到加速数据获取的目的。由于缓存空间的成本较高，在实际设计架构中要考虑访问延迟和成本的权衡问题。

三个关键点：

1. 时间局限性原理。即被获取过一次的数据在未来会被多次引用，比如一条微博被一个人感兴趣并阅读后，它大概率还会被更多人阅读，当然如果变成热门微博后，会被数以百万/千万计算的更多用户查看。
2. 空间换时间。因为原始数据获取太慢，所以开辟一块高速独立空间，提供高效访问，来达到数据获取加速的目的。
3. 性能成本权衡。构建系统时，希望系统的访问性能越高越好，访问延迟越低越好。但是维持相同数据规模的存储以及访问，性能越高延迟越小，成本也会越高，因此在系统架构设计时，需要在系统性能和开发运行成本做取舍。



## 优点

- 提升访问性能。缓存存储原始数据，可以大幅提升访问性能。
- 减少网络流量，降低网络拥堵。在实际业务场景中，缓存中存储的往往是需要频繁访问的中间数据甚至最终结果，这些数据相比 DB 中的原始数据小很多，这样就可以减少网络流量，降低网络拥堵。
- 减轻服务负载。由于减少了解析和计算，调用方和存储服务的负载也可以大幅降低
- 增强可扩展性。缓存读写性能很高，预热快，在数据访问存在性能瓶颈或遇到突发流量，系统读写压力大增时，可快速部署上线，同时在流量稳定后，也可以随时下线，从而使系统的可扩展性大大增强。



## 缺点（代价）

任何事情都有两面性，缓存也不例外，在享受缓存带来好处的同时，也注定需要付出一定的代价。

- 服务系统中引入缓存，会增加系统的复杂度；
- 由于缓存相比原始 DB 存储的成本更高，所以系统部署及运行的费用也会更高。
- 由于一份数据同时存在缓存和 DB 中，甚至缓存内部也会有多个数据副本，多份数据就会存在==**一致性问题**==，同时缓存体系本身也会存在可用性问题和分区的问题。这就需要加强对缓存原理、缓存组件以及优秀缓存体系实践的理解，从系统架构之初就对缓存进行良好设计，降低缓存引入的副作用，让缓存体系成为服务系统高效稳定运行的强力基石。

## 读写模式

业务系统读写缓存有 3 中模式：

- Cache Aside（旁路缓存）
- Read/Write Through（读写穿透）
- Write Behind Caching（异步缓存写入）

### Cache Aside（旁路缓存）

#### 套路

对于写请求：

- 先更新 DB 后，然后将 key 从 cache 中删除，最后由 DB 驱动缓存数据的更新；

对于读请求：

- 先读 cache，如果 cache 没有，则读 DB，同时将从 DB 中读取的数据回写到 cache。

#### 特点

- 业务端处理所有数据访问细节，同时**利用 Lazy 计算的思想，更新 DB 后，直接删除 cache 并通过 DB 更新，确保数据以 DB 结果为准**，则可以大幅降低 cache 和 DB 中数据不一致的概率。

#### 适合场景

- 对于**没有专门的存储服务**，同时是对数据一致性要求比较高的业务，或者是**缓存数据更新比较复杂的业务**，这些情况都比较适合使用 Cache Aside 模式。

  如微博发展初期，不少业务采用这种模式，这些缓存数据需要通过多个原始数据进行计算后设置。在部分数据变更后，直接删除缓存。同时，使用一个 Trigger 组件，实时读取 DB 的变更日志，然后重新计算并更新缓存。如果读缓存的时候，Trigger 还没写入 cache，则由调用方自行到 DB 加载计算并写入 cache。

### Read/Write Through（读写穿透）

对于 Cache Aside 模式，业务应用需要同时维护 cache 和 DB 两个数据存储方，过于繁琐，于是就有了 Read/Write Through 模式。

在这种模式下，业务应用只关注一个存储服务即可，业务方的读写 cache 和 DB 的操作，都由存储服务代理。

#### 套路

对于写请求：

- 首先查 cache，如果数据在 cache 中不存在，则只更新 DB，如果数据在 cache 中存在，则先更新 cache，然后更新 DB。

对于读请求：

- 如果命中 cache 直接返回，否则先从 DB 加载，回种到 cache 后返回响应。

#### 特点

存储服务封装了所有的数据处理细节，业务应用端代码只用关注业务逻辑本身，系统的隔离性更佳。另外，进行写操作时，如果 cache 中没有数据则不更新，有缓存数据才更新，内存效率更高。

#### 适合场景

数据有冷热区分。微博 Feed 的 Outbox Vector（即用户最新微博列表）就采用这种模式。一些粉丝较少且不活跃的用户发表微博后，Vector 服务会首先查询 Vector Cache，如果 cache 中没有该用户的 Outbox 记录，则不写该用户的 cache 数据，直接更新 DB 后就返回，只有 cache 中存在才会通过 CAS 指令进行更新。

### Write Behind Caching（异步缓存写入）

与 Read/Write Through 模式类似，也由数据存储服务来管理 cache 和 DB 的读写。

不同点是，数据更新时，Read/write Through 是同步更新 cache 和 DB，而 Write Behind Caching 则是只更新缓存，不直接更新 DB，而是采用异步批量的方式来更新 DB。

#### 套路

对于写请求：

- 只更新缓存，缓存服务异步更新 DB；
- miss 后由缓存服务加载，并写入 cache。

#### 特点

数据存储的写性能最高，定期异步刷新，存在数据丢失概率。

#### 适合场景

非常适合一些变更特别频繁的业务，特别是可以合并写请求的业务，

比如对一些计数业务，一条 Feed 被点赞 1万 次，如果更新 1万 次 DB 代价很大，而合并成一次请求直接加 1万，则是一个非常轻量的操作。

这种模型有个显著的缺点，即数据的一致性变差，甚至在一些极端场景下可能会丢失数据。比如系统 Crash、机器宕机时，如果有数据还没保存到 DB，则会存在丢失的风险。所以这种读写模式适合变更频率特别高，但对一致性要求不太高的业务，这样写操作可以异步批量写入 DB，减小 DB 压力。



三种模式各有优劣，不存在最佳模式。实际上，也不可能设计出一个最佳的完美模式出来，和空间换时间、访问延迟换低、成本高一样，高性能和强一致性从来都是有冲突的，系统设计从来就是取舍权衡。重点是如何根据业务场景，更好的做权衡取舍，从而设计出更好的服务系统。

## 缓存分类

### 按宿主层次分类

分为本地 Cache、进程间 Cache 和远程 Cache。

- 本地 Cache
  - 业务进程内的缓存；
  - 由于在业务系统进程内，所以读写性能超高，且无任何网络开销；
  - 不足是会随着业务系统重启而丢失。
- 进程间 Cache
  - 本机独立运行的缓存；
  - 缓存读写性能较高，不会随着业务系统重启丢数据，并且可以大幅减少网络开销；
  - 不足是业务系统和缓存都在相同宿主机，运维复杂，且存在资源竞争。
- 远程 Cache
  - 跨机器部署的缓存；
  - 这类缓存因为独立设备部署，容量大且易扩展，在互联网企业使用最广泛。
  - 不足是远程缓存需要跨机访问，在高读写压力下，带宽容易成为瓶颈。

#### 常用缓存

- 本地 Cache 的缓存组件有 Ehcache、Guava Cache 等，也可以用 Map、Set 等轻松构建一个自己专用的本地 Cache；
- 进程间 Cache 和远程 Cache 的缓存组件相同，只是部署位置的差异罢了，这类缓存组件有 Memcached、Redis、Pika 等。

### 按存储介质分类

分为内存型缓存和持久化型缓存。

- 内存型缓存
  - 将数据存储在内存，读写性能很高；
  - 不足是缓存系统重启或崩溃后，内存数据会丢失。
- 持久化缓存
  - 将数据存储到 SSD/Fusion-IO 硬盘中；
  - 相同成本下，这种缓存的容量会比内存型缓存大 1 个数量级以上，而且数据会持久化落地，重启不丢失，
  - 但读写性能相对低 1～2 个数量级。

#### 常用缓存

Memcached 是典型的内存型缓存，而 Pika 以及其他基于 RocksDB 开发的缓存组件等则属于持久化型缓存。

## 缓存组件选择

在设计架构缓存时，首先要选定缓存组件。

用 Local-Cache，还是 Redis、Memcached、Pika 等开源缓存组件。

如果业务缓存需求比较特殊，可以考虑是直接定制开发一个新的缓存组件，还是对开源缓存进行二次开发，来满足业务需要。

## 缓存数据结构设计

确定好缓存组件后，要根据业务访问的特点，进行缓存数据结构的设计。

- 对于直接简单 KV 读写的业务，这些业务数据封装为 String、Json、Protocol Buffer 等格式，序列化成字节序列，然后直接写入缓存中。读取时，先从缓存组件获取到数据的字节序列，再进行反序列化操作即可；
- 对于只需要存取部分字段或需要在缓存端进行计算的业务，可以把数据设计为 Hash、Set、List、Geo 等结构，存储到支持复杂集合数据类型的缓存中，如 Redis、Pika 等。

## 缓存分布设计

确定缓存组件，设计好缓存数据结构，接着就是设计缓存的分布。从 3 个维度来进行缓存分布设计：

1. 选择分布式算法，是采用==取模==还是==一致性 Hash== 进行分布。

   - 取模分布的方案简单，每个 key 只会存在确定的缓存节；
   - 一致性 Hash 分布的方案相对复杂，一个 key 对应的缓存节点不确定。但一致性 Hash 分布，可以在部分缓存节点异常时，将失效节点的数据访问均衡分散到其他正常存活的节点，保证缓存系统的稳定性。

2. 分布读写访问如何进行实施，是由==缓存 Client 直接进行 Hash 分布定位读写==，还是==通过 Proxy 代理==来进行读写路由

   - Client 直接读写，读写性能最佳，但需要 Client 感知分布策略。在缓存部署发生在线变化时，需要及时通知所有缓存 Client，避免读写异常，Client 实现也较复杂；
   - 通过 Proxy 路由，Client 只需直接访问 Proxy，分布逻辑及部署变更都由 Proxy 来处理，对业务应用开发最友好，但业务访问多一跳，访问性能会有一定的损失；

3. 缓存系统运行过程中，如果待缓存的数据量增长过快，会导致大量缓存数据被剔除，缓存命中率会下降，数据访问性能会随之降低，这样就需要将数据从缓存节点进行动态拆分，把部分数据水平迁移到其他缓存节点。

   这个迁移过程需要考虑，是由==Proxy 进行迁移==还是==缓存 Server 自身进行迁移==，甚至根本就==不支持迁移==。

   - 对于 Memcached，一般不支持迁移；
   - 对 Redis，社区版本是依靠缓存 Server 进行迁移；
   - 对 Codis 则是通过 Admin、Proxy 配合后端缓存组件进行迁移。

## 缓存架构部署及运维管理

设计完毕缓存的分布策略后，接下来就要考虑缓存的架构部署及运维管理。

架构部署主要考虑如何对**缓存进行分池、分层、分 IDC（互联网数据中心，Internet Data Center），以及是否需要进行异构处理。**

1. 核心的、高并发访问的不同数据，需要分别分拆到独立的缓存池中，进行分别访问，避免相互影响；访问量较小、非核心的业务数据，则可以混存；
2. 对海量数据、访问超过 10～100万 级的业务数据，要考虑分层访问，并且要分摊访问量，避免缓存过载；
3. 如果业务系统需要多 IDC 部署甚至异地多活，则需要对缓存体系也进行多 IDC 部署，要考虑如何跨 IDC 对缓存数据进行更新：
   1. 采用直接跨 IDC 读写，
   2. 采用 DataBus 配合队列机进行不同 IDC 的消息同步，然后由消息处理机进行缓存更新，
   3. 还可以由各个 IDC 的 DB Trigger 进行缓存更新；

4. 某些极端场景下，需要把多种缓存组件进行组合使用，通过缓存异构达到最佳读写性能。
5. 站在系统层面，要想更好得管理缓存，要考虑缓存的服务化，考虑缓存体系如何更好得进行集群管理、监控运维等。

## 考虑点

| 考虑点               |                                                      |                                                              |
| -------------------- | ---------------------------------------------------- | ------------------------------------------------------------ |
| 读写方式             | 全部整体读写<br/>部分读写及变更<br/>是否需要内部计算 | 用户粉丝数，很多普通用户的粉丝有几千到几万，而大 V 的粉丝更是高达几千万甚至过亿，因此，获取粉丝列表肯定不能采用整体读写的方式，只能部分获取。<br/>判断某用户是否关注了另外一个用户时，也不需要拉取该用户的全部关注列表，直接在关注列表上进行检查判断，然后返回 True/False 或 0/1 的方式更为高效。 |
| KV size              | 不同业务数据缓存 KV 的 size                          | 如果单个业务的 KV size 过大，需要分拆成多个 KV 来缓存。<br/>如果不同缓存数据的 KV size 差异过大，也不能缓存在一起，避免缓存效率的低下和相互影响。 |
| key 的数量           | 数据量中/小<br/>数据量大/海量                        | 如果 key 数量不大，可以在缓存中存下全量数据，把缓存当 DB 存储来用，<br/>如果缓存读取 miss，则表明数据不存在，根本不需要再去 DB 查询。<br/>如果数据量巨大，则在缓存中尽可能只保留频繁访问的热数据，对于冷数据直接访问 DB。 |
| 读写峰值             | 峰值 <= 10 w <br/>10 w < 峰值 < 100  w               | 如果小于 10万 级别，简单分拆到独立 Cache 池即可<br/>一旦数据的读写峰值超过 10万 甚至到达 100万 级的 QPS，需要对 Cache 进行分层处理，可以同时使用 Local-Cache 配合远程 cache，甚至远程缓存内部继续分层叠加分池进行处理。微博业务中，大多数核心业务的 Memcached 访问都采用的这种处理方式。 |
| 命中率               | 核心高并发访问<br/>持续监控                          | 缓存的命中率对整个服务体系的性能影响甚大。对于核心高并发访问的业务，需要预留足够的容量，确保核心业务缓存维持较高的命中率。比如微博中的 Feed Vector Cache，常年的命中率高达 99.5% 以上。为了持续保持缓存的命中率，缓存体系需要持续监控，及时进行故障处理或故障转移。同时在部分缓存节点异常、命中率下降时，故障转移方案，需要考虑是采用一致性 Hash 分布的访问漂移策略，还是采用数据多层备份策略。 |
| 过期策略             | 设置时间自动过期<br/>key 带过期时间戳                | 可以设置较短的过期时间，让冷 key 自动过期；<br/>也可以让 key 带上时间戳，同时设置较长的过期时间，比如很多业务系统内部有这样一些 key : key_20190801。 |
| 平均缓存穿透加载时间 | 配置更大容量确保命中率                               | 平均缓存穿透加载时间在某些业务场景下也很重要，对于一些缓存穿透后，加载时间特别长或者需要复杂计算的数据，而且访问量还比较大的业务数据，要配置更多容量，维持更高的命中率，从而减少穿透到 DB 的概率，来确保整个系统的访问性能。 |
| 缓存可运维性         | 集群管理<br/>一键扩容<br/>监控报警<br/>运维工具集成  | 需要考虑缓存体系的集群管理：如何进行一键扩缩容，如何进行缓存组件的升级和变更，如何快速发现并定位问题，如何持续监控报警，最好有一个完善的运维平台，将各种运维工具进行集成。 |
| 缓存安全性           | 限制来源 IP <br/>内网访问<br/>关键指令，增加访问权限 | 对于缓存的安全性考虑，一方面可以限制来源 IP，只允许内网访问；<br/>对于一些关键性指令，需要增加访问权限，避免被攻击或误操作时，导致重大后果。 |

