---
title: 缓存的七个坑
author:
  name: superhsc
  link: https://github.com/happymaya
date: 2020-06-03 24:33:22 +0800
categories: [Database, Cache]
tags:  [Cache]
math: true
mermaid: true
---

在缓存系统设计架构中，有很多坑，如果设计不当会导致很多严重的后果。

设计不当，轻则请求变慢、性能降低，重则会数据不一致、系统可用性降低，甚至会导致缓存雪崩，整个系统无法对外提供服务。

缓存设计中的七个大坑，如下：

1. 缓存失效
2. 缓存穿透
3. 缓存雪崩
4. 数据不一致
5. 数据并发竞争
6. Hot key
7. Big key



## 缓存失效

### 现象描述

由于服务系统查数据，首先会查缓存。如果缓存数据不存在，就进一步查 DB，最后查到数据后回种到缓存并返回！

缓存性能比 DB 高 50~100 倍以上，因此希望数据查询尽可能命中缓存，这样系统负荷最小，性能最佳。

缓存里存储的数据，基本上都是以 key 为索引进行存储和获取。业务访问时，如果大量的 key 同时过期，很多缓存数据访问都会 找不到，进而穿透到 DB，DB 的压力就会明显上升，由于 DB 的性能较差，只在缓存的 1%~2% 以下，这样请求的慢查率会明显上升。

### 原因

导致缓存失效，特别是很多 key 一起失效的原因，和日常写缓存的过期时间息息相关。

写缓存时，根据业务访问特点，给每种业务数据预置一个过期时间，写入缓存时把这个过期时间带上，让缓存数据在这个固定过期时间后被淘汰。<br/>

一般情况下，缓存数据是逐步写入的，所以也是逐步过期被淘汰的。

某些场景，一大批数据会被系统主动或被动从 DB 批量加载，然后写入缓存。这些数据写入缓存时，由于使用相同的过期时间，在经历这个过期时间之后，这批数据就会一起到期，从而被缓存淘汰。此时，对这批数据的所有请求，都会出现缓存失效，从而都穿透到 DB，DB 由于查询量太大，就很容易压力大增，请求变慢。

### 场景

很多业务场景，稍不注意，就出现大量缓存失效，从而导致系统 DB 压力大、请求变慢的情况。

类似的场景有：

- 同一批火车票、飞机票，当可以售卖时，系统会一次性加载到缓存，如果缓存写入时，过期时间按照预先设置的过期值，过期时间到期后，系统就会因缓存失效出现变慢的问题；
- 微博业务，会有后台离线系统，持续计算热门微博，每当计算结束，会将这批热门微博批量写入对应的缓存；
- 很多业务，在部署新 IDC 或新业务上线时，会进行缓存预热，也会一次性加载大批热数据。

### 方案

设计缓存的过期时间时，使用公式：==**过期时间 = baes 时间 + 随机时间。**==

即相同业务数据写缓存时，**在基础过期时间之上，再加一个随机的过期时间，**让数据在未来一段时间内慢慢过期，避免瞬时全部过期，对 DB 造成过大压力。

## 缓存穿透

### 现象描述

缓存穿透是一个很有意思的问题。

因为缓存穿透发生的概率很低，所以一般很难被发现。一旦发现，且量还不小，可能立即就会经历一个忙碌的夜晚。

对于正常访问，访问的数据即便不在缓存，也可以通过 DB 加载回种到缓存。而缓存穿透，意味着有特殊访客在查询一个不存在的 key，导致每次查询都会穿透到 DB，如果这个特殊访客再控制一批肉鸡机器，持续访问系统里不存在的 key，就会对 DB 产生很大的压力，从而影响正常服务。

### 原因

在系统设计时，更多考虑的是正常访问路径，对特殊访问路径、异常访问路径考虑相对欠缺。

缓存访问设计的正常路径，是先访问 cache，cache miss 后查 DB，DB 查询到结果后，回种缓存返回。对于正常 key 访问是没有问题，但是如果用户访问的是一个不存在的 key，查 DB 返回空（即一个 NULL），就不会把这个空写回 cache，不管查询多少次这个不存在的 key，都会 cache miss，都会查询 DB。整个系统就会退化成一个 “前端 + DB“ 的系统，由于 DB 的吞吐只在 cache 的 1%~2% 以下，如果有特殊访客，大量访问这些不存在的 key，就会导致系统的性能严重退化，影响正常用户的访问。

### 场景

- 通过不存在的 UID 访问用户；
- 通过不存在的车次 ID 查看购票信息
- 用户输入错误

偶尔几个这种请求问题不大，如果是大量这种请求，就会对系统影响非常大。

### 解决方案

#### 方案一

查询不存在的数据时，第一次查 DB，即使没查到结果返回 NULL，仍然记录这个 key 到缓存，只是这个 key 对应的 value 是一个特殊设置的值。

此方案需要注意的坑：

如果特殊访客持续访问大量的不存在的 key，这些 key 即便只存一个简单的默认值，也会占用大量的缓存空间，导致正常 key 的命中率下降。

进一步的改进措施是：

- 对不存在的 key 只存较短的时间，让它们尽快过期；
- 将这些不存在的 key 存在一个独立的公共缓存，从缓存查找时，先查正常的缓存组件，如果 miss，则查一下公共的非法 key 的缓存，如果后者命中，直接返回，否则穿透 DB，如果查出来是空，则回种到非法 key 缓存，否则回种到正常缓存。

#### 方案二

构建一个 BloomFilter 缓存过滤器，记录全量数据，访问数据时直接通过 BloomFilter 判断这个 key 是否存在，如果不存在直接返回，无需查缓存和 DB。

此方案需要注意的坑：

BloomFilter 要缓存全量的 key，要求全量的 key 数量不大，10亿 条数据以内最佳，因为 10亿 条数据大概要占用 1.2GB 的内存。

也可以用 BloomFilter 缓存非法 key，每次发现一个 key 是不存在的非法 key，就记录到 BloomFilter 中，这种记录方案，会导致 BloomFilter 存储的 key 持续高速增长，为了避免记录 key 太多而导致误判率增大，需要定期清零处理。

## 缓存雪崩

### 现象描述

指部分缓存节点不可用，导致整个缓存体系甚至服务系统不可用的情况。

缓存雪崩按照缓存是否 rehash（是否漂移）分两种情况：

1. 缓存不支持 rehash 导致的系统雪崩不可用
2. 缓存支持 rehash 导致的缓存雪崩不可用

### 原因

#### 支持 rehash

缓存支持 rehash 时产生的雪崩，一般是由于较多缓存节点不可用，请求穿透导致 DB 也过载不可用，最终整个系统雪崩不可用的。

缓存支持 rehash 时产生的雪崩，大多跟流量洪峰有关，流量洪峰到达，引发部分缓存节点过载 Crash，然后因 rehash 扩散到其他缓存节点，最终整个缓存体系异常

#### 不支持 rehash

缓存节点不支持 rehash，较多缓存节点不可用时，大量 Cache 访问会失败。

根据缓存读写模型，这些请求会进一步访问 DB，而且 DB 可承载的访问量要远比缓存小的多，请求量过大，就很容易造成 DB 过载，大量慢查询，最终阻塞甚至 Crash，从而导致服务异常。

这是因为缓存分布设计时，会选择一致性 Hash 分布方式，同时在部分节点异常时，采用 rehash 策略，即把异常节点请求平均分散到其他缓存节点。

在一般情况下，一致性 Hash 分布 + rehash 策略可以很好得运行，但在较大的流量洪峰到临之时，如果大流量 key 比较集中，正好在某 1～2 个缓存节点，很容易将这些缓存节点的内存、网卡过载，缓存节点异常 Crash，然后这些异常节点下线，这些大流量 key 请求又被 rehash 到其他缓存节点，进而导致其他缓存节点也被过载 Crash，缓存异常持续扩散，最终导致整个缓存体系异常，无法对外提供服务。

### 场景

微博、Twitter 等系统在运行的最初若干年都遇到过很多次。

比如：

- 微博最初很多业务缓存采用一致性 Hash + rehash 策略，在突发洪水流量来临时，部分缓存节点过载 Crash 甚至宕机，然后这些异常节点的请求转到其他缓存节点，又导致其他缓存节点过载异常，最终整个缓存池过载；
- 机架断电，导致业务缓存多个节点宕机，大量请求直接打到 DB，也导致 DB 过载而阻塞，整个系统异常。最后缓存机器复电后，DB 重启，数据逐步加热后，系统才逐步恢复正常。

### 解决方案

#### 方案一

对业务 DB 的访问增加读写开关，当发现 DB 请求变慢、阻塞，慢请求超过阀值时，就会关闭读开关，部分或所有读 DB 的请求进行 failfast 立即返回，待 DB 恢复后再打开读开关。

#### 方案二

对缓存增加多个副本，缓存异常或请求 miss 后，再读取其他缓存副本，而且多个缓存副本尽量部署在不同机架，从而确保在任何情况下，缓存系统都会正常对外提供服务。

#### 方案三

- 对缓存体系进行实时监控，当请求访问的慢速比超过阀值时，及时报警，通过机器替换、服务替换进行及时恢复；
- 通过各种自动故障转移策略，自动关闭异常接口、停止边缘服务、停止部分非核心功能措施，确保在极端场景下，核心功能的正常运行。

这三种方案可以三管齐下，避免缓存雪崩的发生。

## 数据不一致

### 现象描述

- 同一份数据，同时存在 DB 和缓存之中，DB 和缓存的数据不一致；

- 缓存有多个副本，多个缓存副本里的数据也可能会发生不一致现象。

### 原因

不一致的问题大多跟缓存更新异常有关。

比如：

- 更新 DB 后，写缓存失败，从而导致缓存中存的是老数据；
- 如果系统采用一致性 Hash 分布，同时采用 rehash 自动漂移策略，在节点多次上下线之后，会产生脏数据；
- 缓存有多个副本时，更新某个副本失败，也会导致这个副本的数据是老数据。

### 场景

导致数据不一致的场景也不少：

- 在缓存机器的带宽被打满，或者机房网络出现波动时，缓存更新失败，新数据没有写入缓存，就会导致缓存和 DB 的数据不一致。
- 缓存 rehash 时，某个缓存机器反复异常，多次上下线，更新请求多次 rehash。这样，一份数据存在多个节点，且每次 rehash 只更新某个节点，导致一些缓存节点产生脏数据。

### 解决方案

#### 第一个方案

cache 更新失败后，先进行重试，如果重试失败，则将失败的 key 写入队列机服务，待缓存访问恢复后，将这些 key 从缓存删除。

这些 key 在再次被查询时，重新从 DB 加载，从而保证数据的一致性。

#### 第二个方案

缓存时间适当调短，让缓存数据及早过期后，然后从 DB 重新加载，确保数据的最终一致性。

#### 第三个方案

不采用 rehash 漂移策略，而采用缓存分层策略，尽量避免脏数据产生。



以上三个，需要根据实际情况进行选择。

## 数据并发竞争

### 现象描述

互联网系统，线上流量较大，缓存访问中很容易出现数据并发竞争的现象。<br/>数据并发竞争，是指在高并发访问场景，一旦缓存访问没有找到数据，大量请求就会并发查询 DB，导致 DB 压力大增的现象。

### 原因

主要是由于多个进程/线程中，有大量**并发请求获取相同的数据，**而这个数据 key 因为正好过期、被剔除等各种原因在缓存中不存在，这些进程/线程之间没有任何协调，然后一起并发查询 DB，请求那个相同的 key，最终导致 DB 压力大增。

### 场景

在大流量系统比较常见。

比如：车票系统。如果某个火车车次缓存信息过期，仍然有大量用户在查询该车次信息。

### 解决方案

#### 方案一

使用全局锁。当缓存请求 miss 后，先尝试加全局锁，只有加全局锁成功的线程，才可以到 DB 去加载数据。

其他进程/线程在读取缓存数据 miss 时，如果发现这个 key 有全局锁，就进行等待，待之前的线程将数据从 DB 回种到缓存后，再从缓存获取。

#### 方案二

对缓存数据保持多个备份，即便其中一个备份中的数据过期或被剔除了，还可以访问其他备份，从而减少数据并发竞争的情况。

## Hot key

### 现象描述

 对于大多数互联网系统，数据是分冷热的。比如最近的新闻、新发表的微博被访问的频率最高，而比较久远的之前的新闻、微博被访问的频率就会小很多。

在突发事件发生时，大量用户同时去访问这个突发热点信息，访问这个 Hot key，这个突发热点信息所在的缓存节点就很容易出现过载和卡顿现象，甚至会被 Crash。

### 原因

Hot key 引发缓存系统异常，主要是因为突发热门事件发生时，超大量的请求访问热点事件对应的 key，比如微博中数十万、数百万的用户同时去吃一个新瓜。数十万的访问请求同一个 key，流量集中打在一个缓存节点机器，这个缓存机器很容易被打到物理网卡、带宽、CPU 的极限，从而导致缓存访问变慢、卡顿。

### 场景

引发 Hot key 的业务场景很多，比如：

- 明星结婚、离婚、出轨这种特殊突发事件；
- 奥运、春节这些重大活动或节日，
- 秒杀、双12、618 等线上促销活动；

### 解决方案

要解决这种极热 key 的问题，首先要找出这些 Hot key 来。对于重要节假日、线上促销活动、集中推送这些提前已知的事情，可以提前评估出可能的热 key 来。而对于突发事件，无法提前评估，可以通过 Spark，对应流任务进行实时分析，及时发现新发布的热点 key。而对于之前已发出的事情，逐步发酵成为热 key 的，则可以通过 Hadoop 对批处理任务离线计算，找出最近历史数据中的高频热 key<br/><br/>找到热 key 后，就有很多解决办法了。首先可以将这些热 key 进行分散处理，比如一个热 key 名字叫 hotkey，可以被分散为 hotkey#1、hotkey#2、hotkey#3，……hotkey#n，这 n 个 key 分散存在多个缓存节点，然后 client 端请求时，随机访问其中某个后缀的 hotkey，这样就可以把热 key 的请求打散，避免一个缓存节点过载，其次，也可以 key 的名字不变，对缓存提前进行多副本+多级结合的缓存架构设计。<br/><br/>再次，如果热 key 较多，还可以通过监控体系对缓存的 SLA 实时监控，通过快速扩容来减少热 key 的冲击。<br/><br/>最后，业务端还可以使用本地缓存，将这些热 key 记录在本地缓存，来减少对远程缓存的冲击。

## Big key

### 现象描述

  大 Key 的问题。指在缓存访问时，部分 Key 的 Value 过大，读写、加载易超时的现象。

### 原因

1. 大 key 占总体数据的比例很小，存 Mc，对应的 slab 较少，导致很容易被频繁剔除，DB 反复加载，从而导致查询较慢；
2. 大 key 很多，并被大量访问，缓存组件的网卡、带宽很容易被打满，也会导致较多的大 key 慢查询；
3. 如果大 key 缓存的字段较多，每个字段的变更都会引发对这个缓存数据的变更，同时这些 key 也会被频繁地读取，读写相互影响，也会导致慢查现象；
4. 大 key 一旦被缓存淘汰，DB 加载可能需要花费很多时间，也会导致大 key 查询慢问题。

### 场景

比较常见。比如互联网系统中需要保存用户最新 1万 个粉丝的业务，比如一个用户个人信息缓存，包括基本资料、关系图谱计数、发 feed 统计等。微博的 feed 内容缓存也很容易出现，一般用户微博在 140 字以内，但很多用户也会发表 1千 字甚至更长的微博内容，这些长微博也就成了大 key，。

### 解决方案

#### 第一种方案

如果数据存在 Mc 中，设计一个缓存阀值，当 value 的长度超过阀值，则对内容启用压缩，让 KV 尽量保持小的 size，其次评估大 key 所占的比例，在 Mc 启动之初，就立即预写足够数据的大 key，让 Mc 预先分配足够多的 trunk size 较大的 slab。确保后面系统运行时，大 key 有足够的空间来进行缓存。

#### 第二种方案

如果数据存在 Redis 中，比如业务数据存 set 格式，大 key 对应的 set 结构有几千几万个元素，这种写入 Redis 时会消耗很长的时间，导致 Redis 卡顿。此时，可以扩展新的数据结构，同时让 client 在这些大 key 写缓存之前，进行序列化构建，然后通过 restore 一次性写入。

#### 第三种方案

将大 key 分拆为多个 key，尽量减少大 key 的存在。同时由于大 key 一旦穿透到 DB，加载耗时很大，所以可以对这些大 key 进行特殊照顾，比如设置较长的过期时间，比如缓存内部在淘汰 key 时，同等条件下，尽量不淘汰这些大 key。

## BloomFilter

![](https://images.happymaya.cn/assert/db/cache/cache-03.png)

BloomFilter 是一个非常有意思的数据结构:

- 可以挡住非法 key 攻击；
- 可以低成本、高性能地对海量数据进行判断。

比如一个系统有数亿用户和百亿级新闻 feed，可以用 BloomFilter 来判断某个用户是否阅读某条新闻 feed。



### BloomFilter 的目的

==**检测一个元素是否存在于一个集合内**==。



### BloomFilter 的原理

**用 bit 数据组来表示一个集合，对一个 key 进行多次不同的 Hash 检测，如果所有 Hash 对应的 bit 位都是 1，则表明 key 非常大概率存在，平均单记录占用 1.2 字节即可达到 99%，只要有一次 Hash 对应的 bit 位是 0，就说明这个 key 肯定不存在于这个集合内。**

 

### BloomFilter 的算法

首先分配一块内存空间做 bit 数组，数组的 bit 位初始值全部设为 0，加入元素时，采用 k 个相互独立的 Hash 函数计算，然后将元素 Hash 映射的 K 个位置全部设置为 1。检测 key 时，仍然用这 k 个 Hash 函数计算出 k 个位置，如果位置全部为 1，则表明 key 存在，否则不存在。



### BloomFilter 的优势

内存操作，性能很高。另外空间效率非常高，要达到 1% 的误判率，平均单条记录占用 1.2 字节即可。而且，平均单条记录每增加 0.6 字节，还可让误判率继续变为之前的 1/10，即平均单条记录占用 1.8 字节，误判率可以达到 1/1000；平均单条记录占用 2.4 字节，误判率可以到 1/10000。以此类推。

这里的误判率是指，BloomFilter 判断某个 key 存在，但它实际不存在的概率，因为它存的是 key 的 Hash 值，而非 key 的值，所以有概率存在这样的 key，它们内容不同，但多次 Hash 后的 Hash 值都相同。对于 BloomFilter 判断不存在的 key ，则是 100% 不存在的，反证法，如果这个 key 存在，那它每次 Hash 后对应的 Hash 值位置肯定是 1，而不会是 0。